WHY?

WHY?

Why, when the United Nations IPCC is totally refuted…
When Al Gore is totally discredited…
When man-made global warming is totally debunked…
When passing a global warming cap and trade is totally futile…
Why is this book necessary?
Very simple: the environmental activist extremists are not going away.
They are committed to their crown jewel—cap and trade. Whether by

treaty, legislation or regulation, this largest tax increase in American history
must become a reality for them.

And while the public has caught on and believes the global warming issue

is dead, President Obama and the left in Congress are proceeding as if
nothing has happened, inserting expensive CO2 emission controls in virtually
every piece of legislation and regulation.

While in the fall of 2011, the House, the Senate, and the Super Committee

got all the public attention on a spending reduction of one trillion dollars
over ten years, the extremists are pushing an agenda that would cost the
American taxpayers three trillion dollars in that same ten years. And it has
gone almost completely unnoticed for two reasons: one, the media is
obsessed with resurrecting the Gore mantra and, two, MoveOn.org, George
Soros, Michael Moore, and the Hollywood elites have the resources to
reverse the defeat and preserve their crown jewel. And they will do it with
or without President Obama.

This book constitutes the wake-up call for America—the first and only
complete history of The Greatest Hoax, who is behind it, the true motives,
how we can defeat it—and what will happen if we don’t.

Somebody had to do it.

INTRODUCTION

THE HOAX DEBUNKED: DON’T FEEL TOO

SORRY FOR AL GORE

“With all of the hysteria, all of the fear, all of the phony science, could it be
that man-made global warming is the greatest hoax ever perpetrated on the

American people? It sure sounds like it.”

—Senator Jim Inhofe, July 29, 20031

SINCE JULY 2003, when I stood alone on the Senate floor and declared that
man-made catastrophic global warming was the greatest hoax ever
perpetrated on the American people, the credibility of the United Nations
Intergovernmental Panel on Climate Change (IPCC)—which claimed to have
a “consensus” on global warming—has eroded; cap and trade is dead and
never to be resurrected, and, the belief that anthropogenic global warming is
leading to catastrophe is all but forgotten. With the total collapse of the
biggest campaign of his life, one might feel a bit sorry for Al Gore. Almost.

Reprinted with permission

When the Climategate scandal (which includes both the leaked

Climategate emails and the errors in the IPCC science that were discovered
in the wake of these emails) revealed that several of the world’s top climate
researchers were alleged to be cooking the science, Al Gore, the world’s
first potential climate billionaire was running for cover.

When he resurfaced months later with a nearly 2,000 word op-ed in the

New York Times, the fact that he was in denial was clearly evident in the title
of his piece: “We Can’t Wish Away Climate Change.”2 I hated to break it to
him, but global warming alarmism was long dead and buried at that stage.

His op-ed was all about the so-called “overwhelming consensus,” China,
solar and wind power, globalization, rising sea levels, melting glaciers, and
cap and trade—all topics that the American people no longer saw as
relevant, especially in a weak economy.

What was Gore’s take on the Climategate scandal? Climate scientists, he

wrote in that same op-ed, were “besieged” by an “onslaught” of hostile
information requests from climate “skeptics.” He called those who question
climate alarmism members of a “criminal generation,” and said in a separate
interview that these emails were just “sound and fury signifying nothing.” Yet
the Daily Telegraph,3 one of largest publications in the United Kingdom, said
it was the worst scientific scandal of our generation. The Atlantic Monthly,4
the Financial Times,5 the New York Times,6 Newsweek,7 Time8 and many
others, conceded that it was a legitimate scandal and that reform of the IPCC
is absolutely essential.

The fact that Time magazine was one of the publications closely

reviewing the Climategate scandal is important because this is the same
publication that in 1974 told us that another ice age was coming, and we
were all going to die.9 And everyone remembers the Time magazine cover
during the height of the alarmism campaign that pictured the last polar bear
standing on the last cube of ice saying that we should all be worried, very
worried about global warming.10 So even a publication that has unabashedly
promoted alarmism over both global cooling and warming was taking this
scandal very seriously.

One of Gore’s former global warming allies, President Barack Obama,
has since received the message that global warming is no longer politically
popular. Of course, the President was the one who sold his cap and trade
agenda during the 2008 Presidential campaign as the only way to save the
planet from catastrophe, promising that generations from now “we will be
able to look back and tell our children that this was the moment… when the
rise of the oceans began to slow and our planet began to heal.”11 He vowed
that the United States would sign an international treaty with binding limits
on greenhouse gases, and he promised that a cap and trade bill would be
signed into law. But in a rare moment of clarity, then-Senator Obama openly
revealed the hoax for what it is, admitting, “if somebody wants to build a
coal-fired plant they can. It’s just that it will bankrupt them” and “under my

plan of a cap and trade system, electricity rates would necessarily
skyrocket.”12

The central fact about cap and trade is inescapable: as the President said

himself, it’s designed to make the energy we use more expensive. Cap and
trade would have been the largest tax increase in American history. It would
have made electricity, food, and gasoline prices significantly more
expensive, and it would have destroyed hundreds of thousands of jobs and
further weakened the already fragile economy. The Wharton Econometric
Forecasting Associates (or WEFA13) and Charles Rivers and Associates14
found that the Kyoto Treaty would have cost the U.S. between $300 billion
and $400 billion annually to implement. Cap and trade would have around
the same pricetag. To put this in perspective, the Clinton-Gore tax increases
of 1993, which were set at $30 billion, were the largest tax increases in
American history. Cap and trade would have been ten times that amount.

What would Americans actually gain environmentally for all the economic

pain they would be forced to endure? One would think that at the very least
we would save the world from climate catastrophe. Not exactly. When
Environmental Protection Agency (EPA) Administrator Lisa Jackson testified
before the Senate Environment and Public Works Committee, she admitted to
me that the United States acting alone on global warming would have no
impact whatsoever on global carbon levels.15 Administrator Jackson’s
comment echoes the work of Dr. Tom Wigley, one of Al Gore’s own
scientists, who said in 1997 that even if the Kyoto Protocol were fully
implemented by all signatories, it would only reduce global temperatures by
0.06 degrees Celsius by 2050.16 Such a small amount is hardly even
measurable.

The results of such a plan are obvious: as jobs go to places like India,

China, and Mexico, where they don’t have any emissions requirements, much
less the environmental standards we have in the United States, cap and trade
would actually increase worldwide emissions. So in the end, it would have
been all pain for no environmental gain.

President Obama no longer speaks about global warming, much to the

disappointment of Al Gore, who in a June 2011 Rolling Stone article called
“Climate of Denial” said,

President Obama has thus far failed to use the bully pulpit to make the
case for bold action on climate change. After successfully passing his
green stimulus package, he did nothing to defend it when Congress
decimated its funding. After the House passed cap and trade, he did little
to make passage in the Senate a priority […] The failure to pass
legislation to limit global-warming pollution ensured that the much-
anticipated Copenhagen summit on a global treaty in 2009 would also
end in failure.17

But even though global warming hysteria and cap and trade are long dead,

the fight is far from over because President Obama is now moving forward
with a plan to achieve through regulation what could not be achieved through
legislation. In December of 2009, the Obama EPA issued what is called the
“endangerment finding”—a finding that greenhouse gases harm public health
and welfare.18 Armed with this “finding” the EPA is planning to regulate
greenhouse gases instead through the Clean Air Act, which was never meant
to regulate carbon. Like cap and trade, this plan will have the same
$300-$400 billion pricetag, it will put the same amount of jobs in jeopardy,
and it will cause the same amount of havoc for our economy. My fight today
is to stop them from achieving this cap and trade agenda through the back
door.

Unfortunately for Americans, the endangerment finding is just the

beginning. The Obama EPA now has the distinction of implementing and
overseeing the most aggressive regulatory regime in American history, and,
like cap and trade, it is aimed squarely at regulating traditional and
domestically abundant sources of energy like coal, oil, and natural gas out of
existence. The EPA is moving forward with an unprecedented number of
rules for coal-fired power plants and industrial boilers that have now
become known as the infamous “train wreck” for the incredible harm they
will do to our economy. They are set to destroy hundreds of thousands of
jobs, and significantly raise energy prices for families, businesses, and
farmers—basically anyone who drives a car, operates heavy machinery, or
flips a switch.

Indeed, from farm dust to puddles of water on the road, there are very few
aspects of American life that the Obama EPA is not planning to regulate. And
it is businesses and working families who will pay the price.

Meanwhile, President Obama also continues his administration’s

restrictions on deepwater permits in the eastern Gulf of Mexico and the
Pacific and Atlantic coasts, and its constraints on development on federal
lands. He repeatedly calls for increasing taxes on oil and natural gas
producers, even though this will only serve to raise gasoline prices, destroy
oil and gas jobs, and increase our dependency on foreign oil. His
administration is also actively promoting the federal regulation of hydraulic
fracturing, the primary method of extracting natural gas, even though states
are efficiently and effectively regulating the practice, and there have been no
confirmed cases of water contamination since the first use of hydraulic
fracturing in my home state of Oklahoma in 1949. Well, we have seen the
results of Washington’s regulation on federal lands: it leads to less
development, fewer jobs, and less economic growth. America has outpaced
Russia to become the largest producer of natural gas because our immense
shale deposits are located predominantly in areas of the country where states
primarily regulate oil and gas development, not the federal government. In
states like Pennsylvania, Arkansas, Oklahoma, Texas, Louisiana, West
Virginia, Ohio, Michigan, and North Dakota, a boom in natural gas and oil
development is transforming America’s energy outlook—all thanks to the
absence of federal red tape. Putting the federal government in charge of
fracking will severely limit our ability to develop these vast resources.

President Obama has plenty of allies in this war on affordable energy who

are working overtime to restrict our domestic energy supply. This “green
team” includes the new Secretary of Commerce, John Bryson, who once
called the Waxman-Markey cap and trade, a “moderate but acceptable bill”19
; and his pick for Assistant Secretary for Fish Wildlife and Parks at the
Department of the Interior, Rebecca Wodder. She is a staunch opponent of
hydraulic fracturing, which she has said “has a nasty track record of creating
a toxic chemical soup that pollutes groundwater and streams.”20 Also on the
President’s green team is his selection for chairman of the Council of
Economic Advisers, Alan Krueger, who has made it clear that “the
administration believes that it is no longer sufficient to address our nation’s
energy needs by finding more fossil fuels.”21 Perhaps the most telling
comment in this war on affordable energy comes from Secretary of Energy
Steven Chu, who told the Wall Street Journal in 2008, “Somehow we have

to figure out how to boost the price of gasoline to the levels in Europe.”
That’s interesting. Just what are those “levels” in Europe?22

• The United Kingdom: $7.87 per gallon

• Italy: $7.54 per gallon

• France: $7.50 per gallon

• Germany: $7.41 per gallon

So if you think paying over $4.00 is too much for a gallon of regular,

fasten your seat belts. The Obama Administration is here to make it happen.

Meanwhile, Gore is still drowning in a sea of his own global warming

illusions, desperately trying to keep alarmism alive. On September 15, 2011,
Gore launched another global warming awareness event—a twenty-four-hour
Climate Reality Project that was featured in several cities around the world
and streamed live on the internet. But on that same day, in the midst of his
latest campaign, Gore was dealt an inconvenient truth when the Obama
Administration said that it would have to delay action on the EPA’s
greenhouse gas regulations, which were supposed to be announced
September 30, 2011.23

The reason for this delay was obvious: not only would greenhouse gas
regulations by the EPA cost hundreds of thousands of American jobs, they
may cost President Obama his own job, and he knows that all too well.
That’s why he is punting on a number of EPA’s rules until after the 2012
elections.

The reversal of the global warming campaign since the days when Al

Gore and the IPCC both received a Nobel Peace Prize has been remarkable.
In 2007, when the IPCC released its so-called “smoking gun” report, which
claimed that the link between humans and catastrophic global warming was
“unequivocal,” it was met with a blaze of attention. On November 18, 2011,
when the now discredited IPCC released the Summary for Policymakers for
its latest global warming report, very few people even noticed. In fact, I was
one of the only senators who weighed in on the report, as many of my
colleagues were not even aware that it had been published.24

As for Al Gore, a September 2011 Rasmussen poll reported that “Despite
winning a Nobel Prize and an Oscar for his work in the global warming area,
most voters don’t consider former Vice President Al Gore an expert on the
subject.”25 Steven Hayward put it well in a March 15, 2010, article in
Weekly Standard called, “In Denial: The meltdown of the climate campaign,”
when he said that Al Gore’s New York Times op-ed was “the rhetorical
equivalent of stamping his feet and saying, “‘It is too so!’ In a sign of how
dramatic the reversal of fortune has been for the Climate Campaign, it is now
James Inhofe, the leading climate skeptic in the Senate, who is eager to have
Gore testify before Congress.”26

In 2003, I stood alone in saying that the science behind anthropogenic

catastrophic global warming was a hoax and that the so-called “solutions”
were only symbolic and would have no impact on the climate. Now Gore
stands alone in his dismissal of reform, openness, transparency, and peer-
review to ensure good science, as well as his determination that man-made
catastrophic global warming is a serious threat.

But you don’t need to feel too sorry for Al Gore. Just as cap and trade

was about to collapse, the New York Times reported on November 2, 2009,
“Critics say Mr. Gore is poised to become the world’s first ‘carbon
billionaire,’ profiteering from government policies he supports that would
direct billions of dollars to the business ventures he has invested in.”27

Even without cap and trade, Gore had serious money-making potential
from green government policies. As Stephen Spruiell put it in a March 22,
2010, article for the National Review, “Climate Profiteers: For Gore & Co.,
Green Is Gold”:

Only a small part of Gore’s investment portfolio is tied to cap and trade.
Most of the companies in which he invests would benefit from the other
parts of the Democrats’ energy bill—the parts that would be much easier
for Congress to pass. Congress has been subsidizing green programs for
decades, and that support increased dramatically with the 2005 energy
bill. But the Democrats want to pump it up still more, even though the
consensus for dramatic action on climate change is buckling like a
shoddy roof in a blizzard of scientific scandals. The U.S. government,
facing record-setting deficits and debt, cannot afford new subsidies. Yet
with “green jobs” as their rallying cry, Gore and other advocates for

more green-tech largesse will push to pick the taxpayers’ pockets—lining
their own all the while.28
Since then, Gore hasn’t fared too badly. The New York Times article

continues, “He has invested a significant portion of the tens of millions of
dollars he has earned since leaving government in 2001 in a broad array of
environmentally friendly energy and technology business ventures, like
carbon trading markets, solar cells, and waterless urinals. He has also given
away millions more to finance the nonprofit he founded, the Alliance for
Climate Protection, and another group called the Climate Project, which
trains people to present the slide show that was the basis of his documentary
An Inconvenient Truth. Royalties from his book on climate change, Our
Choice, printed on 100 percent recycled paper, will go to the Alliance, an
aide said.”29

©2010 by National Review, Inc. Reprinted by permission.

Then there are his speaking fees—and he has been paid more than

$100,000 for a single speech. “Mr. Gore’s spokeswoman would not give a
figure for his current net worth, but the scale of his wealth is evident in a
single investment of $35 million in Capricorn Investment Group.”30 So he
will be all right.

Don’t feel too sorry for Al Gore. A billion dollars is a lot of comfort.
This book tells the story of my journey to halt the radical global warming
agenda, which, at one time—when President Obama was elected and Gore
was on top of the world—seemed inevitable. It began in 2003, when I was a

one-man truth squad and culminated with my vindication when cap and trade
was ultimately rejected, despite overwhelming Democrat majorities in both
the House and the Senate, and when the Climategate scandal revealed
allegations of cooking the science, which I had been saying all along. This
was truly one of the most important fights of my career in public service
because so much was at stake: as the leading Republican on the Environment
and Public Works Committee and the father and grandfather of twenty kids
and grandkids, I wasn’t going to sit back and allow the largest tax increase
on the American people to be imposed, all for nothing.

Despite what has been achieved, my story won’t end at the conclusion of

this book, much to the chagrin of my environmental friends, I’m sure. The
Obama Administration will continue to pursue its war on affordable energy
and, even if Obama is not reelected, global warming alarmists will continue
fighting to implement their economically damaging agenda— and they have
almost inexhaustible amounts of money and resources to advance it.
MoveOn.org, George Soros, Michael Moore, and the Hollywood elite will
still be going strong. But we will continue fighting back, and I believe the
next chapter that continues after this book ends will offer many successes,
too. In fact, we are already well on our way.

1

WHY I FIGHT

ONE OF MY GRANDKIDS ASKED me one day, “Pop I, why do you do things
nobody else does?”

You see, “I” is for Inhofe, so it’s “Pop I” and “Mom I,” according to our

twenty kids and grandkids.

My answer was simple, “Because nobody else does.”
When I think back on those years when I was alone standing up against the

global warming machine, I can’t imagine what would have happened if I
hadn’t…certainly nobody else would. But I knew that I couldn’t just stand by
and let our government implement policies that would leave our country so
much worse off for my twenty kids and grandkids.

Anyone who has visited me in my office knows how much my family
means to me; they rarely leave without being bombarded with multiple
pictures.

Even my readers are not exempt. For one thing, how many guys can

actually say that they married the girl next door? I did, and we have been
happily married for over fifty years, living in the same house in Tulsa that we
moved into not long after our wedding day. Kay and I feel incredibly blessed
by our four children, Jimmy, Perry, Molly, and Katy, and our twelve
wonderful grandkids. My sons-in-law and daughters-in-law round out the
twenty. Jimmy and his wife, Shannon, have three kids; Perry (a hand surgeon)
and his wife, Nancy (a pediatrician) have two kids; Katy and Brad have three
kids. They live virtually next door to Kay and me, and Molly and Jimmy with
their four kids live only an hour away. How often does that happen? It usually
doesn’t, but it did.

Global warming activists often take the high moral ground and claim that
they are on a crusade to save the planet for future generations. But what they
never acknowledge is that their policies would give our children a

substantially depressed quality of life, forcing them to live in a less free, less
prosperous America. My dream for my children and grandchildren is the
same as the dream of parents all over America: that our kids will reap the
many blessings of living in a free country and that their opportunities will be
even greater than our own. Under a global warming cap and trade regime,
this dream would have just been a fantasy.

When I was ambushed by global warming advocates recently—no, they

haven’t given up—they asked me the same questions they always ask: “What
if you’re wrong?” and “If you’re wrong will you apologize to future
generations?” I always answer, “What if you’re wrong? Will you apologize
to my twenty kids and grandkids for the largest tax increase in American
history?”31 They usually don’t have anything to say after that.

My willingness to take up the global warming fight when no one else

would comes from so many experiences in my past as a father and
grandfather, entrepreneur and public servant, that made me the guy I was to
become when I finally took up the gavel as Chairman of the Senate
Environment and Public Works Committee in 2003—a position that awarded
me the best possible platform.

OVERREGULATION NATION
It all started in Tulsa, Oklahoma, with a fire escape—or more accurately,
with the many ridiculous, unnecessary regulations surrounding that particular
fire escape.

At the time, I was working long, hard hours as a developer. I was making

fortunes, losing fortunes, expanding the tax base, doing the things I thought
Americans were supposed to do. And all that time, the chief opposition I had
to living out my American dream was, ironically, the government.

This was especially clear to me in the late 1970s when I purchased the
Wrightsman Oil Estate, which had been abandoned for many years and had
since fallen to ruin. In fact, it had become the favorite shelter for derelicts
who were breaking in and burning the furniture to keep warm. But the house
was extraordinary and it had a long, rich history. During World War II, the
wealthy owners donated the mansion to the war effort, allowing the aviation
community to live there while building the Douglas aircraft. Anyone who
knows my love of flying would understand why I found its story so
compelling. I saw a wonderful opportunity to restore the home to its former

glory and couldn’t wait to get started. I hired the construction crew
immediately to expedite the renovation.

The only eyesore to the otherwise beautiful estate was one particularly
ugly fire escape that was in plain sight. But I thought, no problem, I’ll just
move it to the north side of the building where it will still serve its purpose
but not be visible from the street.

I was told that in order to do so, I would have to get the City Engineer’s
permission. So I promptly appeared at the City Engineer’s office, made my
request, and within moments I was flat-out rejected. “But I’m not changing
anything to the structure or the foundation,” I said, bewildered. “It won’t
create any safety concerns whatsoever. I’m simply moving the fire escape
from the south side to the north side of the building. It still serves exactly the
same purpose; it’s just in a different location.” But the Engineer was not
moved. He told me I would have to take my request to the city board, put my
name on the agenda, and they would hear my case in about two months.
“Two months?” I exclaimed, “This project can’t be delayed that long

because all the workers are being paid now. That will cost me thousands of
dollars. Are you telling me you won’t help me at all?”

He just looked at me and said, “That’s your problem, not mine.”
So I told him that I was going to run for Mayor and fire him. And I ran for

Mayor and I fired him.

When I became Mayor of the City of Tulsa in 1978, I set out to make sure
the size of the government, both in the operating budget and the numbers of
employees, did not grow. I remember just after taking office a man came up
to me and said, “Congratulations, Mayor on your victory. When would you
like to have the Inhofe Hour, monthly, weekly, or daily?” When I inquired as
to what the Inhofe Hour was, he said, “Well, we will let you go on our cable
station to explain to the people your policies and programs.” I responded,
“You mean so that I can use the public funds to propagandize the electorate?”

The guy agreed that was a pretty good analysis, so I told him, “I don’t

want the Inhofe Hour monthly, weekly, or daily. As a matter of fact, I’m going
to defund you.” In the weeks after that, I found that they were running a script
along the bottom of the screen of this cable station that no one watched,
saying, “Your Mayor is trying to close the doors of government. Call the
Mayor’s office immediately and say that you demand to have this channel.”
We succeeded in defunding the agency and no one seemed to miss it.

Now that doesn’t sound like a big deal, but it is. Most people just roll

over when they are abused by big government. And bureaucrats seem to have
unlimited resources—our tax dollars—to come after us. I hope you stick with
this book to read a later chapter, the “Afterword.” It will give you insight
into the source of the power of the bureaucracy…(hint: bureaucratic
earmarks.) And no one can appreciate the abusive government power until he
has been through it. I’ve been there.

I never forgot how in my daily work as a developer I was bombarded

with regulations from the city, state, and federal government, so much so that
I once had to go to twenty-six different government bureaucracies in order to
get a dock permit for a single condo project.

Having gone through that, I could understand why so many people just

throw in the towel and close their businesses altogether. Unfortunately, this is
exactly what happened to one of the best men I have ever known, my mentor
A.W. Swift. When I was young I used to work with A.W. on his oil rigs, and
he taught me so much about energy development and how to run a successful
business. That was back in the days when you drilled for oil with cable tools
as opposed to rotary bits. It worked on the principle of pounding. The bit
would be raised up and dropped deepening the hole a few inches each time,
until you struck oil—or didn’t. It was backbreaking work, but I loved it and
often worked two eight hour shifts without stopping.

A.W. had one son, Bert, who had become an engineer and the three of us
worked together. One night, there was a terrible tragedy on the rig: the well
that Bert and I were working on exploded and he was fatally burned. After
Bert’s death, I became like a son to A.W. I often thought of following in his
footsteps and still wonder to this day what would have happened if I had
made the oil business my career. But, of course, I took a different path.
About twenty years later, when I drove up to visit him at his home

overlooking Keystone Lake, I was shocked to see all of his cable tool rigs
stacked and sitting idle in his front yard. He told me sadly that he had given
up the drilling business. When I asked why, he said, “It’s because I can’t
handle the government regulations any longer. Here I am out working hard,
trying to produce cheap oil for Oklahomans and the government keeps
imposing regulations until they have just flat driven me out of business.”

A.W. Swift was the epitome of the hard working, frontier spirited

American. He was involved in an honorable profession, worked harder than

most, was very religious, and provided jobs for as many as twenty-five or so
people. Yet he was regulated out of business by the pseudo-intellectuals in
Washington who think they know best. I remember thinking at the time how
ironic it was that the very government that was supposed to create an
environment where A.W. could achieve his American dream was the very
institution that had managed to quell it. Something had to change.

“LONE VOICE IN THE WILDERNESS” ONE-MAN TRUTH SQUAD:
THE EARLY YEARS
Many think that my reputation for being a “one-man truth squad” on Capitol
Hill started with the global warming debate, but actually, it started much
earlier than that.

I remember back when I was first elected to Congress. The total national
debt was $200 billion and I was outraged. I recall an ad on television that
was produced by some national taxpayers group on what $200 billion was.
They stacked up $1 bills in this 30 second spot until it reached the height of
the Empire State Building, and that was $200 billion. Back then, I was a
young, energized zealot who was going to do something about all of this.
Come to think about it, I did do something about it.

When I was first in the Oklahoma Senate in 1968, then-Senator Carl

Curtis, a Republican from Nebraska, asked me if I’d help him convince states
to ratify a balanced budget amendment so that Congress would be forced to
face up to the issue. Senator Curtis told me that when he annually introduced
a budget balancing amendment to the Constitution, that the excuse used by his
fellow senators for not voting for it, was that 3/4 of the states will never
ratify it. So he charged me with the job of leading 3/4 of the states to “pre-
ratify” the budget balancing amendment. I introduced and passed the first
resolution by any state to join an initiative calling for a constitutional
convention for the purpose of adopting a budget balancing amendment to the
Constitution. We got to just one short of the 3/4 of the states necessary to pass
resolutions calling for this constitutional convention. Then one of the national
conservative groups decided that there was too much of a risk in calling for a
convention, so the conservatives became split and the effort failed.

In a column called “A Voice in the Wilderness,” Anthony Harrigan wrote,

“Way out in Oklahoma there is a state senator who is going to balance the

federal budget.”32 I’ve felt like that voice in the wilderness many times since
then.

As a state legislator and as a mayor, there was only so much I could do to

rein in the excessive regulations and out-of-control spending that were
hurting our economy. The greatest overregulator of all was clearly the federal
government, so I knew that if I wanted to be part of the solution, I had to go to
Washington.

When I arrived in D.C. in 1987 as the Congressman from Oklahoma’s
First District, what surprised me the most was that so many of my liberal
colleagues in the House, who were constantly pushing for more and more
job-killing regulations, had never held a “real job” in their entire lives. Then
I discovered that many members never go home. Why should they? There
seem to be more golf courses in Northern Virginia than anywhere else. So I
started the Tuesday—Thursday Club and went back and worked my district
every weekend. Here’s what happens: members who stay in Washington
become part of the problem. You live next door to a lobbyist, a member, or a
staffer. You see, there aren’t any normal people in Washington and you forget
what real people are like. Most of these guys don’t understand the extent to
which overreaching regulations are hurting businesses and job growth.

Much of that comes from the attitude of many on the liberal left that they
know what’s best for everyone else—an attitude that reached new heights
during the global warming debate. D.C. politicians were working overtime to
hide the real costs of their global warming cap and trade regulations because
they knew the American people would never go for it. But as far as they were
concerned, it was none of their business. They were going to save the world
from global warming catastrophe whether their constituents liked it or not.
They just didn’t want voters to realize that this would mean the largest tax
increase in American history, higher energy costs, hundreds of thousands of
lost jobs, a depressed economy, and a much less free country.

I discovered that not only did my liberal colleagues in the House believe
it was none of their constituents’ business how they voted, but that there were
set procedures in place to make sure of that.

There was one particular statement that I often heard whispered in the

Cloakroom: “Vote liberal and press release conservative.” Never once did I
hear, “Vote conservative and press release liberal.” I discovered that in
1932, a very powerful Democrat from Texas, Speaker John Nance Garner,

had set up a system that would allow Democrats from conservative districts
to do just that: vote for liberal causes, while pretending to be conservatives.

This is the way it worked: When a bill is introduced, it is assigned to a

committee in the House of Representatives. In order for it to come to the
Floor for a vote, there would have to be a committee hearing and a public
vote. However, there was another more obscure method to bring it to the
Floor for a vote, and that was for a majority of the Members of the House
(218) to sign a Discharge Petition that was located in a drawer at the
Speaker’s desk. No member of the press or public could view the Discharge
Petition, and no Member of Congress was allowed to have a list of the other
Members who had signed a Discharge Petition. Of course, when you went to
sign, you could see the other ten names on the page with you, but you were
not allowed to write those names down. If any Member disclosed the names
of anyone who had signed the Discharge Petition, the penalty was that the
person could be expelled from House of Representatives.

This system seemed outrageous to me, and it was even more upsetting that
no one had ever fought it. But it occurred to me that something could be done:
I introduced a Resolution that would make the signatures of a Discharge
Petition a matter of public record. Of course, the Speaker and the Democratic
leadership didn’t like that one bit, so they assigned my resolution to a
committee that agreed never to bring it up for a vote. After that, the only way
to get it out for a vote was to file a Discharge Petition on the Discharge
Petition and have that placed in the Speaker’s drawer. I knew that this would
be very heavy lifting for just one Congressman, but I was determined.

Unfortunately, my Discharge Petition suffered the fate of the process I was
trying to reform. So the only way I could get what I wanted was to expose the
names of those Members who signed my Discharge Petition. Of course, most
Members claimed that they had done so as to appear as though they cared
about transparency in government, knowing full well that they would never
be caught. But I had a plan: I found a few Republican House Members who
were willing to take the risk with me to memorize one page of signatures
while they signed themselves, and report those names back to me. I compiled
the list of names, gave it to the Wall Street Journal, and let the chips fall
where they may.

Of course, no Democrats had signed. I spent the rest of August recess on

radio shows in each of the Democratic districts explaining my Discharge

Petition, and I was pleased that there was such a groundswell of support
from across America. The way the system worked is that once the 218th
member signed, the signatures stopped. By the end of that August recess, the
remaining Democratic members, under pressure from their constituents, were
begging me to include their names, since they could not be added after the
218th name.

For sixty years, no one had dared to challenge this rule; even if they had
wanted to, many were intimidated by the threat that they could be expelled.
During that time, I was asked if I was worried about being thrown out of the
House of Representatives. I replied that I would simply run in my own
special election to replace me and then fill my own vacancy. Besides, there
was no way the Democrats could expel me after that.

I was proud to learn later that this became known as one of the most

significant reforms in the history of the House of Representatives and I was
happy that my efforts were appreciated. As the Daily Oklahoman, wrote,
“Inhofe’s victory… is one of the most significant changes in congressional
rules in the modern era.”33

POISED TO TAKE THE GAVEL
When I was elected to the U.S. Senate, I knew from the start exactly what
committees I wanted to serve on: Armed Services, and Environment and
Public Works. My reason for wanting to be on Armed Services was simple. I
am proud to have served in the U.S. Army. It was probably the best thing that
could have happened to me, as I developed a profound appreciation for the
price of freedom.

When I was elected to the Oklahoma Legislature in 1967, one of my first

duties was to travel to Washington to appear before the Environment and
Public Works Committee, when Senator Jennings Randolph was the
Chairman, to protest Ladybird’s Highway Beautification Act of 1965. Little
did I know then that this would be the Committee I would ultimately end up
chairing.

Since my goal was to do whatever I could to rein in the kinds of

regulations that stifle entrepreneurship and job growth, I felt it was important
to serve on the Environment and Public Works Committee. This committee
has primary jurisdiction over the Environmental Protection Agency—an
agency that puts forth some of the most job-destroying regulations in the

country. From water rules, to the regulation of our energy capabilities, the
EPA has the power to affect almost every facet of business and industry in
America, and under the Obama administration, the results have been
devastating for our economy.

These overreaching regulations especially have huge impacts on my state
of Oklahoma, which is one of the top energy producing states in the country.
I’m a pilot and I often fly my plane around the state to different political and
civic events. When I fly into Will Rogers Airport in Oklahoma City and look
out the window, I’m always greeted by multiple pump jacks located just off
the runways, and there are more still pumping alongside the highways as you
travel through the state. It’s second nature to see that in Oklahoma—we’re
known for our energy development, so much so that old “Petunia #1,” an oil
well named for being in the middle of a flower bed, stands tall in front of the
Oklahoma State Capitol building. It no longer produces oil, but it’s a
reminder of past and present Oklahoma.

Oklahoma has been blessed with tremendous resources, and developing
those resources has led to a huge economic boost and the creation of good
paying jobs.34 In fact, Oklahoma’s unemployment rate is a far cry from the
national unemployment rate. This good state economy is in large part due to
our strong energy development industry. The U.S. Energy Information
Administration reports that presently Oklahoma has over 83,000 producing
oil wells and well over 43,000 producing natural gas wells.35 Oklahoma
City University published a study in 2009, which found that the oil and gas
industry in Oklahoma is responsible for 300,000 jobs in the state and
contributes $51 billion to the state economy in just one year alone. In fact,
twenty-percent of state revenues are due to the oil and gas industry.36 In
addition to oil and gas, Oklahoma is also one of the largest wind energy-
generating states. I have always said that the best way to power this machine
called America is through an all-of-the above energy policy that includes
fossil fuels and renewables—and Oklahoma provides a good example of
that.

Far from being a friend of Big Oil, as the accusation is often levied

against me, I am a friend of “Little Oil” or of any Oklahomans who strive to
develop our vast resources. Most of those involved in energy development in
Oklahoma are running small businesses, like A.W. Swift. They are the people
I fight for every day in Washington.

Many environmentalists see me as their enemy because they measure the

“greenness” of politicians by how many federal laws they impose on the
American people. In contrast, I have always believed that the environment is
best served when the economy is strong, and we can develop our resources
while being good stewards of the environment.

On August 3, 2007, Emily Belz of The Hill contacted me to ask if I

consider myself a green lawmaker.37 I said absolutely: I have always strived
to be a good steward of the environment and I see myself as a
conservationist. One of my favorite stories involves Ila Loetscher, a
remarkable person who is known to most as the “Turtle Lady.” I met Ila over
forty years ago while spending time on South Padre Island with my family.
She used to rescue turtles from nets and traps—some of them had torn
flippers and cracked shells. She trained her turtles to clap their flippers and
roll over in the water on cue. Our kids loved to watch them perform. The
Ridley Sea Turtles lay their eggs, cover them, and leave. When the eggs
hatch, the tiny turtles must struggle on their own toward the water, and those
who make it, swim away. It wasn’t long before Ila had all of us out on the
beaches of South Padre late at night guarding and protecting newly hatched
Ridley Sea Turtles as they made their first journey from the beach to the
ocean.

As part of being a conservationist, I have always believed that personal
responsibility breeds environmental stewardship. I saw this so often when I
was Mayor of Tulsa: whenever individuals were involved in efforts to
protect the species and the environment, the outcome is always more
effective and efficient than it would be with regulations solely from the
federal government. One of the best examples of this is the Partners for Fish
and Wildlife Program, which was authorized when I was Chairman of the
Environment and Public Works Committee. Whereas regulations under the
Endangered Species Act have a low success rate in recovering species but
are highly successful in stifling economic growth, the Partners Program is
much more effective in preserving the species and the economy because it
works with property owners instead of against them.

Good energy and environment policy is about achieving a healthy balance
between environmental progress and economic growth. The global warming
regulations promoted by President Obama, whether through cap and trade or
through regulations by the EPA, completely lack that balance, so much so that

they would destroy our economy while doing nothing to help the
environment. Looking back, it is clear that the global warming debate was
never really about saving the world; it was about controlling the lives of
every American. MIT climate scientist Richard Lindzen summed up it up
perfectly in March 2007 when he said, “Controlling carbon is a bureaucrat’s
dream. If you control carbon, you control life.”38

Having every aspect of one’s life controlled is completely against

everything America stands for, and that’s why global warming is one of the
defining debates of our era. These experiences in my life, from reining in
job-killing regulations as Mayor of Tulsa, to being the voice in the
wilderness on the balanced budget amendment, to my one-man truth squad
battle in the House, even to rescuing turtles in South Padre island are the
training grounds that prepared me to fight this huge battle: The Greatest
Hoax.

2

“THE MOST DANGEROUS MAN ON THE

PLANET”: EXPOSING THE HOAX

WHEN I ARRIVED IN MILAN, Italy, in December 2003 for the annual IPCC
global warming conference, I was in for a big surprise. While I was fully
aware that I would be walking straight into the lion’s den, I certainly wasn’t
expecting to find my face on a WANTED poster for being “The most
dangerous man on the planet.” My crime was clearly laid out; under a picture
of me at the dais was the quote that had made me famous with the
environmental crowd just a few months before: “Global warming is the
greatest hoax ever perpetrated on the American people.”

I looked around and realized that the posters were plastered everywhere

throughout the convention center, and there were some taped to telephone
poles on the city streets. When I first discovered that I was the most
dangerous man on the planet, I must admit, I was a little stunned. But then I
thought of how my family and friends would smile when they heard the news
and I knew my grandchildren would be impressed. Up until that point, the
closest I had ever come to being called “dangerous” was when I was
repeatedly referred to as the “Renegade Conservative from Oklahoma”—a
title I have long been proud to have. Now because I had dared to question the
validity of the radical global warming agenda, I was suddenly the world’s
most wanted climate criminal. With so much wrath directed at me, my
colleagues even suggested that I may be a target in Italy and perhaps I should
request that the conference provide me with additional security.

But if they were trying to intimidate me, it didn’t work. My staff tried to

stop me as I made my way over the room of the National Environmental
Trust, the group responsible for the incriminating posters. When they saw me
coming they looked worried—they thought I was going to be mad, but instead

I smiled and shook their hands. I told them, “I’m just glad you guys got my
quote right this time. You know, you usually misquote me.”

The young man behind the table asked me to autograph one of the posters

and I said I’d be honored. “Great to have friends like you, Jim Inhofe,” I
wrote. It turns out that my autographed poster hung framed in their
Washington, D.C., office for years. It may still be there.

TAKING UP THE GAVEL
The WANTED posters incident was a critical juncture when I came to
understand firsthand just how far the environmental left will go to isolate and
silence anyone who dares to call their agenda into question. Instead of
engaging in an intellectual debate about the problems I had exposed in the
scientific process underlying their theories, they resorted to threats and
attacks. And that is precisely why they ultimately lost the debate. I’ve always
said that when you don’t have science on your side, when you don’t have
logic on your side, when you don’t have truth on your side, you resort to
attacks.

For a long time, it worked: once intimidated, many of my colleagues

would either change their position on global warming, or they would stay
quiet so as not to be a target. In fact, the green movement had a clear record
of success in silencing dissenters—that is, until I became Chairman of the
Environment and Public Works Committee in 2003. Not only did I
consistently speak out myself, I made it a priority to ensure that other
silenced voices, especially in the scientific community, were heard as well.
From the moment the new committee leadership was in place, I think the

Environment and Public Works Committee didn’t know what hit them. I could
not have been more different from my predecessors. In fact, as a staunch
conservative from Oklahoma, an energy-developing state, I was a radical
departure from my colleagues who had held my position before, including
Senators Jim Jeffords of Vermont, Robert Smith of New Hampshire, and John
Chafee of Rhode Island, who were, for the most part, less conservative and
from eastern states with much different constituencies. More often than not,
they believed that the more regulations from Washington, the better for our
environment and nation. I never saw it that way; I have always believed that
we need to achieve a healthy balance between environmental progress and
economic growth. In fact, I was the first Chairman to invite witnesses from

industry and energy development sectors to testify on how excessive
environmental regulations may affect their ability to create jobs or expand
their businesses. Also for the first recorded time in the Committee’s history, I
made sure that it was staffed with people who had actually worked for a
living, instead of filling it with the kinds of idealists who often end up in
these jobs.

Because the Environment and Public Works Committee has primary

jurisdiction over the issue of global warming, I realized that as Chairman, I
had a profound responsibility, as any “solution” to global warming would
have far-reaching impacts for our nation. That’s why from the moment I took
up the gavel, I established three key principles for our work on the
committee: (1) it should rely on the most objective science, (2) it should
consider costs on businesses and consumers, and (3) the bureaucracy should
serve, not rule, the people. I knew that without these principles, we could not
make effective public policy decisions. These three principles would guide
us as we continued to improve the environment, while also encouraging
economic growth and prosperity. It was a fundamental shift in the way the
Committee operated. For the first time in environmental policy, instead of
looking at a problem with the mindset of “How can the federal government
solve the problem?” we looked at “What problems were the federal
government causing?”

KYOTO TREATY ALL ECONOMIC PAIN FOR NO
ENVIRONMENTAL GAIN
In the 1990s, as the global warming hysteria was heating up, the so-called
solution was the Kyoto Protocol, a treaty that required nations that were
signatories to reduce their greenhouse gas emissions by considerable
amounts below 1990 levels; specifically, the U.S. would have to reduce its
emissions 31 percent below the level otherwise predicted for 2010. To put
this in perspective, as the Business Roundtable pointed out, that target was
“the equivalent of having to eliminate all current emissions from either the
U.S. transportation sector, or the utilities sector [residential and commercial
sources], or industry.”39

The Clinton Administration, led by then Vice President Al Gore, signed

Kyoto on November 12, 1998, but never submitted it to the Senate for
ratification. That’s because the Senate sent a powerful message to President

Clinton. By a vote of 95-0, the Senate passed the Byrd-Hagel resolution on
July 25, 1997, which stated that the Senate would not ratify a treaty if (1) it
caused substantial economic harm and (2) if developing countries were not
required to participate on the same timetable.40

Of course, Kyoto satisfied neither of the requirements of the Byrd-Hagel

resolution. One definitive study from Wharton Econometric Forecasting
Associates, or WEFA, a private consulting company founded by professors
from the University of Pennsylvania’s Wharton Business School, revealed
that Kyoto would cost 2.4 million U.S. jobs and reduce GDP by 3.2
percent.41 In other words, the pricetag for the United States would be over
$300 billion annually. It found that Americans would face higher food,
medical, and housing costs—for food, an increase of 9 percent; medicine, an
increase of 11 percent; and housing, an increase of 21 percent. At the same
time, an average household of four would see its real income drop by $2,700
in 2010, and each year thereafter. Under Kyoto, energy and electricity prices
would nearly double, and gasoline prices would go up an additional 65 cents
per gallon. It was truly a recipe for economic disaster.

In July 2003, the Congressional Budget Office (CBO) found that “The
price increases resulting from a carbon cap would be regressive— that is,
they would place a relatively greater burden on lower-income households
than on higher-income ones.”42 So it would have been a raw deal for
America and a disaster for the poor, who would have to pay a
disproportionate amount of their incomes on higher energy prices. I have
always found it ironic that the environmental left continually claims the high
moral ground, saying that their policies are to protect the most vulnerable, yet
the very policies they espouse would cause the greatest harm to the poorest
among us.

One witness we called before the Committee, Tom Mullen of the

Cleveland Catholic Charities, put it the most succinctly:

The elderly on fixed limited incomes and the working poor with families
have made it clear to me on a daily basis that they cannot afford increases
in costs for their basic needs. In Cleveland, over one-fourth of all
children live in poverty and are in a family of a single female head of
household. These children will suffer further loss of basic needs as their
moms are forced to make choices of whether to pay the rent or live in a

shelter; pay the heating bill or see their child freeze; buy food or risk the
availability of a hunger center. These are not choices any senior citizen,
child, or, for that matter, person in America should make.43

In 2003, those Americans who made less than $30,000 spent 22 percent of
their take-home pay on energy costs, such as gasoline or their monthly utility
bills. People who made less than $10,000 per year spent 68 percent of their
take-home pay on energy. Many of those people live in my home state of
Oklahoma and other rural and urban areas of the country that the East and
West coast liberal elites refer to as flyover country.

One would think that for all the economic suffering that the United States
would have to endure, surely it would be worth it; surely it would save the
world. Not exactly. The Kyoto Treaty did not bind developing countries like
China, India, Brazil, and Mexico, who were all signatories to Kyoto and
some of the world’s largest emitters of greenhouse gases, to any emission
requirements. It’s not difficult to predict the results: as jobs went overseas
and to countries that do not have emissions limits, worldwide emissions
would actually increase.

In the end, no matter how the environmental left tried to couch it; no matter

what the politicians said to make their case, Kyoto would have been all
economic pain for no environmental gain. Sometimes I feel like a broken
record, as I have been repeating that message ever since I began this fight. In
2003, I told world leaders in Milan that the U.S. would never ratify Kyoto.
Six years later, in Copenhagen in 2009, I told world leaders that the United
States Senate would never pass cap and trade. They didn’t like my message
then and they don’t like it now. But I was right and they were wrong.

KYOTO: NOT ABOUT SAVING THE PLANET
In truth, Kyoto’s objective had nothing to do with saving the globe, because it
was clear that was the one thing it would fail to do—it was purely political.
Before and during the time I was Chairman, the push for the United States

to ratify Kyoto continued. In June 2001, Germany released a statement
declaring that the world needs Kyoto because its greenhouse gas reduction
targets “are indispensable.”44 Also that June, Swedish Prime Minister
Goeran Persson flatly stated that Kyoto is necessary.45 I couldn’t help but
ask: necessary for what? We already knew that it would not reduce

emissions. In fact, at that time, according to the EU, Environment Ministry,
most EU member states were not on schedule to meet their Kyoto targets.

Even from the beginning it was very clear that Kyoto was not about

reducing emissions, but something much more sinister. One indication of this
came from Russia, who ratified Kyoto not because the government believed
in catastrophic global warming, but because ratification was Russia’s key to
joining the World Trade Organization. Also, under Kyoto, Russia could profit
from selling emissions credits to the EU and continue business as usual,
without undertaking economically harmful emissions reductions. They could
make billions of dollars by not developing their own natural resources.

This made a big impression on me because I had the opportunity to fly an
airplane around the world. In fact, I’m one of the relatively few private-plane
pilots who has followed in the tradition of Oklahoman Wiley Post, who
traveled around the world in a small private plane in 1931. There’s an
amazing thing about flying in a small aircraft. You see the world from a
different vantage point—closer to the ground than the major airliners but high
enough to get a distinct perspective on the ground below, the sky above, and
the horizon in the distance. I remember taking off from Moscow and flying
over the vast wilderness of Siberia where they have all these natural
resources. I was flying over time zone after time zone without seeing any sign
of life—nothing but one east-west river that runs through there. All that time I
was thinking, these engines are running pretty rough and its lousy gasoline
they’ve got over here. What will I do if I end up down there? Just being up
there and seeing for myself the vast land that contained a wealth of resources
that would remain untapped—it seemed like such a waste.

But that’s the way the system works: countries are rewarded for not
developing their resources. While in Milan, I met with members of the
Russian delegation, who told me, “of course we will sign the Kyoto Treaty,
we don’t believe in global warming, but we get so much funding from the UN
and developed countries that we would be foolish not to sign. Also, the
Treaty will be long dead before Russia would have to comply.”

It was clear that the point of Kyoto wasn’t to reduce emissions, so what

was it? French President Jacques Chirac provided a good answer to that
question at The Hague in November 2000 when he explained that Kyoto
represents “the first component of an authentic global governance.”46 Margot
Wallstrom, the EU’s Environment Commissioner, took a slightly different

view, but one that reveals the real motives of Kyoto’s supporters. She
asserted that Kyoto is about “the economy, about leveling the playing field
for big businesses worldwide.”47 The meaning behind Chirac and
Wallstrom’s comments was clear to me: (1) Kyoto represented an attempt by
certain elements within the international community to restrain U.S. interests;
and (2) Kyoto was an economic weapon designed to undermine the global
competitiveness and economic superiority of the United States. Canadian
Prime Minister Stephen Harper put it well when he later called Kyoto a
“socialist scheme.”48 In short, Kyoto went against everything the United
States stands for.

THE GREATEST HOAX
When I became Chairman, I said that if the United States was even going to
consider taking drastic measures on global warming, the science behind
those decisions had better be absolutely sound.

I am most remembered for standing on the Senate floor in July 2003 and
declaring that man-made catastrophic global warming is the greatest hoax
ever perpetrated on the American people. I am often accused of coming to
that conclusion flippantly—as many on the environmental left have said, just
because I didn’t like what I heard from the mainstream global warming
alarmists. Nothing could be farther from the truth. I came to that conclusion
only after engaging in a lengthy, rigorous oversight process over the course of
a few years; it was the most thorough investigation of the science by any
senator. In more than twelve thousand words and several hours on the floor
over the course of two days, I brought numerous inconsistencies and gaps in
the mainstream theory to light. Only after that did I conclude that the science
to justify the catastrophic theory simply was not there.

My painstaking oversight was almost to a fault, so much so that one

editorial in the Oklahoman, looking back on a number of my science
speeches, said that my “detailed and highly technical fodder” might “have a
dual use as a cure for insomnia.” Although I may have been putting everyone
to sleep with my exhaustive approach, I was also appreciative that the
editorial recognized the importance of my work. As it also said, “Credit
Inhofe for nimbly making his case. And we think he’s got a point. The science
on human causation of global warming is conflicted and unsettled. There’s

something to be said for a senator who does his homework and is willing to
swim against the stream on this important issue.”49

My “Greatest Hoax” speech was the first speech I gave on the Senate
floor as Chairman of the Environment and Public Works Committee, and I
knew that it was going to be a defining moment for me going forward in this
leadership role. The night before I gave the speech, I spent hours going
through it with one of my most trusted aides. At the end of our meeting, I told
him how I wanted to close: with my famous line. I took out a pen and
handwrote it at the conclusion of the speech, “With all of the hysteria, all of
the fear, all of the phony science, could it be that man-made global warming
is the greatest hoax ever perpetrated on the American people? It sure sounds
like it.”

He looked slightly pained and asked seriously, “Are you sure?” He asked

me that question not because there was any doubt that it was the message I
needed to convey. He was asking me if I was prepared to endure the wrath of
the environmental left for that comment, for the most part all alone—which
clearly manifested itself months later in Milan. He tried to talk me out of it
but at that moment, I couldn’t have been more sure; catastrophic global
warming was the greatest hoax, and it was my responsibility to expose it for
what it was. The next day I asked him, “Now you made sure to put that line in
the speech, right?”

Hopefully without offering my readers a cure for insomnia, it is important

to explain how I came to that conclusion.

CATASTROPHIC GLOBAL WARMING BASED ON FEAR, NOT
SCIENCE
I am a U.S. Senator, and a former mayor and businessman. I am not a
scientist. But I do understand politics. And the more I delved into the science
purporting global catastrophe, the more I saw the extent to which the science
was being co-opted by those who care more about peddling fear of gloom
and doom to further their own, broader political agendas.

As I said on the Senate floor in July 2003, much of the debate about global
warming is predicated on fear rather than science. The alarmists predicted a
future plagued by catastrophic flooding, war, terrorism, economic
dislocations, droughts, crop failures, mosquito-borne diseases, and harsh

weather, and they placed the blamed squarely on man-made carbon
emissions.

One thing that people don’t always realize is that global warming

alarmism became like a religion that divided the world into believers and
skeptics—the latter of which was the most exclusive club in Washington, as
it consisted primarily of me. At any rate, being a skeptic was to them heresy
of the highest order. That, of course, is what landed my face on those
WANTED posters for being the most dangerous man on the planet. I still find
it amazing that, in their minds, I was capable of singlehandedly bringing
about the end of the world. But such was their logic.

Of course, alarmists accused me of attacking the science of global

warming—that is part of their game. But the truth is that throughout my battle
against the hoax, I consistently defended credible, objective science by
exposing the corrupting influences that continually subverted it for political
purposes. Good policy must be based on good science, not on religion, and
that requires science free from bias, whatever its conclusions.

GLOBAL WARMING OR GLOBAL COOLING?
I began my investigation by delving first into some of the most obvious
inconsistencies of the catastrophe rhetoric.

My starting point was a quote from a particular Newsweek magazine

article that said, “There are ominous signs that the Earth’s weather patterns
have begun to change dramatically and that these changes may portend a
drastic decline in food production—with serious political implications for
just about every nation on Earth.”50 Another came from Time magazine: “As
they review the bizarre and unpredictable weather pattern of the past several
years, a growing number of scientists are beginning to suspect that many
seemingly contradictory meteorological fluctuations are actually part of a
global climatic upheaval.”51 All of this climate rhetoric sounds very
ominous. That is, until you realize that these passages come from articles in a
1975 edition of Newsweek magazine and Time magazine in 1974. These
articles weren’t referring to global warming; they were warning of a coming
ice age.

These fears can also be found in a 1974 study by the National Science

Board, the governing body of the National Science Foundation, which stated,
“During the last 20 to 30 years, world temperature has fallen, irregularly at

first but more sharply over the last decade.”52 Two years earlier, the board
had observed: “Judging from the record of the past interglacial ages, the
present time of high temperatures should be drawing to an end… leading into
the next glacial age.”53

Yet, not long after we went through a widespread global cooling scare,

alarmists boldly went forward to assert that the science behind the
phenomenon of global warming was “settled,” the “debate was over,” and
that there was no question that it was man-made and catastrophic.

In truth, since 1895, alarmists have alternated between global cooling and

warming scares during four separate and sometimes overlapping time
periods. From 1895 until the 1930s, the media peddled a coming ice age—
and the world was coming to an end. From the late 1920s until the 1960s,
they warned of global warming—and again the world was coming to an end.
From the 1950s until the 1970s, they warned us again of a coming ice age—
and as before, the world was coming to an end. The latest apocalyptic scare
about global warming is the fourth attempt to promote catastrophe and the
world has yet to come to an end.

From Newsweek, April 28, 1975 © 1975 Newsweek, Inc. All rights
reserved. Used by permission and protected by the Copyright Laws of the
United States. The printing, copying, redistribution, or retransmission of the
Material without express written permission is prohibited.
www.newsweek.com

While alarmists continued their message of climate apocalypse, I

maintained that it was very simplistic to say that a one degree Fahrenheit
temperature increase during the twentieth century means that we are all
doomed. After all, a one degree Fahrenheit rise has coincided with the

greatest advancement of living standards, life expectancy, food production
and human health in the history of our planet. So it is hard to argue that the
global warming we experienced in the twentieth century was somehow
negative or part of a catastrophic trend.

Of course, these particular inconsistencies in the alarmists’ theories were
the kind of observations that one could point out without delving very far into
the debate. As I continued my investigation, more and more serious
inconsistencies with the science came to light.

THE UNITED NATIONS AND THE EMERGENCE OF THE IPCC
Even though the global community has been somewhat critical of the United
States in recent years, our nation remains one of the most generous in the
world. The American people have always gone above and beyond the call of
duty to help our neighbors and far away friends who are in need.

You probably remember the aftermath of 2010’s devastating earthquake in
Haiti. According to one report published by the Inter-American Development
Bank, the disaster killed over 200,000 people and could require more than
$7 billion in recovery costs.54 How did Americans respond? With an
outpouring of generosity and support. According to the Wall Street Journal,
Americans “raised more than $150 million in four days for the Haiti relief
efforts.”55 Countless others traveled to Haiti to help with the cleanup and
rescue efforts. This is not an uncommon headline following natural disasters.
The same response occurred after the tsunamis in Japan and the Indian
Ocean. The American people are generous, and I believe that responses like
these are a natural display of our nation’s character.

This is important when considering the global warming issue because if
the American people truly believed that the effects of climate change were
man-made and going to cause the destruction of the earth, then I believe the
American people would be the first to stand up and provide a solution. But
this has not happened.

While the global warming issue did not come to the attention of the

American people until the late 1980s or early 1990s, the seeds of the hoax
were being sown decades before by environmental elites at the United
Nations.

Why would they want to do this? I believe it is because many who work at
the United Nations would like to see the institution’s mandate expanded well
beyond its original intent. I do not think they are satisfied with having an
influential role in the international arena—I think they want a controlling one.
They want the United Nations to have sovereignty, control of the world’s
economic and political systems, and the ability to tax wealthy nations and
redistribute their resources to poorer ones in an effort, as Margo Wallstrom
said, to “level the playing field.”56

Just like liberals in Washington, the elites at the United Nations truly

believe that they know how to run things better than any individual country
ever could. In this way, they are like “super-liberals” on an international
scale. On one of its websites, the UN even claims that its “moral authority” is
one of its “best properties.”57 But what do the elites and super-liberals at the
UN believe?

Briefly, their guiding philosophy is known as “sustainable development,”

and it is alarmingly similar to the utopian ideals of global socialism.
Unfortunately for us Americans, this philosophy would require those of us in
Western nations to surrender our lifestyles and our resources so that they
could be redistributed to developing nations, just as our friends from Russia
acknowledged. And who would be doing this redistribution? You got it—the
United Nations.

Perhaps what is most alarming is that this is all being done in an effort to
save the “environment,” which is why I believe the leaders at the UN have
been pushing the global warming issue so hard. In fact, the Kyoto Protocol
embodies many of the actual goals and priorities of the UN super elites. That
is why it’s a crucial component of their effort to make the UN more powerful
and important.

More details about the history of sustainable development, including how

the philosophy was developed and what is wrong with it, are included in
Appendix A. I encourage you to check that out and see all that the UN is up
to. Keep these ideas in mind as you read the rest of the book. If you do, you’ll
be able to see how the global warming issue fits in with the bigger picture of
the ambitions of the super-liberals at the UN.

But for now, let’s stay focused on the core of the story. The UN began

officially working on the global warming issue when it decided to create the
IPCC.

IPCC SCIENTIFIC INTEGRITY QUESTIONED
Like most bad things that come to America, the primary science behind
catastrophic global warming came from the United Nations—specifically
from its Intergovernmental Panel on Climate Change (IPCC). Again, a careful
reading of Appendix A is a must to understand thoroughly the origin of the
hoax.

The Kyoto Treaty explicitly acknowledged that man-made emissions,

primarily from fossil fuel development, are causing global temperatures to
rise to catastrophic levels. Those who sign on to Kyoto pledge to cut back
dramatically or even work to eliminate fossil fuels so that the world can
return to global temperatures at “normal” levels. According to the IPCC,
Kyoto was to achieve “stabilization of greenhouse gas concentrations in the
atmosphere at a level that would prevent dangerous anthropogenic
interference with the climate system.”58

But when it came to discovering what those “normal” levels were, the
IPCC couldn’t provide a scientific explanation. That’s because they didn’t
have one. Dr. S. Fred Singer, formerly an atmospheric scientist at the
University of Virginia, said, “No one knows what constitutes a ‘dangerous’
concentration. There exists, as yet, no scientific basis for defining such a
concentration, or even of knowing whether it is more or less than current
levels of carbon dioxide.”59 This was seriously problematic. My question
was: how can we bring greenhouse gas concentrations to normal levels if we
don’t know what those normal levels are?

The more questions I asked, the clearer it became that Kyoto emissions

reduction targets were arbitrary, lacking in scientific basis. This was not just
my opinion, but the conclusion reached by the country’s most recognized
climate scientists. Dr. Tom Wigley, one of Al Gore’s own scientists, was one
of them. After President Clinton signed on to the Kyoto Protocol in 1997, Dr.
Wigley, a senior scientist at the National Center for Atmospheric Research,
found that if the Kyoto Protocol were fully implemented by all signatories, it
would reduce temperatures by a mere 0.06 degrees Celsius by 2050.60 And
that’s if the United States had ratified Kyoto and the other signatories met
their targets. What does this mean? Such an amount is so small that ground-
based thermometers cannot reliably measure it.

Dr. Richard Lindzen, an MIT scientist and member of the National

Academy of Sciences who specializes in climate science, told the
Environment and Public Works Committee on May 2, 2001, that there is a
“definitive disconnect between Kyoto and science. Should a catastrophic
scenario prove correct, Kyoto would not prevent it.”61 Similarly, Dr. James
Hansen of NASA, the father of global warming theory citing Wigley and
Malakoff, said that Kyoto Protocol “will have little effect” on global
temperature in the twenty-first century. In a rather stunning follow-up, Hansen
said it would take thirty Kyotos to reduce warming to an acceptable level.62
If one Kyoto devastates the American economy, what would thirty do? So
even the scientists were saying that it would be all pain for no gain.

In December 2004, when several nations including the United States met

in Buenos Aires for the tenth round of international climate change
negotiations, I was happy that the U.S. delegation held firm both in its
categorical rejection of Kyoto and the questionable science behind it. Paula
Dobriansky, Undersecretary of State for Global Affairs, and the leader of the
U.S. delegation, put it well when she told the conference, “Science tells us
that we cannot say with any certainty what constitutes a dangerous level of
warming, and therefore what level must be avoided.”63

FLAWED IPCC ASSESSMENT REPORTS
Our rigorous oversight of the IPCC began with my “Greatest Hoax” speech in
2003 and continued over the course of many years. In numerous speeches, I
recounted the systematic and documented abuse of the scientific process by
the IPCC, which claims it provides the most complete and objective
scientific assessment in the world on the subject of climate change.

In 2003, I began to expose the flaws in the IPCC process that were

glaringly apparent in the first IPCC Assessment Report in 1990, which found
that the climate record of the past century was “broadly consistent” with the
changes in Earth’s surface temperature, as calculated by climate models that
incorporated the observed increase in greenhouse gases. This conclusion,
however, appeared suspect to me, considering the climate cooled between
1940 and 1975, just as industrial activity grew rapidly after World War II.
How does one reconcile this cooling with the observed increase in
greenhouse gases?

But the flaws revealed themselves in earnest when the IPCC issued its

second assessment report in 1996. The most obvious problem was the
altering of the document on the central question of whether man is causing
global warming. Here is what Chapter 8—the key chapter in the report—
stated on this central question in the final version accepted by reviewing
scientists: “No study to date has positively attributed all or part [of the
climate change observed] to anthropogenic causes.”64

But when the final version was published, this and similar phrases in
fifteen sections of the chapter were deleted or modified. Nearly all the
changes removed hints of scientific doubts regarding the claim that human
activities are having a major impact on global warming. In the Summary for
Policymakers—which is the only part of the report that most reporters and
policymakers read—a single phrase was inserted. It reads: “The balance of
evidence suggests that there is a discernible human influence on global
climate.”65

The lead author for Chapter 8, Dr. Ben Santer, is not fully to blame for

manipulation of the message. According to the journal Nature, the changes to
the report were made in the midst of high-level pressure from the
Clinton/Gore State Department to do so. In fact, after the State Department
sent a letter to Sir John Houghton, Co-Chairman of the IPCC, Houghton
prevailed upon Santer to make the changes. Of course, the impact of this
change was explosive, with media across the world, including heavyweights
such as Peter Jennings, declaring this as proof that man is responsible for
global warming. On September 10, 1995, the New York Times published an
article titled “Global Warming Experts Call Human Role Likely.” According
to the Times account, the IPCC showed that global warming “is unlikely to be
entirely due to natural causes.”66 When parsed, this account means fairly
little. Not entirely due to natural causes? Well, how much, then? One
percent? Twenty percent? Eighty-five percent?

The IPCC report was replete with caveats and qualifications, providing

little evidence to support anthropogenic theories of global warming. The
preceding paragraph in which the “balance of evidence” quote appears
makes exactly that point. It reads, “Our ability to quantify the human influence
on global climate is currently limited because the expected signal is still
emerging from the noise of natural variability, and because there are
uncertainties in key factors. These include the magnitude and patterns of

long-term variability and the time evolving pattern of forcing by, and
response to, changes in concentrations of greenhouse gases and aerosols, and
land surface changes.”

Perhaps one of the most important yet most ignored aspects of the IPCC

report is that it is actually quite explicit about the uncertainties surrounding a
link between human actions and global warming: “Although these global
mean results suggest that there is some anthropogenic component in the
observed temperature record, they cannot be considered compelling
evidence of a clear cause-and-effect link between anthropogenic forces and
changes in the Earth’s surface temperature.” Remember, the IPCC is
supposed to provide the scientific basis for the alarmists’ conclusions about
global warming. Yet, even the IPCC admitted that its own science cannot be
considered compelling evidence.

Dr. John Christy, professor of Atmospheric Science and director of the

Earth System Science Center at the University of Alabama in Huntsville, and
a key contributor to the 1995 IPCC report, participated with the lead authors
in the drafting sessions and in the detailed review of the scientific text. He
wrote in the Montgomery Advertiser on February 22, 1998, that much of
what passes for common knowledge in the press regarding climate change is
“inaccurate, incomplete or viewed out of context.”67 Dr. Christy said that
many of the misconceptions about climate change originated from the IPCC’s
six-page executive summary, rather than the final report. It was the most
widely read and quoted of the three documents published by the IPCC’s
Working Group, but, Christy said— and this point is crucial—it had the
“least input from scientists and the greatest input from non-scientists.”68

IPCC PLAYS HOCKEY AND LOSES
Five years later, the IPCC was back at it again, this time with the Third
Assessment Report on Climate Change. In October 2000, the IPCC Summary
for Policymakers was leaked to the media, which once again accepted the
IPCC’s conclusions as fact.

Based on the summary, The Washington Post wrote on October 30, 2000,
“The consensus on global warming keeps strengthening.”69 In a similar vein,
the New York Times confidently declared on October 28, 2000, “The
international panel of climate scientists considered the most authoritative
voice on global warming has now concluded that mankind’s contribution to

the problem is greater than originally believed.”70 Of course, these accounts
were worded to maximize the fear factor, and upon closer inspection they
had no compelling intellectual content. “Greater than originally believed”?
Was that .01 percent, or 25 percent? And how much is greater? Double?
Triple?

Such reporting prompted testimony by Dr. Richard Lindzen before the

Committee on Environment and Public Works in May of 2001. Lindzen said,
“Almost all reading and coverage of the IPCC is restricted to the highly
publicized Summaries for Policymakers, which are written by
representatives from governments, NGOs and business; the full reports,
written by participating scientists, are largely ignored.”71 Of course, the
Policymaker’s Summary was politicized and radically differed from an
earlier draft. For example, the draft concluded the following concerning the
driving causes of climate change:

From the body of evidence since IPCC (1996), we conclude that there
has been a discernible human influence on global climate. Studies are
beginning to separate the contributions to observed climate change
attributable to individual external influences, both anthropogenic and
natural. This work suggests that anthropogenic greenhouse gases are a
substantial contributor to the observed warming, especially over the past
30 years. However, the accuracy of these estimates continues to be
limited by uncertainties in estimates of internal variability, natural and
anthropogenic forcing, and the climate response to external forcing.

The final version, however, looked quite different and concluded instead,

“In the light of new evidence and taking into account the remaining
uncertainties, most of the observed warming over the last 50 years is likely
to have been due to the increase in greenhouse gas concentrations.”

This kind of distortion was not unintentional. As Dr. Lindzen explained

before the EPW Committee, “I personally witnessed coauthors forced to
assert their ‘green’ credentials in defense of their statements.”72 In short,
some parts of the IPCC process resembled a Soviet-style trial: facts were
predetermined and ideological purity trumped technical and scientific rigor.
The most egregious flaw in the Third Assessment is undoubtedly the now

infamous hockey stick graph produced by Dr. Michael Mann and others,

which the IPCC enthusiastically embraced.

Intergovernmental Panel on Climate Change, Working Group 1, Climate
Change Tool: The Scientific Basis, 2001

Mann’s study concluded that the twentieth century was the warmest on

record in the last one thousand years, showing flat temperatures until 1900,
and then spiking upward. Put simply, it looked like a hockey stick. The cause
for such a shift, of course, is attributed to industrialization and man-made
greenhouse gas emissions. The conclusion is that industrialization, which
spawned widespread use of fossil fuels, was causing the planet to warm.

The hockey stick achieved instant fame as proof that humans were causing
global warming because it was featured prominently in the Summary Report
read by the media—and eventually Al Gore made it mainstream in his
documentary, An Inconvenient Truth.

But the problems with Mann’s study were immense. First of all, it focuses

on temperature trends only in the Northern Hemisphere. Mann extrapolated
that data to reach the conclusion that global temperatures remained relatively
stable and then dramatically increased at the beginning of the twentieth
century. That leads to Mann’s conclusion that the twentieth century has been
the warmest in the last one thousand years. As is obvious, however, such an

extrapolation cannot provide a reliable global perspective of long-term
climate trends.

Mann’s conclusions were also drawn mainly from twelve sets of climate

proxy data, of which nine were tree rings, while the remaining three came
from ice cores. Ice core data was drawn from Greenland and Peru. What’s
left is a picture of the Northern Hemisphere based on eight sets of tree-ring
data. Again, hardly a convincing global picture of the last one thousand
years.

In other words, Mann’s hockey stick completely dismisses both the

Medieval Warm Period (800 to 1300) and the Little Ice Age (1300 to 1900),
two climate events that are widely recognized in the scientific literature.
Mann said the twentieth century was “nominally the warmest” of the past
millennium and that the decade of the 1990s was the warmest decade on
record. The Medieval Warm Period and Little Ice Age are replaced by a
largely benign and slightly cooling linear trend in climate until 1900. But as
is clear from a close analysis of Mann’s methods, the hockey stick is formed
by crudely grafting the surface temperature record of the twentieth century
onto a pre-1900 tree-ring record.

Intergovernmental Panel on Climate Change, Climate Change, The IPCC
Scientific Assessment 202 (1990).

It was a highly controversial and scientifically flawed approach. As was
widely recognized in the scientific community, two data series representing
radically different variables (temperature and tree rings) cannot be grafted
together credibly to create a single series. In simple terms, as Dr. Patrick
Michaels of the University of Virginia explained, this is like “comparing
apples to oranges.”73 Even Mann and his coauthors admitted that if the tree
ring data set were removed from their climate reconstruction, the calibration
and verification procedures they used would undermine their conclusions.

A study from Harvard-Smithsonian Center for Astrophysics strongly

disputed Mann’s methods and hypotheses. As coauthor Dr. David Legates
wrote, “Although [Mann’s work] is now widely used as proof of
anthropogenic global warming, we’ve become concerned that such an
analysis is in direct contradiction to most of the research and written
histories available…Our paper shows this contradiction and argues that the
results of Mann… are out of step with the preponderance of the evidence.”74
Indeed, Mann’s theory of global warming was out of step with most scientific
thinking on the subject. Dr. Hans von Storch, a prominent German researcher
with the GKSS Institute for Coastal Research who believes in global
warming put it this way: “We were able to show in a publication in Science
that the [hockey stick] graph contains assumptions that are not permissible.
Methodologically it is wrong: rubbish.”75

Dr. von Storch was not the only one who felt that way. Three

geophysicists from the University of Utah, in the April 7, 2004, edition of
Geophysical Research Letters, concluded that Mann’s methods used to
create his temperature reconstruction were deeply flawed. In fact, their
judgment was harsher than that. As they wrote, Mann’s results are “based on
using end points in computing changes in an oscillating series” and are “just
bad science.” I repeat: “just bad science.”

My concerns about the “hockey stick” in those early days were validated

later in 2005 when two Canadian researchers, Steven McIntyre and Ross
McKitrick, essentially tore apart its statistical foundation. In essence, they
discovered that Dr. Mann misused an established statistical method called
principal components analysis (PCA). As they explained, Mann created a
program that “effectively mines a data set for hockey stick patterns.”76 In
other words, no matter what kind of data one uses, even if it is random and
totally meaningless, the Mann method always produces a hockey stick. After
conducting some 10,000 data simulations, the result was nearly always the
same. “In over 99 percent of cases,” McIntyre and McKitrick wrote, “it
produced a hockey stick shaped PCI series.”77 Statistician Francis Zwiers of
Environment Canada, a government agency, said he agreed that Dr. Mann’s
statistical method “preferentially produces hockey sticks when there are none
in the data.”78 Even to a non-statistician, this looked extremely troubling. But
that statistical error was just the beginning. On a public Web site where Dr.
Mann filed data, McIntyre and McKitrick discovered an intriguing folder

titled “BACKTO_1400-CENSORED.” What McIntyre and McKitrick found
in the folder was disturbing: Mann’s hockey stick blade was based on a
certain type of tree—a bristlecone pine—that, in effect, helped to
manufacture the hockey stick.

So why was the bristlecone pine important? The bristlecone experienced

a growth pulse in the Western United States in the late 19th and early 20th
centuries. However, this growth pulse, as the specialist literature has
confirmed, was not attributed to temperature. So using those pines, and only
those pines as a proxy for temperature during this period is questionable at
best. Even Mann’s co-author has stated that the bristlecone growth pulse is a
“mystery.” Because of these obvious problems, McIntyre and McKitrick
appropriately excluded the bristlecone data from their calculations. What did
they find? Not the Mann hockey stick, to be sure, but a confirmation of the
Medieval Warm Period, which Mann’s work had erased. As the CENSORED
folder revealed, Mann and his colleagues never reported results obtained
from calculations that excluded the bristlecone data. It appeared to be a case
of selectively using data—that is, if you don’t like the result, remove the
offending data until you get the answer you want. As McIntyre and McKitrick
explained, “Imagine the irony of this discovery…Mann accused us of
selectively deleting North American proxy series. Now it appeared that he
had results that were exactly the same as ours, stuffed away in a folder
labeled CENSORED.”

McIntyre and McKitrick believed there were additional errors in the
Mann hockey stick. To confirm their suspicion, they need additional data
from Dr. Mann, including the computer code he used to generate the graph.
But Dr. Mann refused to supply it. As he told the Wall Street Journal,
“Giving them the algorithm would be giving in to the intimidation tactics that
these people are engaged in.”79

Intimidation tactics? On April 12, 2005, I gave a speech on the Senate

floor explaining McIntyre and McKitrick’s findings and, as I said, “Mr.
McIntyre and Mr. McKitrick were just trying to find the truth. What is Dr.
Mann trying to hide?” Well, as the Climategate scandal eventually revealed,
he may have been trying to “hide the decline” in temperatures, but we will
get to that later.

In June 2006, the National Academy of Science released its study,

“Surface Temperature Reconstructions for the Last 2,000 Years” which

acknowledged that there were “relatively warm conditions centered around
A.D. 1000 (identified by some as the ‘Medieval Warm Period’) and a
relatively cold period (or ‘Little Ice Age’) centered around 1700.” This
report refuted the hockey stick which showed temperatures in the Northern
Hemisphere remained relatively stable over nine hundred years, then spiked
upward in the twentieth century. The NAS report also stated that “substantial
uncertainties” surround Mann’s claims that the last few decades of the
twentieth century were the warmest in last one thousand years. In fact, while
the report conceded that temperature data uncertainties increase going
backward in time, it acknowledged that “not all individual proxy records
indicate that the recent warmth is unprecedented.” In one last blow, the NAS
reports stated, “Even less confidence can be placed in the original
conclusions by Mann et al. (1999) that ‘the 1990s are likely the warmest
decade and 1998 the warmest year in at least a millennium.” When that
report appeared, it only confirmed what I had been saying all along: the
hockey stick was broken.

Climate alarmists have long been attempting to erase this inconvenient

Medieval Warm Period from the Earth’s climate history for at least a decade
because it doesn’t fit in with their theories of catastrophe. David Deming,
who at the time was an Assistant Professor at the University of Oklahoma’s
College of Geosciences, understood this all too well. Dr. Deming was
welcomed into the close-knit group of global warming believers after he
published a paper in 1995 that noted some warming in the twentieth century.
Deming says he was subsequently contacted by a prominent global warming
alarmist who told him point-blank, “We have to get rid of that Medieval
Warm Period.”80 When the “hockey stick” first appeared in 1998, it did just
that. And yet, the IPCC immortalized the hockey stick as incontrovertible
proof of catastrophic global warming.

IPCC: POLITICS, NOT SCIENCE
How could the IPCC so blatantly move forward with arguments based on
such dubious science? I had an answer to that question as early as my 2003
speech.

First, the IPCC is a political institution. Its whole purpose is to support
the efforts of the UN Framework Convention on Climate Change, which has
the basic mission of eliminating the “threat” of global warming. This clearly

creates a conflict of interest with the standard scientific goal of assessing
scientific data in an objective manner. The Summary for Policymakers
illustrates some of the problems with this: it was not approved by the
scientists and economists who contributed to the report. It was approved by
intergovernmental delegates—in short, politicians. It doesn’t take a leap of
imagination to realize that politicians will insist the report supports their
political agenda. Both scientists and economists complained that the
summary did not adequately reflect the uncertainties associated with tentative
conclusions in the basic report. The uncertainties identified by contributing
authors and reviewers seemed to disappear or were downplayed in the
summary.

In other words, the lead authors and the chair of the IPCC control too
much of the process. The old adage “power corrupts, and absolute power
corrupts absolutely” applies. Only a handful of individuals were involved in
changing the entire tone of the second assessment. Likewise, Michael Mann
was a chapter lead author in the third assessment.

One stark example of how the process has been corrupted involves a U.S.
government scientist who was among the world’s most respected experts on
hurricanes, Dr. Christopher Landsea, who eventually resigned as a
contributing author of the fourth assessment. His reason was simple—the
lead author for the chapter on extreme weather, Dr. Kevin Trenberth, had
demonstrated he may pursue a political agenda linking global warming to
more severe hurricanes. Trenberth had spoken at a forum where he was
introduced as a lead author and proceeded forcefully to make the link. The
only problem is that Trenberth’s views may not have been widely accepted
among the scientific community. As Landsea explained, “All previous and
current research in the area of hurricane variability has shown no reliable,
long-term trend up in the frequency or intensity of tropical cyclones, either in
the Atlantic or any other basin.”81

When Landsea brought it to the attention of the IPCC, he was told that
Trenberth—who as lead author is supposed to bring a neutral, unbiased
perspective to his position—would keep his position. Landsea concluded
that, “because of Dr. Trenberth’s pronouncements, the IPCC process on our
assessment of these crucial extreme events in our climate system has been
subverted and compromised, its neutrality lost.”

Landsea’s experience is not unique. Richard Lindzen, a prominent MIT

researcher who was a contributing author to a chapter in the third assessment,
said that the summary did not reflect the chapter to which he contributed. But
when you examine how the IPCC is structured, is it really so surprising? The
IPCC has consistently demonstrated an unreasoning resistance to accepting
constructive critiques of its scientific and economic methods. I said that this
was a recipe for de-legitimizing the entire endeavor in terms of providing
credible information that is useful to policymakers.

I explained a few examples of this on the Senate floor in 2003. First,

malaria is considered one of the four greatest risks associated with global
warming. But the relationship between climate and mosquito populations is
highly complex. There are more than thirty-five hundred species of mosquito,
and all breed, feed, and behave differently. Yet the nine lead authors of the
health section in the second assessment had published only six research
papers on vector-borne diseases among them. Dr. Paul Reiter of the Pasteur
Institute, a respected entomologist who has spent decades studying mosquito-
borne malaria, believes that global warming would have little impact on the
spread of malaria. But the IPCC refused to consider his views in its third
assessment, and completely excluded him from contributing to the fourth
assessment.

Here’s another example: To predict future global warming, the IPCC
estimated how much world economies would grow over the next century.
Future increases in carbon dioxide emission estimates are directly tied to
growth rates, which, in turn, drive the global warming predictions. However,
the method the IPCC used to calculate growth rates was wrong. It also
contained assumptions that developing nations will experience explosive
growth—in some cases, becoming wealthier than the United States. These
combine to greatly inflate even its lower-end estimates of future global
warming. The IPCC, however, bowed to political pressure from the
developing countries that refused to acknowledge the likelihood that they
will not catch up to the developed world. The result: Future global warming
predictions by the IPCC were based on a political choice, not on credible
economic methodologies.

Likewise, the IPCC ignored the advice of economists who concluded that
if global warming is real, future generations would have a higher quality of
life if societies maximize economic growth and adapt to future warming
rather than trying to drastically curb emissions. This problem with the

economics led to a full-scale inquiry by the UK’s House of Lords Select
Committee on Economic Affairs, which found numerous problems with the
IPCC. In fact, the problems they identified were so substantial, it led Lord
Nigel Lawson, former Chancellor of the Exchequer and a Member of the
Committee, to state: “I believe the IPCC process is so flawed, and the
institution, it has to be said, so closed to reason, that it would be far better to
thank it for the work it has done, close it down, and transfer all future
international collaboration on the issue of climate change.”82

These were the red flags that I examined with a vengeance from 2003 on.

As more and more problems continued to surface with the IPCC, I warned
that the entire institution would lose its credibility altogether if it did not take
drastic steps to remedy the situation.

2005: REFORM NEEDED AT THE IPCC
In December 2005, as the IPCC was preparing to issue its fourth assessment
report, I wrote to Dr. R.K. Pachauri, Chair of the Intergovernmental Panel on
Climate Change, explaining my concern that certain scientific conclusions are
selected or excluded from the IPCC’s consideration and presentation, and
how the science has been manipulated in order to reach a predetermined
conclusion. I objected to the comments he made in Montreal earlier that year
where he stated, “In the fourth assessment, we will conduct an extensive
outreach effort. If facts are highlighted, not exaggerated… then it will help in
changing public perception.”83 I told him that such an effort was in direct
conflict with an objective assessment of the science, which should be free of
political goals. Selective presentations of facts, whether accurate or not,
skew the public’s understanding of the issue by eliminating contrary findings
and potentially considerable uncertainty about their accuracy.

I said that these problems must be remedied in order for the IPCC to
present a fair and impartial conclusion as to the current state of climate
science, and therefore regain its credibility. I wrote that the IPCC must adopt
procedures that ensure that impartial scientific reviewers formally approve
both the chapters and the Summary for Policymakers—the latter of which is
the only document that members of the press and members of Congress ever
read. When compared with the actual report, it was clear the Summary for
Policymakers was being co-opted by activists with an agenda to shape the
conclusions to show that man-made emissions were causing catastrophic

global warming. To safeguard against the manipulation of the message,
objective scientists, not government delegates, should be a part of the
approval process. I also said that the IPCC must ensure that any uncertainties
in the state of knowledge be clearly expressed in the Summary for
Policymakers. But Dr. Pachauri dismissed my concerns. Here’s how Reuters
reported his response:

In the one-page letter, [Pachauri] denies that the IPCC has an alarmist
bias and says, ‘I have a deep commitment to the integrity and objectivity
of the IPCC process.’ Pachauri’s main argument is that the IPCC
comprises both scientists and more than 130 governments who approve
IPCC reports line by line. That helps ensure fairness, he says.84

My concerns were confirmed when the IPCC process finally imploded in

2009 with the Climategate scandal and the errors in the IPCC science
revealed in its wake, which showed once and for all that their process was
all politics; science was secondary, even non-essential, to the ultimate goal
of confirming catastrophic global warming and achieving global governance.

STATE OF FEAR: MICHAEL CRICHTON
As I was exposing flaws and inconsistencies of the IPCC science on the
Senate floor, bestselling author Dr. Michael Crichton was doing it through
fiction—it was, as the saying goes, art imitating life. In 2004, Dr. Crichton
published State of Fear, a fascinating novel that questions the scientific
consensus of man-made catastrophic global warming, while also predicting
much of how the global warming debate would pan out. If I was the one-man
truth squad on Capitol Hill, surely Dr. Crichton was the one-man truth squad
in Hollywood.

With his scientific and medical background, Dr. Crichton made it a

practice to do extensive research before writing his books. For Congo, he
studied the Congo. In Airframe, he perfected the details of airframe structure.
But my favorite bestseller of all was State of Fear. Initially, Dr. Crichton
planned to write a novel about celebrities who warn about the dangers of
global warming and the disasters that they predict come true. But the more he
researched, the more he began to understand the true motives of the
movement, and instead wrote a book about how it all turned out to be a scam.
Of course, being from Hollywood and having been the force behind the

blockbuster, Jurassic Park, as well as the very popular series ER, he knew
the Hollywood elite mentality firsthand, and does an excellent job portraying
young actors who go out to evangelize about global warming. In fact, one
theme that State of Fear introduced and features brilliantly is the “religion”
of global warming.

Throughout the book, environmental organizations are focused squarely on
raising money, principally by scaring potential contributors with predictions
of a global apocalypse and claims that they are saving the world.

What is truly remarkable about Dr. Crichton’s book is just how accurately
it establishes the progression of the entire movement. The novel calls out the
media and Hollywood for its political agenda—which had not yet reached its
height when he wrote the book in 2004. At one point in the story, a young
actor called Ted Bradley films a scene where he lectures a group of third
graders about the dangers of global warming. As he says:

But now these magnificent trees—having survived the threat of fire, the
threat of logging, the threat of soil erosion, the threat of acid rain—now
face their greatest threat ever. Global warming. You kids know what
global warming is, don’t you?

Hands went up all around the circle. “I know, I know!”

“I’m glad you do,” Bradley said, gesturing for the kids to put their

hands down. The only person talking today would be Ted Bradley. “But
you may not know that global warming is going to cause a very sudden
change in our climate. Maybe just a few months or years, and it will
suddenly be much hotter or much colder. And there will be hordes of
insects and diseases that will take down these wonderful trees.”

[…]

By now the kids were fidgeting, and Bradley turned squarely to the

cameras. He spoke with the easy authority he had mastered while playing
the president for so many years on television. “The threat of abrupt
climate change,” he said, “is so devastating for mankind, and for all life
on this planet, that conferences are being convened all around the world
to deal with it. There is a conference in Los Angeles starting tomorrow,

where scientists will discuss what we can do to mitigate this terrible
threat. But if we do nothing, catastrophe looms. And these mighty,
magnificent trees will be a memory, a postcard from the past, a snapshot
of man’s inhumanity to the natural world. We’re responsible for
catastrophic climate change. And only we can stop it.”85
The novel also ventures into extensive detail about the science of the
IPCC, demonstrating the manipulation of data from the very beginning. It
even touches on some of the same problems I had exposed on the Senate
floor in 2003. As Dr. Crichton writes,

“… it is unquestionably manipulative. And Hansen’s testimony wasn’t the
only instance of media manipulation that’s occurred in the course of the
global warming sales campaign. Don’t forget the last-minute changes in
the 1995 IPCC report.”

“IPCC? What last minute changes?”

“The UN formed the Intergovernmental Panel on Climate Change in

the late 1980s. That’s the IPCC… a huge group of bureaucrats, and
scientists under the thumb of bureaucrats. The idea was that since this
was a global problem, the UN would track climate research and issue
reports every few years. The first assessment report in 1990 said it
would be very difficult to detect a human influence on climate, although
everybody was concerned that one might exist. But the 1995 report
announced with conviction that there was now a ‘discernible human
influence’ on climate.”

“…a discernible human influence’ was written into the 1995 summary

report after the scientists themselves had gone home. Originally, the
document said scientists couldn’t detect a human influence on climate for
sure, and they didn’t know when they would. They said explicitly, ‘we
don’t know.’ That statement was deleted and replaced with a new
statement that a discernible human influence did indeed exist. It was a
major change.”86

Of course, the “major change” that Dr. Crichton accurately refers to is in

the IPCC’s second assessment, which I also discussed in my “Greatest Hoax”

speech. Dr. Crichton’s book also addresses the change in terms from “global
warming” to “climate change,” which didn’t happen in full force until after
the appearance of Al Gore’s global warming Hollywood hysteria film in
2006. In the novel, two of the characters, Drake and Henley, devise a way to
change the rhetoric to keep money flowing in as the issue of global warming
loses credibility:

“… You can’t raise a dime with it, especially in winter. Every time it
snows people forget all about global warming. Or else they decide some
warming might be a good thing after all. They’re trudging through the
snow, hoping for a little global warming. It’s not like pollution, John.
Pollution worked. It still works. Pollution scares… people. You tell ’em
they’ll get cancer, and the money rolls in. But nobody is scared of a little
warming. Especially if it won’t happen for a hundred years.”87

“So what you need … is to structure the information so that whatever

kind of weather occurs, it always confirms your message. That’s the
virtue of shifting the focus to abrupt climate change. It enables you to use
everything that happens. There will always be floods, and freezing
storms, and cyclones, and hurricanes. These events will always get
headlines and airtime. And in every instance, you can claim it as an
example of abrupt climate change caused by global warming. So the
message gets reinforced. The urgency is increased.”88

In the “Author’s Message” at the end of the book, Dr. Crichton’s words
are calm amid the building hysteria: “We are also in the midst of a natural
warming trend that began about 1850, as we emerged from a 400-year cold
spell known as the Little Ice Age”; “Nobody knows how much of the present
warming trend might be a natural phenomenon”; and, “Nobody knows how
much of the present trend might be man-made.” And for those who were
worried about impending disaster in the coming century, Dr. Crichton says, “I
suspect that people of 2100 will be much richer than we are, consume more
energy, have a smaller global population, and enjoy more wilderness than we
have today. I don’t think we have to worry about them.”

In 2005, I invited Dr. Crichton to testify at a Senate hearing on the “Role
of Science in Environmental Policy Making.” His appearance was ridiculed
by environmentalists and many Democratic Senators who dismissed his

testimony because he was a science fiction writer—of course, only a few
years later, those same Senators would invite the Hollywood elite to testify
on Capitol Hill as experts on the dangers of global warming. Dr. Crichton
was more than just a science fiction author. He was also a scientist and a
medical doctor who held degrees from Harvard College and Harvard
Medical School; was a visiting lecturer in Physical Anthropology at
Cambridge University; and a postdoctoral fellow at the Salk Institute. His
detractors didn’t understand that his testimony pulled back the curtain on the
manipulation of data by climate researchers, which would not be completely
understood until the Climategate scandal, four years later.

What he discovered in researching the science behind climate change was

that most of the research was being conducted by the same insular group of
scientists without any independent verification. His message at the Senate
hearing was very clear and should be endorsed by everyone who desires that
public policy be set by sound science. He stated:

In essence, science is nothing more than a method of inquiry. The method
says an assertion is valid—and will be universally accepted—only if it
can be reproduced by others, and thereby independently verified. The
scientific method is utterly apolitical. A truth in science is verifiable
whether you are black or white, male or female, old or young. It’s
verifiable whether you know the experimenter, or whether you don’t. It’s
verifiable whether you like the results of a study, or you don’t.

Thus, when adhered to, the scientific method can transcend politics.

Unfortunately, the converse may also be true: when politics takes
precedent over content, it is often because the primacy of independent
verification has been abandoned.

Verification may take several forms. I come from medicine, where the

gold standard is the randomized double-blind study. Not every study is
conducted in this way, but it is held up as the ultimate goal.

In climate research, the same small group of scientists conducts the

majority of the research and peer reviews each other’s work. A scientist peer
reviewing a colleague one year knows that that same colleague may be
reviewing their work the next year. To provide a direct contrast with the

research procedures in climate science, Dr. Crichton told the story of a
physician who was in the middle of an FDA study of a new drug. It was a
double-blind study so the different teams conducting the research were not
allowed to have any contact with the other teams as they worked, so as not to
contaminate the results. When this physician innocently met another
researcher from a different team while waiting at the airport, they were both
required to report their encounter to the FDA. As Dr. Crichton explained,
“For a person with a medical background, accustomed to this degree of rigor
in research, the protocols of climate science appear considerably more
relaxed.” As an example of this lax peer review, he specifically evaluated
the work of Dr. Mann and his hockey stick.

The American climate researcher Michael Mann and his co-workers
published an estimate of global temperatures from the year 1000 to 1980.
Mann’s results appeared to show a spike in recent temperatures that was
unprecedented in the last thousand years. His alarming report received
widespread publicity and formed the centerpiece of the UN’s Third
Assessment Report, in 2001. The graph appeared on the first page of the
IPCC Executive Summary.

Mann’s work was initially criticized because his graph didn’t show

the well-known Medieval Warm Period, when temperatures were
warmer than they are today, or the Little Ice Age, when they were colder
than today. But real fireworks began when two Canadian researchers,
McIntyre and McKitrick, attempted to replicate Mann’s study. They found
grave errors in the work, which they detailed in 2003: calculation errors,
data used twice, and a computer program that generated a hockeystick out
of any data fed to it—even random data.

Mann’s work has been dismissed as “phony” and “rubbish” by climate
scientists around the world who subscribe to global warming. Some have
asked why the UN accepted Mann’s report so uncritically. It is unsettling
to learn Mann himself was in charge of the section of the report that
included his work. This episode of climate science is far from the
standards of independent verification.

The hockeystick controversy drags on. But I would direct the

Committee’s attention to three aspects of this story. First, six years

passed between Mann’s publication and the first detailed accounts of
errors in his work. This is simply too long for policymakers to wait for
validated results. Particularly if it is going to be shown around the world
in the meantime.

Second, the flaws in Mann’s work were not caught by climate

scientists, but rather by outsiders—in this case, an economist and a
mathematician. McIntyre and McKitrick had to go to great lengths to
obtain the data from Mann’s team, which obstructed them at every turn.
When the Canadians sought help from the NSF, they were told that Mann
was under no obligation to provide his data to other researchers for
independent verification.

Third, this kind of stonewalling is not unique or uncommon. The

Canadians are now attempting to replicate other climate studies and are
getting the same runaround from other researchers. One leading light in
the field told them: “Why should I make the data available to you, when
your aim is to try and find something wrong with it?”

Even further, some scientists complain the task of archiving is so time-

consuming as to prevent them from getting any work done. But this is
nonsense.

Dr. Crichton was right. It is nonsense that climate science is not properly
peer-reviewed, it is nonsense that scientists do not share data, it is nonsense
for policymakers to set policy on research which cannot be replicated, and it
is nonsense to wreck economies and jobs based on this so-called research.

After the hearing, one of my staffers asked Dr. Crichton if the State of
Fear would ever be made into a movie. He said, “No, Hollywood would
never touch a film like this.” He said he had lost a lot of Hollywood friends
because of this book, but he felt that it still had to be written. Of course, at
the time, Hollywood had just premiered The Day After Tomorrow, a disaster
film that depicts the world’s untimely demise due to our failure to “act” on
global warming.

Even toward the end of his life, Dr. Crichton endured a good deal of

wrath from the environmental community, and many regarded him as a traitor
for portraying the errors in global warming science and exposing the

shallowness of the Hollywood elite. May he rest in peace now that he too is
vindicated. I invite you to read a few excerpts from State of Fear in
Appendix B.

THE PUSH FOR CAP AND TRADE BEGINS
In 2003, when I was fighting against the hoax of the science, I was also
fighting the hoax of the so-called solution to global warming: cap and trade
legislation.

Cap and trade achieved essentially the same outcome as the Kyoto Treaty:

it had the same $300–400 billion annual pricetag; it created the same
increased energy costs; it destroyed the same number of jobs; and brought the
same amount of economic pain to our country. It only bound the United States
to emissions reductions so, of course, jobs would be shipped overseas to
China, India, and across the border to Mexico, where they don’t have cap
and trade, which means that worldwide emissions would actually increase.
Honestly, if I had a dollar for every time I said that Kyoto or cap and trade
would have been all pain for no environmental gain, I think I would have a
million dollars just from that by now.

So what they couldn’t achieve through the Kyoto Treaty, they tried to

achieve through legislation.

Here’s how cap and trade for carbon dioxide emissions works: As the

government imposes caps on emissions, it essentially establishes an artificial
price for carbon. Each regulated entity may only emit a certain amount of
carbon, and if it exceeds that limit, it can buy credits from other entities that
are not exceeding their limits. Of course, higher emitting entities such as
coal-fired power plants, would have to purchase a large number of credits to
continue business as usual, and as President Obama said himself, “electricity
prices would necessarily skyrocket” because these costs “would be passed
on to consumers” in the form of an energy tax. Ultimately, the real losers in
this scenario would have been the American people, who would have had to
shoulder the largest tax increase in American history.

Of course, the philosophy behind cap and trade is that if we restrict

enough supply of fossil fuels, the price will increase, and we can then simply
shift to less costly alternatives. Yet this is wishful thinking. Alternatives are
fine, but in most cases, they aren’t widely available or commercially viable
yet—certainly not in a form that can efficiently, affordably, and reliably meet

our existing energy needs. How are we supposed to run this machine called
America without proven and reliable sources such as oil, coal, and natural
gas? The answer is we can’t.

All I knew was that there was no way cap and trade was going to pass out

of the Environment and Public Works Committee with me as chairman.

The first attempt to impose cap and trade came from Senators McCain and
Lieberman’s bill in 2003. As promised, I blocked it from moving through the
Committee, so in order to have their bill brought up for a vote, they had to
bypass the committee process and have it brought straight to the Senate floor.
At the time, Senator McCain was living up to his reputation for being a
maverick and bucking his own party. He had the support of the liberal media
and the Democrats in Congress. I handled the opposition on the floor, as few
Republican Senators dared to speak out against the bill. To put it simply,
many of my Republican colleagues were afraid that they too would see their
faces on an environmental WANTED poster.

In one lively floor debate, on October 30, 2003, Senator McCain, after

quoting Ernest Hemingway, made his case that global warming was a serious
threat and because of it “the snows of Kilimanjaro may soon exist only in
literature.” He went on to say, “These are facts. These are facts that cannot
be refuted by any scientist or any union or any special interest that is
weighing in more heavily on this issue than any issue since we got into
campaign finance reform.”

When it was my turn to speak, I said that while I appreciated the

comments made by my good friend from Arizona, what he was saying was
simply false. I then quoted an article from the front page of that morning’s
USA Today. It was about James Morison, a scientist with the University of
Washington, who said the temperature increases and the shifts in winds and
ocean currents that occurred early in the 1990s have since “relaxed” and
such changes in the Arctic Circle “are not related to (global) climate
change.”89 So it turned out that Senator McCain’s incontrovertible facts about
man-made catastrophic warming were refuted that very day.

Senator McCain immediately nabbed one of his staffers to ask why he

didn’t know about that article, and within moments, that staffer was rushing
out the door to go find it for his boss. In the end, even though few other
senators would come out publicly to oppose Senators McCain and

Lieberman’s bill, it was soundly defeated on the Senate floor by a vote of
55–43, with only forty-three senators supporting the measure.

Senators McCain and Lieberman, however, were determined to try again.
In 2005, they reintroduced their bill, again having to go around me because
they knew it had no shot at getting passed out of committee. When the 2005
Energy Policy Act (EPACT) was brought up for consideration, the senators
offered their bill as an amendment. McCain arranged an agreement with the
Senate Majority Leader that they would vote first on EPACT, then separately
on McCain and Lieberman’s amendment as a stand-alone bill, thus avoiding
the committee process. It was another lonely battle on the Senate floor, as I
was one of the only senators willing to openly stand in opposition. But while
my colleagues may not have expressed their dislike of the bill in words, they
certainly did so with their votes. McCain-Lieberman was dealt a crushing
defeat on June 22 by a 60–38 vote, with only thirty-eight senators supporting
the measure.

CLEAR SKIES

“The Clear Skies bill is the most aggressive presidential initiative in
history to reduce power plant pollution and provide cleaner air across
the country. The bill reduces emissions of sulfur dioxide, nitrogen oxides,
and—for the first time—mercury from power plants by 70 percent by
2018 through expanding the successful Acid Rain Trading Program. This
program, combined with the historic diesel rules being implemented by
the Bush Administration, provide a national clean air strategy that will
bring nearly all of the nation’s counties that are not meeting clean air
standards into attainment, makes the future for clean coal possible, and
keeps energy affordable, reliable and secure. It is my hope that if the
environmental community and my friends on the other side of the aisle are
truly serious about protecting the environment they will join me and
Senator Voinovich in supporting this important legislation.”

—Senate Environment and Public Works Committee Chairman
James M. Inhofe, January 25, 200590

Most people are surprised to discover that while I was working to defeat

cap and trade, I was also simultaneously working to pass the Clear Skies

Act, which would have been the most aggressive legislation in history to
improve air quality by reducing power plant emissions by 70 percent.

EPA had declared that 474 U.S. counties were in non-attainment for the
new, more stringent National Ambient Air Quality Standards (NAAQS) for
ozone, and that 225 counties did not meet new standards for particulate
matter. Non-attainment is determined by air monitoring devices placed
around cities all over the country. If any particular area violates the NAAQS
standards, then they are considered to be in non-attainment, which triggers a
number of regulatory requirements for that area until they can show that they
have attained the standards. Such designations place a significant burden on
state and municipal governments, forcing them to develop plans to reduce
emissions and reach attainment by a certain date. Typically cities in
nonattainment must reduce their emissions from motor vehicles (including
cars and buses) or by limiting the emissions from power plants and
manufacturing facilities. Failure to reach attainment can mean the rejection of
any permits for new businesses in the community and the threat of losing
federal highway dollars to improve the roads. Clear Skies, coupled with
diesel regulations that had already been finalized by EPA, would have
brought most of those counties into attainment without any additional, local
controls because it would have placed most of the burden on the electricity
sector, instead of state and municipal governments.

President Bush announced the Clear Skies Initiative in February 2002, and
I first introduced the corresponding legislation, along with Senator Voinovich
in November 2003, not long after McCain-Lieberman cap and trade
legislation was defeated the first time. Just before I introduced Clear Skies, I
met with President Bush to discuss the path forward. One thing that stands out
in my memory from that meeting is that he told me how much he envied me
for having so many kids and grandkids to go fishing with. He said that with
two daughters who didn’t like to fish, he’d have to wait until he had
grandkids. Here was a president who could truly relate as a fellow
sportsman, avid outdoorsman, and, most importantly, a family man.

After the Senate rejected cap and trade for greenhouse gases the first time,
I insisted that it was time to pass legislation that would actually provide real
public health and environmental benefits to the American people while
preserving our economy and standard of living. Yes, Clear Skies was a cap
and trade bill, but with one, crucial difference from the McCain-Lieberman
bill: it focused solely on reducing three real air pollutants: sulfur dioxide,

nitrogen oxide, and mercury (for the first time in history); it did not regulate
greenhouse gases.

Global warming advocates often accused Republicans of hypocrisy for
supporting cap and trade for real pollutants but not greenhouse gases. But
they confused the fact that trading mechanisms for real pollutants have
achieved significant environmental benefits without harming jobs and the
economy, claiming the same would happen for carbon despite every credible
economic analysis showing exactly the opposite. Placing a price on carbon
could wreak havoc on the economy. The reason trading programs work for
real pollutants is that they are mostly emitted by large, stationary, power-
generating sources, and controlled by existing technologies. Greenhouse
gases, on the other hand, are emitted everywhere and by every sector of the
economy, not just power plants, and they are not harmful to human health.
Because the foundation for Clear Skies was the successful Acid Rain

trading program advanced by President George H.W. Bush in 1990, we knew
the technology to make these reductions was viable and affordable, and that it
would not raise electricity costs for consumers. On the other hand, there was
no viable technology available for utilities to reduce carbon emissions. The
timetables of Clear Skies achieved significant reductions that were
reasonable, and the electric generating industry knew, from experience, that
they could comply. Also, unlike most of our nation’s environmental laws and
regulations under the Clean Air Act, which have resulted in endless
litigation, the Acid Rain program resulted in virtually no litigation and has
achieved goals of substantial reductions in acid rain at less than the
projected cost. Clear Skies similarly would have avoided that constant
litigation. It would have improved our air by reducing utility emissions
faster, cheaper, and more efficiently than the Clean Air Act as it still stands
today.

The legislation went nowhere during President Bush’s first term. Why

would Democrats want to hand a president they hated a key legislative
victory—on an issue they claim to own—before the 2004 elections?

In March 2005, Senator Voinovich and I brought the bill up again, but

again Democrats blocked its consideration because it still did not regulate
greenhouse gases, despite the fact that it would have made great strides in
improving air quality.

The way most Democrats tell it, the Grim Reaper waits outside your door
each and every day to blow toxins in your face. They would never admit that
at the time I re-introduced Clear Skies in the opening days of the 109th
Congress, emissions of the six main air pollutants had actually decreased 51
percent since 1970. That is a significant improvement over a thirty-five-year
period during which U.S. gross domestic product increased by 176 percent,
vehicle miles traveled increased 155 percent, energy consumption rose 45
percent, and population expanded by 39 percent. We could have built on
thirty-five years of success, but politics prevailed.

Well-financed environmental NGOs such as the National Resources

Defense Council and Sierra Club, or “Big Green” as I call them collectively,
launched an aggressive, politically coordinated campaign against Clear
Skies. The basis of their opposition was the false and utterly absurd claim
that Clear Skies was a “rollback” of existing Clean Air Act provisions. This
was simply not true. The real problem for Big Green was that the bill did not
address greenhouse gases. And my Democrat colleagues did not like that it
had been advanced by George W. Bush and me. Opponents amusingly called
Clear Skies “Orwellian.” The bill proposed the first-ever cap to reduce
mercury emissions from the power plants by 70 percent, yet, true to form,
these Big Green groups said it would allow more mercury to go into the air.
Go figure. Clear Skies opponents knew that it would be more difficult to pass
greenhouse gas regulations in addition to a three-pollutant bill. Because of
this, they held it hostage, making it very clear that politics, not the
environment, was the priority. Real and meaningful results for air quality
were shamefully sacrificed for worthless rhetoric.

In March 2005, the bill unfortunately failed by a 9–9 tie vote in

committee, thanks chiefly to Senator Lincoln Chafee, who went on to lose his
Republican-held seat in the Senate despite his calculated opposition to
appease the Big Green machine. Senator Voinovich and I made a number of
compromises to advance the legislation, and even agreed to postpone
scheduled markups three times in a good faith effort to work with opponents
to reach a consensus. In the end, after several months of markup delays,
twenty-four hearings, five years, and more than 10,000 pages of modeling
data, the Democrats succeeded in killing Clear Skies. If anyone was denied
an environmental victory, it was the American people.

3

THE BUILD-UP OF ALARMISM

CELEBRITIES AND THE “CLIMATE CRISIS”
Most people know that I’m not much of a movie-goer, and I can’t tell one
Hollywood star from another. Let’s just say that if you would have told me a
few years earlier that I would be giving PowerPoint presentations that
included quotes from the likes of Leonardo DiCaprio, Barbra Streisand, and
John Travolta, I wouldn’t have believed you. But the alarmists had taken the
fight to pop culture, so I was determined to join in the fray.

After suffering two overwhelming defeats in Congress with the McCain-
Lieberman cap and trade bill, the environmental left realized that they had to
go bigger than Capitol Hill and take their alarmist message of fear directly to
the people. Conveniently, they had Hollywood and the mainstream media on
their side. It was a marriage made in heaven—or so they thought at the time.

By 2006, the American public was inundated with an unprecedented

parade of environmental alarmism though films and celebrities, all
portending a future of natural disasters, wars, displacements, and plagues.
They were also going full force with the major news organizations, which
had completely dismissed any pretense of balance and objectivity on climate
coverage and instead focused squarely on promoting global warming
advocacy. Basically, everywhere you turned you were bombarded with the
message that the end of the world was nigh.

There I was, this not particularly glamorous Senator from Oklahoma,

standing up against these young, beautiful celebrities who had come together
to proclaim with one voice that the science was settled and we had to act
immediately to avoid climate catastrophe.

But they had a little bit of a problem: their “solutions” were absurd.

Cameron Diaz stood on a stage at one particular climate concert and
proclaimed that women could mitigate global warming by turning off the

water in the shower when they shaved their legs;91 Laurie David, an activist
with strong ties to Hollywood, said that we needed to change our standard
light bulbs to fluorescents to avert crisis;92 and of course the most famous
“solution” came from Sheryl Crow who said a “limitation should be put on
how many squares of toilet paper can be used in one sitting.”93 That is,
unless there was a serious emergency, in which case more than one square
would be acceptable. She later claimed that she was just joking, but as I said
on Fox News, “I’m just glad she didn’t define what that serious emergency
was.”

There is certainly nothing wrong with the desire to conserve energy, but if

we were indeed facing a crisis of the proportions they predicted, the
suggestion that we can save the world one square of toilet paper at a time
was just absurd. In the midst of all this hysteria, my message remained calm,
consistent and clear: the science is not settled. Their so-called “solutions”
were only symbolic and would have no actual impact on the climate.

Looking back on these years, I’m sure that many people have probably

forgotten just how prevalent global warming was in the mainstream;

it seemed as if a day didn’t go by without hearing about the impending

doom. Now it is all but forgotten, but they will be back.

HOLLY WOOD HYSTERIA
Of course, of all the efforts to push alarmism, nothing was bigger than Al
Gore’s documentary, An Inconvenient Truth, which rocketed the global
warming movement into pop culture. Suddenly the alarmists had the hero they
were craving: Gore was now the “climate prophet” or the “Goricle” as
journalist Howard Fineman once called him.94 Katie Couric famously said
that Gore was a “Secular Saint,”95 and Oprah Winfrey said that he was the
“Noah”96 of our time. This is what I’m talking about when I say that it was a
religion to them.

But if Gore was a prophet, he was certainly a prophet of doom. Even the
trailer to the film flashes ominous images of destruction accompanied with
phrases such as “Nothing is scarier than the truth;” “By far the most terrifying
film you will ever see;” it’s a “film that has shocked audiences worldwide.”
Given the twenty significant scientific errors I found in Gore’s film, it was
clear that the science was secondary to the primary goal of promoting fear

and pushing the message that the debate was over and the science was
settled.

Gore chose to ignore those inconvenient scientists such as Richard

Lindzen of MIT who put it well when he said, “A general characteristic of
Mr. Gore’s approach is to assiduously ignore the fact that the earth and its
climate are dynamic; they are always changing even without any external
forcing. To treat all change as something to fear is bad enough; to do so in
order to exploit that fear is much worse.”97

The architect behind bringing the global warming alarmism message to
Hollywood was Laurie David, an environmental activist with close ties to
Hollywood. She convinced Gore to turn the PowerPoint presentation that he
was giving across the world into a documentary and became a co-producer
of the film. She was also behind numerous one-sided fear mongering
documentaries that aired on TBS, CNN, HBO, and even Fox News. She
teamed up with her husband to produce a two-hour comedy on TBS called
“Earth to America,” which included numerous stars—most notably Will
Ferrell—in a hilarious spoof of President George Bush complaining that
liberals were trying to make him “look bad” by “using such things as facts
and scientific data.”98

Later in 2007, Leonardo DiCaprio followed suit and completely tossed
objective scientific truth out the window in a documentary scarefest called
The 11th Hour. Like Gore and David, DiCaprio refused to acknowledge any
scientists who disagreed with his dire vision of the future of the Earth. In
fact, his film featured physicist Stephen Hawking making the unchallenged
assertion that “the worst-case scenario is that Earth would become like its
sister planet, Venus, with a temperature of 250 [degrees] centigrade.”99 In
other words, worst-case scenarios pass for science in Hollywood—in fact,
they are preferred. DiCaprio was not shy about stating that this was the
purpose of the film. As he said, “I want the public to be very scared by what
they see. I want them to see a very bleak future.”100 They may have been too
scared even to attend as the film itself was apparently a miserable flop.

While the target was essentially everyone, the alarmists were particularly

focused on planting the seed of fear in the young. DiCaprio announced his
goal was to recruit young eco-activists to the cause: “We need to get kids
young,” He said in an interview with USA Weekend.101 Laurie David also
coauthored a children’s global warming book with Cambria Gordon for

Scholastic Books titled, The Down-To-Earth Guide to Global Warming.
David made it clear the purpose of her book was to influence young minds
when she wrote in an open letter to her children, “We want you to grow up to
be activists.”102

After a successful campaign to have An Inconvenient Truth shown in
classrooms around the nation, the alarmists were unfortunately having the
impact they wanted on children: Nine-year-old Alyssa Luz-Ricca was quoted
in The Washington Post on April 16, 2007, as saying: “I worry about [global
warming] because I don’t want to die.”103

I remember one morning, when I was grocery shopping back home in

Tulsa with Kay, a young mother introduced herself and told me how worried
she was that her child had been made to watch An Inconvenient Truth in
several classes at school without being told there was another side to the
story. These poor kids were being bombarded with a scientifically unfounded
doomsday message designed to create fear, nervousness, and ultimately
recruit them to liberal activism.

MEDIA MANIA: COOLING OR WARMING CATASTROPHE?
Through it all, the mainstream media had nothing but praise for these efforts.
I almost don’t blame the major news outlets for jumping on global warming
hysteria with a vengeance. Let’s be honest: catastrophe sells news. So rather
than focus on the hard science of global warming, and looking at all the
possibilities, the media instead became prime advocates for hyping
scientifically unfounded climate alarmism.

Such tactics had certainly worked to their advantage before. Take, for

example, a quote from the New York Times reporting fears of an approaching
ice age: “Geologists Think the World May be Frozen Up Again.”104 That
sentence appeared more than one hundred years ago in the February 24,
1895, edition of the New York Times. Then a front-page article in the October
7, 1912, New York Times, just a few months after the Titanic struck an
iceberg and sank, declared that a prominent professor “Warns Us of an
Encroaching Ice Age.”105 The very same day in 1912, the Los Angeles Times
ran an article warning that the “Human race will have to fight for its
existence against cold.”106 An August 10, 1923, a Washington Post article
declared: “Ice Age Coming Here.”107

By the 1930s, the media took a break from reporting on the coming ice age

and instead switched gears to promoting global warming: “America in
Longest Warm Spell Since 1776; Temperature Line Records a 25-year Rise,”
stated an article in the New York Times on March 27, 1933.108 The media of
yesteryear was also not above injecting large amounts of fear and alarmism
into their climate articles. An August 9, 1923, front-page article in the
Chicago Tribune declared: “Scientist Says Arctic Ice Will Wipe Out
Canada.”109 The article quoted a Yale University professor who predicted
that large parts of Europe and Asia would be “wiped out” and Switzerland
would be “entirely obliterated.” A December 29, 1974, New York Times
article on global cooling reported that climatologists believed “the facts of
the present climate change are such that the most optimistic experts would
assign near certainty to major crop failure in a decade.”110 The article also
warned that unless government officials reacted to the coming catastrophe,
“mass deaths by starvation and probably in anarchy and violence” would
result. In 1975, the New York Times reported that “A major cooling [was]
widely considered to be inevitable.”111 These past predictions of doom have
a familiar ring, don’t they?

During the latest global warming craze, the image that stands out the most
is the Time magazine cover picturing the last remaining polar bear standing
on the one remaining ice cube under the heading “Be Worried, Be Very
Worried.”112 I joked that Americans should be very worried—about such
shoddy journalism. After more than a century of alternating between global
cooling and warming, one would think that this media history would serve a
cautionary tale for today’s voices in the media and scientific community who
were promoting yet another round of eco-doom.

From TIME Magazine, April 3, 2006 © 2006 Time Inc., Used under license.
TIME and Time Inc. are not affiliated with, and do not endorse products or
services, of Licensee.

After I presented the media’s one-hundred-year-history of embarrassing
climate change reporting in a speech on the Senate floor in October 2006,
Newsweek magazine Senior Editor Jerry Adler issued a one-thousand-word
correction113 for its 1975 story on the dangers of global cooling that, as the
article said, “may portent a drastic decline in food production—with serious
political implications for just about every nation on Earth.”114 It took them
thirty-one years to admit that that these statements were “so spectacularly
wrong about the near-term future.”115 But Adler still wasn’t willing to put the
full blame on Newsweek for this incredible gaffe. As he said, “The story
wasn’t ‘wrong’ in the journalistic sense of ‘inaccurate.’”116 His justification

was that “Some scientists indeed thought the Earth might be cooling in the
1970s, and some laymen—even one as sophisticated and well-educated as
Isaac Asimov—saw potentially dire implications for climate and food
production.”117 Yet, he was still unwilling to admit that what the media now
says about global warming could be wrong, as it was in the 1970s. So I’m
guessing that if it takes thirty one years for Newsweek to admit its mistakes,
we can expect them to recant their latest global warming scare around
October 2037.

But Adler was right in one respect. He also said, “All in all, it’s probably

just as well that society elected not to follow one of the possible solutions
mentioned in the Newsweek article: to pour soot over the Arctic ice cap, to
help it melt.”118

Newsweek was not the only publication that responded to me calling them

out over their past climate debacles. The New York Times also weighed in
with an October editorial:

We do not expect Mr. Inhofe to see the light—or feel the heat—any time
soon. He and his staff are serious collectors of opposition research. But
the essence of his strategy is to seize upon a mistaken or overblown story
to try to undermine the broad consensus. If that fails, he can always
question his opponents’ politics and motives, as with his insinuations that
environmentalists dreamed the whole thing up to scare people and raise
money.119
In other words, even though the New York Times was completely wrong
about global cooling in the 1970s, they were outraged that I would have the
audacity to question the validity of their claim that there is a “broad
consensus” among scientists regarding global warming today.

Even my favorite global warming reporter, Miles O’Brien, wasn’t too

happy when I challenged him on his past climate cooling gaffes. Miles was
great: so many extremists were mad all the time, but Miles always smiled,
even when he was cutting my guts out, and I appreciated that. Here’s the
exchange in one of our typically lively television interviews:

INHOFE: And I wonder also, Miles, it wasn’t long ago—you’ve got to
keep everyone hysterical all the time. You were the one that said another
ice age is coming just twelve years ago.

O’BRIEN: I said that? I didn’t say that.

INHOFE: You didn’t say that. Let me quote you…

O’BRIEN: No, no, no. I’d be willing to tell you there are stories like
that. But there’s not…

INHOFE: … quote you so I’ll be accurate. I don’t want to be inaccurate.

O’BRIEN: All right, go ahead.

INHOFE: You said, in talking about a shift that was coming—you said,
“If the Gulf Stream were to shift again, the British Isles could be engulfed
in polar ice and Europe’s climate could become frigid. [From CNN
Transcript titled Scientists Research the Rapidity of the Ice Age dated
December 19, 1992.]” That’s another scary story.

O’BRIEN: But that also is a potential outgrowth of global warming
when you talk about the ocean currents being arrested. This is “The Day
After Tomorrow” scenario that we’re talking about.

So global cooling was actually global warming. They were determined to

have their catastrophe no matter what.120

MEDIA MANIA OVER GORE
Not surprisingly, Al Gore had the full backing of the media to promote his
movie, and leading the cheerleading charge was none other than the
Associated Press. On June 27, 2006, an AP article written by Seth
Borenstein boldly declared: “Scientists give two thumbs up to Gore’s
movie.”121 The article states that the top scientists were giving his movie five
stars for accuracy and that its prospects of “a flooded New York City, an
inundated Florida, more and nastier hurricanes, worsening droughts,
retreating glaciers and disappearing ice sheets” were mostly accurate, with
only some minor adjustments.

Of course, the AP chose to ignore the scores of scientists who have
harshly criticized the science in Gore’s movie. I said that the AP should
release the names of the “more than 100 top climate researchers” that they

attempted to contact, and the nineteen scientists who gave Gore “five stars
for accuracy.” Most importantly, the AP chose to ignore Gore’s reliance on
the “hockey stick” by Dr. Michael Mann, which claims that temperatures in
the Northern Hemisphere remained relatively stable over nine hundred years,
then spiked upward in the twentieth century, and that the 1990s were the
warmest decade in at least one thousand years. Only a week before the AP
article was published, the National Academy of Sciences report dispelled
Mann’s oft-cited claims by reaffirming the existence of both the Medieval
Warm Period and the Little Ice Age.122 That’s when I declared on the Senate
floor that the hockey stick was broken. Yet, this highly significant breaking
news is not even acknowledged in this article which instead features only
glowing praise from the likes of Robert Corell, chairman of the worldwide
Arctic Climate Impact Assessment group of scientists, who said he was
“amazed at how thorough and accurate” it was; he was “blown away” and
“could find no error.”123

WHAT BALANCE?
This AP article epitomizes the attitude of many journalists around that time:
balance in global warming reporting simply wasn’t valued, and many were
not at all afraid to admit that openly. The quote that perhaps encapsulates the
global warming mania in the mainstream media most comes from Bill
Blakemore with ABC News, who explained on August 30, 2006, “After
extensive searches, ABC News has found no such [scientific] debate” on
global warming.124

ABC News put forth its best effort to secure its standing as an advocate
for climate alarmism when the network put out a call for people to submit
their anecdotal global warming horror stories in June 2006 for use in a future
news segment.125 Then, in July of that year, the Discovery Channel presented
a documentary on global warming, narrated by former NBC anchor Tom
Brokaw. You don’t have to take my word on the program’s overwhelming
bias; a Bloomberg News TV review noted, “You’ll find more dissent at a
North Korean political rally than in this program” because of its lack of
scientific objectivity.126

Brokaw, who had affiliations with the Sierra Club, lavishly praised

Gore’s film as “stylish and compelling,” called the science behind
catastrophic human caused global warming “irrefutable” and specifically

presented climate alarmist James Hansen’s views as unbiased. Of course, he
failed to note Hansen’s quarter-million-dollar grant from the partisan Heinz
Foundation or his endorsement of Democrat Presidential nominee John Kerry
in 2004 and his role promoting former Vice President Gore’s Hollywood
movie.

Brokaw, however, did find time to impugn the motives of scientists

skeptical of climate alarmism when he featured paid environmental partisan
Michael Oppenheimer of the Environmental Defense Fund, accusing skeptics
of being bought out by the fossil fuel interests. Whenever the media asked me
how much I have received in campaign contributions from the fossil fuel
industry, my unapologetic answer was “not enough”—especially when you
consider the millions partisan environmental groups pour into political
campaigns.

Former Colorado state climatologist and professor emeritus of

atmospheric sciences at Colorado State University, Senior Research Scientist
in the Cooperative Institute for Research in Environmental Sciences
(CIRES), and Senior Research Associate in the Department of Atmospheric
and Oceanic Sciences (ATOC) Roger Pielke Sr., viewed an advance copy of
Brokaw’s special and declared that it contained “errors and misconceptions”
and “relied on just a few scientists with a particular personal viewpoint on
this subject which misleads the public on the broader view that is actually
held by most climate scientists.”127

In March of that year, 60 Minutes profiled NASA scientist and alarmist
James Hansen, who was once making allegations of being censored by the
Bush administration.128 Many at that time pointed out the irony of a man who
claimed to be censored, yet was appearing frequently on every major
network.

In this segment, objectivity and balance were again tossed aside in favor
of a one-sided glowing profile of their hero Hansen. It made no mention of
Hansen’s ties to quarter-of-a-million-dollar grant from the left-wing Heinz
Foundation run by Teresa Heinz Kerry. Neither did 60 Minutes inform
viewers that Hansen appeared to concede in a 2003 issue of Natural Science
that the use of “extreme scenarios” to dramatize climate change “may have
been appropriate at one time” to drive the public’s attention to the issue.129
To put the severity of this lack of balance in perspective, one of Laurie

David’s one-sided documentaries, featuring David herself as well as Robert

Kennedy Jr., was even aired on Fox News in November 2005.130 As the Fox
News promotional article said, they were “committed to teaching everyday
Americans and the rest of the world about what can be done to cut down on
greenhouse gases that threaten our children’s future.” The surprise of seeing
such a one-sided documentary on Fox was not lost on the rest of the
mainstream media. The Los Angeles Times reported, “When Dan Becker,
director of the Sierra Club’s global-warming program, got a call from the
network this summer asking him to be interviewed for the documentary, he
initially thought it was a prank call. ‘I asked whether they were joking,’ said
Becker, who participated and noted that he was ultimately impressed by the
network’s questions.”131 When I found out that Fox News was moving
forward with this unbalanced documentary, I immediately called Roger
Ailes, who is the president of the network, and he conceded that it was not a
journalistically balanced approach. He agreed then to feature another
documentary that tells the other side of the story.

Perhaps the funniest example of the lack of balance in environmental

reporting comes from a particular ad by CBS News seeking a “Hip
Environmental Reporter” with no need to have “knowledge of the enviro
beat” but must be “funny, irreverent and hip, oozing enthusiasm and creative
energy.” This employee must be “vibrant” and bring “a dash of humor to our
coverage.” I actually gave them credit for coming out and admitting that being
“hip” was the most important thing for selling what was then the “cool”
environmental agenda.132

LAST POLAR BEAR STANDING ON THE LAST CUBE OF ICE
Through it all, the last polar bear standing on the last cube of ice remained
the image seared in everyone’s mind from the media of why we should be
worried—very worried about global warming.

On August 3, 2006, the New York Times ran an op-ed by Bob Herbert

called “Hot Enough Yet?” which claimed that polar bears were “drowning
because they can’t swim far enough to make it from one ice floe to
another.”133 Then a Reuters article dated September 15, 2006, by
correspondent Alister Doyle, quoted a visitor to the Arctic who claimed that
he saw two distressed polar bears: “one of [the polar bears] looked to be
dead and the other one looked to be exhausted.”134 Importantly the article did
not state that the bears were actually dead or exhausted, rather that they

“looked” that way. At the time, I asked, have we really arrived at the point
where major news outlets in the United States are reduced to analyzing
whether or not polar bears in the Arctic appear to be tired? How does
reporting like this get approved for publication by the editors at Reuters?
Where was the rigorous scientific discussion?

What was missing from this Reuters news article was the fact that

according to biologists who actually study the animals for a living, polar
bears were doing quite well. Biologist Dr. Mitchell Taylor from the Arctic
government of Nunavut, a territory of Canada, refuted these claims in May of
that year when he noted: “Of the 13 populations of polar bears in Canada, 11
are stable or increasing in number. They are not going extinct, or even appear
to be affected at present.”135 At that stage, it was clear that it was more
interesting to hype up a polar bear catastrophe in the media than to engage in
the basic tenets of balance in journalism.

In September 2006, a report based on unproven computer models found
that polar bear populations were allegedly going to be devastated by 2050
due to global warming.136 The report was issued as part of the U.S. Fish and
Wildlife Service’s consideration of listing the polar bear under the
Endangered Species Act. It was a classic case of reality versus speculation,
when you consider that the Fish and Wildlife Service estimated that the polar
bear population was at 20,000–25,000 bears that year, whereas in the 1950s
and 1960s, estimates were as low as 5,000–10,000 bears. That same month,
Dr. Taylor, referring to the Fish and Wildlife computer modeling report
rightly said, “It is just silly to predict the demise of polar bears in 25 years
based on media-assisted hysteria.”137

The report with unproven computer models from the Fish and Wildlife

Service, of course, was one of the key factors that led to the decision to list
the polar bear as threatened under the Endangered Species Act later in May
2008, despite the fact that actual data was showing that the number of polar
bears was not in decline.138

THE DREADED DENIERS
Why would these new outlets blatantly ignore the basic tenets of journalism,
objectivity, and balance? A reporter at the time for CBS, Scott Pelley,
provided a telling answer. According to him, excluding scientists skeptical of
global warming alarmism was justified because skeptics are the equivalent

of “Holocaust deniers.”139 Similarly, Robert F. Kennedy Jr. said at one of
Gore’s LIVE Earth concerts, “This is treason. And we need to start treating
them as traitors.”140 Dave Roberts of Grist openly called for Nuremberg-
style trials for climate skeptics.141

There is one particular story, however, that stands out. Heidi Cullen who,

at the time, hosted a weekly global warming program called, The Climate
Code, wrote on her blog on the Weather Channel Web site that meteorologists
should be stripped of their scientific certification if they express skepticism
about predictions of man-made catastrophic global warming. As she said:

If a meteorologist can’t speak to the fundamental science of climate
change, then maybe the AMS shouldn’t give them a Seal of Approval.
Clearly, the AMS doesn’t agree that global warming can be blamed on
cyclical weather patterns…. It’s like allowing a meteorologist to go on-
air and say that hurricanes rotate clockwise and tsunamis are caused by
the weather. It’s not a political statement…it’s just an incorrect
statement.142
With so much wrath directed at skeptics, you can imagine the names they
called me. Let’s just say that if Al Gore was a secular saint, a Noah figure,
and a prophet, I was dubbed at worst as the devil and at best as the
“Doubting Thomas,” or as the New York Times put it, the “Doubting
Inhofe.”143 In a response to that particular New York Times editorial, Debra
Saunders of the San Francisco Chronicle wrote a very clever column called
“Inhofe the Apostate,” which made much of the global warming “religious”
rhetoric:

The Times’ focus was on Inhofe’s refusal to bow to “the consensus
among mainstream scientists and the governments of nearly every
industrialized nation concerning man-made climate change.” That is,
Inhofe has had the effrontery to challenge elite orthodoxy. Or, as the
editorial put it, Inhofe “has really buttressed himself with the will to
disbelieve.”

Get thee away, Satan.

“I see a sense of desperation that I haven’t seen before,” Inhofe told

me by phone Thursday, “and frankly I’m enjoying it.”

CNN’s Miles O’Brien also challenged Inhofe in a similar vein.
O’Brien cited the NAS study, then assailed Inhofe with quotes from
notable Republicans—President Bush, Gov. Arnold Schwarzenegger and
Rep. Chris Shays of Connecticut—who recognize global warming. Note
that Schwarzenegger gets into global-warming heaven just for believing,
despite his four Hummers and use of a private jet.144

Hollywood and the mainstream media called me every name in the book:

the guy who “thinks global warming is debunked every time he drinks a
slushy and gets a brain freeze,”145 “the noisiest climate skeptic,”146 “banged
on the head too many times,”147 the “Senate’s resident denier bunny,”148
“Traitor,”149 “Dumb,” “crazy man,” “science abuser,”150 “Holocaust denier,”
“villain of the month,” “hate-filled,” “warmonger,” “Neanderthal,” “Genghis
Khan,” and “Attila the Hun.” Later in 2010, I had the distinction of being
included in Rolling Stone’s list of the “Planet’s Worst Enemies.” My only
disappointment was that I should have been No. 1, not No. 7.151

During those years, I couldn’t help but wonder: if the advocates for global

warming alarmism were so confident they had the broad consensus and the
science was settled and the evidence was overwhelming, why were they so
determined to silence me?

A MEMORABLE INTERVIEW: MILES O’BRIEN AND THE ONE-
MAN TRUTH SQUAD
The interview with CNN’s Miles O’Brien that Debra Saunders mentioned in
her column was a particularly memorable exchange that is one of the best
examples of the alarmists’ narrative that everyone was against me and I was
the only one not buying everything they were telling me.

On September 25, 2006, I gave a speech on the Senate floor called “Hot
& Cold Media Spin Cycle: A Challenge to Journalists Who Cover Global
Warming” which exposed the glaring media bias on global warming. A few
days later, on September 28, 2006, CNN ran a segment criticizing my speech
and attempting to refute the scientific evidence I presented to counter media-
hyped climate fears.152 CNN reporter Miles O’Brien inaccurately claimed

that I was “too busy” to appear on his program. They were told simply that I
was not available on Tuesday or Wednesday and that I preferred to do the
segment live.

On January 31, 2007, Miles and I finally had the chance to argue live on

national TV. Right away Miles began building up the idea that I was
completely alone amid the overwhelming “consensus.” He began with the
science:

O’BRIEN: Let’s talk about the science first. We’ve got a big report
coming out, this United Nations report, 2,500 of the world’s leading
scientists. It’s being called a smoking gun report with a link between
humans and global warming. Let’s listen to what one of the leading
scientists has to say about it.

JAMES HANSEN, DIR., GODDARD INST. FOR SPACE STUDIES:
The human link is crystal clear. There is no question, the increase from
280 to 380 parts per million in CO2 is due to the burning of fossil fuels.
O’BRIEN: That’s James Hansen, one of the leading climate scientists.
He says it’s crystal clear. What do you say?153
I said that’s James Hansen, who is paid $250,000 by the Heinz

Foundation. I told Miles he was wrong to say that the report was going to
come out that Friday. He would not be reading the actual report but the
Summary for Policymakers—the report would not surface until weeks later.
Of course, the Summary is all the media and members of Congress were ever
going to look at. They were never going to talk about anything else—and this
Summary was written by politicians, not scientists. In fact, the IPCC’s own
guidelines explicitly state that the scientific reports have to be “change[d]” to
“ensure consistency with” the politically motivated Summary for
Policymakers: “Changes (other than grammatical or minor editorial changes)
made after acceptance by the Working Group or the Panel shall be those
necessary to ensure consistency with the Summary for Policymakers or the
Overview Chapter.”154

I wasn’t alone in expressing my concerns with the UN mandating that

scientific work be altered to fit its political agenda. As Steve McIntyre, who
had debunked the hockey stick graph, said:

So the purpose of the three-month delay between the publication of the
(IPCC) Summary for Policymakers and the release of the actual WG1
(Working Group 1) is to enable them to make any ‘necessary’ adjustments
to the technical report to match the policy summary. Unbelievable. Can
you imagine what securities commissions would say if business
promoters issued a big promotion and then the promoters made the
‘necessary’ adjustments to the qualifying reports and financial statements
so that they matched the promotion.155

Harvard University Physicist Lubos Motl also slammed the UN saying,
“These people are openly declaring that they are going to commit scientific
misconduct that will be paid for by the United Nations. If they find an error in
the summary, they won’t fix it. Instead, they will ‘adjust’ the technical report
so that it looks consistent.”156

Another aspect Miles made a point of challenging me on was my religion.

As Miles correctly noted, I take my religion seriously—I always say I’m a
Jesus guy—so why wasn’t I buying into what evangelicals such as Rev.
Richard Cizik were saying? In May 2006, Vanity Fair featured a photo of
Cizik dressed like Jesus and walking on water—another savior figure in this
“religion” of global warming. Cizik was being sponsored by many
environmentalist groups who were trying to break into the National
Association of Evangelicals. Fortunately, evangelicals had already rejected
him, and for good reason. In May 2006, in a speech to the World Bank, Cizik
said, “I’d like to take on the population issue… We need to confront
population control and we can—we’re not Roman Catholics after all—but
it’s too hot to handle now.”157 So those were his true views, and he did not
represent evangelicals.

Miles wasn’t the only one who was pushing this idea that religious

leaders were buying into global warming fears. In our joint interview with
Larry King on January 31, 2007, Senator Boxer said that in addition to the
Big Oil companies that were calling her to express their support of cap and
trade, “evangelicals are coming to me. So a consensus is building and my
dear friend Jim Inhofe is just being left all alone.”158 Later on October 13,
2009, when she was announcing hearings on her own cap and trade bill, she
said, “I can report that Evangelical groups and other religious communities
have expressed their commitment to help us move the bill quickly.”159

But this was misleading: directly countering the views of Cizik and those
in the mainstream media who were pushing the idea that evangelicals were
on board with global warming hysteria, in 2000, the Cornwall Alliance
released the Cornwall Declaration which reminds us, “We should respect
creation and be wise stewards, but we must be careful not to fall into the trap
of secular environmentalists who believe that man is an afterthought on this
earth, and is principally a polluter of it.” Later in 2009, the Cornwall
Alliance’s evangelical arm issued a statement called “An Evangelical
Declaration on Global Warming,” which stated that “recent progress in
climate research suggests that:

1. Observed warming and purported dangerous effects have been
overstated.
2. Earth’s climate is less sensitive to the addition of CO2 than the alleged
scientific consensus claims it to be, which means that climate model
predictions of future warming are exaggerated.

3. Those climate changes that have occurred are consistent with natural
cycles driven by internal changes in the climate system itself, eternal
changes in solar activity, or both.160
I do not pretend to be a biblical scholar, but I have read a lot of work by

scholars on the topic of man’s relation to creation and what stewardship
means from a biblical perspective. It seems to me that we should make use of
the resources God has given us, and remember that it is God, not God’s
creation, that should be praised, as exemplified in Romans 1:25: “They gave
up the truth about God for a lie, and they worshiped God’s creation instead of
God who will be praised forever. Amen.”161

I have always greatly admired Bill Bright, the founder of Campus Crusade

for Christ, who was a close friend of mine from 1984 until his death. He
wrote a daily devotional book called Promises, which I have read nearly
every day for twenty-five years. Many times during my global warming fight,
I turned to Day 36 of Promises which features one of my favorite Bible
verses, Genesis 8:22:

As long as the earth remains

there will be springtime and harvest,

cold and heat, winter and summer,

day and night.

The devotional associated with the verse in Promises is a wonderful story

about how it is God who “maintains the seasons”:

On his way to a country church on Sunday morning, a preacher was
overtaken by one of his deacons.

“What a bitterly cold morning,” the deacon remarked. “I am sorry the
weather is so wintry.”

Smiling the minister replied, “I was just thanking God for keeping His
Word.”

“What do you mean?” the man asked with a puzzled look on his face.

“Well,” the preacher said, “more than 3,000 years ago God promised that
cold and heat should not cease, so I am strengthened by this weather
which emphasizes the sureness of His promises.”162

And this is what a lot of alarmists forget: God is still up there, and He

promised to maintain the seasons and that cold and heat would never cease
as long as the earth remains.

Miles also asked me why I remained obstinate when corporate

heavyweights were coming to Capitol Hill to testify that they could compete
in a “greenhouse gas constrained world.” As I said to Miles, if anyone thinks
these corporations are willing to reduce their emissions out of the goodness
of their hearts, think again. These companies were climate profiteers that
would gain market share against their competitors while the economy flattens
and jobs are sent to China. At that time, ten companies, ranging from Duke to
GE, announced they were joining together to create the Climate Action
Partnership, but it was abundantly clear that they would individually profit

from this plan. As I said to Miles, coal is responsible for over 50 percent of
our electricity in America. These companies have nuclear, hydroelectric,
solar, and wind, so it was to their financial advantage to do away with coal.
The biggest losers in the plan would not be big businesses, but the American
people, who would have to foot the bill for these companies’ profits under
this cap and trade regime. If I were on the board of directors of GE, who is
making solar equipment and wind turbines, I’d say, “Sure, let’s jump on this
bandwagon. We’d make a fortune.”

At the time, Republicans had begun to feel the heat of the global warming

alarmism campaign, and Miles couldn’t wait to ask me why I remained
skeptical when my Republican counterparts such as Senator John McCain
had proclaimed, “I believe climate change is real. I believe that we need to
act as quickly as possible.” But it was the State of the Union address by
President George W. Bush that was the defining moment for the media. When
President Bush said America was addicted to oil and that global warming
was a serious problem, they saw this as a victory. According to them, Bush
had caved and I was just being stubborn among my Republican colleagues
with my insistence that the science wasn’t settled. Miles also used the Bush
EPA’s Web site as further evidence:

O’BRIEN: First two lines there are, “According to the National
Academy of Sciences, the Earth’s surface temperature has risen by about
a degree Fahrenheit in the past century, with accelerated warming during
the past two decades. There is new and stronger evidence that most of the
warming over the last fifty years is attributable to human activities.” This
is the Environmental Protection Agency from the Bush administration
saying that. I mean, there is a lot of consensus here, isn’t there?163

Later in 2008, I’ll never forget the vision of Newt Gingrich sitting on a

couch in front of the Capitol holding hands with Nancy Pelosi, saying while
he and Pelosi rarely agree, “We do agree our country must take action to
address climate change,” and “If enough of us demand action from our
leaders, we can spark the innovation that we need.” Al Gore’s Alliance for
Climate Protection sponsored the ad. I applaud Newt for saying on October
9, 2011, three years after the couch episode, “That is probably the dumbest
single thing I’ve done in recent years.”164

But I told Miles that he was wrong to say that I was alone in this

particular sense: I reminded him that the last time we had a vote on cap and
trade, led by John McCain, we won 60–38. So there were fifty-nine other
senators who agreed with me on that point, even if they weren’t willing to
say it. Then I went on to explain that even if it’s true that the planet is
warming due to anthropogenic gases, if every developed nation signed on to
the Kyoto Treaty, it would only reduce the earth’s temperature by 0.06
degrees Celsius by 2050.

His reaction was very interesting: “Well, we’re not talking about Kyoto…

We’re not talking about Kyoto. We’re just talking about whether global
warming is real.”165

That last comment from Miles pretty much encapsulates the entire fear

campaign from about 2005 to 2007. It was completely centered on drumming
up terror: what they didn’t want to acknowledge was whether it was the
Kyoto Treaty or cap and trade, whatever “solution” there was to this so-
called “problem” was about as effective as demanding that every American
use only one square of toilet paper in one sitting. Indeed, cap and trade—the
greatest tax increase in American history—was actually more terrifying than
the catastrophe they were perpetrating; and for them, that was a serious
problem.

A HEATED HEARING
In December 2006, just before I had to hand the gavel of the Environment and
Public Works Committee over to the new Chairman, Senator Boxer, we held
one of my favorite hearings on how the mainstream media was over-hyping
the coverage of global warming to scare the public. One of our guests,
Paleoclimate researcher Bob Carter of Australia’s James Cook University,
who has had over one hundred papers published in revered scientific
journals, said that “there is huge uncertainly in every aspect of climate
change.” He explained:

If you look at the ice core records, you will discover that yes, changes in
carbon dioxide are accompanied by changes in temperature, but you will
also discover that the change in temperature precedes the change in
carbon dioxide by several hundred years to a thousand or so years.
Reflect on that. And reflect on when you last heard somebody say that

they thought lung cancer caused smoking. Because that is what you are
arguing if you argue on the glacial time scale that changes in carbon
dioxide cause temperature changes. It is the other way around.166

Carter also made the important point that uncertainty is even present in the

way scientists make their claims. And that was something I always found
ironic about the debate. They were always going on about how the science is
settled and there was unequivocal certainty of catastrophe, but their rhetoric,
as Carter said, was completely couched in words like “if, could, may, might,
probably, perhaps, likely, expected, projected”:

Wonderful words. So wonderful, in fact, that environmental writers
scatter them through their articles on climate change like confetti. The
reason is that—in the absence of empirical evidence for damaging
human-caused climate change—public attention is best captured by
making assertions about ‘possible’ change. And, of course, using the
output of computer models in support, virtually any type of climatic
hazard can be asserted as a possible future change.167

David Deming, a geophysicist from the University of Oklahoma, also

echoed these concerns about the media saying:

Every natural disaster that occurs is now linked [by the media] with
global warming, no matter how tenuous or impossible the connection. As
a result, the public has become vastly misinformed on this and other
environmental issues.168

I found the entire discussion compelling but I noticed there was one

person who was less than riveted. In the back of the room, there was Miles
O’Brien with his head on the press table, fast asleep.169 Oh well, I thought,
there wasn’t much hope of him converting anyway.

HUNGRY FOR THE TRUTH
The American people had been fed an unprecedented diet of hysterical
rhetoric: they were hungry for the truth. So amid the deafening noise of
Hollywood and the media, I found two powerful forums where I could
bypass the mainstream media and reach out directly to the public: talk radio

and my Environment and Public Works Committee Web site—a Web site that
launched one the first blogs and YouTube channels in Congress.

My appreciation for talk radio goes back many years. During the height of
the depression, I had a great experience that I would not fully understand for
several decades. My father worked with, and came through the depression
with, a sports announcer from WHO Radio in Des Moines by the name of
Ronald Reagan, or “Dutch Reagan” as he was called then. He was
considered family at the time and, several years later after we moved to
Tulsa, we thought of him differently as he became an important political
figure. I was a small child at the time and my father worked most every day
of the week. We rarely went to movies, but whenever there was a Dutch
Reagan movie, we would go without fail. I recall one night driving six hours
round trip to Duncan, Oklahoma, just to see a Dutch Reagan movie.

From watching Reagan throughout his career, I learned about the power of

talk radio to reach a wide audience—I knew it would be one of the best
ways to get my message about global warming far and wide. Whether through
the Steve Malzberg show in New York City or the Steve Hennen show in
North Dakota, I have appreciated all the opportunities to subvert the
mainstream media. Now with Twitter and Facebook, it has been easy to alert
people across the country whenever I’m going to speak on a local Oklahoma
radio show.

Now there is also a large global warming community online that offers
diverse perspectives on the ongoing debate. There are several top skeptic
sites that I recommend, but to name a few I would suggest R.A. Pielke Sr.,
Colorado State Climatologist Emeritus and Senior Research Scientist CIRES
University of Colorado at Boulder (http://pielkeclimatesci.word-press.com),
his son Roger Pielke Jr., professor of environmental studies at the Center for
Science and Technology Policy Research at the University of Colorado at
Boulder (http://rogerpielkejr.blogspot.com), Judith Curry, Professor and
Chair of the School of Earth and Atmospheric Sciences at the Georgia
Institute of Technology and President (co-owner) of Climate Forecast
Applications Network (CFAN) (http://judithcurry.com), Steve McIntyre
(ClimateAudit.org), Steve Malloy (JunkScience.com), and Anthony Watts
(WattsUpWithThat.com). For one-stop shopping of the best headlines of the
day, my former committee staffer Marc Morano runs the site
www.climatedepot.com. While these sites do not necessarily represent my

views, I think they are wonderful vehicles that show the aggressive ongoing
debate about climate science.

Of course, one of the most effective vehicles for the truth was my

committee website where people could hear the viewpoints that had been
silenced so long in the mainstream. My approach to an online presence was
based largely on the principles put forward by conservative radio host Hugh
Hewitt in his book Blog published in 2005. I know it seems strange today to
think that my Web site, which featured a blog addressing global warming,
was innovative, but at that time, not many of my colleagues on Capitol Hill
were blogging, so it was actually quite a new thing. An article in the Wall
Street Journal put it well:

Pundits do it. Scientists do it. Even Donald Trump does it. So why
shouldn’t Congress blog too?

As the former Chairman of the Senate Environment and Public Works

Committee, Republican Jim Inhofe was a coruscating critic of climate
change alarmism. Now in the minority, he plans to make sure his voice is
heard over the din of the media-savvy environmental groups through a
new blog […] And their new blog is already making waves, not to
mention causing some congressional tech malfunctioning.170

The technical malfunctioning mentioned in the Wall Street Journal article
was due to one particular blog post where we called out Heidi Cullen of the
Weather Channel for saying that meteorologists who did not believe in global
warming should be stripped of their scientific certification. The Drudge
Report subsequently linked to that post, and the response was so
overwhelming that the entire Senate web system crashed, including all my
colleagues’ personal pages and committee pages.

The Hill newspaper quoted an email sent to the Senate offices by the

Sergeant at Arms that day which read, “Drudgereport.com established a link
on their Web site to a press release on a Senate committee Web site. This link
was creating 30–50,000 queries per hour to Senate.gov, which in turn was
generating a query to the Press Application for each of those hits.”171

That was not the only time Drudge linked to our blog. It also picked up my

speech on the Senate floor about the hot-and-cold media spin cycle in
September 2006, which created a firestorm of support pouring into my office.

My committee staff, at the time, was overwhelmed by the phone ringing off
the hook from people across America, thanking us for having a refreshing
voice of reason amid so much hysteria.

In thousands of emails and phone calls, Americans expressed not only
their thanks but also their frustration with the major media outlets because
they knew in their gut that what they had been hearing was false and
misleading. Here’s a brief sampling of what people were saying:

Janet of Saugus, Massachusetts: “Thank you Senator Inhofe. Finally

someone with the guts to stand up and call it what it is—a sham. I think you
have taken over Toby Keith’s place as my favorite Oklahoman!!”

Al of Clinton, Connecticut, wrote: “It’s about time someone with a loud
microphone spoke up on the global warming scam. You have courage—if
only this message could get into the schools where kids are being brow-
beaten with the fear message almost daily.”

Kevin of Jacksonville, Florida, wrote: “I’m so glad that we have leaders
like you who are willing to stand up against the onslaught of liberal media,
Hollywood and the foolish elected officials on this topic. Please keep up the
fight!”

Steven of Phoenix, Arizona, wrote: “As a scientist, I am extremely

pleased to see that there is at least one member of congress who recognizes
the global warming hysteria for what it is. I am extremely impressed by the
Senator’s summary and wish he were running for President.”

Craig of Grand Rapids, Michigan, wrote: “As a meteorologist I strongly

agree with everything you said.”

Dan of Westwood, Massachusetts, wrote: “This is the most concise, well

researched, eloquently presented argument against Global Warming I have
ever seen. Somebody in Congress has finally gotten it right!”

Adam of Salmon, Idaho, wrote: “Thank you for the brave speech made all
about the hyping about alleged global warming and its causes. It took guts.”

Once launched, the Inhofe Environment and Public Works Committee

Press Blog quickly developed into a watchdog on news media excesses; it
provided the badly needed balance that was lacking in mainstream
environmental reporting. In 2007, I was honored to receive the Gold Mouse
Award for having one of the best Web sites on Capitol Hill. The Gold Mouse
Report and Awards were part of the “Connecting to Congress” research
project, funded by a grant from the National Science Foundation. For this

project CMF, partnered with researchers from the John F. Kennedy School of
Government at Harvard University, University of California-Riverside, and
Ohio State University to study how Members of Congress can use the Internet
to improve communications with their constituents and to promote greater
participation in the legislative process.

Later, in 2009, even liberal bloggers admitted that my Web site wasn’t too

bad. As Jonathan Hiskes of Grist wrote,

Listen up, James Inhofe, because this might be the only compliment Grist
ever pays you: You’ve got a decent website. Despite your wackedout
view that climate change is a “hoax” and your opposition to a climate
bill, inhofe.senate.gov does a fair job of making your climate and energy
positions clear and accessible to the Oklahomans who voted to send you
to Washington. In fact, your website is more transparent than the sites of
many senators who completely disagree with your views on global
warming, including Democratic leaders Harry Reid (Nev.) and Richard
Durbin (Ill.), along with two of the most influential senators when it
comes to environmental policymaking—Barbara Boxer (Calif.) and Jeff
Bingaman (N.M.).172

One of the best aspects of our Web site (and one of the reasons we won

the Gold Mouse award) is that we had dueling headlines on the Environment
and Public Works homepage between the minority and the majority, which
clearly showed that there was a strong debate on global warming. Of course,
we were far more active than Senator Boxer’s staff and the impact of our
Web site would eventually become too much for her. She unfortunately
decided to eliminate the dueling headlines and told my staff we could only
post on the minority Web site, not on the homepage.

The day we shut down the Senate web system because of the Drudge link

was in January 2007. To me it was an auspicious beginning to a year that,
from the outside, looked as though it was going to be bad for skeptics.
Democrats had taken back the Senate after the 2006 elections. I had just
handed over the gavel to the new Chairman, Senator Barbara Boxer, after
losing our majority. Of course, Chairman Boxer was a strong global warming
advocate who promised in the early days of her Chairmanship that cap and
trade was going to pass.

I may have given up the gavel, but as the Drudge incidents revealed, I

knew that I had the ear of the American people, and they were hungry for the
truth. As Winston Churchill said, “Truth is incontrovertible. Panic may resent
it. Ignorance may deride it. Malice may destroy it, but there it is.”

In many ways, the crash of the Senate Web site was a metaphor for what
our message of the truth was capable of doing: exposing the global warming
cap and trade hoax for what it was, and subsequently letting it all come
crashing down.

4

SKEPTICISM REIGNS

IFAL GORE WAS RIGHT about one thing in 2007, it was his claim that global
warming had reached a tipping point—but it was tipping in my direction, not
his. In October 2007, I stood on the Senate floor and said that “future climate
historians will look back at 2007, as the year global warming fears began
crumbling”—and I was right.

HOLLYWOOD HYPOCRISY: GORE REFUSES TO TAKE THE
PLEDGE
One of the greatest ironies of the global warming movement is that Al Gore
and the Hollywood elite, who were so successful in launching the issue into
the spotlight, were the very ones who ended up sealing their own campaign’s
doom with the American people.

They were their own worst enemies. Here they were, screaming about
climate catastrophe and demanding that Americans use only one square of
toilet paper, take cold showers or two-minute showers, not eat meat, and take
public transportation, while clearly they were not willing to make these
sacrifices themselves. In those years, it was not the science that was
overwhelming but rather the hypocrisy of those who were emitting so much
more carbon than the average American with their multiple mansions and
private jets, telling everyone else that they had to cut back.

I wasn’t going to let them get away with it. I said repeatedly that Al Gore
and his Hollywood friends were happy to talk the talk but refused to walk the
walk. How hard is it for these elitists to become as frugal in their energy
consumption as the average American? Apparently it was impossible. They
could have their limos and their private jets, but everyone else had to suffer.
The American public knew they were being had.

One of the most humorous examples of this came when Al Gore and

Leonardo DiCaprio teamed up to have their message “Ride mass transit”
flashed on the TV screen during the 2007 Academy Awards. As Charles
Krauthammer succinctly put it, “This to a conclave of Hollywood plutocrats
who have not seen the inside of a subway since the moon landing and for
whom mass transit means a stretch limo seating no fewer than ten.”173
Although I have to give some credit to global warming advocate John
Travolta, who finally admitted, “I’m probably not the best candidate to ask
about global warming because I fly jets.”174

If John Travolta wasn’t the best candidate, Gore was the worst of all. He

went around saying things like: the world must embrace a “carbonneutral
lifestyle”175 and we have just “ten years to avert a major catastrophe that
could send our entire planet into a tailspin” while having a house that
consumes more electricity every month than the average American household
uses in an entire year.176 And this is not to mention the multiple trips he takes
on multiple carbon-emitting private jets. No, flying commercial would not
do.

So Al Gore, one of the biggest carbon emitters in the world, was chosen
to be the face of the campaign to eliminate carbon from our lives. This was a
recipe for failure.

At the end of An Inconvenient Truth, the last message flashed on the

screen is a challenge to America and the world: “Are you ready to change the
way you live?” So I said, yes, Al Gore, are you ready to change the way you
live? When Gore came to testify at an Environment and Public Works
Committee hearing in March 2007, I asked him if he would be willing to sign
a pledge—frankly, of his own making—that as a believer in catastrophic
man-made global warming, would he consume no more energy at his
residence than the average American household by March 21, 2008?177 I
even gave him a year to do it!

Of course, during the hearing he wouldn’t answer the question and

waffled around the issue, going on as he always does about how we need to
transition to a clean energy future. So I said I’d be anxious to hear his answer
in the questions for the record. Here’s what I received months later:

Inhofe: As a believer that human-caused global warming is a moral,
ethical, and spiritual issue affecting our survival; that home energy use is
a key component of overall energy use; that reducing my fossil fuel-based
home energy usage will lead to lower greenhouse gas emissions; and that
leaders on moral issues should lead by example; I pledge to consume no
more energy for use in my residence than the average American
household by March 21, 2008.

Given that hundreds of Americans—a great many of whom could not

afford offsets—would follow your example by significantly reducing
their home energy consumption, will you now agree to take the pledge?

Gore: No.

DESPERATION IN THE ALARMISTS CAMPS
As one can imagine, this particular hearing caused quite a stir in the media,
with every global warming reporter weighing in. The most memorable recap
was aired on Keith Olbermann’s show. He invited journalist Howard
Fineman, who at the time was with Newsweek and now is a senior editor at
the Huffington Post, to discuss what he called “the link between Inhofe and
the smear campaign against Gore,” which was, according to Olbermann,

“venomous.”178 Their evidence was that my communications director at the
time, who had once worked for Rush Limbaugh, was handing out our press
release of the day, which called out Gore for his refusal to take our energy
pledge. As Fineman said about my so-called “smear campaign”:

Well it is extensive. The guy you were talking about was buzzing around
the press table I was sitting at, handing around press releases before,
during, and after the Vice President’s appearance. I think there’s a few
things going on here. For one, Al Gore is winning this argument,
scientifically and politically.179

My staffer was not doing anything out of the ordinary: press staff always

distribute releases and opening statements to members of the press during
hearings. It is standard operating procedure. The real problem here was that I
was drawing national attention to one of the movement’s greatest weaknesses
—its hypocrisy—so they were going overboard to compensate, practically
shouting that I was losing and Gore was winning:

FINEMAN: If Inhofe is the best arguer they can come up with for their
side Al Gore is in even better shape politically than he even realizes.

OLBERMAN: Yeah we have a great chance of saving the planet then.
Mr. Inhofe is putting the fossil back in fossil fuels.180

As for Gore, Fineman called him the “‘Goricle.’ He’s the guy everyone’s
listening to. He’s got his answers down pat on all this thing. He goes around
being a sage. He doesn’t have to run for anything. He’s the unofficial
president of the environment now.”181

Talk about “global governance.” Their only problem was that it wasn’t
true: 2007 was the tipping point—the Great Goricle’s prophesies of doom
were wearing thin with the American people and they were coming over to
my side in droves.

In the summer of 2007, Gore launched his “Live Earth” Concerts, which

were held across the world—including Washington, D.C., New York,
Sydney, Shanghai, and Rio de Janeiro. These performances featured “more
than 150 of the world’s best music acts—a mix of both legendary music acts
like The Police, Genesis, Bon Jovi and Madonna with the latest headliners

like Kanye West, Kelly Clarkson, Black Eyed Peas and Jack Johnson.” The
mission, according to the Web site, was to have twenty-four hours of music
performed on seven continents in order to instill “a worldwide call to action
and the solutions necessary to answer that call. Live Earth launched a multi-
year campaign to drive individuals, corporations and governments to take
action to solve the climate crisis.”

But as MTV put it, the “results were mixed.”182 For one thing, the first

snowfall in years and extremely cold temperatures were blamed for the poor
turnout in Johannesburg. A Rasmussen poll published on July 8, 2007,
showed that “most Americans tuned out.” Only twenty-two percent of
Americans even followed the concerts somewhat or very closely while
seventy-five percent did not follow coverage of the event.183 To add insult to
injury, after all the time and money spent on their campaign, only twelve
percent of Americans said that global warming was the most important issue
to them.

Further, Rasmussen found that some of the biggest carbon emitters in the
world telling everyone else to cut back wasn’t really working out for them:

Skepticism about the participants may have been a factor in creating this
low level of interest. Most Americans (52%) believe the performers take
part in such events because it is good for their image. Only 24% say the
celebrities really believe in the cause while another 24% are not sure.
One rock star who apparently shared that view is Matt Bellamy of the
band Muse. Earlier in the week, he jokingly referred to Live Earth as
“private jets for climate change.”184

Americans were clearly on to them for their hypocrisy, which continued to
be a persistent problem for the Live Earth campaign: for instance, Madonna,
who was a key performer in these climate concerts, had just made a name for
herself as one of the world’s worst polluters. Having already offered the
pledge to Gore, I thought why not extend it to other eco-hypocritical
celebrities as well? In April 2007, in the spirit of Earth Day, I challenged
global warming activists/celebrities such as Laurie David, John Travolta,
Leonardo DiCaprio, and Madonna to do what former Vice President Al Gore
refused to do—live up to their environmental rhetoric by reducing their home

energy usage to the level of the average American household by Earth Day
2008.185

I thought it was the least they could do since they were so worried that

mankind only had ten years left to act in order to avoid a climate catastrophe,
and they had made personal energy use a cornerstone of their pleas to the
general public to save the planet. I have yet to get one of them to sign my
pledge.

THOSE PESKY BLIZZARDS: GLOBAL WARMING OR CLIMATE
CHANGE?
The alarmists also had another serious problem: the weather didn’t always
cooperate with their predictions of tropical doom—they didn’t know what to
say when it turned cold. With a dire future in store of melting ice caps,
drowning polar bears, and rising sea levels, Al Gore and the media had built
up such a strong narrative that global warming was leading to higher
temperatures, any cold spell threw them for a loop. Keith Olbermann’s clip
on Gore’s visit to Capitol Hill provides a good example of the kind of
rhetoric that was always getting them into trouble:

OLBERMAN: What’s Inhofe’s story? …Does he explain the oil
companies who are funding some of the garbage science that says oh no
everything’s great. Don’t anybody panic. That sweat on your brow in the
middle of February has nothing to do with global warming.186

But the problem was that places across America were not experiencing

sweat on their brows in the middle of February but, instead, record cold
temperatures—and that was kind of inconvenient for them. At the March
2007 hearing, I challenged Gore on that point. He had mentioned that the fires
that were occurring in Oklahoma that year were due to global warming. So I
asked him, “How come you guys never seem to notice it when it gets
cold?”187 I held up a document from the National Oceanic and Atmospheric
Administration that showed that there were 183 record cold temperatures in
January 2007—183 of them! As for Oklahoma, we had three days that year
that were the coldest days in history. So I asked: where is global warming
when you really need it?

That was a question that continued to haunt the movement. Their heated

rhetoric failed every time the snow fell: they needed to modify it to
accommodate those inconvenient blizzards and record cold temperatures. Al
Gore later went on to claim that heavy snowfalls are completely consistent
with man-made catastrophic global warming, saying in a February 27, 2010
New York Times op-ed, “Just as it’s important not to miss the forest for the
trees, neither should we miss the climate for the snowstorm.” The American
people were suspicious to say the least, especially when they noticed a
deliberate shift in terms over this period from catastrophic “global warming”
to “climate change.”188

AL GORE’S NEW HOME
“Snowmageddon”—the intense blizzard that shut down Washington, D.C., for
weeks in February 2010 was what really spurred a crisis of messaging in the
global warming camps.

Because of the blizzard, the airport was closed, all the federal buildings
were closed, and very few people were even leaving their houses. Just days
before the storm struck, my daughter Molly and her family arrived in
Washington for an annual event that I sponsor called the African Dinner. This
year was special because my beautiful granddaughter Zegita Marie was
going to speak at the dinner. You see, we found Zegita in Ethiopia as a baby
—she had been left an orphan. Molly and her husband already had three boys
and always wanted a girl. They instantly loved Zegita Marie, as we all did,
and decided to adopt her.

My daughter Molly’s family at the igloo

God may have been having a little fun with the alarmists that week too: the

scheduled Environment and Public Works Committee hearing on “Global
Warming Impacts, Including Public Health, in the United States” had to be
postponed because the blizzard had essentially shut the government down. Of
course, the logic of the alarmists was that the global warming hearing was
cancelled due to a blizzard that was caused by global warming. Senator John
Kerry reminded us that we were still facing the grave threat of global
warming and that the “solution” was still on the horizon: as he said in an
article for The Hill, “those who think climate change legislation is dead for
the year are ‘dead wrong.’ Those who think blizzards and record snow falls
in Washington will make it tough to move a global warming bill are guilty of
‘inside the beltway’ thinking.”189

Molly and her husband and kids decided to make the most of the snow and
built an elaborate igloo right outside the Library of Congress—you could put
four people in there. Then they put on the finishing touch: a sign that read,
“Al Gore’s New Home—Honk if you love global warming.” So as you can
see, skepticism runs in our family.

They sent me a picture and I thought it was so clever I put it up on my

Facebook page right away. The media blizzard that ensued was really
remarkable. But of all the press, Keith Olbermann, who had just a few years
earlier said we’d be wiping sweat off our brow in the middle of February,
was the one who took it the hardest. He featured Molly’s family on his show
that night as “The Worst Family in America.” I guess some people just can’t
take a joke.

I’m a public figure, an elected official, and being a target is part of the
job. It’s quite another thing for my family to be subjected to ridicule and
vicious name-calling. But I’ll let my daughter Molly speak to that:

How An Afternoon of Family Fun Ended Up in Global Warming
Headlines
—By Molly Inhofe Rapert, daughter of U.S. Senator James M. Inhofe

It’s hard to imagine that choosing to spend a family afternoon in the
outdoors together could result in a polarizing political event, but that’s
exactly what happened in February of 2010. It all began with a special
trip to Washington, D.C. to watch our youngest child, Marie, speak at an
African event associated with the 2010 National Prayer Breakfast. Our
family of six lives in Fayetteville, Arkansas and includes my husband,
Jimmy, and our children, Jase, Luke, Jonah, and Zegita Marie. At the time
of this event, the kids were 14, 12, 11, and 9. Marie was born in Addis
Ababa, Ethiopia, in 2001 and beautifully spoke of her adoption journey,
and love for Africa, on Thursday evening, February 6.

The weather took a turn for the worse that evening and, by Friday

morning, had developed into a record-breaking snowstorm. My husband,
Jimmy, always finds great things for our family to do outdoors—hiking,
camping, and more. So he saw this as the perfect opportunity to spend an
afternoon outside, building an igloo near the Library of Congress. Five
hours later, an impressive six-foot igloo had emerged with Capitol Hill
as the backdrop.

Our oldest son, Jase, added a touch of humor with a sign reading “Al

Gore’s New Home” with the flip side: “Honk if You Love Global
Warming.” Jase had completed two science fair projects that examined

the media perceptions of global warming/cooling and the projects just
sparked a natural connection to the irony of being in Washington, D.C. in
a historic storm, steps away from where Vice-President Gore had
worked on his book, An Inconvenient Truth.

Jase’s sign went up on top of the igloo, car horns honked, and folks

stopped to have their photo taken. It all seemed harmless enough—
actually, not just harmless…I’ll admit that, as parents, we were proud
that we had managed to get our kids out of the house, spending 5 hours
playing outside together on a very cold day.

Washington, D.C. was virtually shut down, the Smithsonians were

closed, we were simply making the best out of the situation at hand. We
enjoyed visiting with people as they stopped to look at the igloo, and
everyone that stopped was light-hearted and happy, whether they
believed in man-made global warming or not. It was a great day.

We call my dad PopI (short for Pop-Inhofe). PopI loaded the photo of
the igloo on his Facebook page, where it was picked up by Roll Call then
Heard on the Hill. From there, the photo snowballed with the same
ferocity of the snowstorm. The initial coverage adopted the same tone as
the kind-hearted people that had stopped to talk to us on the street—it
was a funny quip and everyone understood that it was simply a family
outing on a snowy day, with no intense political agenda intended. We
headed back to Fayetteville, with great memories of our snowy trip to
Washington, D.C. Then Keith Olbermann entered our lives.

A common end-of-show segment on the Olbermann show is to identify
the worst person in the world. That Sunday evening, Mr. Olbermann, in a
bizarre twist, far removed from what anyone could deem true journalism,
decided to name our family one of his worst families in the world for
making a joke about climate change “in a storm that killed people.”
Actually no one was killed by the storm in Washington, D.C., but that’s
only one of the many journalistic flaws he commits on a regular basis. He
didn’t even bother to pronounce our names correctly.

We did not see his show when it aired. Our wonderful pediatrician, a
staunch conservative who watches Olbermann to “see what the enemy is

saying,” texted us at midnight to say she had never been so appalled and
that we had to watch it. After viewing it together, Jimmy and I knew we
needed to show it to the kids before they went to school, in case any of
their friends had seen it.

We gathered around our kitchen table. The look on the younger kids’
faces was hard to watch. How confusing this must have been to them—
we had worked hard to teach them to respect authority and to encourage
them to watch the news, learn about the world around them. For them to
see an adult speaking about their family in such an aggressive, abrupt,
highly inappropriate way was insulting. Their faces revealed their
confusion.

You can imagine that, from their point of view, they had spent a day in
the snow building an igloo—no political agenda, no political discussion.
Just a lot of good, old-fashioned work making lots and lots of blocks of
snow. They couldn’t quite equate their igloo experience with the man on
the computer screen ranting and raving about them by name. To say they
were “taken aback” perhaps describes it best. We tried our best to use it
as an opportunity to explain the importance of always conveying the truth
of a situation, illustrating how Mr. Olbermann had used the situation out
of context to further his own agenda. We explained that they had done
nothing wrong; it was a lighthearted day that someone else was
attempting to use to his advantage. Our oldest son, Jase, lightened up the
situation when he said “Worst family in the world? From Olbermann? I
wear that as a badge of honor.”

Olbermann’s show sparked remarkable controversy in the media. We
were shocked with the intensity of the comments on the Internet as well
as the language that was used. Some bloggers harshly criticized us,
saying that we asked for this by mocking the vice-president. I would
argue two things. First, many people make light-hearted jokes about
global warming whenever the weather turns cold…we simply put it on
paper. We didn’t shout it from the mountaintops, we didn’t set out with a
political agenda, we just made an igloo for fun and then put the sign up as
a minor afterthought at the end of an enjoyable day. Second, I don’t know
of any reasonable human being who could possibly equate a sign made by

a fourteen-year-old with the vitriol of Keith Olbermann—an adult who
chose to identify a family on national television in a diatribe. How
Olbermann and his staff could look at a photo of four young children
having fun outdoors and, with malice, go after them in an attempt to
elevate themselves—it is hard for me to fathom.

Others were not amused with Olbermann’s attack. Rush Limbaugh and
Sean Hannity both aired segments commenting on the igloo situation and
commending our family for building it. Blogger Amy Ridenour levied a
harsh criticism at Olbermann’s sponsor, stating “GE should be ashamed
of itself for allowing its personnel to attack children on air. These kids
probably are sophisticated enough to realize Olbermann’s just doing it
for attention, but it’s still pathetic to see a giant corporation going after
kids.” Individual bloggers rallied to our defense, hammering both
Olbermann and the bloggers who had joined his side. But the energy
gathered on both ends of the spectrum. One of my personal favorites
came from blogger, They Gave Us a Republic, who stated on his website
that I suffered from “congenital, multigenerational stupidity.” The
Progressive Electorate blog claimed that PopI had “used” his
grandchildren to make a political point, making us build an igloo which
mocked a snowstorm that killed people. Fortunately, this level of
stupidity didn’t need to be defended as most Americans recognize that
you don’t need to force kids to play in the snow—although perhaps that
blogger had a childhood where he preferred to sit in front of a computer
rather than play outside? Oh yes, and Grant Lawrence’s blogsite called
our children “mentally handicapped” spawns who are academically
inferior to a damp sponge. The poor fellow would be so disappointed to
learn that our kids are actually all honor roll students. The wonderful
blog, Maggie’s Notebook, jumped to our defense, pointing out the
absurdity of these and other statements.

By the end of the week, the igloo story had been picked up by a

variety of news sources including: ABC Nightly News, CNN, Fox News,
Good Morning America, Neil Cavuto Business, The Washington Post,
New York Times, CBN, CBS, CNS, NBC, Newsmax, Rush Limbaugh,
Sean Hannity, and Wall Street Journal.

I’ll admit that as a marketing professor, if I ever hoped to be quoted in
the Wall Street Journal, I never expected it to be for an igloo! Here is a
sampling of the headlines from the various blogs and internet sources,
illustrating the extreme takes on the story:

• Inhofe’s grandchildren build igloo to mock killer snowstorm190

• Jim Inhofe, America’s Worse Senator, also has America’s Worse

Grandchildren191

• Olbermann names Inhofe family worst person in the world

• GOP Sen Inhofe’s global warming denial a family affair

• Global warming denier Sen. James Inhofe’s family builds a very

special igloo for Al Gore

• Inhofe’s Al Gore Igloo

• Inhofe family talks about igloo, national media coverage

• Inhofe family mocks Al Gore’s global warming

• Al Gore’s new house on Capitol Hill

• Global warming snow job

• Family’s igloo draws debate over warming

• Olbermann Inhofe family worst family in world: igloo frosts critics

• 14-year-old Jase Rapert tells off Olbermann

• Inhofe igloo frosts critics

• DC igloo takes aim at global warming

• UA professor’s ice storm story continues to snowball

• Al Gore’s new home don’t you dare

• Family in hot seat for global warming joke

As Tina Korbe stated in an article written for the University of

Arkansas Traveler, “It’s difficult to say which was bigger: the historic
ice storm that prompted UA marketing professor Molly Rapert and her
family to build an igloo on Capitol Hill—or the media blizzard the igloo
inspired.”192 This was an interesting lesson for our family, even more
relevant to society today as we watch how quickly internet sources and
pundits will jump on a story and stretch it beyond the limits of the
original situation. On one hand, it is inspiring that so many people care
about issues and want to be involved. Whether they ardently believe that
government intervention is needed to help protect the earth—or whether
they believe the earth is naturally moving through warming and cooling
cycles and that it is foolhardy to pursue government-based initiatives. On
the other hand, it is disheartening to witness the willingness of people to
publicly express such extreme, intense opinions that are not fact-based.
We were grateful for bloggers, such as Amy Ridenour and Maggie’s
Notebook, who were willing to use their blog forums to counter
comments that stepped beyond the bounds of decency.

An enjoyable family afternoon together; a five-hour-in-the-making six-
foot igloo; and a funny quip that captured the sentiments of many make up
the great igloo saga of 2010. If that makes us the worst family in the
world, I’ll join Jase and wear that badge with honor.

Olbermann, by the way, left the network not long after that.
My daughter Molly is a brilliant marketing professor with a PhD from the

University of Memphis. She received the 2010 Hormel Master Marketing
Teacher Award and was an Inaugural Recipient of the 2011 University of
Arkansas Honors College Distinguished Faculty Award. She is also the
recipient of the Beta Gamma Sigma Outstanding Teaching Award (2007), the
Sam M. Walton College of Business Excellence in Service Award (2006 and
1993), the Arkansas Alumni Association Distinguished Faculty Achievement
in Teaching Award (2002), the Walton College of Business Excellence in

Teaching Award (2001 and 1998), and the Walton College Excellence in
Advising Award (1996).

To my great surprise, one member of the liberal establishment media,
Dana Milbank of The Washington Post—one who had his own fun at my
expense several times—understood the significance of the igloo for his
side’s argument. As he wrote:

Still, there’s some rough justice in the conservatives’ cheap shots. In
Washington’s blizzards, the greens were hoist by their own petard. For
years, climate-change activists have argued by anecdote to make their
case. Gore, in his famous slide shows, ties human-caused global
warming to increasing hurricanes, tornadoes, floods, drought and the
spread of mosquitoes, pine beetles and disease. It’s not that Gore is
wrong about these things. The problem is that his storm stories have
conditioned people to expect an endless worldwide heat wave, when in
fact the changes so far are subtle. Other environmentalists have
undermined the cause with claims bordering on the outlandish; they’ve
blamed global warming for shrinking sheep in Scotland, more shark and
cougar attacks, genetic changes in squirrels, an increase in kidney stones
and even the crash of Air France Flight 447. When climate activists make
the dubious claim, as a Canadian environmental group did, that global
warming is to blame for the lack of snow at the Winter Olympics in
Vancouver, then they invite similarly specious conclusions about
Washington’s snow—such as the Virginia GOP ad urging people to call
two Democratic congressmen “and tell them how much global warming
you get this weekend.” Argument-by-anecdote isn’t working. Consider
the words of Sen. Jeff Bingaman (D-N.M.), chairman of the energy
committee, who told The Hill newspaper last week that the snow “makes
it more challenging” to make the case about global warming’s danger to
people who aren’t “taking time to review the scientific arguments.”193

So he was saying that my family had a point. Alarmists were always going
on about how the overheating planet is causing every weather disaster known
to mankind, but there was always a crisis of messaging when it came to
extremely cold temperatures. Milbank concluded his column writing, “If the
Washington snows persuade the greens to put away the slides of polar bears

and pine beetles and to keep the focus on national security and jobs, it will
have been worth the shoveling.”194

ABC NEWS: EGG ON THEIR FACE
Not everyone in the media received Milbank’s memo about not using weather
to justify global warming policy.

It was a very hot summer day in July 2010 when Jon Karl with ABC News
approached my office to ask if I would give an interview on the prospects of
global warming policy—outside in the heat. Their intentions were obvious,
so my staff asked me if I wanted to do it. Of course I did—I enjoy ambushes
—so we headed outside, ready to turn the tables on them.195

Sure enough, instead of the usual one cameraman and one reporter, there

was a large production crew set up, and a crowd had gathered outside to
watch, so I knew they were planning to do something big. Karl began with
questions on the prospects of cap and trade but then shifted quickly to what
he really wanted to ask me about: the weather. It was so predictable. “How’s
that igloo doing now?” he asked. Here we go again.

Then Karl had brought out his incontrovertible proof that global warming

was happening: a pan with an egg in it. They thought it was so hot outside
that the egg would fry—only it didn’t. After the botched ambush, the crew
tossed the egg out on to the grass. When my communications director started
to take pictures of the unfried proof of global warming, the ABC news crew
came sprinting back to clear the evidence. The crowd that had gathered was
given quite a show, but not the one they expected.

THE “CONSENSUS” BEGINS TO COLLAPSE
In March 2007, the IPCC fourth assessment boldly declared that man’s
contribution to global warming was “unequivocal,” Hollywood and the
mainstream media had been peddling the scientific “consensus” with a
vengeance, and yet, a Los Angeles Times/ Bloomberg poll in August 2006
found that most Americans did not attribute the cause of any recent severe
weather events to global warming, and the portion of Americans who
believed that climate change is due to natural variability has increased over
50 percent in the last five years. After all that money and effort, this must
have been quite a blow.196

If Hollywood hypocrisy had cooled the public’s reception to the

alarmists, their excessive hysteria, which inflated the issue beyond their
wildest dreams, was exactly what propelled so many scientists, even those
from the left wing, to come forward in dissent. Scientists began to feel that
their work was being pushed to the side while activists made the most
outlandish claims; many became increasingly uncomfortable with the
alarmists’ aggressive agenda. As a result, several key scientists made the
“conversion” from a believer to a skeptic.

The biggest shock to the global warming camp was probably the

conversion of Dr. Claude Allegre—a renowned French geophysicist, a
former French Socialist Party leader, a member of both the French and U.S.
Academies of Science, and one of the first scientists to sound the global
warming alarm—who changed around 2006 from being a believer to a
skeptic. This was a guy who marched up and down the street twenty years
ago saying man-made gases are going to bring the world to an end. Now he
was saying that the cause of climate change is unknown and even accused the
climate alarmists of being motivated by money. In a September 2006 article,
he said, “The ecology of helpless protesting has become a very lucrative
business for some people!”197 I thought that it was so ironic that a free market
conservative capitalist in the U.S. Senate and a French Socialist scientist
were both saying that sound science is not what is driving this debate, but
greed by those who would use this issue to line their own pockets.

Allegre was not the only prominent scientist to convert. Astrophysicist Dr.
Nir Shaviv, a sharp young astrophysicist from Israel, also recanted his belief
that man-made emissions were driving climate change, and said that solar
activity can explain a large part of warming in the twentieth century. As he
said in a February 2, 2007, Canadian National Post article:

Like many others, I was personally sure that CO2 is the bad culprit in the
story of global warming. But after carefully digging into the evidence, I
realized that things are far more complicated than the story sold to us by
many climate scientists or the stories regurgitated by the media. In fact,
there is much more than meets the eye.198

Botanist Dr. David Bellamy, a famed UK environmental campaigner,

former lecturer at Durham University, and host of a popular UK TV series on
wildlife, also converted into a skeptic after reviewing the science. Bellamy

said that “global warming is largely a natural phenomenon” and said that
catastrophic fears were “poppycock.” “The world is wasting stupendous
amounts of money on trying to fix something that can’t be fixed,” and
“climate-change people have no proof for their claims. They have computer
models which do not prove anything.”199 Bellamy paid a steep price for his
conversion: he was derided by many environmental groups, and they severed
their ties with him despite his long record of environmental activism, which
included being arrested while trying to prevent loggers from cutting down a
rainforest in Tasmania, saving peat bogs, and other endangered habitats.
Finally, meteorologist Dr. Reid Bryson, the founding chairman of the

Department of Meteorology at the University of Wisconsin (now the
Department of Oceanic and Atmospheric Sciences), who was a key figure
promoting the ice age scare of the 1970s, also converted into a leading
global warming skeptic. In a May 2007 issue of Wisconsin Energy
Cooperative News, he said,

Before there were enough people to make any difference at all, two
million years ago, nobody was changing the climate, yet the climate was
changing, okay? All this argument is the temperature going up or not, it’s
absurd. Of course it’s going up. It has gone up since the early 1800s,
before the Industrial Revolution, because we’re coming out of the Little
Ice Age, not because we’re putting more carbon dioxide into the air.200

Another remarkable thing happened just a year before. On April 11, 2006,

sixty prominent scientists, many of whom advised the Canadian Prime
Minister in the 1990s to ratify Kyoto, wrote in an open letter to Prime
Minister Stephen Harper saying,

Significant [scientific] advances have been made since the [Kyoto]
protocol was created, many of which are taking us away from a concern
about increasing greenhouse gases. If, back in the mid-1990s, we knew
what we know today about climate, Kyoto would almost certainly not
exist, because we would have concluded it was not necessary.201

Also important was that the Czech President Vaclav Klaus said on

February 8, 2007, that fears of catastrophic man-made global warming were
a “myth” and critiqued the UN IPCC process, calling it a “political body.”

He also remarkably said that other government leaders would also speak out
if it were not for the fact that “political correctness strangles their voice.”202

While major media news outlets continued to ignore the prominent
scientists and leaders such as Allegre, Shaviv, Bellamy, Bryson, Czech
President Klaus, and the sixty Canadian scientists who were becoming
increasingly skeptical, I noticed that smaller news outlets across the country
were actually doing their job and providing balanced coverage of global
warming, and in the process, they were uncovering hundreds of skeptical
scientists.

Award-winning Chief Meteorologist James Spann of an Alabama ABC
TV affiliate, who holds the highest level of certification from the American
Meteorological Society, wrote in a January 18, 2007, blog post: “I do not
know of a single TV meteorologist who buys into the man-made global
warming hype. I know there must be a few out there, but I can’t find them.”203
This fascinated me, so I asked my staff to begin watching for and compiling
news clips that mention scientists who disputed man-made global warming
catastrophe. James Spann has the distinction of being the first skeptic
scientist to be included in a list that grew to over 450 in 2007; it reached 750
by 2009, and today my former communications director, Marc Morano, keeps
the list up to date. This list now stands at well over 1,000 scientists.204

“ELECTIONS HAVE CONSEQUENCES”: MY FRIEND, BARBARA
BOXER
Most people are surprised when I say this, but I really like Barbara Boxer.
She’s always wrong on the environment, and our debates have been very
fierce at times, but we’ve been great friends for many years, and we have a
lot of lively memories as we’ve alternated the leadership of the Environment
and Public Works Committee. I handed the Chairmanship over to her in
January 2007, and, as she put it at that particularly fiery hearing with Al
Gore, waving the gavel in front of my face, “You don’t do this anymore.
Elections have consequences.” It was all in good fun, but I joked that she’s
just set herself up for a repeat of those words when things change back again.

So she was making the rules in 2007 and the structure of the committee

was very different than it was when I held the gavel. For one thing, instead of
examining the economic implications of the “solutions” to global warming,
as I was determined to do as Chairman, we had twenty hearings focusing on

the dangers of global warming, including “Global Warming and Wildfire,”
“The Examination of the Views of Religious Organizations Regarding Global
Warming,” and “Global Warming Impacts on the Chesapeake Bay”—my
favorite being the affect of global warming on the tourism industry.

The irony seemed to be lost on my colleagues that we were having

endless discussions about how urgent global warming was and how we must
act now or the world will end; yet they were in no hurry to discuss the
“solution” of cap and trade. It wasn’t until October 2007 that Senator Boxer
finally held a hearing on the newest cap and trade bill introduced by Senators
Joseph Lieberman and John Warner.

In truth, I wasn’t at all surprised that there was very little effort to

examine cap and trade seriously. No one on their side wanted to discuss the
economic pain that it would cause. It was legislation that would, as the
sponsor of the bill admitted, cost “hundreds of billions of dollars,” and, as
The Washington Post put it, “require a wholesale transformation of the
nation’s economy and society.”205 I could see why my good friend and
climate foe didn’t want to broadcast that this bill would be largest tax
increase in American history.

Barbara and I had a big interview with Larry King on January 31, 2007,

just a few weeks after she had taken up the gavel. I was waiting in the
greenroom where they mic you up a few minutes before you go on air, when
Barbara came out of the make-up room ready to battle it out with me on cap
and trade. She had an entourage of staff with her so I turned to them: “Don’t
worry, it won’t get too hostile. You may not believe it but Barbara and I
actually like each other,” I said, patting her on the head. But I apparently
destroyed her hairdo and her staffers were horrified; she immediately ran
back into the make-up room with her staffers close at her heels to get it fixed.
Barbara and I have fought each other on cap and trade and global warming

for many years, but we’ve had some good laughs too. At one memorable
hearing, I gave her a gift: it was a coffee mug with a picture of the world’s
coastline being inundated with water from global warming when one pours
liquid in the cup. But she had a good sense of humor about it: as seriously as
she takes the issue, she couldn’t help but laugh.

Larry King put it well when he said at the close of our interview with him,
“Senator Barbara Boxer, Democrat of California. James Inhofe, Republican
of Oklahoma. And, by the way, they are good friends.”206

CAP AND TRADE AGAIN: LIEBERMAN-WARNER DIES
In October 2007, Al Gore and the IPCC together won the Nobel Peace Prize
for their work to bring widespread attention to the emergency of global
warming. Yet even so, and despite the efforts of the Hollywood elite and the
mainstream media, Senator Boxer still couldn’t sell the Lieberman-Warner
cap and trade bill in the Senate.

Here’s how it all shook down: the United Nations Climate Change

Conference in Bali was just around the corner, so Barbara was in a rush to
get Lieberman-Warner passed out of Committee so she could meet world
leaders with a climate “victory” under her belt. Along with Senators
Voinovich and Barrasso, I requested a full economic analysis of the bill by
the Environmental Protection Agency and the Energy Information
Administration before proceeding to a committee vote. We maintained that it
was irresponsible to move forward without knowing the full extent of the
economic damage resulting from this bill. There were also many attempts by
Republicans to offer amendments that would mitigate its negative economic
consequences—but these were completely rejected and only Democrat
amendments were added.

The business meeting to pass the bill was in December 2007. It was the

first time a cap and trade bill ever made it through the Environment and
Public Works Committee and all Republicans, except for Senator Warner,
voted against it. We had been ignored; in her rush to get it out the door,
Barbara opted to wait to address the more significant obstacles, including the
economic damage of the bill, until it reached the Senate floor.

The “obstacles” were indeed significant. At the time, Duke Energy Corp.
Chairman Jim Rogers warned that the bill will cause a “customer revolt” due
to a rise in electricity bills by as much as 53 percent in 2012.207
Additionally, the widely respected nonpartisan Charles River Associates
issued a November 8 analysis of the bill, revealing it would result in $4
trillion to $6 trillion in welfare costs over forty years and up to $1 trillion
per year by 2050.208 Even the co-author of the bill, Senator Lieberman,
conceded on November 1 that his bill would cost “hundreds of billions of
dollars.” The American Council for Capital Formation’s analysis on
November 8 found the bill would lead “to higher energy prices, lost jobs and
reduced [gross domestic product].”209 Two unlikely groups agreed on this
one point as well: the AFL-CIO was worried, saying that “the bill would

cost jobs by giving a competitive advantage to foreign companies that aren’t
subject to similar restrictions,”210 and the U.S. Chamber of Commerce said
that it “does not adequately preserve American jobs and the domestic
economy.”211

Not only would Lieberman-Warner have been a disaster for the economy,
it also would have brought our country to an abrupt halt. That’s because the
bill did not include nuclear, and without it, we would have been severely
lacking in baseload power, as fossil fuels were gradually phased out. The
lack of nuclear may have been a blessing in disguise for my good friend John
McCain, as it gave him an out not to support a bill that was very similar to
the one he introduced a few years before. He had already launched his
campaign for president and, understandably, he did not want to vote for the
largest tax increase in American history.

Senator Reid brought Lieberman-Warner to the Senate floor in June 2008,

but the results were already in before the vote was even taken. It was
obvious that the Democrats were not at all serious about the bill. To put it in
perspective, the 1990 Clean Air Act amendments were considered on the
Senate floor for five weeks, due to the substantial impact they would have.
Lieberman-Warner was even more consequential economically, yet
Democrats had no interest even in considering our amendments that would
protect American families and workers from the devastating effects of this
bill. On June 3, Roll Call quoted frustrated Democrat staffers saying, “We
have no strategy, no message, and no plan” and, “Boxer is walking us off a
cliff.”212

During the floor debate all the Democrats, true to form, were talking about

nothing but the science and how global warming was real. Senator Boxer
was no exception—in fact, she said very little about the bill itself; it was all
about how we must act now because the science was settled. So I went down
to the Senate floor and conceded the science. I said, “Let’s say we are
headed toward unspeakable catastrophe, would cap and trade save us?”
Absolutely not. That’s because only America would be taking action: China,
Mexico, and India have no intention of inflicting this kind of economic harm
on themselves so as jobs move to those countries where they don’t have any
emissions limits, global temperatures would actually increase. It would be
all economic pain—significantly higher energy costs, hundreds of thousands
of lost jobs, and a depressed economy—for nothing.

This strategy really threw Senators Boxer, Kerry, and others for a loop as

they came to the Senate floor fully prepared to have a science debate with
their doom and gloom pictures of polar bears, melting glaciers and
hurricanes. Instead, what they got was a debate on economics and costs,
which until then had taken a back seat to the dramatic images.

During that week in June when the Lieberman-Warner bill was debated on

the Senate floor, I carried around a memo with four themes based solely on
economics. They were taxes, jobs, gas prices, and nuclear power. The
themes quickly got the attention of the American people and my fellow
Republican colleagues, setting the stage for the economic debate on the
Waxman-Markey bill later in 2009 and 2010.

The news hit them hard: “Government studies confirm this bill will only

raise gas prices”;213 “$6.7 trillion in the form of higher gasoline and
electricity bills”;214 “1.8 million jobs lost by 2020 and 4 million by 2030
according to the National Association of Manufacturers.”215 The economic
impacts of the bill drove the debate and we continued to build on these
themes as the week went on.

I argued how many American jobs would be lost to developing countries
such as China, India, and Brazil that have been allowed to emit greenhouse
gasses without international criticism. For instance, China had built 117
government-approved coal-fired power plants in 2005— a rate of roughly
one every three days, according to official figures. Without international
participation, I argued, which this bill failed to address adequately, global
concentrations of greenhouse gases will continue to increase, even if
America were to nearly eliminate its emissions. This news was indeed
alarming to my opponents, and it continued to throw them off as they hastily
tried to respond to the economic findings with continued emphasis on science
and doom and gloom scenarios.

It wasn’t just industry making these assumptions. The independent Energy
Information Administration said the bill would result in a 9.5 percent drop in
manufacturing output and higher energy costs, and that it would be worse
unless we could build 268 new nuclear plants by 2030.216 This country had
already lost 3 million manufacturing jobs since 2000. The EPA itself even
estimated that the Lieberman-Warner bill would increase fuel costs an
additional 53 cents per gallon by 2030 and $1.40 by 2050.217

Sponsors of the bill also failed to tell the American people that it would

generate over $6.7 trillion in revenues through 2050 from the sale and
auction of carbon allowance to energy users.218 Unfortunately, that $6.7
trillion cost would be passed on to families and workers across the country
in the form of higher gas prices, higher electricity bills, more expensive
consumer goods, and higher workplace costs.219 In fact, new funding for
government programs, minus any set asides for transition assistance or tax
relief to states, industry, or consumers under the Act is a staggering $4.2
trillion. I made sure to shine light on these numbers, emphasizing what it
would mean for American families. For example, according to various
economic models, this bill would cost $3,298 to Oklahoma families in 2020
and cut over 51,000 jobs through the life of the bill.220

We continued to hammer these themes during that week, but even though

my staff suggested that I stick to the economics during the debate afterwards I
still had a few things to say about the science so unbeknownst to them, I went
back down to the floor to give them an extra dose of reality on that point, too.
Senator Boxer maintained Democrats “had 54 Senators come down on the

side of tackling this crucial issue now” following the cloture vote of 48–36
which effectively killed the bill. But she did not take into account a letter that
was signed on June 6 by ten Democratic Senators who explicitly stated that
“they cannot support final passage” of the “Climate Tax Bill.” The letter
showed that Boxer would have only had at most 39 votes to support final
passage of the bill. As the letter stated,

As Democrats from regions of the country that will be most immediately
affected by climate legislation, we want to share our concerns with the
bill that is currently before the Senate. We commend your leadership in
attempting to address one of the most significant threats to this and future
generations; however, we cannot support final passage of the Boxer
Substitute in its current form.221

The ten Democratic Senators were: Debbie Stabenow (D-MI), Senator

John D. Rockefeller (D-WV), Carl Levin (D-MI), Blanche Lincoln (D-AR),
Mark Pryor (D-AR), Jim Webb (D-VA), Evan Bayh (D-IN), Claire
McCaskill (D-MO), Sherrod Brown (D-OH), and Ben Nelson (D-NE). Of
the ten Senators, only Senator Brown voted against cloture.

All in all, the bill didn’t have the slightest chance of passing. Republicans
were prepared to debate the bill and were ready to offer amendments. But the
Democrats did not want to debate, much less vote, on our amendments that
were aimed at protecting American families and workers from the severe
economic impacts of this bill. Even if it made it through the Senate,
Democrats knew the House would never pass it, and even if they jumped that
hurdle, Bush would have vetoed it. But that wasn’t really the point. The point
was to show leaders at the UN Climate Conference that the Senate had taken
action. They had a symbolic victory just in time for Bali and their plan was
to use it as a model for future success—but it would never turn into an actual
victory.

As I predicted, it all came crashing down when the economic reality of
the bill was exposed. When faced with the inconvenient truth of the bill’s
impact on skyrocketing energy prices, very few Senators were able to come
out in support of the bill.

I was surprised that environmentalists were so “stunned that their global
warming agenda was in collapse” in the wake of the bill’s failure because
that was obvious to me. That quote came from a June 6, 2008, article in the
Wall Street Journal which also noted that “green groups now look as
politically intimidating as the skinny kid on the beach who gets sand kicked
in his face. Those groups spent millions advertising and lobbying to push the
cap and trade bill through the Senate.”222

THE GORE EFFECT
God has a sense of humor. December 5, 2007, was a particularly cold and
snowy day to be passing the Lieberman-Warner global warming bill out of
committee. In the middle of the markup, just before I voted no on Lieberman-
Warner, I walked outside the Dirksen Senate Office Building to meet a
reporter from the Business and Media Institute who was braving the cold to
catch me for an interview. I told him that the bill would suffer the same fate
as the McCain-Lieberman bill on the Senate floor due to its enormous
pricetag. Looking around at the snowflakes, I asked the question I asked Gore
in that fateful hearing: Where’s global warming when you need it?

5

THE MOMENT ARRIVES AND THE

MOVEMENT COLLAPSES

THE MOUNTAIN
On December 3, 2009, just days before the Copenhagen climate conference,
Rachel Maddow aired a five-minute segment featuring me, the “unmovable”
denier:

ANNOUNCER: For climate change activists, James Mountain Inhofe is
an inconvenient truth […] The M. stands for Mountain. No really his
name is James Mountain Inhofe and what a mighty unyielding alp he is.
All others are just foothills.

[…]

ANNOUNCER: Yes, elections do have consequences. Senator Inhofe
just won reelection last year. So we’ll be in the shadow of this mountain
until at least January 3, 2015.

RACHEL: Two questions for you Kent. Number one: his middle name is
actually Mountain?

KENT JONES: Yeah.

RACHEL: Not a TMI invention?

KENT: For real. Mountain. James Mountain Inhofe.

RACHEL: Did he also really suggest that the Weather Channel was
trying to boost its ratings?

KENT: He said they’d like that. Yeah, that’d be great. They’d love it if
we were afraid all the time.223

Rachel’s segment was one of the last major efforts to go after me just days

before I landed in Copenhagen and declared vindication, but as I said at a
bloggers’ luncheon at the Heritage Foundation when they asked me what I
thought about the clip, “You know, I’ve really grown to like that gal. She
thinks she’s saying such hateful things about me, but they’re all true”224 —
including the Mountain part. Mountain is my mother’s maiden name. If I was
indeed a “mountain of indignation” for global warming activists, as Rachel’s
segment claimed, it was only because I was a vehicle for the truth and that
was an insurmountable obstacle for them. For all the time, money, and effort
that they poured into their message that global warming was man-made and
catastrophic, Americans were starting to see the truth: the science was not
settled and their “solutions” were dead on arrival.

THE STARS ALIGN
In January 2009, from the outside looking in, it seemed that the United States
was poised to open a new chapter in environmental policy. Barack Obama—
the climate savior who had promised to slow the rise of the oceans and heal
the planet under his plan of a cap and trade system—had just been sworn in
as President of the United States. His allies in Congress were ready to take
action: Speaker of the House Nancy Pelosi and Senate Majority Leader
Harry Reid had made commitments that they were going to pass cap and
trade. The international stage was set for President Obama to arrive in
Copenhagen in December and finally bind the United States to an
international agreement to limit greenhouse gases. The stars were aligned for
the first time ever. The moment they had been waiting for had finally arrived.

But by the end of 2009, it was all over, leaving the UK Guardian to ask

bewildered:

How can everything have gone so wrong so quickly? A year ago, the
prospects for successful climate change regulation were bright: a new
U.S. president promised positive re-engagement with the international
community on the issue, civil society everywhere was enthusiastically
mobilising to demand that world leaders ‘seal the deal’ at Copenhagen,

and the climate denial crowd had been reduced to an embarrassing rump
lurking in the darker corners of the internet. Now there seems to have
been a complete reversal.225

In January of that year, while my environmental friends were popping the
champagne, blinded by the euphoria of being so close to victory, I went to the
Senate floor to give them a dose of reality. I said that no matter how closely
stars were aligned on Capitol Hill and Pennsylvania Avenue, cap and trade
would never pass and the science would implode under the pressure of its
own flaws.

MEDIA MANIA CONCEDES TO SKEPTICISM
In fact, the irony is that the movement fell from inevitability to failure
precisely because the stars were so aligned. With the full control of both
Houses of Congress and the White House, many believed that cap and trade
could become the law of the land, so they began to take a serious look at
what that would actually mean.

I began my own investigation into the science in 2003, because I found out

how much the “solution” would cost and I said that if the United States was
even going to consider such expensive, drastic measures that would
fundamentally change our economy, the science driving that decision had
better be solid. After my rigorous research, I found that it was not—and over
the course of six years, more and more flaws continued to surface.
Importantly, this was exactly the process that the media and many members of
Congress undertook during that year: they realized that we may be seriously
considering a solution that would destroy our already ailing economy so they
decided to take a harder look at the science—and when they did, they found
the man-made catastrophic theory wanting.

By then the Hollywood hysteria had faded into the sunset and the media
could no longer indulge in hyped-up fears of climate catastrophe focusing
only on the “problem” and ignoring the “solution” as Miles O’Brien did in
our 2007 interview when he dismissed my concerns about Kyoto: “We’re not
talking about Kyoto…We’re talking about whether global warming is real.”
Now that the “solution,” was on the horizon, they were forced to face up to it
and the economic catastrophe that it would cause. In droves, they started
going back to take another look at the science.

In 2007, Washington Post staff writer Juliet Eilperin conceded the

obvious, writing that climate skeptics “appeared to be expanding rather than
shrinking.”226 The mainstream media was finally beginning to take notice. A
November 25, 2008, article in Politico reported that a “growing
accumulation” of science is challenging warming fears, and added that the
“science behind global warming may still be too shaky to warrant cap and
trade legislation.”227 On October 20, 2008, Canada’s National Post said that
“the number of climate change skeptics is growing rapidly.”228 And New York
Times environmental reporter, Andrew Revkin, noted on March 6, 2008, “As
we all know, climate science is not a numbers game (there are heaps of
signed statements by folks with advanced degrees on all sides of this
issue).”229

So even before it all collapsed at the end of the year with the Climategate
scandal, the science was already well on its way to imploding and the media
was starting to catch on to that fact. It was truly the “Year of the Skeptic.”

WAXMAN MARKEY PASSES IN THE HOUSE: “WE’LL KILL IT IN
THE SENATE”
On Friday, June 26, 2009, Democrats made history. For the first time, a cap
and trade bill—sponsored by Representatives Henry Waxman and Ed
Markey—passed in the House of Representatives. The House debated the
bill all night, and John Boehner stood his ground throwing the 1,400-page
document on the desk, saying: “Are we really going to pass a bill that will
remake our entire economy in one night, when no one has even read it?”
With the passage of Waxman-Markey that morning, Americans were
subjected to yet another significant change in environmental rhetoric. No
longer were we moving forward with this effort to avert major climate
catastrophe; we were doing this, more importantly, because we needed to
transition to a clean energy future. Notably absent in President Obama’s
ringing endorsement of the bill was any mention of global warming or
climate change—or cap and trade for that matter:

This week, the House of Representatives is moving ahead on historic
legislation that will transform the way we produce and use energy in
America. This legislation will spark a clean energy transformation that

will reduce our dependence on foreign oil and confront the carbon
pollution that threatens our planet.

This energy bill will create a set of incentives that will spur the
development of new sources of energy, including wind, solar, and
geothermal power. It will also spur new energy savings, like efficient
windows and other materials that reduce heating costs in the winter and
cooling costs in the summer.

These incentives will finally make clean energy the profitable kind of

energy. And that will lead to the development of new technologies that
lead to new industries that could create millions of new jobs in America
— jobs that can’t be shipped overseas.

At a time of great fiscal challenges, this legislation is paid for by the

polluters who currently emit the dangerous carbon emissions that
contaminate the water we drink and pollute the air that we breathe. It also
provides assistance to businesses and communities as they make the
gradual transition to clean energy technologies.

[…]

We all know why this is so important. The nation that leads in the

creation of a clean energy economy will be the nation that leads the 21st
century’s global economy. That’s what this legislation seeks to achieve—
it’s a bill that will open the door to a better future for this nation. And
that’s why I urge members of Congress to come together and pass it.230

Suddenly we had to pass this bill so that we could reduce carbon

“pollution” and create “green jobs”? What happened to saving the world?
This kind of language was also the headline of Senator Boxer’s first climate
change hearing in July 2009 when she said, “Today’s hearing is the kickoff of
a historic Senate effort to pass legislation that will reduce our dependence on
foreign oil, create millions of clean energy jobs, and protect our children
from pollution.”231 Just as there was a decided shift in rhetoric from global
warming to climate change around 2005–2007, there was a sudden shift in
rhetoric from averting climate catastrophe to transitioning to a clean energy

economy in 2009. They had to change their strategy to try to counter the
reality that cap and trade would destroy jobs.

This did not go unnoticed by the mainstream media. As the New York

Times put it:

The problem with global warming, some environmentalists believe, is
‘global warming.’ The term turns people off, fostering images of shaggy-
haired liberals, economic sacrifice and complex scientific disputes,
according to extensive polling and focus group sessions conducted by
ecoAmerica, a nonprofit environmental marketing and messaging firm in
Washington.232
The LA Times also reported:

Scratch ‘cap and trade’ and ‘global warming,’ Democratic pollsters tell
Obama. They’re ineffective…Control the language, politicians know and
you stand a better chance of controlling the debate. So the Obama
administration, in its push to enact sweeping energy and healthcare
policies, has begun refining the phrases it uses in an effort to shape
public opinion. Words that have been vetted in focus groups and polls are
seeping into the White House lexicon, while others considered too scary
or confounding are falling away.233

As the LA Times rightly points out, cap and trade had also become a

forbidden phrase. Senator Kerry later made his famous statement in
September, “I don’t know what ‘cap and trade’ means. I don’t think the
average American does. This is not a cap and trade bill, it’s a pollution
reduction bill.”234 I said that if Kerry didn’t know what cap and trade meant,
he only had to ask his Democratic colleague in the House, John Dingell, who
had defined it beautifully: “a tax and a big one.”235

President Obama was clearly moving away from the catastrophe rhetoric,

Senator Kerry was pushing green jobs and energy security, and their ranks
were in disarray with the messaging. Several Democrats still hadn’t let go of
the idea of preventing bad weather by acts of Congress. Even in the same
opening statement where Senator Boxer headlined with green jobs, she
mentioned toward the end the “devastating effects that will come in the future
if we do not take action to cut global warming pollution. Droughts, floods,

fires, loss of species, damage to agriculture, worsening air pollution and
more.” Senator Debbie Stabenow also hadn’t let go of the catastrophe, saying
in August 2009, “Climate change is very real. Global warming creates
volatility. I feel it when I’m flying. The storms are more volatile. We are
paying the price in more hurricanes and tornadoes.”236

The final tally of Waxman-Markey’s passage in the House portended its

demise. The vote was 219–212, which meant that a large number of
Democrats had voted against it. If a 1,400-page bill that’s designed to
transform our economy only passed by seven votes, I knew it didn’t stand a
chance in the Senate.

The day before the vote, I was traveling back to Tulsa and stopped by the
Countrywide and Sun newspaper in Shawnee, Oklahoma. A reporter asked
me what would happen if the House passed cap and trade and I said, “It
doesn’t matter because we’ll kill it in the Senate anyway.”237

CAP AND TRADE COMES TO THE SENATE
It was like Lieberman-Warner all over again, except this time Senator Boxer
couldn’t even get cap and trade to the Senate floor.

From the moment Waxman-Markey passed in the House, the Environment
and Public Works Committee under the Chairmanship of Boxer held hearing
after hearing on such topics as “Update on the Latest Global Warming
Science,” “Moving America toward a Clean Energy Economy and Reducing
Global Warming Pollution: Legislative Tools,” “Clean Energy Jobs, Climate-
Related Policies and Economic Growth—State and Local Views,” “Climate
Change and National Security”—and as before, what Senator Boxer was not
as anxious to discuss was cap and trade legislation itself.

But we weren’t going to let her get away with that. It didn’t matter what

the topic of the hearing was. My Republican colleagues and I made it a
priority to shine the spotlight on the bill itself and the economic burden it
would place on our country. If our friends in the House had debated the bill
under the cover of night, we were determined to expose it to the light of day.

At the end of each hearing, I asked my staff to put together a series of

YouTube videos that exposed particular faults in cap and trade legislation.
There were three pivotal videos that made a significant impact on the debate.

HEARING HIGHLIGHTS: LISA JACKSON CONFIRMS CAP AND
TRADE ALL COST, NO GAIN
When President Obama came into office, he brought with him a substantial
green team, but I was very pleased that his choice for EPA Administrator,
Lisa Jackson, was someone I really liked and respected. Before she joined
the Obama Administration, Lisa had testified before the Environment and
Public Works Committee on a range of issues from chemical security to
mercury legislation when she served as the Director of the New Jersey
Environmental Protection Agency office. I had the chance to meet with her
again before her nomination hearing, and we talked at length about our
families, values, and about the importance of having a healthy respect for our
differences of opinion. Lisa left with two items in her hand: my report
documenting hundreds of scientists who dispute global warming alarmism
and a Christmas card with a picture of my family. She told me that she has
that picture hanging on her wall in her office. Lisa and I rarely agree when it
comes to environmental policy, but we are good friends and I am always
happy to welcome her to the Environment and Public Works Committee.

I especially appreciated her testimony during a particular hearing in July
2009 when she conceded what I had been saying all along: climate change
legislation in the United States would be all pain for no gain. At one point
during the hearing, I put up a chart of an EPA analysis, which showed that the
United States acting alone to reduce greenhouse gas emissions would have no
affect on global temperatures, and asked her to confirm that this chart was
accurate. She said, “I believe the central parts of the [EPA] chart are that
U.S. action alone will not impact world CO2 levels.”238 One of the reasons I
respect Lisa so much is that when you ask her a question, she gives you an
honest answer—as she did that day. To have the head of the EPA come out
and admit that this bill would have no impact on the climate was a game
changer.

Interestingly, just a few weeks before, President Obama in a June 25,
2009, interview covered by the San Francisco Chronicle, said, “A long-
term benefit [of cap and trade] is we’re leaving a planet to our children that
isn’t four or five degrees hotter.” He clearly failed to consult his own EPA on
that matter.239

HEARING HIGHLIGHTS: GOVERNORS REJECT WAXMAN
MARKEY
That July, we also welcomed several governors to testify before the
committee for a hearing titled, “Clean Energy Jobs, Climate-Related
Policies, and Economic Growth—State and Local Views.” Of course, cap
and trade was not the intended topic of the hearing, but I made it a point to
ask each Governor present how they felt about the Waxman-Markey bill,
which put them all in a very uncomfortable position. I knew that Democratic
Governor Bill Ritter of Colorado had previously endorsed cap and trade so I
was anxious to see if he could still support Waxman-Markey as the governor
of a strong oil, gas and agriculture state. Interestingly, when pressed, he
could not say that he supported it:

Here’s what I support. I support a national energy policy that’s married to
a national climate policy that gets at these goals that we have for
greenhouse gas reductions. And I believe that if you do that, that there
will some vehicle that may not look exactly like Waxman-Markey,
particularly after the Senate finishes its work. But I very much support
climate legislation that is joined with a national energy policy to get us to
the greenhouse gas emission reduction goals that are set for 2050.240

Also telling was that Governors Gregoire (D-WA) and Corzine (D-NJ)

would not directly say that they endorsed Waxman-Markey.

HEARING HIGHLIGHTS: GREEN JOBS DEBUNKED
During one particularly lively hearing on the clean energy economy also in
July, we invited Harry Alford, President and CEO of the National Black
Chamber of Commerce, to discuss a new study conducted by the Black
Chamber of Commerce and CRA International on the economic effects of
Waxman-Markey. As Alford explained:

Climate change is a vital issue that must be addressed […] Regretfully,
the current legislation out of the U.S. House of Representatives will
negatively impact the most vulnerable of our society. I’m sure that those
who proposed it had the best intentions, but the bill doesn’t do what it’s

supposed to do, and it does so at a very high cost—especially high for
working families and small business owners.241

Alford also said that according to this study, “green jobs gained would be
swamped by jobs lost in old industries and businesses, leading to a net loss
of 2.3 million to 2.7 million jobs,” because so many industries supported by
fossil fuel development would be destroyed.

Then Senator Boxer let the cat out of the bag. She explained that the goal
of her forthcoming legislation was “softening the blow on our trade sensitive
industries and our consumers. I just want you to know that, that’s the goal.”242
I was glad that she finally admitted that cap and trade would impose
significant economic harm. Basically what she was saying was that
Washington would tax hard-working Americans, put them out of work, and
then cut them a check to help “soften the blow.” That didn’t sound like a very
effective “jobs bill.”243

THE MOMENT ARRIVES: KERRY-BOXER?
After a long summer of anticipation, Senators Kerry and Boxer finally held
an outdoor press conference on September 30, 2009, to introduce their cap
and trade bill in the Senate. Right away they faced several problems. First of
all, what they introduced that day was an outline—it was not a full bill.
Second, it had been clear for a while that a number of conservative
Democrats and moderate Republicans in the Senate would not support a bill
that was exactly the same as Waxman-Markey, so everyone was wondering
what changes they would make on the Senate bill to gain the support of
wavering members. Yet, when the bill surfaced, it appeared to be nearly
identical to Waxman-Markey. It was also curious that the bill was named
Kerry-Boxer rather than Boxer-Kerry since Boxer was, after all, the
Chairman of the Environment and Public Works Committee.

CRSREPORT: AMERICA HAS THE LARGEST RESERVES OF
FOSSIL FUELS OF ANY COUNTRY IN THE WORLD
On October 23, 2009, just weeks after cap and trade—a bill that would force
Americans into policies of energy austerity—came to the Senate, a report
from the nonpartisan Congressional Research Service (CRS) revealed for the

first time that America’s combined energy resources are the largest on
earth.244 Democrats trying to force us off of fossil fuels were not too pleased.
The report tells us that America’s combined recoverable natural gas, oil,
and coal endowment is the largest on Earth—far larger than those of Saudi
Arabia (third), China (fourth), and Canada (sixth) combined. I said, thanks to
the non-partisan Congressional Research Service, we now know what
resources we have and what we aren’t getting out of the ground. I requested
the report along with Senator Lisa Murkowski because we had grown tired
of the Democrats’ refrain that America only has 3 percent of global oil
reserves—which, according to this view, meant more drilling and production
at home would be futile. The 3 percent mantra is their bread and butter
talking point.

President Obama himself has said that with 3 percent of the world’s oil
reserves, the U.S. cannot drill its way to energy security. My Democratic
colleagues in the Senate have also said that the United States has only 3
percent of known oil reserves, yet we use 25 percent of the world’s oil
production. But the non-partisan CRS shows the full, complete, accurate
picture of America’s resources—and shows that, yes, we can produce our
way to energy security.

That’s because CRS shows more than just our proven oil reserves, which

is what the Democrats conveniently cite. America’s proven reserves, of
course, are a modest 28 billion barrels. The word “proven” is important
here. The only way to estimate proven reserves is to drill. But that’s not
possible because federal policies, supported by President Obama and many
Democrats, have made of America’s federal land inaccessible to drilling.
Knowing what vast resources literally lay at our feet, it seemed all the
more outrageous that we were even considering such drastic measures to
limit access to these resources.

THE “NUCLEAR OPTION”
Although Kerry-Boxer was “introduced” at the end of September 2009, it
took Senators Kerry and Boxer almost a month to draft the full bill, which we
received just before midnight on October 23. Senator Boxer planned to begin
marking up the bill November 3, 2009, in the Environment and Public Works
Committee.

As was the case with Lieberman-Warner, Republicans understandably

wanted to have an economic analysis of Kerry-Boxer completed before
voting on the bill in committee. Leading the charge on this request was
Senator Voinovich from Ohio, a state that would be hit hard by the
legislation.

At that stage, EPA had already completed an analysis of Waxman-Markey
but they had used unrealistic assumptions—one of them being that the United
States would construct 103 new nuclear power plants to make up for lost
baseload power from the gradual phasing out of fossil fuels. The idea that we
could license and construct that many nuclear power plants was absurd
considering that no new power plants have been licensed for thirty years.
Senator Voinovich requested that EPA conduct an economic analysis of
Kerry-Boxer using more realistic assumptions. But Senator Boxer claimed
that our request was only a delay tactic. She said we had enough economic
information in our hands to move the bill through Committee.

This disagreement over economic modeling set the stage for a

confrontation in the Environment and Public Works Committee that was
unprecedented. We stood our ground that we could not move forward with a
markup until we had adequate economic modeling of the bill. Senator Boxer
decided to hold the markup anyway, breaking longstanding precedent that two
members of the minority must be present for a markup to begin. Again, as
with Lieberman-Warner, the Copenhagen conference was just around the
corner, so Senator Boxer was determined to have this bill passed out of
committee to show world leaders that we were taking action.

But as Darren Samuelsohn of Politico correctly noted, the consequences
for breaking precedent were high: “Going this route […] could spell trouble
for the overall legislation as Boxer and her allies continue their search for 60
votes among moderate Democrats and Republicans. ‘That product is totally
toxic,’ the former staffer warned. ‘It’s basically worthless.’”245

Senator Voinovich attended the markup only on the first day to ask Senator

Boxer one last time to allow us to complete the economic analysis before
proceeding; when she refused, he left. I briefly attended the last day of the
markup to reinforce our message, but she would not concede.

Cap and trade was already dead, but Senator Boxer’s decision to break
with committee precedent put the last nail in the coffin. I walked straight out
of the committee room to the Russell rotunda to give an interview with

Martha MacCallum of Fox News on November 5, 2009. I explained how it
would all turn out:

INHOFE: We’re not going to pass this huge bill. This would be the
largest tax increase in the history of America—[we said we would] not
pass it out of committee until we had an EPA analysis. We’ve requested it
now for two months. And they’re waiting to do it but they won’t give
them the go ahead. I can only conclude that they don’t want the public to
know how much money this thing’s going to cost. So they’ve done
something that’s never been done in the history of this committee. They
passed a bill out without one member of the minority—not one
Republican. You know, Martha, let’s keep in mind all the town meetings
that we went through. That was about health care but that was also about
this global warming monstrous tax. And I’ll tell you this is unprecedented
and I think the bill is dead. By the way, one Democrat voted against it and
that was Max Baucus. He’s from Montana but he’s also the chairman of
the Finance Committee. So that bill is dead. […] I really honestly believe
this is history in the making right now because it’s never happened
before. I can’t see any way in the world that Harry Reid is going to bring
this bill up and it could pass. It just can’t happen.246

CAP AND TRADE IS DEAD
It wasn’t long before I could declare victory. By the middle of November, the
Environment and Public Works Committee was back to work on other issues
in the hearing room, and I had the chance to have a little fun after Barbara’s
“elections have consequences” jab. It turns out that votes have consequences
too. I was happy to declare, “We won, you lost, get a life!”247

6

CLIMATEGATE = VINDICATION

THE COLLAPSE OF THE SCIENCE

When I said I’ve been called every name in the book, I wasn’t kidding. In
stark contrast to Rachel Maddow’s depiction of me as a “mountain of
indignation,” Dana Milbank wrote in an October 28, 2009, in The
Washington Post column, “It must be very lonely being the last flat-earther.”
It was probably the last time a reporter could get away with singling me out
as the only one who wasn’t buying into the flawed science behind the global
warming campaign. Milbank continued making his case:

“Eleven academies in industrialized countries say that climate change is
real; humans have caused most of the recent warming,” admitted Sen.
Lamar Alexander (R-Tenn.). “If fire chiefs of the same reputation told me
my house was about to burn down, I’d buy some fire insurance.” An oil-
state senator, David Vitter (R-La), said that he, too, wants to “get us
beyond high-carbon fuels” and “focus on conservation, nuclear, natural
gas and new technologies like electric cars.” And an industrial-state
senator, George Voinovich (R-Ohio), acknowledged that climate change
“is a serious and complex issue that deserves our full attention.” Then
there was poor Inhofe.248

Just a few weeks after that column appeared, it was all over: Climategate,

the greatest scientific scandal of our time, broke. So I said Milbank didn’t
have to feel too sorry for me. What I had been saying about the IPCC all
along was confirmed. I was vindicated.

On November 18, 2009, just two days before Climategate, I went back
down to the Senate floor to speak about how the “consensus” was already
shattered and Copenhagen would fail. I said that 2009 would go down in

history as the “Year of the Skeptic.” I had a few allies in this assertion: the
Telegraph, a UK Newspaper, was predicting Copenhagen would be a
disaster on November 15, 2009: “The worst kept secret in the world is
finally out—the climate change summit in Copenhagen is going to be little
more than a photo opportunity for world leaders.”249 I said I would be there
to tell them the truth:

And I will be travelling to Copenhagen, leading what I call the “Truth
Squad” to say exactly what I said six years ago in Milan, Italy: The
United States will not support a global warming treaty that will
significantly damage the American economy, cost American jobs, and
impose the largest tax increase in American history. Further, as I stated in
2003, unless developing nations are part of the binding agreement, the
U.S. will not go along. Given the unemployment rate of 10 percent, and
given all of the out of control spending in Washington, the last thing we
need is another thousand-page bill that increases costs and ships jobs
overseas, all with no impact on climate change.

I also said in Milan that the science is not settled. That was an

unpopular view back then. But today, since Al Gore’s science fiction
movie, more and more scientists, reporters, and politicians are
questioning global warming alarmism. I proudly declare 2009 as the
“Year of the Skeptic”—the year in which scientists who question the so-
called global warming consensus are being heard.250

So Copenhagen was already well on its way to failure. When Climategate
hit, it only added superfluous nails to a coffin that was already tightly nailed
shut.

Climategate revealed leaked emails from the world’s top climate

scientists at the University East Anglia’s Climactic Research Unit, many of
whom had been lead authors of the IPCC reports and were intimately
involved in writing and editing the IPCC’s science assessments. My Senate
report showed that many of these scientists may be obstructing the release of
information that was contrary to their “consensus” claims; may be
manipulating data using flawed climate models to reach preconceived
conclusions; may be pressuring journal editors not to publish work

questioning the “consensus”; and assuming activist roles to influence the
political process.251

The implications of this were huge considering that the “consensus” claim

was based on the foundation of the IPCC science. Noted science historian
Naomi Oreskes wrote, the “scientific consensus” of climate change “is
clearly expressed in the reports of the Intergovernmental Panel on Climate
Change.”252 One top Obama Administration official said that the IPCC’s
assessments were the “gold standard” on climate science “because of the
rigorous way in which they are prepared, reviewed, and approved.”253

Each of the IPCC’s four assessment reports made the scientific case—
more definitely over time—that anthropogenic gases were causing global
warming. The IPCC’s Fourth Assessment Report’s Summary for
Policymakers in 2007 claimed that “warming of the climate system is
unequivocal” and that “[m]ost of the observed increase in globally averaged
temperatures since the mid-20th century is very likely due to the observed
increase in anthropogenic (human) greenhouse gas concentrations.”254

Climategate finally destroyed what was left of the façade of the

“consensus.” Contrary to their repeated public assertions that the “science is
settled,” the emails show climate scientists were arguing over critical issues,
questioning key methods and statistical techniques, expressing concerns
about historical periods (such as whether the Medieval Warming Period
[MWP] was global in extent) and doubting whether there is “consensus” on
the causes and the extent of climate change.

The press reaction in the wake of the scandal was remarkable considering
how just a few years before they had nothing but praise for the IPCC. George
Monbiot, a British writer, known for his environmental and political
activism, wrote in his weekly column in the Guardian: “Pretending that this
isn’t a real crisis isn’t going to make it go away. Nor is an attempt to justify
the emails with technicalities. We’ll be able to get past this only by grasping
reality, apologising where appropriate, and demonstrating that it cannot
happen again.”255 The Daily Telegraph said that “this scandal could well be
the greatest in modern science.”256 Clive Crook of the Atlantic magazine
wrote, “The closed-mindedness of these supposed men of science, their
willingness to go to any lengths to defend a preconceived message, is

surprising even to me. The stink of intellectual corruption is
overpowering.”257

But comedian Jon Stewart was the best—he said, “Poor Al Gore: global

warming completely debunked via the very Internet you invented. Oh the
irony!” He went on:

STEWART: Value added data? What is that, numbers fortified with art?
Truth plus, now with lemon? It doesn’t look good. Now does it disprove
global warming? No, of course not. But it does put a fresh set of
energizers in the Senate’s resident denier bunny.

SENATOR JAMES INHOFE, (R-OKLA.): The fact that this whole
idea on the global warming. I’m glad that’s over, gone, done. We won.
You lost. Get a life.

STEWART: Alright. We knew Inhofe was going to say that. That guy
thinks global warming is debunked every time he drinks a Slushee and
gets a brain freeze. “If global warming is real, why does my head hurt?”
But by the way, that quote was from BEFORE he found out about the
leaked email story. But that’s the point. If you care about an issue, and
want it to be your life’s work, don’t cut corners.258

It was one of the first times someone called me out for being a “denier”

while also giving me credit for predicting how it would all end.

REWIND TO 2005
In 2005, I stood on the Senate floor to discuss the flaws in the IPCC process
that had been manifesting themselves for years, and said it was time to face
up to the “systematic and documented abuse of the scientific process by
which an international body that claims it provides the most complete and
objective science assessment in the world on the subject of climate change,
the United Nations IPCC.”259

At the time of my speech, the IPCC’s fourth assessment, which was meant

to be the “smoking gun” report—attempting to prove there was an
“unequivocal” link between humans and catastrophic global warming—was
set to come out in 2007.260 I said that if the IPCC and its fourth assessment

were to have any credibility, fundamental changes to the IPCC scientific
process would need to be made. Most importantly, I said that the IPCC must
adopt procedures that ensure that impartial scientific reviewers formally
approve both the chapters and the Summary for Policymakers—the latter of
which was the only document that members of the press and members of
Congress ever read. When compared with the actual report, it was clear the
Summary for Policymakers was being co-opted by activists with an agenda
to shape the conclusions to show that man-made emissions were causing
catastrophic global warming. To safeguard against the manipulation of the
message, objective scientists, not government delegates should be a part of
the approval process. I also said that the IPCC must ensure that any
uncertainties in the state of knowledge be clearly expressed in the Summary
for Policymakers.

But of course, the IPCC remained committed to its path and, as

Climategate eventually revealed, it was unsustainable and it was only a
matter of time before it collapsed.

THE “GOLD STANDARD”
Phil Jones, the head of the Climatic Research Unit, put it mildly when he
admitted that the Climategate emails “do not read well.”261

The emails themselves raised the important question: what, if any, are the

boundaries between science and activism? Perhaps the statement that best
exemplifies the unusual political tendency among the scientists in the CRU
controversy came from Dr. Keith Briffa, the Deputy Director of the CRU, and
lead author of the IPCC’s Fourth Assessment Report, who wrote in one of the
CRU emails, “I tried hard to balance the needs of the science and the IPCC,
which were not always the same.”262 The most famous example comes from
an email from Phil Jones, which reads, “I’ve just completed Mike’s Nature
trick of adding in the real temps to each series for the last 20 years (i.e. from
1980 onwards) and from 1961 for Keith’s to hide the decline.”263 Of course,
he means hide the decline in temperatures, which caused another scientist,
Kevin Trenberth, to write: “The fact is we can’t account for the lack of
warming, and it’s a travesty that we can’t.”264

Climategate is significant in that it confirmed in the minds of many what

we strongly suspected all the time. It is imperative that you read Appendix C,

excerpts from our report on the CRU controversy which was published in
February 2010. It clearly documents the specific participants and statements
in Climategate.

HOCKEY STICK ANNIHILATED
If the hockey stick was already broken all the way back in 2003, after the
Climategate revelations, in 2009, it is more accurate to say that it was
shattered, as revealed in the following excerpts of my staff report on
Climategate emails.

Possibly the most egregious example of scientists trying to silence skeptic
voices was the reaction to a paper published in the journal Climate Research
in 2003, which posed a serious challenge to the “hockey stick” graph
constructed by Professors Michael Mann, Raymond Bradley, and Malcolm
Hughes. Of course, the hockey stick, which was featured prominently in the
IPCC’s Third Assessment Report in 2001, supported the conclusion that the
1990s, and 1998, were likely the warmest decade, and the warmest year,
respectively, in at least a millennium.

Dr. Sallie Baliunas and Dr. Willie Soon, researchers at the Harvard-

Smithsonian Center for Astrophysics, contested the hockey stick conclusion;
they reviewed more than two hundred climate studies and “determined that
the 20th century is neither the warmest century nor the century with the most
extreme weather of the past 1000 years.” Their study “confirmed that the
Medieval Warm Period of 800 to 1300 A.D. and the Little Ice Age of 1300 to
1900 A.D., were worldwide phenomena not limited to the European and
North American continents. While 20th century temperatures are much higher
than in the Little Ice Age period, many parts of the world show the medieval
warmth to be greater than that of the 20th century.”265

As the leaked emails show, Michael Mann, the author of the hockey stick,
and Phil Jones, a climatologist at the University of East Anglia, were not too
happy about this. In an email on March 11, 2003, titled “Soon and Baliunas,”
Jones writes that he and his colleagues “should do something” about the
Soon-Baliunas study, the quality of which he found “appalling”: “I think the
skeptics will use this paper to their own ends and it will set paleo
[climatology] back a number of years if it goes unchallenged.” Jones then
went a step further, threatening to shun Climate Research until “they rid
themselves of this troublesome editor.”266

That same day, Mann responded, complaining that the skeptics had

“staged a bit of a coup” at Climate Research, implying that scientists who
disagree with him could never get published in peer-reviewed literature
solely on the merits of their work. Mann echoed Jones’ suggestion to punish
Climate Research by encouraging “our colleagues in the climate research
community to no longer submit to, or cite papers in, this journal:”

This was the danger of always criticising the skeptics for not publishing
in the “peer-reviewed literature.” Obviously, they found a solution to that
—take over a journal! So what do we do about this? I think we have to
stop considering “Climate Research” as a legitimate peer-reviewed
journal. Perhaps we should encourage our colleagues in the climate
research community to no longer submit to, or cite papers in, this journal.
We would also need to consider what we tell or request of our more
reasonable colleagues who currently sit on the editorial board.267

In April 2003, Timothy Carter with the Finnish Environment Institute

suggested changes to the editorial process at Climate Research in an email to
Tom Wigley, a scientist formerly with the University Corporation for
Atmospheric Research (UCAR). Noting communications with “Mike”
(Michael Mann) the previous morning, Carter wondered how to remove
“suspect editors,” presumably those who approve research by skeptics. In
reply, Wigley described a campaign to discredit Climate Research through a
letter signed by more than fifty scientists. He also mentioned Mann’s
approach to “get editorial board members to resign”:

One approach is to go direct to the publishers and point out the fact that
their journal is perceived as being a medium for disseminating
misinformation under the guise of refereed work. I use the word
“perceived” here, since whether it is true or not is not what the
publishers care about—it is how the journal is seen by the community
that counts. I think we could get a large group of highly credentialed
scientists to sign such a letter—50+ people. Note that I am copying this
view only to Mike Hulme and Phil Jones. Mike’s idea to get editorial
board members to resign will probably not work—must get rid of von
Storch too, otherwise holes will eventually fill up with people like
Legates, Balling, Lindzen, Michaels, Singer, etc. I have heard that the

publishers are not happy with von Storch, so the above approach might
remove that hurdle too.268

Along with these discussions about removing journal editors who held

contrary views on climate science, the emails show that the scientists tried to
prevent publication of papers they disagreed with. On July 8, 2004, Jones
suggested that he and a colleague could keep the work of skeptics from
appearing in the IPCC’s Fourth Assessment report: “I can’t see either of
these papers being in the next IPCC report. Kevin and I will keep them out
somehow—even if we have to redefine what the peer-review literature
is!”269

The conclusion is obvious: Mann and his colleagues were not

disinterested scientists. They acted more like a priestly caste, viewing
substantive challenges to their work as heresy. And rather than welcoming
criticism and debate as essential to scientific progress, they launched a
campaign of petty invective against scientists who dared to question their
findings and methods.

“HIDE THE DECLINE”
The following is more from my Environment and Public Works Committee’s
minority staff report on Climategate:

“I am not sure that this unusual warming is so clear in the summer
responsive data. I believe that the recent warmth was probably matched
about 1,000 years ago.” Keith Briffa, Deputy Director, CRU, September
22, 1999.

I asked what Dr. Mann was trying to hide in a speech on the Senate floor

on April 13, 2005, and Climategate emails provided the answer: he was
arguably hiding the decline in temperatures. One of the most famous emails is
written by CRU’s Jones in 1999: “I’ve just completed Mike [Mann]’s Nature
trick of adding in the real temps to each series for the last 20 years (i.e. from
1981 onwards) and from 1961 for Keith’s to hide the decline.”270

Jones’s “trick” arose because of disagreement over Dr. Mann’s “hockey

stick” temperature graph. Of course, the hockey stick showed a relatively
straight shaft extending from 1000 AD to 1900, when a blade turns sharply

upward, suggesting that warming in the 20th century was unprecedented, and
caused by anthropogenic sources. Remember, the hockey stick was featured
prominently on page one of the IPCC’s Summary for Policymakers in its
Third Assessment Report.

In defending himself, Jones said, “The word ‘trick’ was used here

colloquially as in a clever thing to do. It is ludicrous to suggest that it refers
to anything untoward.”271 Similarly, echoing Jones, Dr. John Holdren,
President Obama’s Science Adviser, asserted that “trick” merely means a
clever way to tackle a problem.272 Both Holdren and Jones’ explanation of
“trick” used in this context has evidentiary support. Unfortunately, neither
Jones nor Holdren addressed the “problem” that confronted Jones and his
colleagues. The problem in this case is the so called “divergence problem.”
The divergence problem is the fact that after 1960, tree ring reconstructions
show a marked decline in temperatures, while the land-based, instrumental
temperature record shows just the opposite.

For some scientists, the divergence of data was a cause of great concern,

but not necessarily for scientific reasons. For instance, IPCC author Chris
Folland warned in an email that such evidence “dilutes the message rather
significantly” that warming in the late 20th century relative to the last 1,000
years is “unprecedented.”273

Specifically, Jones et al. expressed concern about a temperature

reconstruction authored by Keith Briffa, a senior researcher with CRU.
Because reliable thermometer data go back only to the 1850s, scientists use
proxy data such as tree rings to reconstruct annual temperatures over long
periods (e.g., 1,000 years) (it must be noted that proxy reconstructions are
rife with uncertainties).274

Unfortunately for those in the email chain, Briffa’s reconstruction relied
on tree ring proxies that produced a sharp and steady decline in temperature
after 1960. This conflicted with the instrumental temperature readings that
showed a steep rise. Briffa’s graph was, according to Dr. Michael Mann, a
“problem”:

Keith’s series…differs in large part in exactly the opposite direction that
Phil’s does from ours. This is the problem we all picked up on (everyone
in the room at IPCC was in agreement that this was a problem and a

potential distraction/detraction from the reasonably consensus viewpoint
we’d like to show w/the Jones et al and Mann et al series.275

Briffa later addressed the “pressure to present a nice tidy story” about the

“unprecedented” warming in the late 20th century. In his view, “the recent
warmth was matched about 1,000 years ago.” Here is the email from Briffa
in full:

I know there is pressure to present a nice tidy story as regards ‘apparent
unprecedented warming in a thousand years or more in the proxy data but
in reality the situation is not quite so simple. We don’t have a lot of
proxies that come right up to date and those that do (at least a significant
number of tree proxies) some unexpected changes in response that do not
match the recent warming. I do not think it wise that this issue be ignored
in the chapter. For the record, I do believe that the proxy data do show
unusually warm conditions in recent decades. I am not sure that this
unusual warming is so clear in the summer responsive data. I believe that
the recent warmth was probably matched about 1,000 years ago. I do not
believe that global mean annual temperatures have simply cooled
progressively over thousands of years as Mike appears to and I contend
that there is strong evidence for major changes in climate over the
Holocene (not Milankovich) that require explanation and that could
represent part of the current or future background variability of our
climate.276

Mann was apparently nervous that “skeptics” would have a “field day” if

Briffa’s decline was featured in the IPCC’s Third Assessment Report. He
said “he’d hate to be the one” to give them “fodder.” On September 22, 1999,
Mann wrote:

We would need to put in a few words in this regard. Otherwise, the
skeptics have a field day casting doubt on our ability to understand the
factors that influence these estimates and, thus, can undermine faith in the
paleoestimates. The best approach here is for us to circulate a paper
addressing all the above points. I’ll do this as soon as possible. I don’t
think that doubt is scientifically justified, and I’d hate to be the one to
have to give it fodder!277

As UK’s Daily Mail reported, “All [Jones] had to do was cut off Briffa’s
inconvenient data at the point where the decline started, in 1961, and replace
it with actual temperature readings, which showed an increase.”278

So it seems that, rather than employing a “clever way”—or “trick”—to
solve the post-1960 decline, Jones allegedly manipulated data to reach a
preconceived conclusion. His method has been criticized by fellow
scientists. Philip Stott, emeritus professor of biogeography at London’s
School of Oriental and African Studies said, “Any scientist ought to know
that you just can’t mix and match proxy and actual data. They’re apples and
oranges. Yet that’s exactly what [Jones] did.”279

“RECKLESS ENDANGERMENT”
What they couldn’t achieve through Kyoto they tried to achieve through cap
and trade legislation. And what they couldn’t achieve through legislation,
they are currently trying to achieve through regulation.

In the midst of the cap and trade debate, as support for the bill was
dwindling, the Obama EPA was working behind the scenes to finalize a
“finding” that greenhouse gases harm public health and welfare, known as the
“endangerment finding.” During a key case, Massachusetts v. EPA,280 the
Supreme Court ruled that if EPA determined that greenhouse gases endanger
human health, then they must regulate them under the Clean Air Act. The key
word here is “if.” Proponents of the endangerment finding claim that the court
forced the EPA to move forward with this finding, but this is not the case.
The courts were clear that the EPA Administrator first had to determine if
greenhouse gases endanger the public, and that determination would require a
scientific investigation. They had a choice, and they made the wrong choice.
They chose to make an endangerment finding based on the flawed scientific
conclusions of the IPCC.

As the cap and trade battle waged on, EPA Administrator Lisa Jackson
and President Obama said repeatedly that passing the bill was preferable
because EPA regulations would be much more costly and complicated. As
one astute April 2009 editorial in the Wall Street Journal put it, the
Administration essentially played Russian roulette with regulations:

President Obama’s global warming agenda has been losing support in
Congress, but why let an irritant like democratic consent interfere with

saving the world? So last Friday the Environmental Protection Agency
decided to put a gun to the head of Congress and play cap and trade
roulette with the U.S. economy.

The pistol comes in the form of a ruling that carbon dioxide is a
dangerous pollutant that threatens the public and therefore must be
regulated under the 1970 Clean Air Act. This so-called ‘endangerment
finding’ sets the clock ticking on a vast array of taxes and regulation that
EPA will have the power to impose across the economy, and all with
little or no political debate.281

They were determined to have cap and trade no matter what.
Part of the reason EPA regulation of greenhouse gases would be more

complicated is that the Clean Air Act thresholds are only meant to regulate
real, localized pollutants such as SO2, NOX, and Mercury. Numerous legal
experts, including Democrat Representative John Dingell, who wrote the
Clean Air Act amendments, have said that the Clean Air Act was never
designed to regulate greenhouse gas emissions. That’s because emissions of
greenhouse gases are far greater than conventional pollutants, if EPA
regulated them at the thresholds required by the Clean Air Act, the Agency
would have to regulate almost everything including schools, hospitals,
nursing homes, commercial buildings, churches, restaurants, hotels, malls,
colleges and universities, food processing facilities, farms, sports arenas,
soda manufacturers, bakers, brewers, wineries, and even some private
homes. The results of that would be absurd, so EPA tailored the Clean Air
Act to create much higher thresholds for greenhouse gases—but this tailoring
will not likely hold up in the courts because it directly contradicts the law.
And if the courts throw the “tailoring rule” out, it will be as Representative
John Dingell put it, “a glorious mess.”282

And the entire foundation of this bureaucratic nightmare is flawed

science: EPA Administrator Lisa Jackson admitted to me publicly that EPA
based its action on the IPCC science, saying that the proposal, the Agency
relied in large part on the assessment reports developed by the
Intergovernmental Panel on Climate Change (IPCC).

At an Environment and Public Works Committee hearing on December 2,

2009, I challenged Administrator Jackson on that matter, saying that given

what has come to light in the Climategate scandal, EPA should halt this
agenda. She replied, “While I would absolutely agree that these emails show
a lack of interpersonal skills, as I would say to my kids, be careful who you
write, and maybe more, I have not heard anything that causes me to believe
that that overwhelming consensus that climate change is happening and that
man-made emissions are contributing to it, have changed.”283 At an
Environment and Public Works Committee hearing in February 2010,
Administrator Jackson told me that EPA accepted the findings of the IPCC
without any serious, independent analysis.284

Directly in line with Administrator Jackson’s points, the endangerment

finding states, “it is EPA’s view that the scientific assessments” of the IPCC
“represent the best reference materials for determining the general state of
knowledge of the scientific and technical issues before the agency in making
an endangerment decision.”285 In the finding’s Technical Support Document
(TSD), in the section on “attribution,” EPA claims that climate changes are
the result of anthropogenic greenhouse gas emissions and not natural forces.
In this section EPA has 67 citations, 47 of which refer to the IPCC.286

If there are any objective readers of this book who still give credibility to

the IPCC science on which the entire hoax is based and the basis of the
endangerment finding, then reading Appendix C is a must.

DR. ALAN CARLIN
Rewind for a moment to March 9, 2009, when Dr. Alan Carlin, a PhD
economist in EPA’s Office of Policy, Economics, and Innovation released a
key report that called into question the scientific process underlying the
agency’s proposed endangerment finding. According to Carlin, a thirty-eight-
year veteran of EPA, and a fellow agency employee:

We have become increasingly concerned that EPA and many other
agencies and countries have paid too little attention to the science of
global warming. EPA and others have tended to accept the findings
reached by outside groups, particularly the IPCC and the CCSP, as being
correct without a careful and critical examination of their conclusion and
documentation…We believe our concerns and reservations are
sufficiently important to warrant a serious review of the science by EPA
before any attempt is made to reach conclusions on the subject.287

But Carlin’s request was denied. In a series of emails, Al McGartland,
Carlin’s boss, forbade him from having “any direct communication” with
anyone outside of his office concerning his study. On March 16, Carlin tried
again, but McGartland made clear what his superiors thought of the report:
“The time for such discussion of fundamental issues has passed for this
round. The administrator and the [Obama] administration have decided to
move forward on endangerment, and your comments do not help the legal or
policy case for this decision… I can only see one impact of your comments
given where we are in the process, and that would be a very negative impact
on our office.” But that wasn’t all. McGartland also wrote to Carlin: “With
the endangerment finding nearly final, you need to move on to other issues
and subjects. I don’t want you to spend any additional EPA time on climate
change. No papers, no research etc., at least until we see what EPA is going
to do with Climate.”288 So much for transparency.

The endangerment finding was finalized December 7, 2009, just in time

for the Copenhagen climate conference.289 Cap and trade had failed, but
President Obama could still face world leaders armed with his backup plan.
The cost of “doing something” for the conference was high: the endangerment
finding, like cap and trade would cost American consumers around $300 to
$400 billion a year, significantly raise energy prices, and destroy hundreds of
thousands of jobs.

CRISIS OF CONFIDENCE IN THE IPCC
After Climategate there was an interesting reversal in the mainstream media:
all those outlets that had praised Al Gore and the IPCC to the heights just a
few years prior were suddenly tearing apart the IPCC’s assessments—and
more and more and more flaws came to light. When ABC News,290 the
Economist,291 Time,292 Newsweek,293 and the Financial Times294 —among
many others—reported that the IPCC’s research contains embarrassing flaws,
and that the IPCC chairman and scientists knew of the flaws, but published
them anyway—well, you have the makings of a major scientific scandal. In
the end, well over a hundred different errors in the IPCC science were
revealed in the wake of the Climategate email scandal.

One of the most publicized errors was, of course, IPCC’s claim that the

Himalayan glaciers would melt by 2035. It’s simply false, yet it was put into
the IPCC’s Fourth Assessment report. Here’s what we know:

•      According to the Telegraph, “the IPCC [has] since admitted it was

based on a report written in a science journal and even the scientist who
was the subject of the original story admits it was not based on facts.”295

•      “When finally published,” the Telegraph wrote, “the IPCC report did

give its source as the WWF study but went further, suggesting the
likelihood of the glaciers melting was ‘very high’.” (The IPCC, by the
way, defines this as having a probability of greater than 90%.)296

Time magazine, the very publication that once told us to be afraid— very
afraid of global warming, said that “Glaciergate,” was a “black eye for the
IPCC and for the climate-science community as a whole.”297

There was more. According to the Telegraph, Dr. Rajendra Pachauri, the

head of the IPCC, “was informed that claims about melting Himalayan
glaciers were false before the Copenhagen summit.298

So why was the Himalayan error included? We now know from the very
IPCC scientist who edited the report’s section on Asia that it was done for
political purposes—it was inserted to induce China, India, and other
countries to “take action” on global warming. According to the UK’s Sunday
Mail, Murari Lal, the scientist in charge of the IPCC’s chapter on Asia, said
“We thought that if we can highlight it, it will impact policymakers and
politicians and encourage them to take some concrete action.” In other
words, as the Sunday Mail wrote, Lal “admitted [the glacier alarmism] was
included purely to put political pressure on world leaders.”299

So what had the IPCC done to rectify this fiasco? I went into the IPCC
report to see if a correction had been made: the 2035 claim was still there.
Of course, there was a note attached and it said the following:

It has, however, recently come to our attention that a paragraph in the
938-page Working Group II contribution to the underlying assessment
refers to poorly substantiated estimates of rate of recession and date for
the disappearance of Himalayan glaciers. In drafting the paragraph in
question, the clear and well-established standards of evidence, required
by the IPCC procedures, were not applied properly.300

It turns out that the IPCC’s fourth assessment also found observed

reductions in mountain ice in the Andes, Alps, and Africa—all caused by, of
course, global warming. In an article titled, “UN climate change panel based
claims on student dissertation and magazine article,” the Telegraph reported:

… one of the sources quoted was a feature article published in a popular
magazine for climbers which was based on anecdotal evidence from
mountaineers about the changes they were witnessing on the
mountainsides around them. The other was a dissertation written by a
geography student, studying for the equivalent of a master’s degree, at the
University of Berne in Switzerland that quoted interviews with mountain
guides in the Alps.301

On top of this, we found that the IPCC was exaggerating claims about the

Amazon. The report said that 40 percent of the Amazon rainforest was
endangered by global warming. But, again, as we’ve seen, this was taken
from, yes, a study by the World Wildlife Federation, and one that had nothing
to do with global warming. Even worse, it was written by a green activist.

In the wake of the scandal, even my good friend Barbara Boxer was

careful about how she talked about the IPCC. As she said in a hearing on
EPA’s Budget on February 23, 2010, “In my opening statement, I didn’t quote
one international scientist or IPCC report…We are quoting the American
scientific community here.”

This was the “gold standard” of climate research; it was the body that was

awarded the Nobel Peace Prize in 2007. It obviously did not win a Nobel
science award.

ONLY A MATTER OF TIME
This crisis of confidence in the IPCC translates to a crisis of confidence for
EPA’s endangerment finding, which rests in large measure on the IPCC’s
conclusions. The endangerment finding’s scientific foundation has already
disintegrated and I believe it will only be a matter of time before the finding
itself follows suit.

Once I had it confirmed from Lisa Jackson that the EPA had relied on the
science of the IPCC to establish the endangerment finding, I asked the EPA
Office of Inspector General (OIG) in April 2010 to investigate the process

leading up to the endangerment finding to determine if EPA had come to that
conclusion properly. In September 2011, the OIG completed its report and
found that the EPA had not come to this conclusion properly—in fact, it found
that the scientific assessment underpinning the Obama EPA’s endangerment
finding for greenhouse gasses was inadequate and in violation of the
Agency’s own peer review process.302 The report calls the scientific
integrity of EPA’s decision-making process into question and undermines the
credibility of the endangerment finding.

The Inspector General’s investigation uncovered that the EPA failed to

engage in the required record-keeping process leading up to the
endangerment finding decision, and it also did not follow its own peer
review procedures to ensure that the science behind the decision was sound.
Regardless of what one thinks of the UN science, the EPA is still required—
by its own procedures—to conduct an independent review. Dr. Alan Carlin
is now vindicated as his concerns that EPA was relying too much on the
science of the IPCC, and that the Agency was not engaging in a rigorous
scientific process, turned out to be valid.

I was reminded of Jon Stewart’s warning “don’t cut corners!” when the

press began weighing in on the IG report. The headline from the AP was
“U.S. watchdog: EPA Took Shortcut on Climate Finding.”303 EPA
immediately responded saying, as it did during the Climategate scandal, that
this still does not affect the validity of the science, but Stewart’s admonition
of the scientists of Climategate applies to the EPA: if the Agency is so sure
the science is completely sound, why did they cut corners? Why can’t they be
transparent if there is nothing to hide?

EPA’s process to determine the endangerment finding was rushed, biased,

and appears to have been predetermined. Now that all of this has come to
light, the only conclusion is that the endangerment finding should be thrown
out, and if it is, it will be a tremendous victory for the American people.

7

COPENHAGEN

Speaking to reporters in Copenhagen

BACK INTO THE LION’S DEN
I was only in Copenhagen for three hours but they were the most exhilarating
three hours of my political life.

I landed at 7 a.m. on a particularly frigid morning to be holding a

conference on global warming. Because I was coming to Copenhagen as a
Senator in the minority party, on my own, and not part of any Congressional
Delegation, the UN Climate Conference would not give me a meeting room to
visit with UN officials or hold a press conference. But with the President
planning to make an appearance, over one hundred members of the press
were locked in a press room so I saw the perfect opportunity to deliver my

message. There was a staircase at the front of the press room which gave me
the best vantage point. Nothing was going on at the time: Al Gore and
Senator John Kerry had just left—so even if the press wasn’t very happy to
see me, at least my visit was some news. I know it sounds strange to say it
but the experience was really quite enjoyable. I will always remember all
those people in the room—hundreds of them—and all the cameras. And they
all had one thing in common: they all hated me.

Fox News had it right when they said I had just walked into the “lion’s

den”:

Inhofe is the second member of congress to arrive in Copenhagen. Kerry
addressed reporters last night and got a standing ovation inside the Bella
Convention Center. By contrast, Inhofe was mobbed inside the press
center and was not offered a speaking slot…Surrounded by reporters
from around the world, including many who believe global warming is
real, Inhofe often looked like a lamb on his way to slaughter.304

I was just happy to see that I had maintained my distinction of being the
“most dangerous man on the planet” after all these years. As Der Spiegel
would later write:

He only came for a few hours, but he was also decisively responsible for
the chaos that marked the negotiations: James Inhofe, a Republican
politician who does whatever he can in Washington to inhibit Obama’s
efforts to impose CO2 limits. He is not only ridiculous in describing
climate change as made up by “the Hollywood elite,” but outright
dangerous…Men like Inhofe, who in Copenhagen warned that nations
shouldn’t be “deceived into thinking the US would pass cap and trade
legislation,” have the effect of poison when it comes to the urgently
needed global trust-building.305

I still find it amazing that they thought me capable of single-handedly

destroying the planet.

ONE-MAN TRUTH SQUAD

Leading up to the conference, President Obama was trying to make it sound
like there was still hope that the United States would act on global warming.
On September 22, 2009, he made a point to highlight the efforts of cap and
trade legislation in Congress as if that were going to lead to something:

The House of Representatives passed an energy and climate bill in June
that would finally make clean energy the profitable kind of energy for
American businesses and dramatically reduce greenhouse gas emissions.
One committee has already acted on this bill in the Senate and I look
forward to engaging with others as we move forward.306

But by November 17, 2009, Wall Street Journal said it best in their

editorial entitled, “Copenhagen’s Collapse—The Climate Change Sequel Is a
Bust”:

“Now is the time to confront this challenge once and for all,” President-
elect Obama said of global warming last November. “Delay is no longer
an option.” It turns out that delay really is an option-the only one that has
world-wide support. Over the weekend Mr. Obama bowed to reality and
admitted that little of substance will come of the climate-change summit
in Copenhagen next month. For the last year the President has been
promising a binding international carbon-regulation treaty a la the Kyoto
Protocol, but instead negotiators from 192 countries now hope to reach a
preliminary agreement that they’ll sign such a treaty when they meet in
Mexico City in 2010. No doubt. The environmental lobby is blaming
Copenhagen’s pre-emptive collapse on the Senate’s failure to ram
through a cap and trade scheme like the House did in June, arguing that
“the world” won’t make commitments until the U.S. does. But there will
always be one excuse or another, given that developing countries like
China and India will never be masochistic enough to subject their
economies to the West’s climate neuroses. Meanwhile, Europe has
proved with Kyoto that the only emissions quotas it will accept are those
that don’t actually have to be met.307

Then as Anna Fifield of the Financial Times put it in a story called “U.S.

Senator Calls Global Warming a Hoax,” on December 5, 2009, even
President Obama had “conceded that the Copenhagen summit is not going to

result in a binding international treaty.”308 Yet he was still planning to
commit the United States to a 17 percent reduction in greenhouse gas
emissions from 2005 levels by 2020, which were the cuts prescribed in the
Waxman-Markey bill.

The President was clearly in a difficult position. He had failed even to
bring together his own party in the Senate to pass the cap and trade bill, so
there was no chance he would bring the world together to implement an
international treaty as he had once promised. For weeks there was plenty of
speculation as to whether President Obama would even attend in person due
to the fact that the effort was going to fail, but just days before the conference
we learned that the President would indeed be traveling to Copenhagen to
commit the United States to emissions cuts and deliver billions of taxpayer
dollars to the global warming effort.

I had the opportunity to debate the President’s plan with Ed Markey on

FOX News Sunday with Chris Wallace the weekend before the Conference:

CHRIS WALLACE, HOST: Senator Inhofe, in Copenhagen, the
president is reportedly going to pledge the U.S. will reduce greenhouse
gas emissions by 17 percent by the year 2020 and will contribute billions
of dollars to developing countries to help them reduce their emissions.
How much authority will the president’s pledge have?

SEN. JAMES INHOFE, (R-OK): Well, see, Chris, that’s the reason I’m
going, to make sure people in these other 191 countries know the
president can’t do that.

The initial reductions he’s talking about are what you find in Markey’s
bill, and that isn’t going to happen. And of course, that bill’s dead. It will
never even be brought up again.

And on top of that, he’s going to commit, I understand, to some $10

billion a year to the developing countries. Now, here’s China that holds
$800 billion of our debt and we’re going to give them $10 billion to stop
generating electricity? I don’t think that’s going to happen. Representative
Markey, however, was in denial. As he said:

REP. ED MARKEY, (D-MA): So as the president goes there, based
upon the Waxman-Markey bill, which has passed through the House of
Representatives, which is a 17 percent reduction of greenhouse gases by
2020—the bill that’s already passed through the Senate Environment
Committee, which is also a 17 to 20 percent reduction, combined with all
of the other activity that Senator Lindsey Graham, Senator Joe
Lieberman, Senator John Kerry…

WALLACE: Yeah, but none of this has passed.

MARKEY: Senator Susan Collins are all moving towards, there is real
momentum now building for a bipartisan bill to pass through the United
States Senate.309

Especially after that interview, I felt that I needed to travel to

Copenhagen, as the left was in full media spin cycle and clearly in denial.
But as I had predicted in a New York Times article in November, Senate
votes were scheduled on the healthcare bill conveniently at the time of the
Copenhagen conference, so that cast doubt on the attendance of any senators.
Senator Boxer cancelled her trip and instead gave a video address to the
conference. Given the significance of the votes on the healthcare bill and
believing my colleagues would not attend, I met with the rest of my Truth
Squad and we decided to cancel.

However, I found out later that Senator Kerry was going to sneak over to
the conference after all. Once I heard that, I knew I had to be there as well
because Senator Kerry would not tell them the truth. I immediately booked a
flight that would have me on the ground in Copenhagen for only three hours. I
scheduled it so that my message could be delivered and still be back in
Washington in time for some key votes.

And as I predicted, Senator Kerry claimed in an interview that I was

wrong and cap and trade still had a shot:

REPORTER: Senator Inhofe’s staff are here meeting with a number of
foreign delegations. He’s coming tomorrow. He’s giving a lot of
interviews to foreign media saying essentially this is going to be a repeat
of Kyoto, that we cannot pass this.

SENATOR KERRY: Well he’s wrong, he’s just dead wrong. He’s
wrong. We have a president of the United States that isn’t George Bush,
and we have a Senate that doesn’t reflect Senator Inhofe’s view. Senator
Inhofe does not accept the science.

REPORTER: But how might the rest of the world take that message?

SENATOR KERRY: I think everybody understands there are doubters
and skeptics and there are people, as I said, who accept the science. The
vast majority of people in the world who understand the science accept
it. And they want action. I don’t think he represents the majority. What he
represents is the fight that we have over these issues but I don’t think he
represents the majority on this issue.

REPORTER: The latest numbers show that people don’t believe—in
America—that this is as much a problem as they’ve been led to believe.
And that they don’t want to have to change the way they live…those
numbers are going south.

SENATOR KERRY: You don’t have to change the way you live.
Nobody is suggesting… nobody has to give up any comfort. Nobody has
to change the way they live. They can live better. They can live safer, live
healthier, live with more income in their household, live with less money
going to wasteful energy. This is a better road for the creation of jobs and
the security of our country and I believe that, again, that most Americans
understand that.310

Senator Kerry’s statement is interesting not only because it turned out that

he was the one who was “dead wrong” about cap and trade, but because it
also clearly exemplifies the crisis of messaging from which the global
warming alarmists continually suffered. It was almost impossible to keep
track of all the different ways global warming was sold: first they told us that
the planet was warming to a dangerous degree and we were going to be
wiping sweat off our brows in the middle of February; then the message was
that cold weather and every weather event was due to global warming; then
Al Gore asked us in his movie if we were ready to change the way we live,
while the Hollywood elite went on an extensive campaign to tell us to take

public transportation, avoid planes and cars, and change our light bulbs; then
the debate was no longer about saving the world but about transitioning to a
clean energy economy, achieving energy security, and reducing pollution;
then in one desperate final attempt, Senator Kerry tells us that we don’t have
to change the way we live or give up any comfort. In fact, according to
Senator Kerry, cap and trade will make us a more prosperous nation.

While my environmental friends changed their tune constantly throughout
the years to justify imposing the largest tax increase in American history, and
as each one of their rhetorical tactics failed with the American people, my
message has been exactly the same from the very beginning: the science is not
settled; their solutions—whether they be Kyoto, cap and trade, or global
warming regulations by the EPA—are only symbolic and will only be all
economic pain for no environmental gain; and as I said first in Milan and
again in Copenhagen, the United States would never ratify Kyoto or impose
cap and trade. On the steps of the press room, I told them the same thing I’ve
been saying all along:

I want to turn back the clock to December 2003, when the United Nations
convened the “9th Conference of the Parties” in Milan, Italy, to discuss
implementation of the Kyoto Protocol. At the time, I was leading the
Senate delegation to Milan as Chairman of the Senate Committee on
Environment and Public Works. Fast forward to December 2009: the UN
is holding its 15th global warming conference-and the delegates are
haggling over the same issues that were before them in 2003. I know this
because I was there. Recently, with the Copenhagen talks underway, I
reread the speech I delivered in Milan. I found that the issues at stake in
2003 are nearly the same as those in 2009. In short, nothing has changed
and nothing has been done.

So let’s go back to 2003. In my speech, I told the conference that the

Senate would not ratify Kyoto. Here’s what I said: “The Senate, by a vote
of 95 to 0, approved the Byrd-Hagel resolution, which warned the
President against signing a treaty that would either economically harm the
United States or exempt developing countries from participating.” I went
on to say this: “Both those conditions then, and still to this day, have not
been satisfied. So, it’s worth noting that even if President Bush wanted to
submit the treaty to the Senate, it couldn’t be ratified.” That was 2003.

Is that still true today? Of course it is. And yet here we go again:
China, India, and other developing countries want nothing to do with
absolute, binding emissions cuts. China and India have pledged to reduce
the rate of growth, or intensity, of their emissions. But that’s not
acceptable to the US Senate. Moreover, China is opposed to a mandatory
verification regime to prove it is actually honoring its commitments.

[…]

My stated reason for attending Copenhagen was to make certain the

191 countries attending COP-15 would not be deceived into thinking the
US would pass cap and trade legislation. That won’t happen. And for the
sake of the American people, and the economic well-being of America,
that’s a good thing.311

I may have been the most hated man at the conference, but I was the only
one that was willing to tell them the truth. As I joked later on Wolf Blitzer’s
show, “They’re in the middle of kind of a group therapy right now.”312

LEAVING THEIR FOOTPRINT
In the end, President Obama’s “agreement” was one that did not bind the
United States to any new targets or timelines, with no way to verify or
enforce any type of emission cuts. The entire effort had failed, and that was a
victory for the American people.

But as CBS reported, the United States made its mark in other ways:

Few would argue with the U.S. having a presence at the Copenhagen
Climate Summit. But wait until you hear what we found about how many
in Congress got all-expense paid trips to Denmark on your dime. CBS
investigative correspondent Sharyl Attkisson reports that cameras spotted
House Speaker Nancy Pelosi at the summit. She called the shots on who
got to go. House Majority Leader Steny Hoyer, and embattled Chairman
of the Tax Committee Charles Rangel were also there…

Senator Inhofe (R-OK) is one of the few who provided us any detail.

He attended the summit on his own for just a few hours, to give an
“opposing view.” “They’re going because it’s the biggest party of the

year,” Sen. Inhofe said. “The worst thing that happened there is they ran
out of caviar.”

Nobody we asked would defend the super-sized Congressional

presence on camera. One Democrat said it showed the world the U.S. is
serious about climate change. And all those attendees who went to the
summit rather than hooking up by teleconference? They produced enough
climate-stunting carbon dioxide to fill 10,000 Olympic swimming pools.
Which means even if Congress didn’t get a global agreement—they left
an indelible footprint all the same.313

8

THE ATTEMPTS TO RAISE CAP AND TRADE

FROM THE GRAVE

“One of the things I don’t think happens often enough in our society in part
because it doesn’t happen so often that we have public figures who stand up
who set their feet squarely forward and say this is nonsense. We have to be
fact based, we have to be rational and this nonsense has to end. James Inhofe
has been such a man over the past six to seven years. He sometimes stood
absolutely alone and was demonized, vilified, ridiculed by the national
media. He stands now in 2010 as a man utterly vindicated.”

—Lou Dobbs, February 24, 2010314

BY THE END OF 2009, cap and trade was dead and buried with so many nails
in the coffin, yet Senator Kerry, Senator Graham, and Senator Lieberman still
got their shovels out and started trying to dig it out in a fruitless attempt to
resurrect the bill.

In November 2009, even as Senator Boxer was marking up in Committee
her version of cap and trade, Kerry-Boxer, without any Republicans, Senator
Kerry was already in talks about introducing a separate cap and trade bill—
he called it a “dual track” measure.315 He undoubtedly saw the writing on the
wall that Kerry-Boxer was failing and he didn’t want to go to the UN
Copenhagen climate conference without a Plan B. So while Obama faced
national leaders with the endangerment finding in hand, Kerry came armed
with the promise of a bill that he would unveil with Senators Graham and
Lieberman. On December 10, 2009, the three presented not their bill but the
framework for a bill316 —it sounded just like the process for Kerry-Boxer.
They were trying to distract the public with multiple bills in a “smoke and
mirrors” campaign. Here we go again.

This tripartisan trio’s plan was to implement the same targets as the

Waxman-Markey bill: a 17 percent reduction by 2020. Their strategy right
away was to say that this was the “last call” before regulations would set in.
Of course, President Obama and EPA Administrator Lisa Jackson often
reminded us that regulations under the Clean Air Act would be much worse
than a cap and trade bill. Senators Kerry and Graham echoed this point in a
joint New York Times op-ed:

Failure to act comes with another cost. If Congress does not pass
legislation dealing with climate change, the administration will use the
Environmental Protection Agency to impose new regulations. Imposed
regulations are likely to be tougher and they certainly will not include the
job protections and investment incentives we are proposing.

The message to those who have stalled for years is clear: killing a

Senate bill is not success; indeed, given the threat of agency regulation,
those who have been content to make the legislative process grind to a
halt would later come running to Congress in a panic to secure the kinds
of incentives and investments we can pass today. Industry needs the
certainty that comes with Congressional action.317

And, as with Kerry-Boxer, they didn’t want to call it cap and trade, even

though that’s exactly what it was. As Steve Benen of the Washington
Monthly aptly put it:

Apparently, “cap and trade” no longer polls well. The White House
seems to now prefer “energy independence legislation.” These three
senators are using the phrase “market-based approach.” (“You remember
the artist formerly known as Prince?” Lieberman said. “This is the
market-based system for punishing polluters previously known as ‘cap
and trade.’”)318
More “smoke and mirrors.” Months later, Senator Reid reinforced this
statement saying that even though the bill does create caps on greenhouse
gases, he doesn’t like the term “cap and trade.” As he said, “We don’t use the
word ‘cap and trade.’ That’s something that’s been deleted from my
dictionary. Carbon pricing is the right term.”319

While Kerry-Boxer was branded as a “pollution reduction bill,” Kerry-
Graham-Lieberman was going to be “energy independence legislation” that
will “punish polluters”—only the polluters would not be punished; even by
President Obama’s own admission, it would be the American people who
would be punished. When the President said himself that under his plan of a
cap and trade system “electricity rates would necessarily skyrocket” he
explained that it was because power plants “would have to retro-fit their
operations. That will cost money. They will pass that money on to the
consumers.”320 No wonder they didn’t like to say cap and trade.

In fact, not only would the “polluters” not be punished, they were cutting

some pretty good deals for themselves.

THE APPEASERS
When I think of all the backroom deals that took place during the Waxman-
Markey and Kerry-Graham-Lieberman days, the maxim that comes to mind
is: “No man survives when freedom fails, the best men rot in filthy jails, And
those who cry, ‘appease, appease’ Are hanged by those they tried to please”
(Hiram Mann).

I remember so well the Hillary healthcare push of 1993—it was the first

attempt to impose socialized medicine. As I was flying back from D.C.
through Chicago, I was so pleased we had virtually won the fight and
defeated Hillary Care. I was on the plane reading the Wall Street Journal
where I saw a full page ad by the American Medical Association embracing
Hillary Care. Needless to say, I called the AMA from Chicago. They were
appeasing the enemy.

And the same thing was happening with many of the energy providers

during the cap and trade push.

Democrats were always so pleased with themselves when the corporate

heavyweights would come to the table, but I maintained that there was no
way they were doing this out of the goodness of their hearts or because they
wanted to be good stewards of the environment. They were in this because
they knew that they could profit if they played their cards right—and the
American consumer would have to foot the bill.

During the Waxman-Markey days, many of the utility companies thought

cap and trade was inevitable with Democrats controlling both houses of

Congress and the White House. So instead of fighting against the bill because
it was bad for jobs and the economy, they started negotiating with the enemy:
they were appeasers. But as what always happens to the appeasers, they are,
as the saying goes, hanged by those they try to please. And of course, that’s
exactly what happened in the deals that were made with Waxman-Markey.
One of the reasons they wanted to negotiate was because they felt that

regulations under legislation would be at least somewhat better than
regulations by the EPA under the Clean Air Act. They also wanted to get a
good deal on the allocation of carbon credits, which were limited. But in the
end, the electric utilities didn’t get what they wanted in the Waxman-Markey
bill—it sold the utilities short. And even worse, the Waxman-Markey bill did
not fully take away the threat of EPA regulations. It did have language to
restrict EPA’s ability to set National Ambient Air Quality Standards for
greenhouse gases but this restriction was on a limited time frame so it left the
door open for both legislation and EPA regulations.

When various companies came to the Senate to shop amendments, I said

the last thing I wanted to do was improve the bill. I wanted to kill the bill, so
I wasn’t much use to them.

Especially during Kerry’s last cap and trade push, many of the utilities,

which primarily used natural gas and nuclear energy as opposed to coal, saw
cap and trade as inevitable. When they realized that coal was getting some
extra provisions in the form of a mill tax to support clean coal technologies,
they wanted to carve out as many allowances as they could. Now no one is a
bigger supporter of natural gas and nuclear energy than I am; I truly believe
we need it all, so I told them that coal may be on the chopping block now—
as it was the industry that cap and trade proponents’ ultimately sought to
destroy—but you’ll be next.

So when the next Kerry-Graham-Lieberman proposal came along, I asked

these appeasers: do you think a cap and trade bill is good for the economy,
good for your members, good for workers, good for consumers? Don’t forget
what happened with Waxman-Markey: some utilities thought they had a deal,
but when the language was actually drafted, the deal made Waxman and
Markey happy, but not the utilities.

During the Kerry-Graham-Lieberman debate, one of the biggest

appeasers, as many will be surprised to realize was an oil company, BP. The
trio had what they thought was a clever plan to get Republicans over to their

side: they were going to put in provisions that would encourage offshore
drilling for oil and gas, increase nuclear power capacity, and encourage
clean coal technologies—and the promise of deals would bring industry to
the table. It was a classic divide and conquer strategy—a strategy that was
destined for failure because whenever you get votes on one side by using
these tactics, you always lose votes on the other side. Besides, even if you
were for offshore drilling, you still couldn’t get away from the fact that any
cap and trade bill would just mean more dependence on foreign oil, more
taxes, and fewer jobs. At the same time, President Obama’s FY budget
planned to impose well over $40 billion in new taxes on the oil and gas
industry, which would only discourage production.

The worst part about Kerry-Graham-Lieberman was that on top of the

higher electricity prices it would force on consumers, it also contained a gas
tax. This gas tax was supported by none other than BP and it was a great deal
for them:321 the American consumer, not the company, would be footing the
bill for this greenhouse gas regime—the same consumers were suffering from
high unemployment. A new tax was the last thing they needed. When it
became clear that the bill contained a gas tax, the Kerry-Graham-Lieberman
house of cards came tumbling down. The White House came out opposing it
and blamed the idea on Senator Graham.322 The sponsors and supporters of
the bill were left trying to explain how legislation that raises the cost of
gasoline for consumers, isn’t really a tax.

On April 19, 2010, I spoke with Jeanne Cummings of Politico about

Kerry, Graham, and Lieberman’s latest cap and trade effort.323 Jeanne
specifically focused on the Democrats’ aggressive effort to bring the energy
development industries to the table and asked if the trio will be successful
this time given that the deals they are offering were more generous now than
ever. I said that their strategy is flawed. They’ve tried it before and it didn’t
work: it’s called “divide and conquer” when they go to the various oil, gas,
nuclear, and coal industries and carve out special deals. I said that the
problem with that is you may be able to get some votes that way but you will
lose votes on the other side. At that time, ten Democrats had already signed a
letter telling the bill’s sponsors that if they add an offshore drilling provision
they will no longer support the bill. So with offshore drilling, they might pick
up four votes on one side but they lose ten on the other. I said that it would be

something that they’ll introduce on Earth Day and have a big celebration and
then it will fade away quickly.

Only a few days after my interview with Jeanne Cummings, BP, the

company that was supposed to help make the bill successful, was the very
company responsible for the oil spill that began in April 2010. Not
surprisingly, the provisions planned for expanding offshore drilling were
immediately abandoned and then an interesting shift in rhetoric took place:
whereas initially increasing offshore drilling was going to be a tool to help
pass the bill, after the spill, the message was that we had to pass this bill to
stop offshore drilling altogether.

In the wake of the spill, President Obama announced that now was the

time to put a price on carbon.324 People had died, people’s economic
livelihoods were at stake, and the environment was being harmed, yet the
President was taking this opportunity to push cap and trade. That’s when
President Obama began his moratorium on deepwater drilling—something
that environmental groups have sought for years. It was an exercise in
overreach that would do far more harm than good. The Louisiana Department
of Economic Development was estimating that thousands of good paying jobs
would be killed in the moratorium’s wake. It was a time when we should
have been concentrating on putting a cap on the spill, not putting a cap on the
economy.

After the drilling provisions were taken out, Senator Graham could no

longer support the bill—so their divide and conquer strategy even lost them a
former sponsor. The bill that was, from then on, known as Kerry-Lieberman
was well on its way to failure.

Whereas Jeanne Cummings of Politico wanted to know if cap and trade
had a better shot with the drilling provisions, Stuart Varney of Fox News, in
an interview in May 2010, wanted to know if the bill had a better shot now
that drilling provisions had been taken out. I said no, it is still dead and for
the same reasons. Without the drilling provisions, they would just lose those
members who supported offshore drilling and they’d be right back where
they started.

The reaction from the environmental community to the spill in the Gulf

was not unlike their reaction during the time of the Exxon-Valdez incident in
1989. At that time, I was serving on two committees in the U.S. House of
Representatives that investigated the oil spill. Four days after the tragedy, I

spoke to the U.S. Chamber of Commerce in Washington and was taken aback
by environmental activists who were celebrating. They were saying that they
were glad the accident happened because they could parley this tragedy into
stopping development and exploration of the Arctic National Wildlife
Refuge. The same thing was happening with the BP oil spill: they were trying
to parley this into killing all kinds of offshore drilling. Of course, the Exxon-
Valdez incident was a transportation accident, and decreasing offshore
production would only increase our dependence on foreign oil, which would
increase transportation of oil and therefore increase the possibility of another
oil spill. But that wasn’t the point—they just want to stop drilling altogether.

To all those who still after all this time thought that cap and trade was

inevitable, I reminded them that opposition has only grown stronger and more
intense. They could have all the backroom secret deals they wanted to try to
get to sixty votes, but try as they might, it was never going work. If we look
back on the votes in the Senate, it is overwhelmingly clear that support for
cap and trade over that time has dropped considerably. In 2003, they got
forty-three; in 2005, they got thirty-eight; and in 2008, with Lieberman-
Warner, they got forty-eight. But let’s not forget that just after the cloture vote
on Lieberman-Warner, ten Democrats, nine of whom voted for cloture, very
quickly sent a letter stating that they could not vote for Lieberman-Warner “in
its current form.” So subtract nine, and you get thirty-nine votes. That’s a far
cry from sixty. There was no way Kerry-Lieberman was going to pass.

CAP AND TRADE IS DEAD BUT THE ENDANGERMENT
FINDING IS ALIVE AND WELL
Later in June, even as Kerry and Lieberman were still trying to dig cap and
trade out of the grave, the attention began to shift to stopping EPA’s
greenhouse gas regulations from taking effect.

Here’s where the idea of playing “Russian roulette” with regulations, as

the Wall Street Journal brilliantly pointed out a year earlier, comes in.
Democrats’ mantra was the EPA regulations would be so much worse and
much more expensive than legislation. We had managed to kill cap and trade
handily, so now Democrats were going to be responsible for some of the
messiest, most expensive and most onerous regulations in American history.
With elections coming up, many of them were having second thoughts.

As I mentioned before, one of the reasons EPA regulation of the

greenhouse gases would be such a disaster is that the Clean Air Act contains
very specific emissions thresholds for regulated pollutants. Under a program
to maintain air quality, facilities that emit 250 tons or more per year of a
given pollutant must obtain a Prevention of Significant Deterioration, or
PSD, permit before they can build or make major modifications to existing
facilities. Two hundred and fifty tons is a big number for traditional
pollutants such as sulfur dioxide or nitrogen oxide, but not for greenhouse
gases. A large commercial building, for example, emits about 100,000 tons
of CO2 a year. We’re talking about six million sources potentially subject to
EPA regulation. To get around this unmitigated administrative and economic
disaster, EPA just decided to ignore the law by instituting the “tailoring rule.”
That’s right: it randomly decreed that regulations would apply only to
facilities that emit more than 100,000 tons. That threshold would be tweaked
over time and apply to sources at differing stages. But the Clean Air Act is
clear. EPA can’t just change a law clearly laid out by Congress. Two hundred
and fifty tons is two hundred and fifty tons.

What will be the results of this? Imagine heading to church on Sunday to

find the doors locked because it couldn’t afford to install Best Available
Control Technology to reduce its greenhouse gas emissions. Of course, EPA
dismisses this and similar examples as nothing more than empty scare tactics.
They contend that they have already exempted smaller entities through the
tailoring rule so no one has to worry.

Not so fast. EPA’s so-called tailoring rule is now being challenged with
several lawsuits. It is very likely that the D.C. Circuit will overturn it and
force EPA to grapple with the regulatory nightmare of its own creation. If the
tailoring rule is thrown out—and almost everything is regulated by the EPA
including farms, churches, coffee shops, and restaurants—what will be the
economic impacts? According to EPA’s own documents, PSD permits cost an
average of $125,120 and impose a burden of 866 hours on the applicant. In
addition, the nation’s largest employers, such as refineries, electric utilities,
and industrial manufacturing facilities, will be forced to install (currently
undefined) best available control technology (BACT) at their plants to
reduce CO2. EPA has also admitted that if the tailoring rule does not hold up
in court, they will have to hire 230,000 new employees and spend an
additional $21 billion to implement their greenhouse gas regime.325

THE MURKOWSKI RESOLUTION AND ROCKEFELLER
COVER VOTES
In June of 2010, Senator Murkowski introduced a resolution that would
prevent the EPA from regulating greenhouse gases under the authority of the
Clean Air Act. The resolution would allow Congress to overturn regulations
from the executive branch by gaining a majority in both the House and the
Senate.

At a press conference to discuss the resolution, several Republicans came
together in opposition to EPA’s greenhouse gas regime, even Senator Graham
who still believed in his heart that man-made greenhouse gases were leading
to catastrophe. I was on the other end of the spectrum as the one who said
that man-made catastrophic global warming is the greatest hoax ever
perpetrated on the American people. Everyone else who was standing up
there with me and Senator Graham were somewhere in between. But we all
agreed on one thing: EPA regulation of greenhouse gases would be a huge
disaster. It would hand the agency the greatest regulatory power in history.

When Senator Murkowski first brought up the idea of a resolution, it

seemed unlikely that she would reach a majority in a Democrat run Senate.
But later it became clear that a lot of members who have elections in 2010 or
2012 wouldn’t want to go back to their constituents and say “Look at me.
Aren’t you proud? I allowed the most massive government take-over of every
aspect of our economy to take place”—and this is not to mention all for
nothing as it would have no impact on the climate.

Of course, after the oil spill, the Democrat talking point was that the

Murkowski resolution was a “Big Oil bailout” that will allow oil companies
such as BP to pollute the air. They were exploiting the tragedy in the Gulf to
advance a political agenda rooted in the belief that fossil fuels are a
destructive nuisance that must be eradicated. And it sees an unrelenting
bureaucracy and regulation as the means of realizing a future without them.
But that belief, if carried out in the form of EPA’s impending greenhouse gas
regulatory regime, will mean a radical change to our way of life—a change
that will mean fewer jobs, fewer American businesses, higher taxes, energy
rationing, and more control of people’s lives by a massive, unforgiving
bureaucracy in Washington. As with healthcare, the Obama Administration,
through the endangerment finding, was and still is on the verge of taking over
yet another facet of our economy. Murkowski’s resolution presented a

fundamental challenge to that view, as it would have prevented EPA from
realizing their radical agenda.

Democrats were clearly in a difficult position: if they overturned EPA
greenhouse gas regulations, it would be a huge blow to President Obama
who would be forced to veto a resolution coming out of a Democrat
controlled Senate; but if they allowed them to go unchallenged, many
Democrats worried that they would lose their jobs. In an effort to avoid an
unpleasant situation for the President, Senator Reid promised moderate
Democrats that they would have the opportunity to vote on a bill sponsored
by Senator Rockefeller, which would provide a two-year delay of EPA’s
regulations if they agreed to vote against Murkowski’s resolution. On June 9,
an article in The Hill explained how things were panning out:

Democratic leaders are scrambling to prevent the Senate from delivering
a stinging slap to President Barack Obama on climate change. They have
offered a vote on a bill they dislike in the hopes of avoiding a loss on
legislation Obama hates. The president is threatening to veto a resolution
from Sen. Lisa Murkowski (R-Alaska) that would ban the Environmental
Protection Agency (EPA) from regulating carbon emissions. But if the
president were forced to use his veto to prevent legislation emerging
from a Congress in which his own party enjoys substantial majorities, it
would be a humiliation for him and for Democrats on Capitol Hill. So
Senate Majority Leader Harry Reid (Nev.) and other Democratic leaders
are doing what they can to stop it. They are floating the possibility of
voting on an alternative measure from Sen. Jay Rockefeller, a Democrat
from the coal state of West Virginia, which they previously refused to
grant floor time, Senate sources say.326
Seven Democrats took the deal offered to them by Senator Reid and did

not vote for the Murkowski resolution. The vote was on June 10, 2010, and it
was defeated by 47–53. If these seven senators had voted for the motion to
proceed to Senator Murkowski’s resolution, the motion to proceed not only
would have passed, it would have passed handily. The whole plan was
transparent: Democratic leaders, in order to ensure that EPA can
micromanage farms and other sources, had to develop a scheme to give cover
to Democrat members who opposed the EPA takeover. Those seven members
were clearly conflicted. They understood the economic harm that an

unfettered EPA bureaucracy could mean for their constituents: fewer jobs,
more regulations, higher taxes, and a slower economy. But they were
pressured by the president and the base of the Democratic Party—they were
warned against defying the president on one of his top initiatives. So they
turned to the Rockefeller bill as an alternative. It was the two-year delay of
Rockefeller—rather than overturning the endangerment finding—that seemed
more politically acceptable.

On the Republican side, every Republican Senator voted for the

Murkowski Resolution, which means that the Senate Democrats are solely
responsible for whatever regulations the EPA implements. Every lost job,
every closed factory, every increase in utility bills or gasoline prices due to
the EPA’s greenhouse gas regulations are the responsibility of Senate
Democrats.

The defeat of the Murkowski resolution wasn’t the end of the road. I said
that if we ever get a vote on the Rockefeller bill, I trust these seven members
—and possibly others who voted “no” on the motion to proceed to
Murkowski—will vote with their constituents for Rockefeller and against
EPA taking jobs, businesses, and energy out of our struggling economy. Even
with the promise a vote on his two-year delay, Rockefeller voted for the
Murkowski resolution. Interestingly the vote on Rockefeller didn’t come up
until the next Congress when we forced Reid to face the problem again.

“LAST CALL”: 2010 ELECTIONS
With the 2010 election looming, the supporters of cap and trade knew that
Republicans were going to make significant gains in the next Congress and
time was running out. As Senator Kerry put it, it was the “last call” to pass
cap and trade.

The town hall meetings in 2009 and 2010 were not just about the passage
of Obamacare; they were also about cap and trade. In fact, cap and trade is
one of the main reasons that Democrats had so many losses in the House. The
2010 elections also brought an interesting reversal in campaign strategies for
Republicans. While in the 2008 elections, few candidates would dare to
question the science behind man-made catastrophic global warming, in 2010,
many Republicans not only campaigned against cap and trade but also
established themselves strongly as skeptics. Americans voted the global

warming advocates out and the skeptics in. And the same thing is happening
as the 2012 elections approach.

With the Murkowski Resolution, the excuse of many Senators was that

even if they voted to rein in the EPA, the resolution would die in a Democrat-
led House. After the 2010 elections, they could no longer use this excuse.

UPTON-INHOFE: THE ENERGY TAX PREVENTION
ACT OF 2011
In March of 2011, I introduced S.482, the Energy Tax Prevention Act of
2011, along with Congressman Fred Upton, Chairman of the Energy and
Commerce Committee, who has introduced the same bill in the House. Like
the Murkowski Resolution, the Energy Tax Prevention Act stops the Obama
EPA’s backdoor cap and trade regulations from taking effect: it protects jobs
in America’s manufacturing sector; protects consumers from higher energy
costs; puts Congress in charge of the nation’s climate change policies; and
ensures that the public health provisions of the Clean Air Act are preserved.

On February 9, 2011, I was privileged to testify to the House

Subcommittee on Energy and Power on the Energy Tax Prevention Act of
2011. Just as Democrats called the Murkowski resolution the “Big Oil
Bailout,” at the hearing, Representative Henry Waxman (D-California) said
that our bill should be called the “Big Polluter Protection Act.”327 He went
on to say that it would “repeal the only authority the administration has to
protect our health and the environment without providing any alternative.”328
Here we go again. Contrary to Representative Waxman’s claims, our bill
leaves all the essential provisions of the Clean Air Act intact. It simply
prevents the EPA from regulating greenhouse gases which are not harmful to
human health. Imposing energy taxes through EPA’s cap and trade regulations
and blocking economic development won’t make Americans healthier—it
will only mean fewer jobs, a higher cost of living, and less growth and
innovation.

Amy Harder of the National Journal in an article called “Reid Might Be

Forced to Deal with His EPA Problem,” wrote:

It wasn’t supposed to go this way if you’re a Democrat.

The 111th Congress was supposed to enact comprehensive climate
change legislation as President Obama prodded action with looming
carbon rules.

Instead, efforts to price greenhouse-gas emissions collapsed in the

Senate last year and left Obama with his hands tied around his climate-
change regulations.

Obama continues to publicly support the regulations and has punted the

debate down Pennsylvania Avenue to Congress.329

In April 2011, Senate Minority Leader Mitch McConnell offered the

Energy Tax Prevention Act as an amendment, which finally forced Reid into
action on allowing a vote on the Rockefeller bill—a two year delay—which
was also offered as an amendment. Senator Max Baucus also joined in the
“cover vote” fray and offered an amendment that stipulates that only the
industries would be regulated and not farms and ranches.

The vote on these amendments was a moment of truth for Obama’s job-

killing greenhouse gas regulations: sixty-four senators voted that day, in
various ways, against EPA’s cap and trade agenda. Each one chose whether
to take the “cover vote” or actually to vote with their constituents for the only
real solution to the problem: the Energy Tax Prevention Act. The other
amendments only delayed or put some limitations on how EPA can regulate.

During the debate, most members publicly aligned themselves with

concerned constituents, especially manufacturers or farmers, who oppose
EPA’s greenhouse gas regulations. Democratic Senator Sherrod Brown, for
example, wrote a letter to EPA Administrator Lisa Jackson in February,
arguing that “any approach to reducing greenhouse gas emissions must
recognize the unique situation of energy-intensive manufacturers.” Of course,
EPA’s regulations don’t, and can’t: “It is disconcerting,” the senator wrote,
“that, to my knowledge, the EPA has neither a plan in place nor the authority
to provide these protections to U.S. manufacturing, a sector of the economy
critical to the continued economic recovery of my state and so many
others.”330

Well put. Yet Senator Brown voted against the Energy Tax Prevention Act,

the only solution that would fully address the aforementioned concerns.
Delays, carve-outs, and exemptions won’t solve the underlying problem:

now he and all the others who did not vote to stop EPA will have to explain
why they stood by and let it happen.

I was very pleased that the House handily passed the Upton-Inhofe bill
with overwhelming bipartisan support. In the Senate, we are still ten votes
short, but with sixty-four members sending a clear message to the
administration that the Obama-EPA needs to be reined in, we will continue
fighting. At present, the Senate Democrats have a majority of three, but there
are twenty-three Democratic senators up for re-election in 2012. I call
eleven of those Senators an “endangered species.” Several have announced
they will retire rather than run again. I fully expect to be Chairman of the
Environment and Public Works Committee again and I hope it may just be a
matter of time until we can declare final victory, by stopping EPA from
regulating greenhouse gases.

“CAP AND TRADE IS DEAD. LONG LIVE CAP AND
TRADE”
In November of 2010, they finally admitted what I had been saying since
2003: Senator Kerry conceded that cap and trade is dead and Senator
Lieberman said “whether we like it or not, cap and trade has no chance of
passage in the next Congress…And so we’ve got to find separate ways to go
at it.”331

So, just as Lieberman said, they have since been trying to find separate
ways to go at it—that is, to put a price on carbon. One of these ways has
been to claim that America has been falling behind other nations in the “clean
energy race.” That was exactly the message in President Obama’s State of the
Union address in January 2011. He declared that this is our “Sputnik
moment” and that the same progress that we achieved in space technologies
can be achieved if we invest in clean energy technologies:

This is our generation’s Sputnik moment. Two years ago, I said that we
needed to reach a level of research and development we haven’t seen
since the height of the Space Race. And in a few weeks, I will be sending
a budget to Congress that helps us meet that goal. We’ll invest in
biomedical research, information technology, and especially clean energy
technology—an investment that will strengthen our security, protect our
planet, and create countless new jobs for our people.332

In this speech, the president set the goal of having 80 percent of America’s
electricity come from clean energy sources by 2035 and it later became clear
that he wanted to do this by establishing a “Clean Energy Standard” (CES).
Under a CES, utilities would be required to trade and sell clean energy
credits, increasing the mandate required until 80 percent of electricity needs
are met by clean energy. As is explained in the Obama Administration
Economic Report:

To meet this goal, the Administration is proposing a Clean Energy
Standard (CES) that would require electric utilities to obtain an
increasing share of delivered electricity from clean sources—starting at
the current level of 40 percent and doubling over the next 25 years.
Electricity generators would receive credits for each megawatt-hour of
clean energy generated; utilities with more credits than needed to meet
the standard could sell the credits to other utilities or bank them for future
use.333

This plan was just cap and trade turned inside out, and it was so short

lived that it barely even made the political radar before it was dead—in fact,
it was dead on arrival in Congress. Kim Strassel of the Wall Street Journal
put it well when she wrote just days after the President’s speech:

Listen carefully to Mr. Obama’s speech and you realize he spent plenty of
it on carbon controls. He just used a different vocabulary. If the president
can’t get carbon restrictions via cap and trade, he’ll get them instead with
his new proposal for a “clean energy” standard. Clean energy, after all,
sounds better to the public ear, and he might just be able to lure, or
snooker, some Republicans into going along.334

Of course, President Obama had been pushing this “Sputnik moment”

since the beginning. In his 2009 State of the Union address, he held up China
as an example of how other countries are taking greater strides than the
United States on clean energy:

We know the country that harnesses the power of clean, renewable energy
will lead the 21st century. And yet, it is China that has launched the
largest effort in history to make their economy energy efficient … Well I

do not accept a future where the jobs and industries of tomorrow take
root beyond our borders—and I know you don’t either. It is time for
America to lead again.335

In December 2010, my Environment and Public Works Committee staff

released a report that showed that the so-called “clean energy race” between
the United States and China—and the lament that America is losing—is an
idea concocted by activists to promote cap and trade, renewable energy
mandates, and greater government control of the economy.336 It is premised
on a biased, narrow picture of China’s energy development and the
demonstrably false notion that economic growth and innovation are best
realized through government mandates. If China is embracing anything, it is
the reality that fossil fuels, along with nuclear power, are the engines of
economic growth and prosperity.

Although the CES was gaining no traction whatsoever, President Obama
announced at Penn State that as part of his clean energy push, “So you show
us the best ideas to change your game on the ground; we’ll show you the
money. We will show you the money.”337

Well the solar company, Solyndra, was one of those companies that was

shown the money at the beginning of Obama’s presidency, as it was a
recipient of the stimulus funds under the American Recovery and
Reinvestment Act. At the time, Solyndra was touted as the model to follow
by the Administration for its green energy economy, and it was hailed as one
of the Recovery Act’s biggest success stories. As President Obama said in a
speech:

When it’s completed in a few months, Solyndra expects to hire a thousand
workers to manufacture solar panels and sell them across America and
around the world […] It’s here that companies like Solyndra are leading
the way toward a brighter and more prosperous future.338

Vice President Joe Biden said that the loan guarantee to Solyndra was an

“unprecedented investment this Administration is making in renewable
energy and exactly what the Recovery Act is all about” and Energy Secretary
Steven Chu said that it was part of “a broad, aggressive effort to spark a new
industrial revolution that will put Americans to work.”339

Now as the country struggles with unemployment and Solyndra has
completely collapsed and gone bankrupt, it is clear that the President’s
policies of “showing the money” has meant throwing the money away. In the
end, Solyndra is more than just a bankrupt company: it is a metaphor for the
failure of Obama’s war on affordable energy and American fossil fuel jobs.

A REAL SOLUTION TO JOBS AND ENERGY SECURITY
In March 2011, in the midst of this “Sputnik moment” clean energy economy
push that was clearly failing, the Congressional Research Service report that
first surfaced in 2009 was updated and it shows us again that America’s
combined recoverable natural gas, oil, and coal endowment is the largest on
Earth. In fact, our recoverable resources are far larger than those of Saudi
Arabia, China, and Canada combined.

While the Obama Administration goes forward with a conscious policy
choice to raise energy prices, accomplished in good measure by restricting
access to domestic energy supplies, we find out that those supplies are,
according to the Congressional Research Service, the largest on Earth.
Here’s what CRS says about America’s tremendous resource base:

Oil
CRS offers a more accurate reflection of America’s substantial oil resources.
While America is often depicted as possessing just 2 or 3 percent of the
world’s oil—a figure which narrowly relies on America’s proven reserves
of just 28 billion barrels—CRS has compiled U.S. government estimates
which show that America, the world’s third-largest oil producer, is endowed
with 163 billion barrels of recoverable oil. That’s enough oil to maintain
America’s current rates of production and replace imports from the Persian
Gulf for more than fifty years.

Natural Gas
Further, CRS notes the 2009 assessment from the Potential Gas Committee,
which estimates America’s future supply of natural gas is 2,047 trillion cubic
feet (TCF)—an increase of more than 25 percent just since the Committee’s
2006 estimate. At today’s rate of use, this is enough natural gas to meet
American demand for ninety years.

Coal
The report also shows that America is number one in coal resources,
accounting for more than 28 percent of the world’s coal. Russia, China, and
India are in a distant second, third, and fifth, respectively. In fact, CRS cites
America’s recoverable coal reserves to be 262 billion short tons. For
perspective, the United States consumes just 1.2 billion short tons of coal per
year. And though portions of this resource may not be accessible or
economically recoverable today, these estimates could ultimately prove to be
conservative. As CRS states: “…U.S. coal resource estimates do not include
some potentially massive deposits of coal that exist in northwestern Alaska.
These currently inaccessible coal deposits have been estimated to be more
than 3,200 billion short tons of coal.”

Oil Shale
While several pilot projects are underway to prove oil shale’s future
commercial viability, the Green River Formation located within Colorado,
Wyoming, and Utah contains the equivalent of 6 trillion barrels of oil. The
Department of Energy estimates that, of this 6 trillion, approximately 1.38
trillion barrels are potentially recoverable. That’s equivalent to more than
five times the conventional oil reserves of Saudi Arabia.

America’s newly tapped shale deposits, such as the Marcellus in

Pennsylvania, the Barnett in Texas, the Haynesville in Louisiana, and the
Woodford in Oklahoma have added hundreds of trillions of cubic feet of
natural gas to our recoverable resource base. Recent estimates of the Bakken
shale suggest that North Dakota could become second only to Texas as the
nation’s largest producer of crude. And thanks to the Canadian oil sands, the
EIA has recently estimated that our neighbors to the north posses a mammoth
178 billion barrels of oil reserves—second only to those of Saudi Arabia.

Methane Hydrates
Although not yet commercially feasible, methane hydrates, according to the
Department of Energy, possess energy content that is “immense…possibly
exceeding the combined energy content of all other known fossil fuels.”
While estimates vary significantly, the United States Geological Survey
(USGS) recently testified that: “the mean in-place gas hydrate resource for
the entire United States is estimated to be 320,000 TCF of gas.” For

perspective, if just 3 percent of this resource can be commercialized in the
years ahead, at current rates of consumption, that level of supply would be
enough to provide America’s natural gas for more than four hundred years.

Instead of continuing down the failed path of this job-killing global

warming green agenda, we could help bring affordable energy to consumers,
create new jobs, and grow the economy if the Obama Administration would
simply get out of the way so America can realize its true energy potential.340

CAP AND TRADE IS DEAD
Even with the final demise of the Kerry-Lieberman bill, it is important to
remember that the fight is not over: Moveon.org, the Hollywood elite and the
anti-sovereignty internationalists are still out there and working overtime to
implement their agenda. But as far as cap and trade as legislation is
concerned, Stuart Varney put it well in our May 12, 2010, interview when he
said, after I told him that Kerry-Lieberman didn’t stand a chance, “There you
have it: Last word on that: It’s dead.”

EPILOGUE

GETTING OUR COUNTRY BACK ON TRACK

ON JULY 1, 2011, one of the headlines in the Tulsa World read, “Inhofe
believes swimming in Grand Lake cause of his illness.”341 In fact, I was sure
of it: I saw green algae in Grand Lake earlier that week while I was
swimming and a few days later, when I was back in Washington, I became
deathly ill and had to fly back to Tulsa to recover. I joked about what the
headlines would be the next day: “The environment strikes back” or “Inhofe
attacked by the environment at last.”

The Sierra Club’s reaction was great: they sent me a rose and a get-well-
soon card with the message, “We hope you have a speedy recovery and that
we can work together to ensure all of our nation’s lakes are safe for
swimming, drinking and fishing.”342 I really appreciated their humor and
even brought the card to show everyone at our next Environment and Public
Works Committee hearing.

Reprinted with permission.

The jokes continued when I had to miss a big conference on climate
change sponsored by the Heartland Institute due to my algae illness. As
Stephen Stromberg of The Washington Post wrote in a piece called “A
Funny Thing Happened at the Climate Denier Conference.”

This is one of the most unintentionally hilarious turns of phrase I’ve seen
in a while. On Thursday, Sen. Jim Inhofe (R-Okla.) was supposed to
deliver the opening keynote address at the Heartland Institute’s sixth
International Conference on Climate Change, a conclave committed to
“abandoning the failed hypothesis of man-made climate change.” That is,
until he sent this statement to the conference’s organizers:

“I am sorry that I will not be able to join you today at the Heartland

Institute’s sixth International Conference on Climate Change.
Unfortunately, I am under the weather, but I did want to send a short note
to say thank you for all of your hard work and dedication.”

That must be some extreme weather.343

It was indeed extreme—I don’t think I’ve ever been so sick, but not too
sick not to address the conference from afar. I reminded them that the last
time this conference was held was just weeks after the House of
Representatives passed the Waxman-Markey global warming cap and trade
bill. With an overwhelming Democratic majority in the Senate, many
predicted that the bill would sail through the Senate and be signed into law
by President Obama in time for the UN Climate Conference in Copenhagen.

But, we succeeded in defeating the bill by exposing the huge costs that

would be imposed on the American people for no environmental gain. I said
that Senate Democrats would not be able to go back to their constituents and
say aren’t you proud of me? I just voted for the largest tax increase in
American history. Further undermining their effort was the latest science
which showed that there was no “consensus” on global warming.

I had a similar message for the UN climate conference held in Cancun in
December 2010, which I delivered via a YouTube address from Washington.
I said, “The fact is, nothing is going to happen in Cancun this year and

everyone knows it. I couldn’t be happier and poor Al Gore couldn’t be more
upset: it has been widely reported that he is ‘depressed’ about Cancun.”344

CLIMATEGATE 2.0: CRISIS OF CONFIDENCE IN THE
IPCC CONTINUES
Of course, nothing happened at the UN global warming conference in
December 2011 in Durban, South Africa, either.

But true to form, just in time for the conference, the IPCC was at it again,

with the release of the Summary for Policymakers for its latest report on
extreme weather and global warming. But this time it faced an increasingly
skeptical public. In 2007, the IPCC’s so-called “smoking gun” report, which
claimed that there was an unequivocal link between humans and catastrophic
global warming, was all anyone ever talked about. On November 18, 2011,
when the discredited IPCC released its Summary for Policymakers, nobody
except me and Representative Markey even noticed.

If the IPCC wasn’t discredited enough, another batch of Climategate

emails, now known as Climategate 2.0, also surfaced on November 22, 2011,
just weeks before the UN climate conference in December. These emails
show more of the same manipulation of data and politicization of the science
by scientists contributing to the IPCC that was revealed in the original
Climategate scandal. One particular email with the title “inhofe & mann &
me” from journalist Andrew Revkin to Stefan Rahmstorf, a lead author of the
IPCC Fourth Assessment Report, shows an interesting behind-the-scenes take
on how they felt about my message. As Rahmstorf writes, “Hi Andy, from
over here, it is hard to see this kind of Inhofe speach [sic] as anything else
than an irrelevant piece of absurd theatre. It doesn’t even bother me any more
—he’s simply lost it. Cheers, Stefan”

This email was written on September 26, 2006, and the speech to which

Rahmstorf is referring is my speech on the “Hot and Cold Media Spin Cycle”
which called out the media for using global warming hysteria to sell news,
and challenged them instead to take an objective approach to the science.

Revkin replies, “I know, but he still speaks to and for a big chunk of

America—people whose understanding of science and engagement with such
issues is so slight that they happily sit in pre-conceived positions.”

If Revkin is right about one thing in this email, it is that I speak to and for
a big chunk of America. Remember after I delivered that speech in 2006, my
Committee office was inundated with calls from Americans thanking me for
having the voice of reason amid all the hysteria. But my ability to reach a
wide audience is not due to Americans having only a “slight” understanding
of the science as Revkin alleges. It is due to the fact that American people
understand all too well the politicization of the science, the manipulation of
the data, and the “tricks” implemented by many IPCC scientists to come to
preconceived solutions.

The IPCC can only blame itself for its irrelevance today and everyone
knows it, including many in the liberal media. As Joe Romm of Climate
Progress said about these emails being released just before the UN climate
conference in Durban, “It’s so refreshing that anybody thinks those climate
talks actually matter.”345

WHAT WILL THEY TRY NEXT?
Of course, Al Gore and Big Green may be down at the moment but they are
not out. These global warming alarmists have not given up their efforts to
continue to push their agenda. At the climate conferences in Cancun and
Durban, some leaders were stepping up their attacks on capitalism and
United Nations officials were saying they need to do more to “spread the
wealth around.”

It was just more of the same, only now they no longer have global

warming hysteria to back it up. So what will they do next to try to implement
their green regime?

Looking back, it is crystal clear that this debate was never about saving

the world from man-made global warming; it was always about how we live
our lives. It was about whether we wanted the United Nations to “level the
playing field worldwide” and “redistribute the wealth.” It was about
government deciding what forms of energy we could use.

From the alarmism to their so-called solutions, the issue has always pitted

big government supporters against strong individualism. That mindset has
always led me to believe that the liberal agenda pushing the global warming
movement will never win. But that doesn’t mean they won’t keep on trying.

The first big question they will be forced to answer: will they keep Al

Gore, Rajendra Pachauri, and James Hansen as the three faces of the
movement? While the trio did wonders in catching the media’s attention
during the glory years of alarmism, each has faced their own troubles ever
since. Al Gore’s problems are obvious, but there have also been calls for
Pachauri to resign as head of the IPCC. Hansen also faces questions from his
own side because of his environmental activism and his inconvenient
campaign against cap and trade.346

The other big question they will be forced to answer is: will they stay on
track with their green energy future talking points or go back to try and scare
the American people through alarmism again?

They tried alarmism in October 2010, and it failed miserably. A group

called 10:10 released a film, No Pressure, that featured children who don’t
believe in global warming being blown up by their teacher. The group said
that it was meant to be funny, but it created so much outrage with the public
that the video had to be immediately pulled down from the organization’s
Web site. It was the most outrageous, last-ditch effort to scare little kids into
thinking that they could be killed if they don’t believe what they’re told to
believe.

And it was no amateur production team. As the UK Guardian reported,
the film featured “film star Gillian Anderson and England footballer Peter
Crouch, with music donated by Radiohead and shot by a forty-strong
professional film crew led by director Dougal Wilson, it was intended to
galvanise viewers into taking personal action to reduce their own carbon
footprint.”347

The Guardian also writes this line from one of the child actors: “Jamie
Glover, the child-actor who plays the part of Philip and gets blown up, has
similarly few qualms: ‘I was very happy to get blown up to save the
world.’”348

Then in October 2011, the media had the chance they had been waiting for
to promote alarmism again. Richard Muller, a professor at the University of
California at Berkeley, released a report from the Berkeley Earth Surface
Temperatures project team (BEST) that claimed that the world was warming
at an alarming rate—conveniently just in time for the 2011 UN climate
change conference in Durban, South Africa.349 In an op-ed the Wall Street

Journal Muller boldly declared that the age of skepticism was over and that
“you should not be a skeptic, at least not any longer.”350

The media, which had gone through a prolonged cooling spell on global
warming, jumped on the story immediately. AP reporter Seth Borenstein, the
same reporter who wrote the glowing article, “Scientists give two thumbs up
to Gore’s movie” in 2006, wrote an article on Muller’s report called
“Skeptic finds he now agrees global warming is real.”351 Of course, in
typical fashion, a fawning media storm ensued. David Rose wrote in an
October 30, 2011, Mail Online article called, “Scientist who said climate
change sceptics had been proved wrong accused of hiding truth by
colleague” about the media mania that was drummed up, explained that “It
was cited uncritically by, among others, reporters and commentators from the
BBC, the Independent, the Guardian, the Economist, and numerous media
outlets in America. The Washington Post said the BEST study had ‘settled
the climate change debate’ and showed that anyone who remained a skeptic
was committing a ‘cynical fraud.’”352

This indulgence in alarmism, however, was so short lived it was over

before many even realized it started. Upon further review, it turned out that
Muller’s study did not evaluate if the warming was man-made and it also
was not properly peer reviewed. In fact, Muller released his findings without
even informing his colleagues in the study, Professor Judith Curry and
Anthony Watts. As Professor Curry said, “Of course this isn’t the end of
skepticism. To say that is the biggest mistake he [Prof Muller] has made.”353

By the way, Muller is far from being a former skeptic. In fact, we
specifically excluded him from our Senate report on skeptic scientists
because he was clearly on the other side of the debate. We haven’t heard
about Richard Muller since.

We also haven’t heard much about green jobs lately.
With the total collapse of Solyndra and dismal unemployment numbers,

their “clean energy future” and “clean energy jobs” talking points have
clearly failed as well so it will be interesting to see what they try next.

2012 ELECTIONS: STOPPING OBAMA’S WAR ON
AFFORDABLE ENERGY

Today the mood in Washington is significantly different. Everyone readily
admits that cap and trade legislation is dead on Capitol Hill—even our good
friend, Senator Boxer.354

Now as the 2012 elections approach Presidential candidates are dropping
global warming like a hot potato. Presidential candidate and former Speaker
of the House Newt Gingrich told Fox News on November 8, 2011, that
holding hands with Nancy Pelosi on the sofa saying we must do something
about global warming in an ad sponsored by Al Gore’s climate alliance is
“probably the dumbest single thing I’ve done in recent years.”355

I couldn’t agree more, and I appreciate his honesty.
With the hysteria about catastrophic man-made global warming behind us,

our fight against the hoax has shifted to stopping President Obama from
imposing through regulation what he was unable to achieve through
legislation.

With nineteen House Democrats supporting the Upton-Inhofe Energy Tax

Prevention Act, and sixty-four senators on the record in some way against
EPA, all eyes are on EPA and the White House. Will EPA change course?
Will President Obama accept that his cap and trade agenda is wildly
unpopular, and agree to drop it? Don’t hold your breath.

Now in the wake of the EPA Inspector General report which reveals that

EPA did not engage in the proper record-keeping process or follow the
required peer review procedures leading up to the endangerment finding—on
top of the fact that the science comes from the IPCC, whose credibility is
seriously called into question—the very foundation of the endangerment
finding is quickly crumbling, and I think it will only be a matter of time
before it collapses.356

In the meantime, the debate continues, and the battle over the Energy Tax
Prevention Act and Obama’s war on affordable energy carries on. The bill
will come to the floor again, and soon, so members will once again have to
decide whether they stand with consumers, manufacturers, farmers, and small
businesses, or with EPA’s barrage of greenhouse gas regulations that will
harm all of them.

My hope is that if the endangerment finding does not collapse under the
weight of its own flaws, which I believe it will, the Energy Tax Prevention
Act will have a clear path towards victory after the 2012 elections.

MOVING ON FROM THE HOAX
What I said on the Senate floor in July 2003 is exactly the same message I
have had ever since. The science behind man-made catastrophic global
warming simply isn’t there and the United States Senate would never ratify
Kyoto or pass cap and trade. I am vindicated on these points today and now
that the global warming hysteria is behind us, its time to get back to powering
this amazing machine called America, which is the surest way to revive our
ailing economy. The United States has a clear choice: we can continue
implementing Al Gore and President Obama’s global warming agenda, which
is destroying jobs and doing great harm to our economy, or we can elect
officials who support developing our nation’s vast natural resources, which
are the key to our nation’s recovery. With the November 2012 elections on
the horizon, we finally have the chance to stop Obama’s cap and trade agenda
and its mindless restrictions on our ability to develop and produce our own
resources.

But the demise of the Obama Administration does not mean that they

won’t keep trying. They’ll be back: MoveOn.org, the Hollywood elite, the
anti-sovereignty internationalists are still out there and they still have the
resources to be dangerous.

However, I firmly believe that when the history of our era is written,

future generations will look back and wonder why we spent so much time
and effort on global warming and pointless “solutions” like Kyoto and cap
and trade. In the end, through all the hysteria, all the fear, and all the phony
science, what global warming alarmists have often forgotten is that God is
still up there, and as Genesis 8:22 reminds us:

“As long as the earth remains,

there will be springtime and harvest,

cold and heat, winter and summer,

day and night.”357

AFTERWORD

WHAT GLOBAL WARMING AND EARMARKS

HAVE IN COMMON

OR THIS COULD BE ENTITLED, “President Obama’s greatest victory…a gift
from Republicans.”

This will be the most difficult concept for conservatives to comprehend,

let alone embrace. As I watched this “gift” unfold, this ceding of our
Constitutional authority to President Obama, I felt perhaps the greatest
frustration of all in my over ten-year battle against the hoax. How would the
Obama bureaucracy be emboldened with their enhanced authority? How
could they use it to advance their effort to pass the greatest tax increase in
history, cap and trade? As difficult as it is, I will ask you to read on.
Difficult? Yes, because it contradicts a basic tenant that most conservatives
have been led to believe. However, there is a happy ending.

THE LONE VOICE IN THE WILDERNESS
On November 22, 2010, Roll Call published an article entitled, “Inhofe
Happy to Stand Apart” which laid out the reasons why I am often alone
opposing some key “politically correct” issues—most recently the
moratorium on earmarks:

In an interview last week, [Inhofe] recalled a time when one of his
grandchildren “came up to me and said, ‘Pop-I, Why do you always do
things that nobody else does?’ … and I said, ‘because nobody else
does.’”

Case in point: As many of his GOP colleagues reversed long-held
positions on an earmark ban last week, Inhofe proudly defended his
support of the practice.

Republicans adopted an internal rules change to create a voluntary ban
on requesting any kind of earmark—including transportation projects, tax
cuts for particular industry sectors or other Congressionally directed
funding.358

Of course, I would so much rather join the crowd and hold hands and have

a ban on earmarks and everybody’s happy. But I do what nobody else does
because I’ve got twenty kids and grandkids. The things we’re doing today are
not for me, they’re for the future. In the 1970s, I was the “lone voice in the
wilderness” in Oklahoma pushing for the balanced budget amendment; I
risked expulsion in the House of Representatives to overturn the discharge
petition that was allowing members to hide their votes from the public; for
many years, I was the only one willing to take the most politically incorrect
position at the time and challenge the science behind the global warming
hoax; and now I am one of the few senators standing up against the earmark
moratorium in Congress. But, as before, I will eventually be proven right.

Before I go any further, let me redeem myself with my conservative

readers. The American Conservative Union has ranked me the number one
most conservative member in the United States Senate. Human Events, in
editorializing on the “Top 10 Most Outstanding Conservative Senators,”
ranked me number one, calling me an “unabashed conservative; he’s unafraid
to speak his mind.” I was also recognized by The National Journal as the
number one conservative in the United States Senate for 2009.

While I am often the lone voice in the wilderness, I am glad that my good

friend Paul Weyrich, co-founder of the Heritage Foundation and a leading
conservative, agreed with me in a column for Renew America called
“Senator Inhofe: Transportation, Work, and Achievement,” which was
published September 12, 2006. This piece is very important to me because it
was one of the last things Weyrich wrote before his death. As Weyrich rightly
said, “[Inhofe] and I always tell our fellow conservatives that the two
matters as to which the Federal Government is authorized to spend money are
defense and infrastructure.” He called me a work horse not a show horse
because of my efforts on bills that receive less attention, such as the Water
Resources Development Act (WRDA)—an important bill that funds water
infrastructure projects. Of course, Weyrich put it best so I’ll let him speak to
that:

Senator Inhofe: Transportation, Work, and Achievement
By Paul Weyrich
September 12, 2006

When I came to work in the United States Senate, 40 years ago this
January, I quickly learned that there are two kinds of Senators—
workhorses and show horses. I dare say few, if any, high school students
could name all 100. Indeed most teachers would be impressed if their
high-schoolers could name the two Senators from their own State.

I have watched over the years the Senators who never met a

microphone they didn’t try to get in front of. Then I have watched the
Senators who work quietly on matters vital to the nation but who get very
little coverage for doing so.

One of the workhorse Senators is James M. Inhofe (R-OK). His is

hardly a household name outside his own state, where he wins by
landslide margins. In the Senate he doggedly works on various pieces of
non-sexy legislation. Often his work pertains to national defense. I have
seen him go toe to toe with both the Clinton and Bush Administrations.
And he won. I have seen him clash with the Congressional leadership of
his own party. For example, he got the rules changed so that Congressmen
who sign a discharge petition (to force a bill to the floor against the
wishes of the leadership) must do so in broad daylight. The rules
previously permitted them to hide behind procedure.

Having been trained by two workhorse Senators I appreciate them a

lot more than those who will say anything to get on television. The reason
I mention Jim Inhofe is because of the 100 Senators I would put him as
the top workhorse Senator. He works on many projects at once. He
pursues them until they are complete. Do not get me wrong, he is good on
television. Since the advent of the Fox News Channel, he now has begun
to get some exposure and he does well. Primarily, however, he does what
he is now doing—that is, working on an infrastructure bill that has almost
no national following. He is shepherding something called the Water
Resources Development Act (WRDA). He and I always tell our fellow
conservatives that the two matters as to which the Federal Government is
authorized to spend money are defense and infrastructure. Two summers

ago Inhofe secured passage of the Transportation Bill, which took
incredible skill on his part. Yes, it has a few questionable items but by
and large that bill was an extraordinary piece of work. I praised him for
it at the time and I do so again today, despite all the criticism. We both
believe that spending outside of defense and infrastructure is stretching
the Constitution to a point beyond recognition.

Anyway, back to this legislation, the bill Inhofe is now working on
authorizes the Corp of Engineers to do flood control, navigation and
environmental restoration projects. For example, the average
transportation cost savings of users of the inland waterway system is
$10.76 per ton hauled or $7 billion annually over rail, highways and air
transportation.

Flood control, as demonstrated during Katrina and Rita, is a critical

service provided by the Army Corps of Engineers. Money was
appropriated to fix those infamous levies in New Orleans but local
politicians always diverted the money to their own projects and now we
are all paying the price. Nevertheless, according to the American Society
of Civil Engineers, flood control structures on average prevent $22
billion in flood damage per year. That is a saving of $6 for every $1
spent.

Clearly, projects that promote economic growth through good

movements or prevent damage due to flooding are not pork. Yet many in
the media, who never understand the big picture in this country, pick on
some project in a Congressman’s district and charge him with bringing
home the pork. Not always so. Recognizing that not all proposed flood
control or navigation projects are necessary, the Senate has established
firm criteria for evaluating project requests.

First, projects have to have a chief report, which means that the Corps

of Engineers has determined that the project is technically feasible,
environmentally sound and economically justified. Second, Inhofe and his
committee attempt to oppose any environmental infrastructure project
which is outside the scope of the main mission. You can imagine that
there are Senators on Inhofe’s committee who do bring pork to the table.
Inhofe won’t budge on that point. Finally, Inhofe’s Environment and

Public Works Committee opposes cost waivers, thus following the policy
established in the WRDA Bill of 1986 which established cost-sharing
requirements. In order for a project to be built local communities must be
willing to pay some cost of the project. The same is true in the
Transportation Bill only in that measure there is a huge disparity between
highways and transit. With highways the Federal Government pays 80 to
90% of the project. With transit, say a light rail line in Denver, the
Federal Government will only pay on average around 50%.

Just as in the Transportation Bill (known around here as SAFETEA-

LU), in which the Senator got his Committee to agree that projects
eligible for Highway Trust Fund dollars be on the State’s transportation,
the Senate WRDA Bill established and stayed with strict criteria for
WRDA projects in an attempt to avoid funding any project which is not
justified.

Work on this measure has been long and hard. Inhofe wants to get the

final bill passed in these waning days of the 109th Congress. But for
Senators like Inhofe (and there are not many—eight or nine at best) who
are willing to do the non-exciting, non-sexy work, the real business of the
Senate would not go on. The WRDA Bill is important and we can be
thankful that Inhofe is behind it, inching it along to enactment.359

I am not including the Weyrich op-ed a as self ingratiating gesture but to
set up the concept that bureaucratic earmarks, as opposed to congressional
earmarks, played a significant role in promoting the whole hoax, using the
bureaucracy as a brainwashing mechanism. Read on, you’ll see.

LEGISLATORS VS. BUREAUCRATS
There is a very important difference between legislators and bureaucrats.
Legislators are elected officials—at the federal level, Congresspersons
serving in the United States House of Representatives, and Senators serving
in the United States Senate. Legislators pass laws that are signed into effect
by the president as the chief administrator of the government. Legislators are
periodically re-elected, or un-elected, by their constituencies. They are
accountable to the people.

In very sharp contrast, agency bureaucrats are appointed officials—they
are part of the executive branch of government, ultimately approved by the
President. They are not elected. They are not accountable to the people. They
serve and are accountable to the president.

The key question for every citizen is: Who do you want making decisions
about your life? Someone who is elected that you can vote out of office, or an
unelected, unaccountable bureaucrat?

A POWER GRAB BY THE EXECUTIVE BRANCH
Through an overwhelming number of regulations, we are experiencing a
masterful power grab on the part of the Executive Branch—in other words,
the president.

This is especially obvious in the efforts by the Obama administration to
take over regulation of a number of sectors of our society such as healthcare
and banking. More recently, the power grab has extended to control of
energy, power, global warming, and climate change. Obama’s power-grab
through the EPA is as massive as that of the Obama healthcare bill.

When I was privileged to testify before a House of Representatives

Subcommittee on Energy and Power in February 9, 2011, concerning the
“Energy Tax Prevention Act of 2011.” I explained the goal of the bill is to
keep the EPA from imposing climate-change regulations.

The legislation is designed to reassert the authority of Congress rather
than to allow unelected bureaucrats to decide regulations on all forms of
energy and to specifically hold the EPA accountable.

Of course, Representative Henry Waxman (D-CA) said that our bill

should be called the “Big Polluter Protection Act.”360 He went on to say that
our bill would “repeal the only authority the administration has to protect our
health and the environment without providing any alternative.”361 However,
Congressman Waxman should know there is no “alternative” even necessary
because there is no need for the administration to take such economically
severe measures to protect our health and environment. There is no threat to
our health and environment.

What happened to push Congressman Upton and me to action? Since the
normal legislative process has failed the Obama Administration due to lack
of support from the Congress and the American people, they are pursuing

regulations to enact what would amount to a cap-and-tax law. The good news
at this point is that the House of Representatives is eager to pass such a
change in EPA powers. The Senate isn’t quite there yet. I’m eager for that day
to come.

THE EPA CLAIMS AND MY REBUTTAL
The EPA claims that it has both justification and Supreme Court authority to
regulate CO2 as a pollutant and to impose regulations that will end up
seriously impacting virtually every aspect of American business and daily
life.

My leading contention before the subcommittee was that the Clean Air Act

—a bill designed to deal with air quality—was passed by Congress with a
deliberate intent not to regulate so-called greenhouse gases. The Clean Air
Act had nothing to do with climate change. Its primary directives were
related initially to the regulation of what were called “criteria pollutants”
such as ozone, particulate matter, and lead.

Furthermore, the Waxman-Markey bill that the House passed did have to

do with greenhouse gases. That bill died with Senate inaction and the
election of a new Congress. Greenhouse gasses were an issue in the 2010
election that saw control of the House move to the Republicans. I have no
doubt that a bill such as Waxman-Markey would not pass the House today.

In her testimony before the subcommittee, EPA Administrator Lisa

Jackson referenced the 5–4 decision by the Supreme Court, Massachusetts v.
EPA,362 testifying that the Supreme Court concluded that the Clean Air Act’s
definition of air pollutants includes greenhouse gases.363

What the Supreme Court said was this: the EPA has the discretion to
decide whether greenhouse gases endanger public health and welfare.
However, Administrator Jackson proceeded in making the endangerment
finding that greenhouse gases are a threat to human health, and welfare.

THE GREATER TREND TOWARD PRESIDENTIAL RULE
The issues involving the EPA are symptomatic of a much broader escalating
trend toward greater and greater Presidential authority through bureaucratic
agencies. This trend continues to a great extent, I believe, because people use
terminology that hides the process.

In a word…earmarks. I consider it one of the most misunderstood words

in our American language today.

WHAT REALLY IS AN EARMARK?
If you ask people today what’s wrong with the federal government, they may
say runaway deficits or wasteful spending, and I agree with that. However,
they may also say earmarks, and I believe they do that because very few
people really understand just what an earmark is. In order to understand
earmarks, you must know the difference between Congressional
authorizations and Congressional appropriations, and you must further
understand the difference between Congressional earmarks and Obama
Administration earmarks. Yes, there are two types of earmarks.

Congress passes laws that authorize specific programs and either

authorizes the overall dollar amount for the program or simply states “such
sums as may be necessary.” An example of an authorization act is the Energy
Independence and Security Act in 2007.364

After a law has been passed, Congress then appropriates funds to carry
out programs contained in the law, across all of the federal departments and
agencies. During the appropriations process, Congress may designate,
through an earmark, that a certain amount of the funds available must be spent
on a particular program or issue. For example, they may specify that under
the Highway program, $100,000 must be used to help a small community
deal with a deteriorating highway. This $100,000 is taken out of the larger
pool of money that Congress authorizes for the program. These earmarks or
appropriations are not additional funds added to the federal budget.

Here’s the crux of the issue: The vast majority of funds are not earmarked
by Congress. Executive-branch officials decide how to spend the funds in the
vast majority of all programs. The Energy Independence and Security Act of
2007 provides for greenhouse gas and climate research in a variety of
sections of the law. The money is spent by either political appointees in the
Executive Branch, or in most cases, by career bureaucrats in each of the
agencies and departments. But in the final analysis, it’s Obama.

One starts to see the connection between global warming and climate

research and earmarks. During the decade-long battle against cap and trade
and other climate legislation, I was not only fighting the global-warming
enthusiasts but also the process that was allowing billions of dollars that

were given to unelected bureaucrats to dish out as they desired. Keep the
concept very clear: unelected bureaucrats are given supervisory control over
bureaucratic-agency earmarks.

You might ask why Executive branch or bureaucratic earmarks are any

worse than Congressional earmarks. The answer is that in bureaucratic
earmarks, most of the decisions are made by the career bureaucrats who are
unaccountable to the people.

The people elect their 535 Congressional representatives, the House and

the Senate, who have a much better idea of how the people want their tax
dollars spent. On the other hand, they elect one president, who is responsible
for the entire Executive branch. The Congressional Research Service reports
that the president, in turn, appoints approximately one thousand cabinet and
subcabinet officials who are confirmed by the Senate,365 with additional
hundreds of political appointees. In all, these bureaucrats run government
agencies—fielding requests for grants and other types of funding, submitting
those requests to the president for his budget and then, when Congress fails to
do its part, receiving jurisdiction over that money to pass it on to those of
their choosing.

While having a thousand Senate confirmed “managers” and a few

thousand related staffers might seem like enough, you must also keep in mind
that these employees of the federal government oversee hundreds of
thousands of federal employees who make decisions every day from
Washington, D.C., about how your federal tax dollars are spent.

Are you aware that most federal employees serve regardless who is

elected president? Just under the political appointees are a group of
managers called the Senior Executive Service. These Senior Executives, in
turn, oversee at least thousands of additional managers, all of whom have
authority to spend federal dollars. Our federal workforce is growing at an
alarming rate; the Obama Administration has reportedly added more than
200,000 new positions.366 The exact number may be disputable. What is not
disputable is that these unelected bureaucrats, who do not answer to the
public or the voters, make decisions every day on how to spend your tax
dollars.

Even if every member of Congress and all one thousand of the president’s
political appointees try to guard against wasteful spending, they are grossly
outnumbered by the career bureaucratic managers and staff.

How does this impact climate change? Quite simply, the career

bureaucrats at the EPA, the National Oceanic and Atmospheric
Administration, the U.S. Fish and Wildlife Service, and numerous other
departments and agencies have been earmarking climate science money to
their pet researchers for decades; regardless of the will of Congress or who
the president might be. It may shock you to know which presidential
administration spent more on climate research. It wasn’t Bill Clinton. The
Administration of George W. Bush spent more than three times more money
on climate research than the Clinton Administration. Who decided how to
spend the money? It wasn’t President Bush or Vice President Cheney.

Career bureaucrats across the federal government spent hundreds of

millions of dollars during the Bush Administration funding the work of Dr.
Mann and his cohorts, and it wasn’t through Congressional earmarks. A
Congressional earmark to fund Dr. Mann would have never passed Congress,
not as long as I was in the U.S. Senate.

WHERE DOES THE MONEY GO?
In March 2010, talk show host and political pundit Sean Hannity had a series
about what he considered to be the 102 worst “earmarks” related to
government spending of tax dollars. He listed these in reverse order from
102 to 1. Read them all. It will make you appreciate the surprise ending at
the conclusion of the list.

102. Protecting a Michigan insect collection from other insects

($187,632)

101. Highway beautified by fish art in Washington ($10,000)

100. University studying hookup behavior of female college coeds in
New York ($219,000)

99. Police department getting 92 Black Berries for supervisors in Rhode
Island ($95,000)

98. Upgrades to seldom-used river cruise boat in Oklahoma ($1.8
million)

97. Precast concrete toilet buildings for Mark Twain National Forest in
Montana ($462,000)

96. University studying whether mice become disoriented when they
consume alcohol in Florida ($8,408)

95. Foreign bus wheel polishers for California ($259,000)

94. Recovering crab pots lost at sea in Oregon ($700,000)

93. Developing a program to develop “machine-generated humor” in
Illinois ($712,883)

92. Colorado museum where stimulus was signed (and already has $90
million in the bank) gets geothermal stimulus grant ($2.6 million)

91. Grant to the Maine Indian Basketmakers Alliance to support the
traditional arts apprenticeship program, gathering and festival ($30,000)

90. Studying methamphetamines and the female rat sex drive in Maryland
($30,000)

89. Studying mating decisions of cactus bugs in Florida ($325,394)

88. Studying why deleting a gene can create sex reversal in people, but
not in mice in Minnesota ($190,000)

87. College hires director for project on genetic control of sensory hair
cell membrane channels in zebra fish in California ($327,337)

86. New jumbo recycling bins with microchips embedded inside to track
participation in Ohio ($500,000)

85. Oregon Federal Building’s “green” renovation at nearly the price of a
brand new building ($133 million)

84. Massachusetts middle school getting money to build a solar array on
its roof ($150,000)

83. Road widening that could have been millions of dollars cheaper if
Louisiana hadn’t opted to replace a bridge that may not have needed
replacing ($60 million)

82. Cleanup effort of a Washington nuclear waste site that already got
$12 billion from the Department of Energy ($1.9 billion)

81. Six woodlands water taxis getting a new home in Texas ($750,000)

80. Maryland group gets money to develop “real life” stories that
underscore job and infrastructure-related research findings ($363,760)

79. Studying social networks, such as Facebook, in North Carolina
($498,000)

78. Eighteen (18) North Carolina teacher coaches to heighten math and
reading performance ($4.4 million)

77. Retrofitting light switches with motion sensors for one company in
Arizona ($800,000)

76. Removing graffiti along 100 miles of flood-control ditches in
California ($837,000)

75. Bicycle lanes, shared lane signs, and bike racks in Pennsylvania
($105,000)

74. Privately-owned steakhouse rehabilitating its restaurant space in
Missouri ($75,000)

73. National dinner cruise boat company in Illinois outfitting vessels with
surveillance systems to protect against terrorists ($1 million)

72. Producing and transporting peanuts and peanut butter in North
Carolina ($900,000)

71. Refurnishing and delivering picnic tables in Iowa ($30,000)

70. Digital television converter box coupon program in D.C. ($650,000)

69. Elevating and relocating 3,000 feet of track for the Napa Valley Wine
Train in California ($54 million)

68. Hosting events for Earth Day, the summer solstice, in Minnesota
($50,000)

67. Expanding ocean aquaculture in Hawaii ($99,960)

66. Raising railroad tracks 18 inches in Oregon because the residents of
one small town were tired of taking a detour around them ($4.2 million)

65. Professors and employees of Iowa state universities voluntarily
taking retirement ($43 million)

64. Minnesota theatre named after Che Guevara putting on “socially
conscious” puppet shows ($25,000)

63. Replacing a basketball court lighting system with a more energy
efficient one in Arizona ($20,000)

62. Repainting and adding a security camera to one bridge in Oregon
($3.5 million)

61. Missouri bridge project that already was fully funded with state
money ($8 million)

60. New hospital parking garage in New York that will employ less
people ($19.5 million)

59. University in North Carolina studying why adults with ADHD smoke
more ($400,000)

58. Low-income housing residents in one Minnesota city receiving free
laptops, WiFi, and iPod Touches to “educate” them in technology ($5
million)

57. University in California sending students to Africa to study why
Africans vote the way they do in their elections ($200,000)

56. Researching the impact of air pollution combined with a high-fat diet
on obesity development in Ohio ($225,000)

55. Studying how male and female birds care for their offspring and how
it compares to how humans care for their children in Oklahoma
($90,000)

54. University in Pennsylvania researching fossils in Argentina (over $1
million)

53. University in Tennessee studying how black holes form (over $1
million)

52. University in Oklahoma sending 3 researchers to Alaska to study
grandparents and how they pass on knowledge to younger generations
($1.5 million)

51. Grant application from a Pennsylvania university for a researcher
named in the Climategate scandal ($500,000)

Don’t give up. There’s a reason for this. You’re half-way there.

50. Studying the impact of global warming on wild flowers in a Colorado
ghost town ($500,000)

49. Bridge built over railroad crossing so 168 Nebraska town residents
don’t have to wait for the trains to pass ($7 million)

48. Renovating an old hotel into a visitors center in Kentucky ($300,000)

47. Removing overgrown weeds in a Rhode Island park ($250,000)

46. Renovating 5 seldom-used ports of entry on the U.S.-Canada border
in Montana ($77 million)

45. Testing how to control private home appliances in Martha’s
Vineyard, Massachusetts, from an off-site computer ($800,000)

44. Repainting a rarely-used bridge in North Carolina ($3.1 million)

43. Renovating a desolate Wisconsin bridge that averages 10 cars a day
($426,000)

42. Four new buses for New Hampshire ($2 million)

41. Repaving a 1-mile stretch of Atlanta road that had parts of it already
repaved in 2007 ($490,000)

40. Florida beauty school tuition ($2.3 million)

39. Extending a bike path to the Minnesota Twins stadium ($500,000)

38. Beautification of Los Angeles’ Sunset Boulevard ($1.1 million)

37. Colorado Dragon Boat Festival ($10,000)

36. Developing the next generation of supersonic corporate jets in
Maryland that could cost $80 million each ($4.7 million)

35. New spring training facilities for the Arizona Diamondbacks and
Colorado Rockies ($30 million)

34. Demolishing 35 old laboratories in New Mexico ($212 million)

33. Putting free WiFi, Internet kiosks, and interactive history lessons in 2
Texas rest stops ($13.8 million)

32. Replacing a single boat motor in a government boat in D.C.
($10,500)

31. Developing the next generation of football gloves in Pennsylvania
($150,000)

30. Pedestrian bridge to nowhere in West Virginia ($80,000)

29. Replacing all signage on 5 miles of road in Rhode Island
($4,403,205)

28. Installing a geothermal energy system to heat the “incredible
shrinking mall” in Tennessee ($5 million)

27. University in Minnesota studying how to get the homeless to stop
smoking ($230,000)

26. Large woody habitat rehabilitation project in Wisconsin ($16,800)

25. Replacing escalators in the parking garage of one D.C. Metro station
($4.3 million)

24. Building an airstrip in a community most Alaskans have never even
heard of ($14,707,949)

23. Bike and pedestrian paths connecting Camden, N.J., to Philadelphia,
Pennsylvania, when there’s already a bridge that connects them ($23
million)

22. Sending 10 university undergrads each year from North Carolina to
Costa Rica to study rain forests ($564,000)

21. Road signs touting stimulus funds at work in Ohio ($1 million)

20. Researching how paying attention improves performance of difficult
tasks in Connecticut ($850,000)

19. Kentucky Transportation Department awarding contracts to
companies associated with a road contractor accused of bribing the
previous state transportation secretary ($24 million)

18. Amtrak losing $32 per passenger nationally, but rewarded with
windfall ($1.3 billion)

17. Widening an Arizona interstate even though the company that won the
contract has a history of tax fraud and pollution ($21.8 million)

16. Replace existing dumbwaiters in New York ($351,807)

15. Deer underpass in Wyoming ($1,239,693)

14. Arizona universities examining the division of labor in ant colonies
(combined $950,000)

13. Fire station without firefighters in Nevada ($2 million)

12. “Clown” theatrical production in Pennsylvania ($25,000)

11. Maryland town gets money but doesn’t know what to do with it
($25,000)

10. Investing in nation-wide wind power (but majority of money has gone
to foreign companies) ($2 billion)

9. Resurfacing a tennis court in Montana ($50,000)

8. University in Indiana studying why young men do not like to wear
condoms ($221,355)

7. Funds for Massachusetts roadway construction to companies that have
defrauded taxpayers, polluted the environment, and have paid tens of
thousands of dollars in fines for violating workplace safety laws
(millions)

6. Sending 11 students and 4 teachers from an Arkansas university to the
United Nations climate change convention in Copenhagen, using almost
54,000 pounds of carbon dioxide from air travel alone ($50,000)

5. Storytelling festival in Utah ($15,000)

4. Door mats to the Department of the Army in Texas ($14,675)

3. University of New York researching young adults who drink malt
liquor and smoke pot ($389,357)

2. Solar panels for climbing gym in Colorado ($157,800)

1. Grant for one Massachusetts university for “robobees” (miniature
flying robot bees) ($2 million)

Grand Total: $4,891,645,229367

After Hannity’s program aired, I repeated his list in a speech I gave on the

Senate Floor. I asked, “What do all of these 102 ‘earmarks’ have in
common? Answer: Not one is a Congressional earmark. They were all
enacted by President Obama and his bureaucrats.”368

I could not agree more that these are examples of unnecessary, silly,

wasteful spending. That $4.9 BILLION could certainly have been spent more
wisely… or not at all!

But not one of these expenditures was authorized by the Senate. They

were expenditures authorized, and spent, by President Obama and his
bureaucrats because the Senate did not do its constitutional duty. They were
bureaucratic earmarks!

Why do I place the burden of blame on the Senate for these expenditures?

Because the House Republicans had invoked a one-year moratorium on
“earmarks,” defining “earmarks” in the House rules, which I will explain
later. The House Republicans not only initiated a “year-long ban” on
earmarks, but encouraged both the House Democrats and the Senate to do the
same. This Republican decision in the House came a day after House
Democrats said they wouldn’t fund special projects for defense contractors,
energy firms, or private companies in general.

Democrat Congressman David Obey said, “The political reality right now

is that the public has lost some confidence in this institution and one of the
reasons is the past abuse of the earmark process.”369

My reaction to Congressman Obey was that the earmark process is not

what has caused the public to lose confidence. The public has lost
confidence in the legislative branch when it comes to spending issues
because we have created deficits in discretionary spending and we have

failed to tackle the GIANTS of spending—the entitlement programs that
continue to grow without restraint.

The Democrat Senator Daniel Inouye, Chair of the Senate Appropriations
Committee, agreed to a two-year moratorium on earmarks, explaining, “The
handwriting is clearly on the wall,” Inouye said in a statement. “The
president has stated unequivocally that he will veto any legislation containing
earmarks.”370 Of course he would. He and his bureaucrats can still spend it.
Big win for President Obama.

So there we have it. Democrats and Republicans alike in the House of
Representatives and in the Senate jumped on a phony “abolish earmarks”
bandwagon. So let’s examine what they, the House Democrats and
Republicans, are saying.

Those who advocate the end of Congressional earmarks are saying, in

effect, “Let’s give all authority for the function of government programs to a
centralized executive branch.”

While there is spending that should be refused appropriation or

authorization, an across-the-board ban is wrong. Items should be defeated by
Congress on the basis of their substance, not as part of a sweeping ban on all
appropriations and authorizations. Let me give you examples. Improved
armor for our soldiers with MRAP vehicles (Mine Resistant and Ambush
Protected) and Unmanned Aerial Vehicles, such as the Predator drone, are
examples of Congressional appropriations that have improved our national
defense. In these cases, appropriations that some erroneously call
“earmarks,” saved lives.371

Banning or eliminating Congressional appropriations they call earmarks,
simply sends the money to the Executive Branch. The expenditure of taxpayer
dollars remains at the same level, only with bureaucrats and administration
officials allocating the funds rather than Congress. Not one dime is saved.
The Democrats are more than happy to see Republicans abandon their

authorization and appropriation responsibilities. It allows the current
Democrat-run executive branch to spend, spend, spend.

I was appalled at what the House Republicans chose to do. It was flat-out

wrong on several accounts. First, the resolution they passed was this:
“Resolve, that it is the policy of the Republican Conference that no Member
shall request a congressional earmark… as such terms are used in Clause 9
of Rule XXI of the Rules of the House for the 111th Congress.” Clause 9 of

Rule XXI applies to all legislation in the House of Representatives, whether
it be authorization, appropriation, tax, or tariff legislation.372

THE REVERSAL OF A SOLEMN OATH
It was clear to me that these elected officials abrogated their responsibility,
and their actions resulted in a large expenditure of money that they could
have stopped. I see a real dereliction of duty.

Every House Republican who voted for the resolution was a Member of

Congress who had taken an oath and solemnly swore, “I will support and
bear true allegiance to the Constitution of the United States… so help me
God.” In the wake of this Republican resolution in the House, some sought to
make a case that the Republicans had just trashed their oath of office and the
Constitution. And they were right. But, as I mentioned previously, the House
Democrats jumped on the bandwagon and were equally guilty.

THE VALUE OF DEBATE ABOUT SPENDING
One of the main benefits of spending in the hands of legislators is that
national programs can be better tailored to fit regional needs. It is the rightful
role of Congress to participate in directing federal funds to the areas where
the funds are most beneficial.

In 2008, Congress was faced with the task of appropriating and

authorizing the spending of funds intended to pay for transportation programs,
including congestion mitigation. That year, Congress distributed the millions
of dollars in the program’s funding through Congressional earmarks for one
hundred projects in thirty-five states. The year before, when no
Congressional earmarks were permitted, the Transportation Department (of
the Executive Branch of government) funded projects through bureaucratic
earmarks through grant competition. It didn’t appear that much competition
occurred. All of these bureaucratic earmarks went to only five big cities—
Miami, Minneapolis-St. Paul, New York City, San Francisco, and Seattle.
All of these cities, by the way, were considered to be Democrat strongholds.
And that was during the Bush Administration! It doesn’t really matter who
sits in the White House. If Congress doesn’t do its job, the Executive-Branch
bureaucrats who hold jobs for a lifetime are able to wield a great deal of

economic power. Their political bias cannot be challenged or stopped,
unless Congress steps in before they have a chance to distribute funds.

A DIVERSION FROM THE BIGGER ISSUE
The real issue is the bloated government budget, mostly in the area of
entitlements and other large programs. Rather than take on those spending
behemoths, the media and others are turning attention on smaller amounts that
are disturbing, but certainly miniscule in size compared to the sink holes of
entitlement programs. And not just entitlements. How about the $700 billion
TARP, nearly $890 billion stimulus, and the $2.5 trillion Obamacare? I’m
embarrassed to admit that TARP was the Republicans’ fault as much as the
Democrats. On October 1, 2008, seventy-four senators including thirty-four
Republicans gave an unelected bureaucrat $700 billion with no
accountability. I call it Group Therapy. People will forget, and they did.
What did they do with the TARP money? Bailouts: AIG, Chrysler, GM, and
the rest of them. And the very Republicans who voted for the $700 billion
TARP spent hours on the Senate Floor complaining about the bailouts that
TARP funded. And sure enough, everyone forgot. But they didn’t forget about
“earmarks.” That diversionary tactic worked beautifully.

In only 2009 and 2010, of the $3.1 trillion and $3.6 trillion budgets,
Congressional earmarks make up less than half a percent. However, they
seem to get 100 percent of the attention.

RAUCH GOT IT RIGHT!
Jonathan Rauch wrote an article titled “Earmarks Are a Model, Not a
Menace” that appeared in the National Journal (March 14, 2009). Here are
my favorite excerpts from that article:

Beating up on earmarks is fun. But if you interrupt the joy long enough to
take a closer look, you may discover that the case against earmarks has
pretty much evaporated over the past few years. In fact, reformers seem
to want to hound out of existence a system that actually works better than
much of what Washington does…

As transparency has taken over, the case against earmarks has melted
away. Their budgetary impact is trivial in comparison with entitlements

and other large programs. Obsessing about earmarks, indeed, has the
perverse, if convenient, effect of distracting the country from its real
spending problems, thus substituting indignation for discipline….

Some earmark spending is silly, but then so is some non-earmark

spending, and there is a lot more of the latter….

And earmark spending today is, if anything, more transparent, more

accountable, and more promptly disclosed than is non-earmark spending.
Indeed, executive agencies could stand to emulate some of the online
disclosure rules that apply to earmarks.373
I pretty well covered the argument succinctly in an op-ed piece I did for

the Washington Times on December 3, 2010. I wrote:

I am used to being all alone. I was the only “no” vote in the Senate on the
Everglades Restoration Act. Three years later, major publications said I
was right and the other 99 were wrong. In 2002, I was alone in exposing
the global-warming hysteria as a hoax. Now I have been vindicated on
that issue as well. As the only conservative Republican to vote against
the earmark moratorium within our conference, I find myself alone once
again. But, as before, I eventually will be proved right. My opposition to
the moratorium is based on my concern that Congress would be ceding its
constitutional authority to the president, while failing to save a single
taxpayer dime and distracting from the real issue of out-of-control deficit
spending.

A politically correct ban on congressional earmarks will give

President Obama even greater power and authority in the expenditure of
taxpayer funds. In other words, in the case of Mr. Obama, he would have
more money to pursue his liberal agenda. No wonder he was so quick to
endorse a ban on congressional earmarks.

With this greater power, the Obama administration will embark on its

own bureaucratic earmarks, which will result in the same type of
spending that we saw from the stimulus bill, which did not contain a
single congressionally directed spending item. These types of
presidential earmarks will mean spending millions of tax dollars for

turtle walkways, toilets in national parks, research on the mating habits of
insects, and equipment to find radioactive rabbit droppings. Lobbyists
already have been hitting up federal agencies at increased rates. A ban on
congressional earmarks will only further increase the number of lobbyists
seeking influence with the executive branch.

Congress would then be nothing more than a rubber stamp for Mr.

Obama’s spending requests. Transparency, accountability and the
public’s recourse would greatly diminish. Currently, members of
Congress must make public notice of their spending requests in advance,
then be held accountable by voters. However, a ban on congressional
earmarks would result in the public being kept in the dark until a year or
more after a presidential earmark already has been spent. At the same
time, voters would be powerless to hold faceless bureaucrats
accountable. What’s worse is that the whole process would be pushed
into Washington’s darkest corners, outside the public’s purview.

That is not how our Founding Fathers envisioned our government. That

is why, when writing the U.S. Constitution, they gave Congress, not the
executive branch, the power of the purse. Writing in the Federalist
Papers, James Madison noted that Congress holds this power for the very
reason that it is closer to the people. Supreme Court Justice Joseph Story
noted in 1833 that if this authority were given to the president, “the
executive would possess an unbounded power over the public purse of
the nation; and might apply all its monied resources at his pleasure. The
power to control, and direct the appropriations, constitutes a most useful
and salutary check upon profusion and extravagance, as well as upon
corrupt influence and public peculation.” Congress should not cede this
authority to the executive branch.

Minus real reforms from Congress to reduce spending, federal

spending will continue to spiral out of control. Why? Because banning
congressional earmarks won’t save a single taxpayer dime. If an
appropriations item that is directed by Congress is removed (or an
attempt is made to remove the item), the money does not return to the
Treasury to pay down the debt. Instead, the bottom-line expenditure
amount remains the same, and the money is put into the hands of the

executive branch, in this case, Mr. Obama, to spend how it sees fit. Given
that the overall number and dollar amount of earmarks has decreased
steadily over the past several years while the federal debt has increased
by $3 trillion in just two years, an earmark ban is not the answer to our
fiscal problems.

Eliminating all earmarks would have additional consequences. Vitally

important earmarks, such as to provide improved armor that has saved
lives for our troops in Iraq and Afghanistan, and the Predator drone
program, which has been vital in the war on terrorism, would not be
possible if the ban were in place. Both are examples of congressional
earmarks that never would have been funded by the administration.

Let me explain it a different way. This is how it is supposed to work. The
President submits his budget to Congress. The authorization committee then
evaluates that budget and makes changes within the President’s bottom line.
In other words, the authorization committee recommends to Congress the
areas where we can best provide funding for items in the President’s budget.
Let’s take an example: I serve on the U.S. Senate Armed Services Committee
which is an authorization committee staffed with defense experts from strike
vehicles to missile defense. Before the ban on congressional earmarks, the
President’s budget contained $350.6 million for a launching system referred
to as a “box of rockets” (otherwise known as the Non Line of Sight—
Launching System). The Senate Armed Services Committee agreed that the
launching system was good but we had a greater need for strike vehicles. So
we struck the launching system and used the money to buy six new F18 E and
F aircrafts. It didn’t cost any more, it just redirected the money to something
that would enhance our defense system in a greater way.374

But wait. These kinds of changes can be characterized as earmarks.
However, without these kinds of changes, the President would make all
spending decisions. This is not what Article 1 Section 9 of the Constitution
says. The legislative branch is supposed to do the authorizing and
appropriating. To be clear, many things that are proposed to be authorized
and appropriated should be defeated. But we should defeat them based on the
substance, not simply because they are called earmarks. I continued in my
op-ed:

Unfortunately, the years of demagoguing earmarks have distracted the
American people from the real fiscal problems that face our nation. We
must do something to stop runaway spending. Ironically, the authors of the
ban both supported the $700 billion bailout and the $50 billion
President’s Plan for Emergency AIDS Relief bill, two of the largest
measures of 2008. That same year, the Office of Management and Budget
calculated total earmark spending at $15 billion. So, by supporting just
those two measures, they obligated the government to 50 times the total of
all earmarks for that year.

There is a simple solution to the earmark problem that I have been

advocating for more than five years. All we have to do is redefine
“earmark” as spending that has not been authorized, meaning it has not
been approved by the committee of jurisdiction. Then eliminate all
earmarks—no exceptions. That’s all. Problem solved. Then we can go
after the real problems, the big stuff like the debt and the deficit.

Let me repeat. The only reason I bring up the earmark issue is to

demonstrate that my decade-long fight against global warming has ALSO
been a simultaneous fight against bureaucratic earmarks, and the unwise
spending of hundreds of millions of taxpayer dollars.375

I applaud Congressman Ron Paul (R-TX) in being one of the only

conservative Republicans in the House to defy the demagoguery and join me.
Congressman Paul and I recently issued a joint new release titled “Earmark
Ban a Huge Victory for Obama” in which we said the following:

The current ban on congressional earmarks gives the Obama
administration and federal bureaucrats even greater power and authority
over the expenditure of taxpayer funds. It’s no surprise, then, that Mr.
Obama was quick to endorse the ban on congressional earmarks—it
grants his administration more power to pursue its agenda by exercising a
power that properly resides with Congress. The president even endorsed
the ban with his veto threat during this year’s State of the Union Address
and in subsequent speeches.

With this greater power, the Obama administration (and future

administrations) will embark on its own bureaucratic earmarks through a

process that will mean less transparency and little accountability for the
American people. The infamous $787 billion Stimulus bill, which both of us
vigorously opposed and voted against, did not contain a single Congressional
earmark. Instead, millions in bureaucratic earmarks were spent for programs
to determine the affects of intoxication on mice, the protection of insects from
other insects, the mating habits of bugs and rodents, and walkways for turtles.

We were the only two conservatives to be outspoken against the

current ban, because it was the right thing to do—politically unpopular,
but the right thing. With the current earmark ban in place, Congress
becomes nothing more than a rubber stamp for President Obama’s
spending request, and our opposition to the ban is based on the fact that
this is not how our Founding Fathers envisioned our government. When
writing the U.S. Constitution, they gave Congress, not the executive
branch, the power of the purse.

According to James Madison’s view outlined in the Federalist

Papers, Congress holds this power for the very reason that it is closer to
the people. Supreme Court Justice Joseph Story noted in 1833 that if this
authority were given to the president, “the executive would posses an
unbounded power over the public purse of the nation; and might apply all
its monied resources at his pleasure.”

The Constitutional power has been ceded to the president.

What’s worse is the fact that banning Congressional earmarks won’t

save a single taxpayer dime. Instead of saving taxpayer money, the
spending authority is shifted from Congress to the Executive Branch. In
this case, the money is put into the hands of President Obama, to spend
how he sees fit.

Unfortunately, the years of demagoguing earmarks have distracted the

American people from the real fiscal problems that face our nation.

Is it any wonder that Obama supported the big push behind the ban on

congressional earmarks? With Congress out of the way, he is now the
king of earmarks.376

NOW THE GOOD NEWS
There is some good news. Senator John McCain and other conservatives
have joined me in a solution to the earmark issue. We have introduced
legislation to redefine “earmarks” as “an appropriation that has not been
authorized.” That should solve the Congressional earmark problem.377
As Senator McCain stated on the floor, “Some of those earmarks are

worthy. If they are worthy then they should be authorized.”378 He also said,
“You’ve got to get the definition of an earmark: that is, an unauthorized
appropriation.”

And as Senator Coburn (R-OK) said, “It is not wrong to want to help your

state. It is not wrong to go through an authorizing process where your
colleagues actually see it.”379 He and I believe if something is really bad you
have two chances to kill it: in authorization and appropriations.

So, I repeat, the only reason I bring “earmarks” into this book, is because

bureaucratic Obama earmarks—unelected bureaucrats putting grant money
into liberal causes, brainwashing the public—has been a major obstacle I
have had to overcome in combating the hoax.

I will never forget when, years ago, my granddaughter came home from

school and asked, “Why is it you don’t understand global warming?” She had
been brainwashed by a grant. We traced what she had been taught all the way
from the EPA to our public school system in Oklahoma—the false
information was the result of a grant designed to brainwash our kids, using a
bureaucratic earmark.

One of the great frustrations is that it will take years for conservatives and

some talk show hosts to appreciate this chapter. If you ask the vast majority
of talk show hosts, or even newly elected and many long-term members of
Congress to define earmarks, they cannot do it. But, even though they may
know better, many politicians have continued to mischaracterize earmarks,
often out of a desire for personal political gain.

Webster defines a demagogue as “a leader who makes use of popular

prejudices and false claims and promises in order to gain power.”380
Demagoguery is popular and sends approval numbers soaring.

For the first seven years of my ten-year battle to stop the global warming

cap and trade bills no one would join me because they were convinced
people had already made up their minds. They said taking it on was

politically stupid, and they were right. I went through seven years of misery.
But we won. And we are continuing to win. And we are determined to win
until there are no more global-warming lies, cap-and-tax tricks, and phony-
baloney Obama bureaucratic earmarks to battle.

YOU DECIDE WHO RULES
In the end, this issue of an obvious executive-branch power grab is an issue
the voters will have to decide.

Do you want elected officials to pass laws? These officials are your

representatives, sent to Washington, D.C., to do your business.

Or do you want appointed bureaucrats to establish regulations over which

you have very little recourse?

Again, we need to keep the authorization and appropriation process in the
hands of Congress where the Constitution placed it and go after bureaucratic
earmarks. It is a fact bureaucratic earmarks have been just as much a part of
the global-warming “lie machine” as Al Gore’s movie and they need to be
eradicated the same as Al Gore’s science fiction movie has been refuted.

APPENDIX A

WHAT’S IN IT FOR THE UNITED NATIONS?

SUSTAINABLE DEVELOPMENT AND THE QUEST FOR
AUTONOMY…WHERE GLOBAL WARMING BEGAN
The United Nations was founded after World War II to replace the League of
Nations. Its expressed purpose at that time, in 1945, was to help nations
work together and talk to avoid wars and promote social progress.381 The
stated aims of the UN have remained fairly stable over the decades, at least
officially. Article 1 of the UN Charter states that the UN is to, among other
things, “maintain international peace and security382 …develop friendly
relations among nations383 … achieve international cooperation in solving
international problems of an economic, social, cultural, or humanitarian
character,”384 and to serve as “a center for harmonizing the actions of
nations.”385 Nations hoped that by encouraging cooperation on these issues,
countries would be moved toward peaceful relationships instead of warring
ones.

I believe that many globalist elites have worked within the United Nations

to expand its responsibility to an alarming degree. Now, instead of
facilitating international cooperation, I believe the UN’s primary institutional
goal—in practice, if not in word—is to actively build a global utopia. The
UN believes that it can—with enough power and influence— determine what
is best for the world by reaching agreements by majority agreement, or better
yet—consensus—among all of the member states participating at the United
Nations.

Each country represented at the UN has an equal voice.386 No nation—

regardless of their population, landmass, or global influence— has any more
power at the United Nations than another. Each country has one vote.387

When the United Nations works to pass resolutions, it is unwavering in its

desire to do so by consensus. I think that the UN wants all nations to agree
because many elites within the UN believe that where you find agreement,
there you will find peace. If the sum of all nations can agree how to tackle
common problems, then global peace should materialize with relative ease.
The UN proclaims that its adamancy toward reaching agreement by the
consensus of all nations gives it “moral authority” over all other institutions.
This, the UN describes, is one of its “best properties.”388

Over the past several decades, the United Nations has turned much of its

attention to the crafting of solutions for two problems I believe it has
designated as high priorities. It has worked tirelessly to provide a consensus-
based solution for them. The environment is one. Development is the other.
According to the bureaucratic elites at the UN, the environment is on the
brink of disaster. They believe that anywhere you go, you will find evidence
of a global population paying little attention to the impact their activities
have on the environment, and caring even less about the long-term
consequences of those actions.

Concurrently, these elites see billions of people around the world living

in absolute poverty. The global poor—heavily concentrated in the least
developed countries—often live on just dollars a day and have extreme
difficulty living even subsistence lifestyles. All the while, they watch those
in developed nations enjoy the prosperity their wealth provides, but see them
paying little attention to the social dilemma inherent in this inequality.

On a fundamental level, the United Nations elites see three core spheres

of humanity: the society, the economy, and the environment, and they see
major problems in all three. They believe society has little regard for the
inequality across people groups around the world. To them, the global
economic structure appears to be skewed in favor of those already
possessing and consuming most of the world’s resources— leaving little left
over for the poor. The environment has been disregarded by many in favor of
faster economic growth—to the peril of future generations.

The UN elites believe the condition of these three spheres is

unsustainable; they are out of balance; causing inequality; and risking serious,
irreversible damage to the global community. As long as they remain out of
sync, they believe that establishing global utopia will be impossible.
Conflicts—armed or otherwise—will undoubtedly sprout from this

unsustainable trajectory, underscoring the need to develop a workable
solution to reverse this alarming trend.389

Sustainable development is the guiding philosophy that the UN elites have

constructed to solve the structural problems they have identified, and they
seek to use this philosophy to bring complete harmony to each of humanity’s
three spheres.

Doing this will demand fundamental and comprehensive change to a

number of international frameworks, and fully implementing the philosophy
could put the UN on a dangerous path towards autonomy. This is not
something we can allow to happen.

SUSTAINABLE DEVELOPMENT & SOCIETAL
CHANGES
The United Nations’ elite believe that the core of the environmental problem
facing the world is one of societal outlook. Al Gore articulated this well
when he wrote that the “the twentieth century has not been kind to the
constant human striving for a sense of purpose in life. Two world wars, the
Holocaust, the invention of nuclear weapons, and now the global
environmental crisis have led many of us to wonder if survival—much less
enlightened, joyous, and hopeful living—is possible.”390 He believes—and
the UN elites agree—that this questioning about the future has caused many to
recklessly seek material gain without any regard for the potential
consequences to the environment or society.

Solving this problem at the philosophical level will require the

restoration of faith in the belief that we do have a future that is worth
preparing for. Gore believes this is “essential to [restoring] the balance now
missing in our relationship to the earth.”391

As a lawmaker, I am keenly aware of the challenges facing anyone who

wants to change another person’s behavior. Solving a philosophical problem
rooted in faith will prove impossible for an institution like the UN. But the
UN elites have worked hard to outline the changes they believe need to
happen within the different spheres of society to bring everything back into
balance.

Widespread modern concern for the environment began in the 1960s and

1970s. In 1962, Rachel Carson wrote her landmark book Silent Spring,

questioning the actual benefits of DDT, a widely used pesticide at the time.
The book led to broader public questioning of using pesticides and chemicals
in everyday life.

In 1968, Paul Ehrlich published another influential book, Population

Bomb. In it, he predicted that mass global starvation would occur as a result
of overpopulation. He did not believe that global food production would
expand as rapidly as the global population, a theory he based on the idea that
the earth has a predictable carrying capacity that can sustain only a certain
number of people at a middle-class standard of living. To address the
problem, he recommended the immediate implementation of population
control policies.392

Carson’s theory questioned whether or not all human advancement is truly
beneficial. Ehrlich’s work sparked skepticism that the earth has the ability to
sustain widespread global industrialization. Their common theme is that they
coincided with and heavily influenced a rising concern about humanity’s
overall impact on the environment.

Al Gore is one leader who was impacted by the work of Rachel Carson.

He wrote that dinner table conversations during his childhood about her book
“made an impression” on him because they made him “think about threats to
the environment that are much more serious than washed-out gullies—but
much harder to see.”393 This revelation would make him sympathetic to
emerging global environmental problems like global warming and the ozone
hole, which predicted impending disaster because of the release by humans
of seemingly harmless chemicals and compounds.

Believing that the earth could be heading down a path toward

environmental catastrophe caused the elites within the UN to push for action.

Their involvement formally began with the planning of the 1972

Conference on the Human Environment, which would ultimately serve as the
birthplace of the sustainable development philosophy.

Sustainable development aims to solve the problems caused by

environmental degradation by organizing society in a way that preserves the
present status of the environment. The UN elites believe that doing this will
preserve an environment suitable for future generations to provide for their
own needs. The UN elites further believe that enshrining an environment that
is sustainable demands that the natural systems that support life on earth be

preserved as development occurs. These systems include “the atmosphere,
the waters, the soils, and the living beings.”394

The elites contend that limiting the rate of depletion of nonrenewable

resources is an important component of sustainable development because of
the predictable carrying capacity theory. According to the theory, future
generations will be unable to meet their development needs if today’s needed
resources have been exhausted. Limiting the depletion of nonrenewables to
the lowest level necessary to sustain society is the goal, as it will result in
the longest enjoyment of those resources. Because these resources, like oil
and coal, will eventually be depleted, sustainable development has an
overwhelming bias toward the immediate adoption of renewable resources
and recycling patterns, no matter the cost. The UN elites believe that the
adoption of consumption and production patterns respectful of these trends
will ensure the availability of resources—and therefore equal opportunity—
for future generations.

The predictable carrying capacity theory also doubts the ability of the

earth’s natural systems to support complete industrialization of the world’s
population. In other words, it fears that the unintended consequences of
industrialization, mainly global warming, will push the delicate balance of
the earth out of whack and into disaster.

This presents an interesting ethical dilemma. If the earth cannot handle the

rising living standards of the global poor, should those who are wealthy be
allowed to maintain lavish lifestyles?

The UN elites believe that such intragenerational inequality is unfair. As
such, the UN believes that accomplishing sustainable development requires a
standardization of living standards that is sensitive to the earth’s predicted
carrying capacity. The elites are fully aware that this will demand a
reduction in the living standards of the populations in developed countries.
Our Common Future, a key UN sustainable development document that is
heralded by the elites, states that “living standards that go beyond the basic
minimum are sustainable only if consumption standards everywhere have
regard for long-term sustainability…sustainable development requires the
promotion of values that encourage consumption standards that are within the
bounds of the ecological possible and to which all can reasonably aspire.”395

In other words, the UN elites believe that they need the power to

determine—and enforce—an appropriate global standard of living that is

sensitive to the earth’s predictable carrying capacity. In doing this, the UN
believes it will be able to ensure that no one individual’s development
potential is cut short by the lavish lifestyles of others.

SUSTAINABLE DEVELOPMENT & CHANGES TO
SOCIETY’S ECONOMIC STRUCTURE
Developing nations have not always gone along with the UN’s plans to limit
their ability to pursue development. This is not surprising. As the home to
most of the global poor, developing nations should be looking for every
development avenue possible to improve the living standards of the people.
Initial discussions leading up to the UN’s first environmental conference,
the 1972 Conference on the Human Environment, were mainly focused on the
problems of the developed world. And because economic growth was seen
by many as a key cause of environmental degradation, some of the solutions
being considered included the idea of slowing economic growth. Developing
nations feared that the effort to limit environmental damage would yield
policy changes that curtailed their ability to develop, which is what they
needed to harness to climb out of poverty. If the fragility of the environment
was the only concern of the conference, then they wanted no part of it, so they
threatened to boycott the event.396

The potential balk by developing nations was important to the UN elites
for one reason: the UN’s desire to craft agreements by majority. Under the
institution’s one-nation, one-vote system, developing nations have
extraordinary power to influence and frame whatever discussion takes place
at the UN. This is because they make up a large majority of the total number
of member states. Without the support of developing nations, a UN resolution
cannot even obtain a simple majority, let alone consensus.

The concentration of this power occurred in 1964 when seventy-seven

nations banded together to create the Group of 77 (G-77). This group works
at the UN to advance the goals and agenda of the developing world. Since
then, its membership has climbed to a total of 131, a significant majority in
the UN’s General Assembly.397

A threat by the G-77 to avoid the 1972 Conference on the Human

Environment, therefore, had to be taken seriously. Without their support, there
would be no way to reach a consensus—or even a majority—and the UN’s

effort to address the global environmental problem would be significantly
weakened.

To garner their support, conference administrators, led by entrepreneur-

turned-environmentalist Maurice Strong, agreed to amend the draft
conference framework so that it would be more inclusive of the developing
world’s concerns. To figure out how to do this, he decided to hold an
informal meeting in Founex, Switzerland, in 1971, where he and the
conference participants would answer any questions and put to rest any
concerns that there may be a conflict of interest between environmental
protection and economic growth/development.398

Strong was able to allay developing nations’ fears by incorporating their

development ideals into the broader fight to save the environment. He did
this by expanding the definition of the word “environment.” The Founex
Panel ultimately resulted in the environment no longer being considered
“simply the biophysical sphere,” but instead it would also include the
“socio-economic structures.” The Founex Panel stated that the bio-
environment and society “[form] an interdependent and inextricable web.”399
No longer would the debate on the environment be “concerned only with
pollution and conservation, nor was the damage to the environment
attributable solely to the process of development. In many cases, the damage
was due to the very same socio-economic forces and causes that were at the
root of poverty, underdevelopment, and inequality, and could be overcome
only through the process of development, economic growth, and social
change.”400

The tangible result of Founex was the modern framework of sustainable
development. It shifted the entire “environmental” debate to become more
centrally focused on the development of impoverished nations.

Throughout the history of the UN, the G-77 has worked to announce its
frustrations with the global economic power structure. In 1974, developing
nations worked together to pass a UN resolution declaring the need to
establish a New International Economic Order.401 In short, the resolution
blames colonialism and the western-style capitalistic system as the root
cause of the impoverished state of their own countries. They point to the fact
that the vast majority of the world’s resources are consumed by a relatively
small group: the people living in industrialized countries. Developing nations
believe this gives them little ability to procure the resources necessary for

development. This was inherent in the Cocoyoc Declaration, which was
made the same year, which states that “pre-emption by the rich of a
disproportionate share of key resources conflicts directly with the longer-
term interests of the poor by impairing their ultimate access to resources
necessary to their development and by increasing their cost.”402 In other
words, they believe the economic development being pursued by developed
nations pushes the price of raw materials up to a level that is unaffordable to
those in developing nations. They believe this limits their potential to
develop.

They further state that the developed nations use their enormous power in

the global economic system to construct trade and other international
agreements to be in their favor, again at the expense of developing nations’
development.

Sustainable development aims to directly combat these challenges to

development that it believes are fostering widespread global inequality by
encouraging the restructuring of the global economic landscape in favor of
developing nations. It also aims to provide more ready and affordable access
to the resources necessary to accomplish development.

Establishing the right to development is one of the key ways that

sustainable development does this. This right demands that all nations and
people be guaranteed the benefits of development. To the extent that
developing nations cannot provide the benefits of development for their
people, sustainable development implies that the global community should
provide it for them.

Because affordable access to raw and other materials is required to
competitively construct goods for trade and consumption, sustainable
development demands that these resources be provided to developing nations
affordably. Access to technology is another resource that must be secured
affordably to take advantage of the maximum benefits of development. To the
extent that individuals in developing nations cannot afford the most advanced
technology available, it should be made available to them for free, or at a
price they can afford.

Because so much of the world’s resources are controlled and consumed

by developed nations, they maintain a dominant hand in international
economics. The UN bureaucracy believes developed nations like the United
States use this influence to unfairly construct trade agreements and

multinational financial institutions, like the IMF, to be overwhelmingly
biased toward their own goals. The UN believes that this causes significant
costs and barriers to development for developing nations. Consequently,
sustainable development demands the realignment of these agreements and
institutions to be overwhelmingly in favor of developing nations so that they
can more easily enjoy the benefits of development.

In short, sustainable development demands the removal of global wealth
and income inequality by completely shifting the global economic landscape
in the favor of developing nations. In many ways, doing this allows the
environmental leaders at the UN to simultaneously accomplish its societal
goals detailed earlier.

The regulation of raw materials and their guaranteed distribution in

accordance with sustainable development needs would require control over
natural resources supplies. To the extent that demand cannot be controlled,
supply must be regulated.

This control over all resources would ultimately allow the UN to reduce
the developed nations’ living standards so that they would be in line with the
UN’s prediction of the world’s carrying capacity. This would protect the
environment. Simultaneously, the UN could redirect the extracted materials
toward the needs of the developing world so that their standards of living can
rise to that set by the earth’s predicted carrying capacity. At its most basic
level, this regime aims to eliminate global inequality through socialism.

Once all people have reached this “sustainable” standard of living, the

UN would then be able to maintain sustainability by directing economic
growth and technological change on a sustainable path, allowing standards of
living to rise only as technological advancement and the discovery of new
resources allow.

Ultimately, the environmental corps at the UN believes that pursuing

sustainable development will reduce conflict and promote peace by restoring
equity to the relationships between the earth and between fellow man. With
sustainable development, there will be no need for conflict, because the
global conditions that so often lead to conflict would be permanently
suspended in harmony.

IMPLEMENTATION & AUTONOMOUS POWER

Considering that the actual implementation of sustainable development is
daunting, it would require nothing short of total control over the earth’s
resources. There is simply no other way to guarantee environmental security,
global equality in living standards, and equal potential for development
across the world.

As discussed before, the UN elites believe that the inherent nature of the
environment’s threat is one that is global and deep within society. Because
the problem is global, the consequences of unsustainable development are
also global. Enacting sustainable development as a corrective instrument
demands a comprehensive shift in the way decisions are made within the
international community so they are responsive to the global problems we
face.

Convincing the world to surrender sovereignty to an international body

will not be easy, and the environmental lobby knows this. History shows that
the global community rarely cooperates in a united way in the absence of an
impending catastrophe. Al Gore once observed that motivating nations to act
collectively “has usually been secured only with the emergence of a life-or-
death threat to the existence of society itself.”403 I believe that this
understanding has led the UN elites to use alarmism to scare the public into
action with the claim that catastrophe will be imminent without immediate
implementation of sustainable development. Without this life-or-death threat
of unsustainable development purported by the United Nations, there would
be no need for their autonomous control over global economic resources.

This is a key reason why the global warming issue fits so perfectly within

the plans of those at the UN who want to use the institution to construct
utopia. Without a tangible, impending disaster, there is no catalyst to act.
Without a catalyst, the UN’s quest to restore equity to relationships and
establish utopia would die.

The formal development of the sustainable development philosophy

occurred over a period of about thirty years, between the late 1960s and the
early 1990s. Since then, the sustainable development philosophy has infected
nearly every United Nations initiative, and the body has used its indefinable
goal of “societal equity” to bring forth policy problems of all sorts so that the
UN can prescribe the solution.

A few examples:

In 1995, the UN held a World Summit for Social Development. The

documents released as a result of the summit directly reference the
establishment of sustainable development as the overriding goal of the
summit’s policy proposals. Among its recommendations is the regulation of
multinational corporations for the purposes of making economic growth more
conducive to social development in developing countries. It also calls into
question the accumulation of wealth, and proposes that it be taxed for the
purposes of improving stability in financial markets, as if it caused instability
in the first place.404

In 1996, the UN held its Second United Nations Summit on Human

Settlements. There, the Summit reaffirmed the UN’s commitment to making
the provision of “adequate shelter” a basic human right. In doing so, it
elevates this issue to the international level, which is unnecessary. To ensure
this right is protected, it states that it is the responsibility of governments to
“enable people to obtain shelter and to protect and improve dwellings.”
While this certainly does not sound bad, it suggests that the government
should be in charge of assigning housing to people of low income. The
conference leader also suggested that each nation cut its defense budget by 5
percent to address the housing needs in an affordable way.405

In 2002, the UN produced a document called “A World Fit for Children”

that reaffirms the rights established for children at the Convention on the
Rights of the Child. It made clear that the right to sexual health privacy is a
necessary element of sustainable development. Without this right enacted,
unsustainable development will ensue, which in their opinion is a bad
thing.406

There is no shortage of alarming changes demanded by the UN in the name

of accomplishing sustainable development, and since launching sustainable
development, the body has stepped well beyond the original intuitions of
what the philosophy should accomplish. It has, in practice, become the catch-
all method by which the UN allows itself to elevate any problem it identifies
within society to the international level. The mentality that it can solve all the
world’s problems and bring about utopia is centered in the philosophy that
governing by the consensus of nations is the surest way to avoid conflict.

WHY WE SHOULDN’T PURSUE SUSTAINABLE
DEVELOPMENT AT THE UN

Surrendering sovereignty to an international body like the United Nations so
heavily influenced by environmental elites pursuing a utopian agenda of
sustainable development is a dangerous idea. From the United States’
perspective, the sustainable development agenda provides no benefit.

Its goals to restore relationships between the earth and humanity, and its
attempt to restore equity between and among generations are farfetched and
are not grounded in reality. The societal changes demanded to improve
environmental wellness are all based around the idea that the earth’s carrying
capacity is predictable. I believe that the earth has a carrying capacity to
some degree, but I also believe that our knowledge of the future is severely
limited. Who would have thought two hundred years ago, that we would be
able to feed our entire country with less than two percent of the entire
workforce dedicated to agriculture?407 This has been made possible because
of technological and other advances that have improved our productivity.
Why should we expect this trend to stop?

The Industrial Revolution and the Information Age have made

technological change occur at incredible speeds. Consequently, our demand
for resources changes every minute. Further limiting already limited non-
renewable resources in the name of sustainable development will only push
the price of those materials up as we continue to use them. This will make
products—and development—more expensive, which is the exact opposite
of what many at the UN want sustainable development to do.

The economic changes demanded by the implementation of sustainable
development are equally befuddling. Giving developing nations a right to
development, per se where the international community pitches in to provide
for a country’s needs without regard to the decisions made by those
developing nations, is a bad idea. I believe that it is in the best interest of the
United States to help developing nations prosper. I also believe it is our
moral obligation. Assistance, however, should never come without strings
attached.

In America, we know that our prosperity is a direct result of the

government founded by our forefathers. They understood the value of a stable
government with limited, separated powers, and they insisted that the
freedom of the individual be protected. These are a few of the many
characteristics of our nation that have provided us with significant material
blessing.

Similarly, many nations have pursued highly damaging economic policies
—like totalitarianism and communism—and they have reaped what they have
sown.

Promoting a right to development, where the international community

pitches in and attempts to direct and guarantee prosperity, overlooks this core
truth. Nations must have the ability to choose whatever development path
they want, and they must also live with the consequences of their decisions.
The right to development, by undermining the need for accountability, runs
the risk of propping up bad leaders in foreign countries. If this development
aid is given to nations whose leaders are awful, such that the benefits of aid
can be more easily felt by a nation’s people, then the nation’s people will be
less outraged at the awful decisions made by their own government. This
will dull their appetite to demand leadership changes, which are often
necessary in bad situations like these.

Instead of encouraging a right to development, the United States and other
countries should look to help developing nations improve their institutional
rules, frameworks, and governing philosophies to be friendlier toward
business, tougher on crime, and stable for the long run. Implementing these
changes will go much further to help developing nations take advantage of
development than hand outs and bailouts ever could.

Similarly, the assertion that the wealth of developed nations has caused
the developing nations to be subjected to poverty is wrong. This argument
has been made since at least the mid 1970s, and since that time it has been
refuted. P.T. Bauer, a renowned economist, wrote in 1977 that developing
nations with the most open relationships with developed countries saw the
most economic growth. Nations that were the most isolated—those with the
fewest connections to the West—experienced worse economic growth
compared to their peers.408 For a modern example, we need to go no further
than the Korean Peninsula. North Korea, with a communistic/totalitarian
state, has isolated itself from the world. It is one of the poorest nations on
earth, and its people suffer from severe poverty. South Korea, on the other
hand, is a democracy that values freedom. It has opened itself up to
relationships with the West, and it is now one of the most advanced
economies in the world. This fact single handedly disarms the argument that
the success of developed nations is preventing developing nations from
enjoying the benefits of growth and development.

Economic growth occurs because individuals are free to make decisions

of personal benefit in an open, safe, and reliable society. This means that
there is no need for a centrally controlled system of raw materials. Such a
system might try to shuffle resources from developed nations to developing
nations, but in the end, it will yield an economic system that is irrational and
not properly sensitive to the changing needs and desires of the seven billion
people on the planet.

CONCLUSION
The desire of many to work through the UN to establish utopia under the
ideals of sustainable development is real. The desire is also dangerous. Past
attempts to build utopias have failed miserably, and they have often resulted
in mass murder and genocide. Promoting an agenda that demands the
centralization of power into the hands of a single institution is a recipe for
disaster. We must remain vigilant and aware of what the UN is doing to
accomplish these goals. As you’ve learned from reading this book, the
clearest and most direct way they’re trying to do this is through the global
warming agenda and the pursuit of a Kyoto-like treaty. A treaty of that
magnitude would require a true shift of power from sovereign nations like
our own to the United Nations so that they can determine how to distribute
our wealth and resources around the world—in a way that meets their
definition of “fair.” We must remember to look beyond the headlines and
press releases and do the hard work required to dig deeply into UN
documents to understand their true intentions and motivations. By doing this,
we will be able to more effectively prevent our nation’s sovereignty from
being eroded by the goals of the super-liberals at the United Nations. To that
end, we must demand that the UN remain open and transparent about all that
it does.

Doing anything less will result in a severe threat to our own freedom and

prosperity.

APPENDIX B

EXCERPTS FROM MICHAEL CRICHTON’S

NOVEL: STATE OF FEAR

*All excerpts are copyrighted and printed with permission from Michael

Crichton, State of Fear , Harper Collins, 2004.

EXCERPT #1:
The Political Manipulation of the Science.

Dr. Crichton clearly understood that the science was being manipulated

from the very beginning.

“‘IPCC? What last minute changes?’

“The UN formed the Intergovernmental Panel on Climate Change in the
late 1980s. That’s the IPCC… a huge group of bureaucrats, and scientists
under the thumb of bureaucrats. The idea was that since this was a global
problem, the UN would track climate research and issue reports every
few years. The first assessment report in 1990 said it would be very
difficult to detect a human influence on climate, although everybody was
concerned that one might exist. But the 1995 report announced with
conviction that there was now a ‘discernible human influence’ on
climate.”

“…a discernible human influence’ was written into the 1995 summary

report after the scientists themselves had gone home. Originally, the
document said scientists couldn’t detect a human influence on climate for
sure, and they didn’t know when they would. They said explicitly, ‘we
don’t know.’ That statement was deleted and replaced with a new
statement that a discernible human influence did indeed exist. It was a
major change.”

“… Changing the document caused a stir among scientists at the time,
with opponents and defendants of the change coming forward. If you read
their claims and counter-claims, you can’t be sure who’s telling the truth.
But this is the Internet age. You can find the original documents and the
list of changes online and decide for yourself. A review of the actual text
changes makes it crystal clear that the IPCC is a political organization,
not a scientific one.”

“… When Hansen announced in the summer of 1988 that global
warming was here, he predicted temperatures would increase .35
degrees Celsius over the next ten years.”

“… The actual increase was .11 degrees.”
“And ten years after his testimony, he said that the forces that govern

climate change are so poorly understood that long-term prediction is
impossible.”

“He said, ‘The forces that drive long-term climate change are not
known with an accuracy sufficient to define future climate change.”

EXCERPT #2:
The Shift from Global Warming to Climate Change.

Dr. Crichton clearly saw the shift in terminology, for political purposes.
Two of the characters in his novel, Drake and Henley, devise a way to keep
money flowing as the issue of global warming loses credibility.

“I hate global warming,” Drake said …

“… You can’t raise a dime with it, especially in winter. Every time it
snows people forget all about global warming. Or else they decide some
warming might be a good thing after all. They’re trudging through the
snow, hoping for a little global warming. It’s not like pollution, John.
Pollution worked. It still works. Pollution scares… people. You tell ‘em
they’ll get cancer, and the money rolls in. But nobody is scared of a little
warming. Especially if it won’t happen for a hundred years.”

“… Species extinction from global warming… They’ve heard that

most of the species that will become extinct are insects. You can’t raise
money on insect extinctions, John. Exotic diseases from global warming
—nobody cares. Hasn’t happened. We ran that huge campaign last year

connecting global warming to the Ebola and Hanta viruses. Nobody went
for it. Sea-level rise from global warming—we all know where that’ll
end up. The Vanutu lawsuit is a… disaster. Everybody’ll assume the sea
level isn’t rising anywhere. And that Scandinavian guy, that sea level
expert. He’s becoming a pest. He’s even attacking the IPCC for
incompetence.”

“… Let me explain how you are going to solve your problem,

Nicholas. The solution is simple. You have already told me that global
warming is unsatisfactory because whenever there is a cold snap, people
forget about it.”

“So what you need,”… “is to structure the information so that

whatever kind of weather occurs, it always confirms your message.
That’s the virtue of shifting the focus to abrupt climate change. It enables
you to use everything that happens. There will always be floods, and
freezing storms, and cyclones, and hurricanes. These events will always
get headlines and airtime. And in every instance, you can claim it as an
example of abrupt climate change caused by global warming. So the
message gets reinforced. The urgency is increased.”

“… It’s not logical to say that freezing weather is caused by global

warming.”

“What’s logic got to do with it?… All we need is for the media to

report it. … The US murder rate is as low as it was in the early 1970s,
but Americans are more frightened than ever, because so much more
airtime is devoted to crime, they naturally assume there is more in real
life too.”

“… Think about what I am saying… A twelve-year trend, and they still

don’t believe it. There is no greater proof that all reality is media
reality.”

“… it’ll be even easier to sell abrupt climate change in Europe than in

the US. You just do it out of Brussels. Because bureaucrats get it, …
They’ll see the advantages of this shift in emphasis.”

EXCERPT #3:
The Press-Driven Agenda.

The plain and simple formula of the media business is that money is made
when ratings are enhanced, and one of the sure-fire ways of boosting ratings

is to introduce a strong fear factor. Dr. Crichton captures this sense of self-
importance on the part of media personnel as few other authors ever have.
The scene below involves three media skeptics.

Then the anchors came back onscreen, and one of the men said, “Flood
advisories remain in effect, even though it is unseasonable for this time of
year.”

“Looks like the weather’s changing,” the anchorwoman said, tossing

her hair.

“Yes, Marla, there is no question the weather is changing. And here,

with that story, is our own Johnny Rivera.”

They cut to a younger man, apparently the weatherman. “Thanks, Terry.

Hi, everybody. If you’re a longtime resident of the Grand Canyon State,
you’ve probably noticed that our weather is changing, and scientists have
confirmed that what’s behind it is our old culprit, global warming.
Today’s flash flood is just one example of the trouble ahead—more
extreme weather conditions, like floods and tornadoes and droughts—all
as a result of global warming.”

Sanjong nudged Evans, and handed him a sheet of paper. It was a

printout of a press release from the NERF website. Sanjong pointed to
the text: “… scientists agree there will be trouble ahead: more extreme
weather events, like floods and tornadoes and drought, all as a result of
global warming.”

Evans said, “This guy’s just reading a press release?”
“That’s how they do it, these days,” Kenner said. “They don’t even

bother to change a phrase here and there. They just read the copy outright.
And of course, what he’s saying is not true.”

“Then what’s causing the increase in extreme weather around the

world?” Evans said.

“There is no increase in extreme weather.”
“That’s been studied?”
“Repeatedly. The studies show no increase in extreme weather events

over the past century. Or in the last fifteen years. And the GCMs don’t
predict more extreme weather. If anything, global warming theory
predicts less extreme weather.”

Onscreen, the weatherman was saying, “it is becoming so bad, that the

latest news is—get this—glaciers on Greenland are melting away and
will soon vanish entirely. Those glaciers are three miles thick, folks.
That’s a lotta ice. A new study estimate sea levels will rise twenty feet or
more. So sell that beach property now.”

Evans said, ‘What about that one? It was on the news in LA

yesterday.”

“I wouldn’t call it news,” Kenner said. “Scientists at Reading ran
computer simulations that suggested that Greenland might lose its ice
pack in the next thousand years.”
“Thousand years?” Evans said.
“Might.”
Evans pointed to the television. “He didn’t say it could happen a

thousand years from now.”

“Imagine that,” Kenner said. “He left that one out.”
“But you said it isn’t news…”
“You tell me,” Kenner said, “Do you spend much time worrying about

what might happen a thousand years from now?”

“No.”
“Think anybody should?”
“No.”
“There you are.”

EXCERPT #4:
The Rising Role of Hollywood.

In the novel, Dr. Crichton’s character Ted Bradley is portrayed in a role

that accurately demonstrates how Hollywood actors rejoice in letting
themselves be used to influence public opinion.

The forest floor was dark and cool. Shafts of sunlight filtered down from
the magnificent trees rising all around them. The air smelled of pine. The
ground was soft underfoot.

It was a pleasant spot, with sunlight dappling the forest floor, but even
so the television cameras had to turn on their lights to film the third-grade
schoolchildren who sat in concentric circles around the famous actor and

activist Ted Bradley. Bradley was wearing a black T-shirt that set off his
makeup and his dark good looks.

“These glorious trees are your birthright,” he said, gesturing all around
him. “They have been standing here for centuries. Long before you were
born, before your parents or your grandparents or your great-
grandparents were born. Some of them, before Columbus came to
America! Before the Indians came! Before anything! These trees are the
oldest living things on the planet; they are the guardians of the Earth; they
are wise; and they have a message for us: Leave the planet alone. Don’t
mess with it, or with us. And we must listen to them.”

The kids stared open-mouthed, transfixed. The cameras were trained

on Bradley.

“But now these magnificent trees—having survived the threat of fire,
the threat of logging, the threat of soil erosion, the threat of acid rain—
now face their greatest threat ever. Global warming. You kids know what
global warming is, don’t you?”

Hands went up all around the circle. “I know, I know!”
“I’m glad you do,” Bradley said, gesturing for the kids to put their

hands down. The only person talking today would be Ted Bradley. “But
you may not know that global warming is going to cause a very sudden
change in our climate. Maybe just a few months or years, and it will
suddenly be much hotter or much colder. And there will be hordes of
insects and diseases that will take down these wonderful trees.”

“What kind of insects?” one kid asked.
“Bad ones,” Bradley said. “The ones that eat trees, that worm inside
them and chew them up.” He wiggled his hands, suggesting the worming
in progress.

“It would take an insect a long time to eat a whole tree,” a girl offered.
“No, it wouldn’t!” Bradley said. “That’s the trouble. Because global

warming means lots and lots of insects will come—a plague of insects—
and they’ll eat the trees fast!”

Standing to one side, Jennifer leaned close to Evans. “Do you believe

this…?”

Evans yawned. He had slept on the flight up, and had dozed off again

in the ride from the airport to this grove in Sequoia National Park. He felt
groggy now, looking at Bradley. Groggy and bored.

By now the kids were fidgeting, and Bradley turned squarely to the

cameras. He spoke with the easy authority he had mastered while playing
the president for so many years on television. “The threat of abrupt
climate change,” he said, “is so devastating for mankind, and for all life
on this planet, that conferences are being convened all around the world
to deal with it. There is a conference in Los Angeles starting tomorrow,
where scientists will discuss what we can do to mitigate this terrible
threat. But if we do nothing, catastrophe looms. And these mighty,
magnificent trees will be a memory, a postcard from the past, a snapshot
of man’s inhumanity to the natural world. We’re responsible for
catastrophic climate change. And only we can stop it.”

He finished, with a slight turn to favor his good side, and a piercing

stare, from his baby blues, right into the lens.

She continued. “At first, arctic grasses and shrubs were the only plants

that could take hold in the barren glacial soil. But when they died they
decomposed, and over thousands of years a layer of topsoil built up. And
that initiated a sequence of plant colonization that was basically the same
everywhere in post-glacial North America.

“First, lodgepole pine comes in. That’s around fourteen thousand years
ago. Later it’s joined by spruce, hemlock, and alder—trees that are hardy
but can’t be first. These trees constitute the real ‘primary’ forest, and they
dominated this landscape for the next four thousand years. Then the
climate changed. It got much warmer, and all the glaciers in California

… “Twenty thousand years ago, the Ice Age glaciers receded from
California, gouging out Yosemite Valley and other beauty spots as they
left. As the ice walls withdrew, they left behind a gunky, damp plain with
lots of lakes fed by the melting glaciers, but no vegetation at all. It was
basically wet sand.

“After a few thousand years, the land dried as the glaciers continued to
move farther north. This region of California became arctic tundra, with
tall grasses supporting little animals, like mice and squirrels. Human
beings had arrived here by then, hunting the small animals and setting
fires.”

“Okay so far?” Jennifer said. “No primeval forests yet.”
“I’m listening,” Ted growled. He was clearly trying to control his

temper.

melted. There were no glaciers at all in California back then. It was
warm and dry, there were lots of fires, and the primary forest burned. It
was replaced by a plains-type vegetation of oak trees and prairie herbs.
And a few Douglas fir trees, but not many, because the climate was too
dry for fir trees.

“Then, around six thousand years ago, the climate changed again. It
became wetter, and the Douglas fir, hemlock, and cedar moved in and
took over the land, creating the great closed-canopy forests that you see
now. But someone might refer to these fir trees as a pest plant—an
oversized weed—that invaded the landscape, crowding out the native
plants that had been there before them. Because these big canopy forests
made the ground too dark for other trees to survive. And since there were
frequent fires, the closed-canopy forests were able to spread like mad.
So they’re not timeless, Ted. They’re merely the last in line.”

Bradley snorted. “They’re still 6 thousand years old…”
But Jennifer was relentless. “Not true,” she said. “Scientists have
shown that the forests continuously changed their composition. Each
thousand-year period was different from the one before it. The forests
changed constantly, Ted. And then, of course, there were the Indians.”

“What about them?” …
“The Indians were expert observers of the natural world, so they

realized that old-growth forests sucked. Those forests may look
impressive, but they’re dead landscapes for game. So the Indians set
fires, making sure the forests burned down periodically. They made sure
there were only islands of old-growth forest in the midst of plains and
meadows. The forests that the first Europeans saw were hardly primeval.
They were cultivated, Ted. And it’s not surprising that one hundred fifty
years ago, there was less old-growth forest than there is today. The
Indians were realists. Today, it’s all romantic mythology.” …

After a while, Bradley excused himself and went to the front of the
plane to call his agent. Evans said to Jennifer, “How did you know all
that stuff?”

“For the reason Bradley himself mentioned. The ‘dire threat of global

warming.’ We had a whole team researching dire threats. Because we
wanted to find everything we could to make our case as impressive as
possible.”

“And?”
She shook her head. “The threat of global warming,” she said, “is
essentially nonexistent. Even if it were a real phenomenon, it would
probably result in a net benefit.”

APPENDIX C

CLIMATEGATE: THE CRU CONTROVERSY

BIOS OF KEY PLAYERS AT THE TIME OF THE CRU
CONTROVERSY
Raymond Bradley
Currently a Professor in the Department of Geosciences and Director of the
Climate System Research Center at the University of Massachusetts Amherst.
Served as a Contributing Author in both the IPCC Third and Second
Assessment Report.

Keith Briffa
Deputy Director of the Climatic Research Unit, University of East Anglia.
Served as a Lead Author of the IPCC Fourth Assessment Report, a
Contributing Author and Reviewer of the IPCC Third Assessment Report,
and a Contributing Author of the IPCC Second Assessment Report.

Timothy Carter
Research Professor at the Finnish Environment Institute (SYKE), Helsinki,
Finland. Served as an Expert Reviewer of the IPCC Fourth Assessment
Report, Lead Author and Reviewer of the IPCC Third Assessment Report,
and Convening Lead Author of the IPCC Second Assessment Report.

Edward Cook
Doherty Senior Scholar at the Tree-Ring Laboratory, Lamont-Doherty Earth
Observatory, Palisades, New York. Served as a Contributing Author in the
IPCC Fourth, Third, and Second Assessment Reports.

Malcolm Hughes

Regents’ Professor in the Laboratory of Tree-Ring Research at the University
of Arizona. Served as a Contributing Author and Reviewer of the IPCC Third
Assessment Report.

Dr. Phil Jones
Professor at University of East Anglia’s CRU. Served as a Coordinating
Lead Author in the 2007 IPCC Fourth Assessment Report as well as an
Expert Reviewer. Also was a Contributing Author in both the IPCC Third
and IPCC Second Assessment Reports.

Thomas Karl
Designated Transitional Director of the NOAA Climate Service. Served as a
Review Editor of the IPCC Fourth Assessment Report, Coordinating Lead
Author and Lead Author of the IPCC Third Assessment Report, and both
Lead and Contributing Author on the IPCC Second Assessment Report. Also
has worked on multiple United States Global Change Research Programs
(USGCRP) including his work as a Co-Chair and Synthesis Team Member of
the USGCRP’s 2000 U.S. National Assessment and Co-Chair and one of
three Editors in Chief of the USGCRP’s 2009 Global Climate Change
Impacts in the United States Report. Also served as an Editor, Convening
Lead Author, and Author of the USGCRP’s 2008 Weather and Climate
Extremes in a Changing Climate Report. Was Chief Editor and Federal
Executive Team Member of the United States Climate Change Science
Program’s 2006 Temperature Trends in the Lower Atmosphere report.

Dr. Michael Mann
Professor and Director of Pennsylvania State University’s Earth System
Science Center. Served as an Expert Reviewer of the IPCC Fourth
Assessment Report as well as a Lead Author, Contributing Author, and
Reviewer of the IPCC Third Assessment Report.

Dr. Michael Oppenheimer
Albert G. Milbank Professor of Geosciences and International Affairs in the
Woodrow Wilson School and the Department of Geosciences at Princeton
University. Also is the Director of the Program in Science, Technology and
Environmental Policy (STEP) at the Woodrow Wilson School and Faculty

Associate of the Atmospheric and Ocean Sciences Program, Princeton
Environmental Institute, and the Princeton Institute for International and
Regional Studies. Served as a Lead Author, Contributing Author, and Expert
Reviewer of the IPCC Fourth Report; Lead Author, Contributing Author, and
Reviewer of the IPCC Third Assessment Report; and Contributing Author
and Technical Summary Author of the IPCC Second Assessment Report.

Dr. Jonathan Overpeck
Co-Director of the Institute of the Environment as well as a Professor in the
Department of Geosciences and the Department of Atmospheric Sciences at
the University of Arizona. Served as a Coordinating Lead Author,
Contributing Author, and Expert Reviewer of the IPCC Fourth Assessment
Report; and Contributing Author of the IPCC Third and Second Assessment
Reports.
Dr. Benjamin Santer
Research Scientist for the Program for Climate Model Diagnosis and
Intercomparison at the Lawrence Livermore National Laboratory. Served as
a Contributing Author in both the IPCC Fourth and Third Assessment Reports
as well as Convening Lead Author, Technical Summary and Contributing
Author of the IPCC Second Assessment Report. Also served as a Convening
Lead Author, Lead Author, and Contributing Author in the U.S. CCSP’s 2006
Temperature Trends in the Lower Atmosphere Report and Author of the
USGCRP’s 2009 Global Climate Change Impacts in the United States
Report.

Gavin Schmidt
working at NASA’s Goddard Institute for Space Studies. Served as a
Contributing Author and Expert Reviewer for the IPCC Fourth Assessment
Report.

Dr. Stephen Schneider
Melvin and Joan Lane Professor for Interdisciplinary Environmental Studies,
Professor of Biological Sciences, Professor (by courtesy) of Civil and
Environmental Engineering, and a Senior Fellow in the Woods Institute for
the Environment at Stanford University. Served as a Reviewer of the IPCC

Third Assessment Report and a Lead Author of the IPCC Second Assessment
Report.

Dr. Susan Solomon
Senior Scientist at the Chemical Sciences Division (CSD) Earth System
Research Laboratory (ESRL), NOAA. Served as a Co-Chair of the IPCC
Working Group I, Contributing Author of the IPCC Fourth Assessment
Report, and a Lead Author of the IPCC Third Assessment Report.

Peter Stott
Climate Monitoring Expert and Head of Climate Monitoring and Attribution
at the Met Office Hadley Centre. Served as a Lead Author, Contributing
Author, and Expert Reviewer of the IPCC Fourth Assessment Report and as a
Contributing Author and Reviewer of the IPCC Third Assessment Report.
Dr. Kevin Trenberth
Senior Scientist and Head of the Climate Analysis Section at the National
Center for Atmospheric Research. Served as a Coordinating Lead Author,
Contributing Author, and Expert Reviewer of the IPCC Fourth Assessment
Report; Lead Author, Contributing Author, and Reviewer of the IPCC Third
Assessment Report; and Convening Lead Author, Technical Summary Author,
and Contributing Author of the IPCC Second Assessment Report.

Dr. Thomas Wigley
Senior Scientist in the Climate and Global Dynamics Division, University
Corporation for Atmospheric Research. Served as a Contributing Author of
the IPCC Fourth and Third Assessment Reports as well as a Lead Author and
Contributing Author of the IPCC Second Assessment Report. Also was a
Convening Lead Author and Contributing Author of U.S. CCSP’s 2006
Temperature Trends in the Lower Atmosphere report.

A SAMPLING OF EMAILS AND DOCUMENTS
The following is a preliminary sampling of CRU emails and documents
which I believe seriously compromise the IPCC-backed “consensus” and its
central conclusion that anthropogenic emissions are inexorably leading to

environmental catastrophes, and which represent unethical and possibly
illegal conduct by top IPCC scientists, among others. In the interest of
brevity, many of the emails are not reproduced in their entirety. Therefore,
the reader is encouraged to seek outside sources for broader review and
context of the exposed emails and documents. The emails are reproduced in
chronological order from oldest to newest on a variety of topics related to
the participant’s global warming research. These emails have been widely
circulated:

From: Michael E. Mann [University of Virginia]
To: Tim Osborn [CRU]
July 31, 2003
Subject: Re: reconstruction errors
Tim,
Attached are the calibration residual series for experiments based on
available networks back to:
AD 1000
AD 1400
AD 1600
I can’t find the one for the network back to 1820! But basically, you’ll
see that the residuals are pretty red for the first 2 cases, and then not
significantly red for the 3rd case--its even a bit better for the AD 1700
and 1820 cases, but I can’t seem to dig them up… . p.s. I know I probably
don’t need to mention this, but just to insure absolutely clarify on this, I’m
providing these for your own personal use, since you’re a trusted
colleague. So please don’t pass this along to others without checking
w/me first. This is the sort of “dirty laundry” one doesn’t want to fall into
the hands of those who might potentially try to distort things…

From: Phil Jones [CRU]
To: Michael E. Mann [University of Virginia]
January 16, 2004
Subject: CLIMATIC CHANGE needs your advice—YOUR EYES
ONLY !!!!!
Mike,

This is for YOURS EYES ONLY. Delete after reading—please ! I’m
trying to redress the balance. One reply from Pfister said you should
make all available !! Pot calling the kettle black—Christian doesn’t make
his methods available. I replied to the wrong Christian message so you
don’t get to see what he said. Probably best. Told Steve separately and to
get more advice from a few others as well as Kluwer and legal. PLEASE
DELETE—just for you, not even Ray and Malcolm

From: Phil Jones
To: Tas van Ommen [University of Tasmania, Australia]
Cc: Michael E. Mann [University of Virginia]
February 9, 2004
Subject: Re: FW: Law Dome O18
Dear Tas,
Thanks for the email. Steve McIntyre hasn’t contacted me directly about
Law Dome (yet), nor about any of the series used in the 1998 Holocene
paper or the 2003 GRL one with Mike. I suspect (hope) that he won’t. I
had some emails with him a few years ago when he wanted to get all the
station temperature data we use here in CRU. I hid behind the fact that
some of the data had been received from individuals and not directly
from Met Services through the Global Telecommunications Service
(GTS) or through GCOS. I’ve cc’d Mike on this, just for info. Emails
have also been sent to some other paleo people asking for datasets used
in 1998 or 2003. Keith Briffa here got one, for example. Here, they have
also been in contact with some of Keith’s Russian contacts. All seem to
relate to trying to get series we’ve used.

From: Michael E. Mann [University of Virginia]
To: Phil Jones [CRU]; Gabi Hergerl [Duke University]
August ??, 2004
[Subject: Mann and Jones (2003)]
Dear Phil and Gabi,
I’ve attached a cleaned-up and commented version of the matlab code
that I wrote for doing the Mann and Jones (2003) composites. I did this
knowing that Phil and I are likely to have to respond to more crap
criticisms from the idiots in the near future, so best to clean up the code

and provide to some of my close colleagues in case they want to test it,
etc. Please feel free to use this code for your own internal purposes, but
don’t pass it along where it may get into the hands of the wrong people…
.

From: Tom Wigley [University Corporation of Atmospheric Research]
To: Phil Jones [CRU]
January 21, 2005
Phil,
Thanks for the quick reply. The leaflet appeared so general, but it was
prepared by UEA so they may have simplified things. From their
wording, computer code would be covered by the FOIA. My concern
was if Sarah is/was still employed by UEA. I guess she could claim that
she had only written one tenth of the code and release every tenth line.
Sorry I won’t see you, but I will not come up to Norwich until Monday.

From: Phil Jones [CRU]
To: Tom Wigley [University Corporation of Atmospheric Research]
Cc: Ben Santer [Lawrence Livermore National Laboratory]
January 21st, 2005
Subject: Re: FOIA
Tom,
… As for FOIA Sarah isn’t technically employed by UEA [University of
East Anglia] and she will likely be paid by Manchester Metropolitan
University. I wouldn’t worry about the code. If FOIA does ever get used
by anyone, there is also IPR [intellectual property rights] to consider as
well. Data is covered by all the agreements we sign with people, so I
will be hiding behind them. I’ll be passing any requests onto the person at
UEA who has been given a post to deal with them.

From: Phil Jones [CRU]
To: Michael E. Mann [University of Virginia]
February 2, 2005
[Subject: For your eyes only]
Mike,

I presume congratulations are in order—so congrats etc ! Just sent loads
of station data to Scott. Make sure he documents everything better this
time ! And don’t leave stuff lying around on ftp [file transfer protocol]
sites—you never know who is trawling them. The two MMs have been
after the CRU station data for years. If they ever hear there is a Freedom
of Information Act now in the UK, I think I’ll delete the file rather than
send to anyone. Does your similar act in the US force you to respond to
enquiries within 20 days?—our does ! The UK works on precedents, so
the first request will test it. We also have a data protection act, which I
will hide behind. Tom Wigley has sent me a worried email when he
heard about it—thought people could ask him for his model code. He has
retired officially from UEA so he can hide behind that. IPR [intellectual
property rights] should be relevant here, but I can see me getting into an
argument with someone at UEA who’ll say we must adhere to it !

From: Michael E. Mann [University of Virginia]
To: Phil Jones [CRU]
February 2, 2005
Thanks Phil,
Yes, we’ve learned out lesson about FTP. We’re going to be very careful
in the future what gets put there. Scott really screwed up big time when
he established that directory so that Tim could access the data. Yeah,
there is a freedom of information act in the U.S., and the contrarians are
going to try to use it for all its worth. But there are also intellectual
property rights issues, so it isn’t clear how these sorts of things will play
out ultimately in the U.S. I saw the paleo draft (actually I saw an early
version, and sent Keith some minor comments). It looks very good at
present—will be interesting to see how they deal w/the contrarian
criticisms—there will be many. I’m hoping they’ll stand firm (I believe
they will—I think the chapter has the right sort of personalities for that)
…

From: Phil Jones [CRU]
To: Michael E. Mann [University of Virginia]
Cc: Raymond Bradley [University of Massachusetts, Amherst]; Malcolm
Hughes [University of Arizona]

February 21, 2005
Subject: Fwd: CCNet: PRESSURE GROWING ON CONTROVERSIAL
RESEARCHER TO DISCLOSE SECRET DATA
Mike, Ray and Malcolm,
The skeptics seem to be building up a head of steam here ! Maybe we can
use this to our advantage to get the series updated ! Odd idea to update
the proxies with satellite estimates of the lower troposphere rather than
surface data !. Odder still that they don’t realise that Moberg et al used
the Jones and Moberg updated series ! Francis Zwiers is till onside. He
said that PC1s produce hockey sticks. He stressed that the late 20th
century is the warmest of the millennium, but Regaldo didn’t bother with
that. Also ignored Francis’ comment about all the other series looking
similar to MBH [Mann Bradley Hughes]. The IPCC comes in for a lot of
stick. Leave it to you to delete as appropriate!
Cheers
Phil
PS I’m getting hassled by a couple of people to release the CRU station
temperature data.
Don’t any of you three tell anybody that the UK has a Freedom of
Information Act !

From: Phil Jones [CRU]
To: Eugene R. Wahl [Alfred University]; Caspar Ammann [University
Corporation of Atmospheric Research]
September 12, 2007
Subject: Wahl/Ammann
Gene/Caspar,
Good to see these two out. Wahl/Ammann doesn’t appear to be in CC’s
online first, but comes up if you search. You likely know that McIntyre
will check this one to make sure it hasn’t changed since the IPCC close-
off date July 2006! Hard copies of the WG1 report from CUP have
arrived here today. Ammann/Wahl—try and change the Received date!
Don’t give those skeptics something to amuse themselves with.

From: Phil Jones [CRU]
To: Michael E. Mann [Penn State University]

May 29, 2008
Subject: IPCC & FOI
Mike,
Can you delete any emails you may have had with Keith re AR4 [IPCC
Fourth Assessment Report]? Keith will do likewise. He’s not in at the
moment—minor family crisis. Can you also email Gene and get him to do
the same? I don’t have his new email address. We will be getting Caspar
to do likewise. I see that CA [Climate Audit website] claim they
discovered the 1945 problem in the Nature paper!!
Cheers
Phil

From: Michael E. Mann [Penn State University]
To: Phil Jones [CRU]
May 29, 2008
Subject: Re: IPCC & FOI
Hi Phil,
laughable that CA [Climate Audit] would claim to have discovered the
problem. They would have run off to the Wall Street Journal for an
exclusive were that to have been true. I’ll contact Gene about this
[deleting emails] ASAP. His new email is: …
talk to you later,
Mike

From: Phil Jones [CRU]
To: Gavin Schmidt [NASA Goddard Institute for Space Studies]
Cc: Michael E. Mann [Penn State University]
August 20, 2008
Gavin,
… Thinking about the final bit for the Appendix. Keith should be in later,
so I’ll check with him—and look at that vineyard book. I did rephrase the
bit about the ‘evidence’ as Lamb refers to it. I wanted to use his phrasing
—he used this word several times in these various papers. What he
means is his mind and its inherent bias(es). Your final sentence though
about improvements in reviewing and traceability is a bit of a hostage to

fortune. The skeptics will try to hang on to something, but I don’t want to
give them something clearly tangible. Keith/Tim still getting FOI requests
as well as MOHC [Meteorological Office Hadley Center] and Reading.
All our FOI officers have been in discussions and are now using the same
exceptions not to respond—advice they got from the Information
Commissioner… . The FOI line we’re all using is this. IPCC is exempt
from any countries FOI—the skeptics have been told this. Even though
we (MOHC, CRU/UEA) possibly hold relevant info the IPCC is not part
our remit (mission statement, aims etc) therefore we don’t have an
obligation to pass it on.

From: Phil Jones [CRU]
To: Unknown list
March 10, 2003
[Subject: Soon & Baliunas]
Dear all,
Tim Osborn has just come across this. Best to ignore probably, so don’t
let it spoil your day. I’ve not looked at it yet. It results from this journal
having a number of editors. The responsible one for this is a well-known
skeptic in NZ. He has let a few papers through by Michaels and Gray in
the past. I’ve had words with Hans von Storch about this, but got
nowhere. Another thing to discuss in Nice !
Cheers
Phil

From: Phil Jones
To: Raymond Bradley [University of Massachusetts, Amherst]; Malcolm
Hughes [University of Arizona]; Scott Rutherford [University of Rhode
Island]; Michael E. Mann [University of Virginia]; Tom Crowley [Duke
University]
Cc: Keith Briffa [CRU]; Jonathan Overpeck [University of Arizona];
Edward Cook [Columbia University]; Keith Alverson [IGBP-PAGES]
March 11, 2003
Subject: Fwd: Soon & Baliunas
Dear All,

Apologies for sending this again. I was expecting a stack of emails this
morning in response, but I inadvertently left Mike off (mistake in pasting)
and picked up Tom’s old address. Tom is busy though with another
offspring ! I looked briefly at the paper last night and it is appalling—
worst word I can think of today without the mood pepper appearing on
the email ! I’ll have time to read more at the weekend as I’m coming to
the US for the DoE CCPP meeting at Charleston. Added Ed, Peck and
Keith A. onto this list as well. I would like to have time to rise to the
bait, but I have so much else on at the moment. As a few of us will be at
the EGS/AGU meet in Nice, we should consider what to do there. The
phrasing of the questions at the start of the paper determine the answer
they get. They have no idea what multiproxy averaging does. By their
logic, I could argue 1998 wasn’t the warmest year globally, because it
wasn’t the warmest everywhere. With their LIA [Little Ice Age] being
1300-1900 and their MWP [Medieval Warm Period] 800-1300, there
appears (at my quick first reading) no discussion of synchroneity of the
cool/warm periods. Even with the instrumental record, the early and late
20th century warming periods are only significant locally at between 10-
20% of grid boxes. Writing this I am becoming more convinced we
should do something—even if this is just to state once and for all what
we mean by the LIA and MWP. I think the skeptics will use this paper to
their own ends and it will set paleo[climatology] back a number of years
if it goes unchallenged. I will be emailing the journal to tell them I’m
having nothing more to do with it until they rid themselves of this
troublesome editor. A CRU person is on the editorial board, but papers
get dealt with by the editor assigned by Hans von Storch.
Cheers
Phil

From: Michael E. Mann [University of Virginia]
To: Phil Jones [CRU]; Raymond Bradley [University of Massachusetts,
Amherst]; Malcolm Hughes [University of Arizona]; Scott Rutherford
[University of Rhode Island]; Tom Crowley [Duke University]
Cc: Keith Briffa [CRU]; Jonathan Overpeck [University of Arizona];
Edward Cook [Columbia University]; Keith Alverson [IGBP-PAGES];
Mike MacCracken [Climate Institute]

March 11, 2003
Subject: Re: Fwd: Soon & Baliunas
Thanks Phil,
(Tom: Congrats again!)
The Soon & Baliunas paper couldn’t have cleared a ‘legitimate’ peer
review process anywhere. That leaves only one possibility—that the
peer-review process at Climate Research has been hijacked by a few
skeptics on the editorial board. And it isn’t just De Frietas, unfortunately
I think this group also includes a member of my own department… The
skeptics appear to have staged a ‘coup’ at “Climate Research” (it was a
mediocre journal to begin with, but now its a mediocre journal with a
definite ‘purpose’). Folks might want to check out the editors and review
editors: [1]http://www.int-res.com/journals/cr/crEditors.html In fact,
Mike McCracken first pointed out this article to me, and he and I have
discussed this a bit. I’ve cc’d Mike in on this as well, and I’ve included
Peck too. I told Mike that I believed our only choice was to ignore this
paper. They’ve already achieved what they wanted—the claim of a peer-
reviewed paper. There is nothing we can do about that now, but the last
thing we want to do is bring attention to this paper, which will be ignored
by the community on the whole… It is pretty clear that thee skeptics here
have staged a bit of a coup, even in the presence of a number of
reasonable folks on the editorial board (Whetton, Goodess, …). My
guess is that Von Storch is actually with them (frankly, he’s an odd
individual, and I’m not sure he isn’t himself somewhat of a skeptic
himself), and without Von Storch on their side, they would have a very
forceful personality promoting their new vision. There have been several
papers by Pat Michaels, as well as the Soon & Baliunas paper, that
couldn’t get published in a reputable journal. This was the danger of
always criticising the skeptics for not publishing in the “peer-reviewed
literature”. Obviously, they found a solution to that--take over a journal!
So what do we do about this? I think we have to stop considering
“Climate Research” as a legitimate peer-reviewed journal. Perhaps we
should encourage our colleagues in the climate research community to no
longer submit to, or cite papers in, this journal. We would also need to
consider what we tell or request of our more reasonable colleagues who
currently sit on the editorial board… What do others think?
Mike

From: Michael E. Mann [University of Virginia]
To: Malcolm Hughes [University of Arizona]
March 11, 2003
HI Malcolm,
Thanks for the feedback--I largely concur. I do, though, think there is a
particular problem with “Climate Research”. This is where my colleague
Pat Michaels now publishes exclusively, and his two closest colleagues
are on the editorial board and review editor board. So I promise you,
we’ll see more of this there, and I personally think there *is* a bigger
problem with the “messenger” in this case…

From: Phil Jones [CRU]
To: Unknown List
March 12, 2003
Dear All,
I agree with all the points being made and the multi-authored article
would be a good idea, but how do we go about not letting it get buried
somewhere. Can we not address the misconceptions by finally coming up
with definitive dates for the LIA and MWP and redefining what we think
the terms really mean? With all of us and more on the paper, it should
carry a lot of weight. In a way we will be setting the agenda for what
should be being done over the next few years…

From: Tom Wigley [University Corporation of Atmospheric Research]
To: Phil Jones [CRU]; Keith Briffa [CRU]; James Hansen [NASA
Goddard Institute for Space Studies]; Michael E. Mann [University of
Virginia]; Ben Santer [Lawrence Livermore National Laboratory];
Thomas R Karl [NOAA]; Mark Eakin [NOAA]; et al.
April 23, 2003
Subject: My turn
… This second case gets to the crux of the matter. I suspect that deFreitas
deliberately chose other referees who are members of the skeptics camp.
I also suspect that he has done this on other occasions. How to deal with
this is unclear, since there are a number of individuals with bona fide
scientific credentials who could be used by an unscrupulous editor to

ensure that ‘anti-greenhouse’ science can get through the peer review
process (Legates, Balling, Lindzen, Baliunas, Soon, and so on). The peer
review process is being abused, but proving this would be difficult. The
best response is, I strongly believe, to rebut the bad science that does get
through. Jim Salinger raises the more personal issue of deFreitas. He is
clearly giving good science a bad name, but I do not think a barrage of ad
hominem attacks or letters is the best way to counter this. If Jim wishes to
write a letter with multiple authors, I may be willing to sign it, but I
would not write such a letter myself. In this case, deFreitas is such a poor
scientist that he may simply disappear. I saw some work from his PhD,
and it was awful (Pat Michaels’ PhD is at the same level).
Best wishes to all,
Tom.

From: Mark Eakin [NOAA]
To: Michael E. Mann [University of Virginia]; et al.
April 24th, 2003
[Subject: My turn]
… A letter to OSTP [White House Office of Science and Technology
Policy] is probably in order here. Since the White House has shown
interest in this paper, OSTP really does need to receive a measured,
critical discussion of flaws in Soon and Baliunas’ methods. I agree with
Tom that a noted group from the detection and attribution effort such as
Mann, Crowley, Briffa, Bradley, Jones and Hughes should spearhead
such a letter. Many others of us could sign on in support. This would
provide Dave Halpern with the ammunition he needs to provide the
White House with the needed documentation that hopefully will dismiss
this paper for the slipshod work that it is. Such a letter could be
developed in parallel with a rebuttal article…

From: Timothy Carter [Finnish Environment Institute]
To: Tom Wigley [University Corporation of Atmospheric Research]
April ??, 2003
[Subject: Java climate model]
… P.S. On the CR [Climate Research] issue, I agree that a rebuttal seems
to be the only method of addressing the problem (I communicated this to

Mike yesterday morning), and I wonder if a review of the refereeing
policy is in order. The only way I can think of would be for all papers to
go through two Editors rather than one, the former to have overall
responsibility, the latter to provide a second opinion on a paper and
reviewers’ comments prior to publication. A General Editor would be
needed to adjudicate in the event of disagreement. Of course, this could
then slow down the review process enormously. However, without an
editorial board to vote someone off, how can suspect Editors be removed
except by the Publisher (in this case, Inter-Research).

From: Tom Wigley [University Corporation of Atmospheric Research]
To: Timothy Carter [Finnish Environment Institute]
Cc: Mike Hulme [CRU]; Phil Jones [CRU]
April 24, 2003 Subject: Re: Java climate model
… PS Re CR, I do not know the best way to handle the specifics of the
editoring. Hans von Storch is partly to blame—he encourages the
publication of crap science ‘in order to stimulate debate’. One approach
is to go direct to the publishers and point out the fact that their journal is
perceived as being a medium for disseminating misinformation under the
guise of refereed work. I use the word ‘perceived’ here, since whether it
is true or not is not what the publishers care about—it is how the journal
is seen by the community that counts. I think we could get a large group of
highly credentialed scientists to sign such a letter—50+ people. Note that
I am copying this view only to Mike Hulme and Phil Jones. Mike’s idea
to get editorial board members to resign will probably not work—must
get rid of von Storch too, otherwise holes will eventually fill up with
people like Legates, Balling, Lindzen, Michaels, Singer, etc. I have heard
that the publishers are not happy with von Storch, so the above approach
might remove that hurdle too.

From: Edward Cook [Columbia University]
To: Keith Briffa [CRU]
June 4, 2003
[Subject: Review-confidential REALLY URGENT]
Hi Keith,

Okay, today. Promise! Now something to ask from you. Actually
somewhat important too. I got a paper to review (submitted to the Journal
of Agricultural, Biological, and Environmental Sciences), written by a
Korean guy and someone from Berkeley, that claims that the method of
reconstruction that we use in dendroclimatology (reverse regression) is
wrong, biased, lousy, horrible, etc. They use your Tornetrask recon as the
main whipping boy… . I would like to play with it in an effort to refute
their claims. If published as is, this paper could really do some damage.
It is also an ugly paper to review because it is rather mathematical, with
a lot of Box-Jenkins stuff in it. It won’t be easy to dismiss out of hand as
the math appears to be correct theoretically but it suffers from the classic
problem of pointing out theoretical deficiencies … I am really sorry but I
have to nag about that review—Confidentially I now need a hard and if
required extensive case for rejecting—to support Dave Stahle’s and
really as soon as you can.

From: Andrew Comrie [University of Arizona]
To: Phil Jones [CRU]
May, 2004
[Subject: IJOC040512 review]
Dear Prof. Jones,
IJOC040512 “A Socioeconomic Fingerprint on the Spatial Distribution
of Surface Air Temperature Trends”
Authors: RR McKitrick & PJ Michaels
Target review date: July 5, 2004
I know you are very busy, but do you have the time to review the above
manuscript [from skeptics McKitrick and Michaels] for the International
Journal of Climatology? If yes, can you complete the review within about
five to six weeks, say by the target review date listed above? I will send
the manuscript electronically. If no, can you recommend someone who
you think might be a good choice to review this paper? …
[Note: In the peer review process, reviewer’s names are kept
anonymous.]

From: Phil Jones [CRU]
To: Andrew Comrie [University of Arizona]

May 24, 2004
Subject: RE: IJOC040512 review
Andrew,
I can do this. I am in France this week but back in the UK all June. So
send and it will be waiting my return.
Phil

From: Phil Jones [CRU]
To: Michael E. Mann [University of Virginia]
August 13, 2004
Subject: Fwd: RE: IJOC040512 review
Mike,
The paper ! Now to find my review. I did suggest to Andrew to find
3 reviewers.
Phil

From: Michael E. Mann [University of Virginia]
To: Phil Jones [CRU]
August 13, 2004
[Subject: IJOC040512 review]
Thanks a bunch Phil,
Along lines as my other email, would it be (?) for me to forward this to
the chair of our commitee confidentially, and for his internal purposes
only, to help bolster the case against MM [skeptics McKitrick and
Michaels]?? let me know…
thanks,
Mike

From: Phil Jones [CRU]
To: Michael E. Mann [University of Virginia]
August 13, 2004
Subject: Re: Fwd: RE: IJOC040512 review
Mike,

I’d rather you didn’t. I think it should be sufficient to forward the para
from Andrew Conrie’s email that says the paper has been rejected by all
3 reviewers. You can say that the paper was an extended and updated
version of that which appeared in CR. Obviously, under no circumstances
should any of this get back to Pielke.
Cheers
Phil

From: Phil Jones [CRU]
To: Michael E. Mann [University of Virginia]
July 8, 2004
Subject: HIGHLY CONFIDENTIAL
Mike,
Only have it in the pdf form. FYI ONLY—don’t pass on. Relevant paras
are the last 2 in section 4 on p13. As I said it is worded carefully due to
Adrian knowing Eugenia for years. He knows the’re wrong, but he
succumbed to her almost pleading with him to tone it down as it might
affect her proposals in the future ! I didn’t say any of this, so be careful
how you use it—if at all. Keep quiet also that you have the pdf. The
attachment is a very good paper—I’ve been pushing Adrian over the last
weeks to get it submitted to JGR [Journal of Geophysical Research] or J.
Climate [Journal of Climate]. The main results are great for CRU and
also for ERA-40. The basic message is clear—you have to put enough
surface and sonde obs into a model to produce Reanalyses. The jumps
when the data input change stand out so clearly. NCEP does many odd
things also around sea ice and over snow and ice… . The other paper by
MM is just garbage—as you knew. De Freitas again. Pielke is also losing
all credibility as well by replying to the mad Finn as well—frequently as
I see it. I can’t see either of these papers being in the next IPCC report.
Kevin and I will keep them out somehow—even if we have to redefine
what the peer-review literature is!
Cheers
Phil
     Mike,
For your interest, there is an ECMWF ERA-40 Report coming out soon,
which shows that Kalnay and Cai are wrong. It isn’t that strongly worded

as the first author is a personal friend of Eugenia. The result is rather
hidden in the middle of the report. It isn’t peer review, but a slimmed
down version will go to a journal. KC are wrong because the difference
between NCEP and real surface temps (CRU) over eastern N. America
doesn’t happen with ERA-40. ERA-40 assimilates surface temps (which
NCEP didn’t) and doing this makes the agreement with CRU better. Also
ERA-40’s trends in the lower atmosphere are all physically consistent
where NCEP’s are not—over eastern US. I can send if you want, but it
won’t be out as a report for a couple of months.
Cheers
Phil

From: Stephen Mackwell [Universities Space Research Association]
To: Michael E. Mann [University of Virginia]
Cc: Chris Reason [University of Cape Town]; James Saiers [Yale
University]
January 20, 2005
Subject: Your concerns with 2004GL021750 McIntyre
Dear Prof. Mann
In your recent email to Chris Reason, you laid out your concerns that I
presume were the reason for your phone call to me last week. I have
reviewed the manuscript by McIntyre, as well as the reviews. The editor
in this case was Prof. James Saiers. He did note initially that the
manuscript did challenge published work, and so felt the need for an
extensive and thorough review. For that reason, he requested reviews
from 3 knowledgable scientists. All three reviews recommended
publication. While I do agree that this manuscript does challenge
(somewhat aggresively) some of your past work, I do not feel that it takes
a particularly harsh tone. On the other hand, I can understand your
reaction. As this manuscript was not written as a Comment, but rather as
a full-up scientific manuscript, you would not in general be asked to look
it over. And I am satisfied by the credentials of the reviewers. Thus, I do
not feel that we have sufficient reason to interfere in the timely
publication of this work. However, you are perfectly in your rights to
write a Comment, in which you challenge the authors’ arguments and
assertions. Should you elect to do this, your Comment would be provided

to them and they would be offered the chance to write a Reply. Both
Comment and Reply would then be reviewed and published together (if
they survived the review process). Comments are limited to the
equivalent of 2 journal pages.
Regards
Steve Mackwell
Editor in Chief, GRL [Geophysical Research Letters]

From: Michael E. Mann [University of Virginia]
The following individuals may have been recipients: Tom Wigley
[University Corporation of Atmospheric Research]; Raymond Bradley
[University of Massachusetts, Amherst]; Tom Osborn [CRU]; Phil Jones
[CRU]; Keith Briffa [CRU]; Gavin Schmidt [NASA Goddard Institute for
Space Studies]; Malcolm Hughes [University of Arizona]; [Subject: Your
concerns with 2004GL021750 McIntyre]
January 20, 2005
Dear All,
Just a heads up. Apparently, the contrarians now have an “in” with GRL
[Geophysical Research Letters]. This guy Saiers has a prior connection
w/the University of Virginia Dept. of Environmental Sciences that causes
me some unease. I think we now know how the various Douglass et al
papers w/Michaels and Singer, the Soon et al paper, and now this one
have gotten published in GRL,
Mike

From: Tom Wigley [University Corporation of Atmospheric Research]
To: Michael E. Mann [University of Virginia]
The following individuals may also have been recipients: Raymond
Bradley [University of Massachusetts, Amherst]; Tom Osborn [CRU];
Phil Jones [CRU]; Keith Briffa [CRU]; Gavin Schmidt [NASA Goddard
Institute for Space Studies]; Malcolm Hughes [University of Arizona];
January 20, 2005
[Subject: Your concerns with 2004GL021750 McIntyre]
Mike,

This is truly awful. GRL [Geophysical Research Letters] has gone
downhill rapidly in recent years. I think the decline began before Saiers.
I have had some unhelpful dealings with him recently with regard to a
paper Sarah and I have on glaciers—it was well received by the
referees, and so is in the publication pipeline. However, I got the
impression that Saiers was trying to keep it from being published.
Proving bad behavior here is very difficult. If you think that Saiers is in
the greenhouse skeptics camp, then, if we can find documentary evidence
of this, we could go through official AGU [American Geophysical Union]
channels to get him ousted.

From: Michael E. Mann [University of Virginia]
To: Tom Wigley [University Corporation of Atmospheric Research] The
following individuals may also have been recipients: Raymond Bradley
[University of Massachusetts, Amherst]; Tom Osborn [CRU]; Phil Jones
[CRU]; Keith Briffa [CRU]; Gavin Schmidt [NASA Goddard Institute for
Space Studies]; Malcolm Hughes [University of Arizona];
January 20, 2005
[Subject: Your concerns with 2004GL021750 McIntyre]
Thanks Tom,
Yeah, basically this is just a heads up to people that something might be
up here. What a shame that would be. It’s one thing to lose “Climate
Research”. We can’t afford to lose GRL [Geophysical Research Letters].
I think it would be useful if people begin to record their experiences
w/both Saiers and potentially Mackwell (I don’t know him--he would
seem to be complicit w/what is going on here). If there is a clear body of
evidence that something is amiss, it could be taken through the proper
channels. I don’t that the entire AGU [American Geophysical Union]
hierarchy has yet been compromised! The GRL article simply parrots the
rejected Nature comment--little substantial difference that I can see at all.
Will keep you all posted of any relevant developments,
Mike

From: Michael E. Mann [University of Virginia]
To: Malcolm Hughes [University of Arizona]

The following individuals may also have been recipients: Tom Wigley
[University Corporation of Atmospheric Research]; Raymond Bradley
[University of Massachusetts, Amherst]; Tom Osborn [CRU]; Phil Jones
[CRU]; Keith Briffa [CRU]; Gavin Schmidt [NASA Goddard Institute for
Space Studies]; Malcolm Hughes [University of Arizona]; January 20 or
21, 2005 [Subject: Your concerns with 2004GL021750 McIntyre]
Hi Malcolm,
This assumes that the editor/s in question would act in good faith. I’m not
convinced of this. I don’t believe a response in GRL is warranted in any
case. The MM claims in question are debunked in other papers that are in
press and in review elsewhere. I’m not sure that GRL can be seen as an
honest broker in these debates anymore, and it is probably best to do an
end run around GRL now where possible. They have published far too
many deeply flawed contrarian papers in the past year or so. There is no
possible excuse for them publishing all 3 Douglass papers and the Soon
et al paper. These were all pure crap. There appears to be a more
fundamental problem w/GRL now, unfortunately…
Mike

From: Ben Santer [Lawrence Livermore National Laboratory]
To: Phil Jones [CRU]
March 19, 2009
[Subject: See the link below]
… If the RMS [Royal Meteorological Society] is going to require authors
to make ALL data available—raw data PLUS results from all
intermediate calculations—I will not submit any further papers to RMS
journals.
Cheers,
Ben

From: Phil Jones [CRU]
To: Ben Santer [Lawrence Livermore National Laboratory]
March 19, 2009
Subject: Re: See the link below
… I’m having a dispute with the new editor of Weather. I’ve complained
about him to the RMS Chief Exec. If I don’t get him to back down, I

won’t be sending any more papers to any RMS journals and I’ll be
resigning from the RMS.

From: Kevin Trenberth [University Corporation of Atmospheric
Research]
To: Michael E. Mann [Penn State University]
Cc: Grant Foster; Phil Jones [CRU]; Gavin Schmidt [NASA Goddard
Institute for Space Studies]; et al.
July 29, 2009
Subject: Re: ENSO blamed over warming—paper in JGR
Hi all
Wow this is a nice analysis by Grant et al. What we should do is turn this
into a learning experience for everyone: there is often misuse of filtering.
Obviously the editor and reviewers need to to also be taken to task here.
I agree with Mike Mann that a couple of other key points deserve to be
made wrt this paper… .

From: Keith Briffa [CRU]
To: Chris Folland [UK Met Office]; Phil Jones [CRU]; Michael E.
Mann [University of Virginia]
Cc: Tom Karl [National Climatic Data Center—NOAA]
September 22, 1999
Subject: RE: IPCC revisions
… I know there is pressure to present a nice tidy story as regards
‘apparent unprecedented warming in a thousand years or more in the
proxy data’ but in reality the situation is not quite so simple. We don’t
have a lot of proxies that come right up to date and those that do (at least
a significant number of tree proxies) some unexpected changes in
response that do not match the recent warming… .

From: Phil Jones [CRU]
To: Ray Bradley [University of Massachusetts, Amherst]; Michael
E. Mann [University of Virginia]; Malcolm Hughes [University of
Arizona]
Cc: Keith Briffa [CRU]; Tom Osborn [CRU]

November 16, 1999
Subject: Diagram for WMO [World Meteorological Organization]
Statement
Dear Ray, Mike and Malcolm,
Once Tim’s got a diagram here we’ll send that either later today or first
thing tomorrow. I’ve just completed Mike’s Nature trick of adding in the
real temps to each series for the last 20 years (ie from 1981 onwards)
amd from 1961 for Keith’s to hide the decline. Mike’s series got the
annual land and marine values while the other two got April-Sept for NH
[Northern Hemisphere] land N of 20N. The latter two are real for 1999,
while the estimate for 1999 for NH combined is +0.44C wrt 61-90. The
Global estimate for 1999 with data through Oct is +0.35C cf. 0.57 for
1998. Thanks for the comments, Ray.
Cheers
Phil

From: Giorgi Filippo [International Centre for Theoretical Physics]
To: Chapter 10 LAs
September 11, 2000
Subject: On “what to do?”
Given this, I would like to add my own opinion developed through the
weekend. First let me say that in general, as my own opinion, I feel rather
unconfortable about using not only unpublished but also un reviewed
material as the backbone of our conclusions (or any conclusions). I
realize that chapter 9 is including SRES stuff, and thus we can and need
to do that too, but the fact is that in doing so the rules of IPCC have been
softened to the point that in this way the IPCC is not any more an
assessment of published science (which is its proclaimed goal) but
production of results. The softened condition that the models themself
have to be published does not even apply because the Japanese model for
example is very different from the published one which gave results not
even close to the actual outlier version (in the old dataset the CCC model
was the outlier). Essentially, I feel that at this point there are very little
rules and almost anything goes. I think this will set a dangerous precedent
which might mine the IPCC credibility, and I am a bit uncomfortable that

now nearly everybody seems to think that it is just ok to do this.
Anyways, this is only my opinion for what it is worth.

From: Michael E. Mann [University of Virginia]
To: Phil Jones [CRU]; et al.
June 4, 2003
Subject: Re: Prospective Eos piece?
… Phil and I have recently submitted a paper using about a dozen NH
records that fit this category, and many of which are available nearly 2K
back--I think that trying to adopt a timeframe of 2K, rather than the usual
1K, addresses a good earlier point that Peck made w/regard to the memo,
that it would be nice to try to “contain” the putative “MWP”, even if we
don’t yet have a hemispheric mean reconstruction available that far back
[Phil and I have one in review--not sure it is kosher to show that yet
though--I’ve put in an inquiry to Judy Jacobs at AGU about this]… .

From: David Rind [NASA Goddard Institute for Space Studies]
To: Jonathan Overpeck [University of Arizona]
January 4, 2005
[Subject: IPCC last 2000 years data]
… In addition, some of the comments are probably wrong—the warm-
season bias (p.12) should if anything produce less variability, since
warm seasons (at least in GCMs) feature smaller climate changes than
cold seasons. The discussion of uncertainties in tree ring reconstructions
should be direct, not referred to other references—it’s important for this
document. How the long-term growth is factored in/out should be
mentioned as a prime problem. The lack of tropical data—a few corals
prior to 1700—has got to be discussed. The primary criticism of
McIntyre and McKitrick, which has gotten a lot of play on the Internet, is
that Mann et al. transformed each tree ring prior to calculating PCs by
subtracting the 1902-1980 mean, rather than using the length of the full
time series (e.g., 1400-1980), as is generally done. M&M claim that
when they used that procedure with a red noise spectrum, it always
resulted in a ‘hockey stick’. Is this true? If so, it constitutes a devastating
criticism of the approach; if not, it should be refuted. While IPCC cannot

be expected to respond to every criticism a priori, this one has gotten
such publicity it would be foolhardy to avoid it… .

From: Jonathan Overpeck [University of Arizona]
To: Keith Briffa [CRU]; Eystein Jansen [Bjerknes Centre for Climate
Research]; Tom Crowley [Duke University]
July ??, 2005
ANOTHER THING THAT IS A REAL ISSUE IS SHOWING SOME OF
THE TREE-RING DATA FOR THE PERIOD AFTER 1950. BASED
ON THE LITERATURE, WE KNOW THESE ARE BIASED—RIGHT?
SO SHOULD WE SAY THAT’S THE REASON THEY ARE NOT
SHOWN? OF COURSE, IF WE ONLY PLOT THE FIG FROM CA 800
TO 1400 AD, IT WOULD DO WHAT WE WANT, FOCUS ON THE
MWP ONLY—THE TOPIC OF THE BOX—AND SHOW THAT
THERE WERE NOT ANY PERIODS WHEN ALL THE RECORDS
ALL SHOWED WARMTH—I.E., OF THE KIND WE’RE
EXPERIENCING NOW. TWO CENTS WORTH

From: Michael E. Mann [Penn State University]
To: Tim Osborn [CRU]; Keith Briffa [CRU]
Cc: Gavin Schmidt [NASA Goddard Institute for Space Studies]
February 9, 2006
guys, I see that Science has already gone online w/the new issue, so we
put up the RC [Real Climate website] post. By now, you’ve probably
read that nasty McIntyre thing. Apparently, he violated the embargo on his
website (I don’t go there personally, but so I’m informed). Anyway, I
wanted you guys to know that you’re free to use RC in any way you think
would be helpful. Gavin and I are going to be careful about what
comments we screen through, and we’ll be very careful to answer any
questions that come up to any extent we can. On the other hand, you might
want to visit the thread and post replies yourself. We can hold comments
up in the queue and contact you about whether or not you think they
should be screened through or not, and if so, any comments you’d like us
to include. You’re also welcome to do a followup guest post, etc. think of
RC as a resource that is at your disposal to combat any disinformation
put forward by the McIntyres of the world. Just let us know. We’ll use

our best discretion to make sure the skeptics dont’get to use the RC
comments as a megaphone…

From: Keith Briffa [CRU]
To: Martin Juckes [???]; et al.
November 16, 2006
Subject: Re: Mitrie: Bristlecones
… I still believe that it would be wise to involve Malcolm Hughes in this
discussion—though I recognise the point of view that says we might like
to appear (and be) independent of the original Mann, Bradley and Hughes
team to avoid the appearance of collusion. In my opinion (as someone
how has worked with the Bristlecone data hardly at all!) there are
undoubtedly problems in their use that go beyond the strip bark problem
(that I will come back to later)… . Another serious issue to be
considered relates to the fact that the PC1 time series in the Mann et al.
analysis was adjusted to reduce the positive slope in the last 150 years
(on the assumption—following an earlier paper by Lamarche et al.—that
this incressing growth was evidence of carbon dioxide fertilization), by
differencing the data from another record produced by other workers in
northern Alaska and Canada (which incidentally was standardised in a
totally different way). This last adjustment obviously will have a large
influence on the quantification of the link between these Western US trees
and N.Hemisphere temperatures. At this point, it is fair to say that this
adjustment was arbitrary and the link between Bristlecone pine growth
and CO2 is, at the very least, arguable.
From: Tom Wigley [University Corporation of Atmospheric Research]
To: Phil Jones [CRU]
Cc: Ben Santer [Lawrence Livermore National Laboratory]
September 27, 2009
Subject: 1940s
Phil,
Here are some speculations on correcting SSTs [Sea Surface
Temperatures] to partly explain the 1940s warming blip. If you look at
the attached plot you will see that the land also shows the 1940s blip (as
I’m sure you know). So, if we could reduce the ocean blip by, say, 0.15

degC, then this would be significant for the global mean—but we’d still
have to explain the land blip. I’ve chosen 0.15 here deliberately. This
still leaves an ocean blip, and i think one needs to have some form of
ocean blip to explain the land blip (via either some common forcing, or
ocean forcing land, or vice versa, or all of these). When you look at other
blips, the land blips are 1.5 to 2 times (roughly) the ocean blips—higher
sensitivity plus thermal inertia effects. My 0.15 adjustment leaves things
consistent with this, so you can see where I am coming from… .

From: Tom Wigley [University Corporation of Atmospheric Research]
To: Phil Jones [CRU]
October 5, 2009
[Subject: A Scientific Scandal Unfolds]
Phil,
It is distressing to read that American Stinker item [Oct. 5th article from
the American Thinker which highlights Stephen McIntyre’s discovery that
Keith Briffa apparently cherry picked data regarding tree-rings from
Yamal]. But Keith does seem to have got himself into a mess. As I
pointed out in emails, Yamal is insignificant. And you say that (contrary
to what M&M say) Yamal is *not* used in MBH, etc. So these facts alone
are enough to shoot down M&M is a few sentences (which surely is the
only way to go—complex and wordy responses will be counter
productive). But, more generally, (even if it *is* irrelevant) how does
Keith explain the McIntyre plot that compares Yamal-12 with Yamal-all?
And how does he explain the apparent “selection” of the less well-
replicated chronology rather that the later (better replicated) chronology?
Of course, I don’t know how often Yamal-12 has really been used in
recent, post-1995, work. I suspect from what you say it is much less often
that M&M say—but where did they get their information? I presume they
went thru papers to see if Yamal was cited, a pretty foolproof method if
you ask me. Perhaps these things can be explained clearly and concisely
—but I am not sure Keith is able to do this as he is too close to the issue
and probably quite pissed of. And the issue of with-holding data is still a
hot potato, one that affects both you and Keith (and Mann). Yes, there are
reasons—but many *good* scientists appear to be unsympathetic to these.
The trouble here is that with-holding data looks like hiding something,

and hiding means (in some eyes) that it is bogus science that is being
hidden. I think Keith needs to be very, very careful in how he handles
this. I’d be willing to check over anything he puts together.
Tom.

From: Phil Jones [CRU]
To: John Mitchell [Director of Climate Science—UK Met Office]
October 28, 2009
Subject: Yamal response from Keith
John,
… This went up last night about 5pm. There is a lot to read at various
levels. If you get time just the top level is necessary. There is also a bit
from Tim Osborn showing that Yamal was used in 3 of the 12 millennial
reconstructions used in Ch 6 [of IPCC Fourth Assessment Report]. Also
McIntyre had the Yamal data in Feb 2004—although he seems to have
forgotten this. Keith succeeding in being very restrained in his response.
McIntyre knew what he was doing when he replaced some of the trees
with those from another site.
Cheers
Phil

From: Phil Jones [CRU]
To: Keith Briffa [CRU]
October 28, 2009
Subject: FW: Yamal and paleoclimatology
Keith,
There is a lot more there on CA [Climate Audit website] now. I would be
very wary about responding to this person now having seen what
McIntyre has put up. You and Tim talked about Yamal. Why have the
bristlecones come in now… . This is what happens—they just keep
moving the goalposts. Maybe get Tim to redo OB2006 without a few
more series.
Cheers
Phil …

Dear Professor Briffa, I am pleased to hear that you appear to have
recovered from your recent illness sufficiently to post a response to the
controversy surrounding the use of the Yamal chronology; and the
chronology itself; Unfortunately I find your explanations lacking in
scientific rigour and I am more inclined to believe the analysis of
McIntyre[.] Can I have a straightforward answer to the following
questions 1) Are the reconstructions sensitive to the removal of either the
Yamal data and Strip pine bristlecones, either when present singly or in
combination? 2) Why these series, when incorporated with white noise
as a background, can still produce a Hockey-Stick shaped graph if they
have, as you suggest, a low individual weighting? And once you have
done this, please do me the courtesy of answering my initial email. Dr.
D.R. Keiller

From: Keith Briffa [CRU]
To: Chris Folland [UK Met Office]; Phil Jones [CRU]; Michael E. Mann
[University of Virginia]
Cc: Tom Karl [National Climatic Data Center—NOAA]
September 22, 1999
Subject: RE: IPCC revisions
… I know there is pressure to present a nice tidy story as regards
‘apparent unprecedented warming in a thousand years or more in the
proxy data’ but in reality the situation is not quite so simple. We don’t
have a lot of proxies that come right up to date and those that do (at least
a significant number of tree proxies) some unexpected changes in
response that do not match the recent warming… .

From: Edward Cook [Columbia University]
To: Keith Briffa [CRU]
April 29, 2003
[Subject: Review- confidential]
Hi Keith,
I will start out by sending you the chronologies that I sent Bradley, i.e. all
but Mongolia. If you can talk Gordon out of the latter, you’ll be the first
from outside this lab. The chronologies are in tabbed column format and
Tucson index format. The latter have sample size included. It doesn’t take

a rocket scientist (or even Bradley after I warned him about small sample
size problems) to realize that some of the chronologies are down to only
1 series in their earliest parts. Perhaps I should have truncated them
before using them, but I just took what Jan gave me and worked with the
chronologies as best I could. My suspicion is that most of the pre-1200
divergence is due to low replication and a reduced number of available
chronologies. I should also say that the column data have had their means
normalized to approximately 1.0, which is not the case for the
chronologies straight out of ARSTAN. That is because the site-level
RCS-detrended data were simply averaged to produce these
chronologies, without concern for their long-term means. Hence the
“RAW” tag at the end of each line of indices. Bradley still regards the
MWP [Medieval Warm Period] as “mysterious” and “very incoherent”
(his latest pronouncement to me) based on the available data. Of course
he and other members of the MBH [Mann Bradley Hughes] camp have a
fundamental dislike for the very concept of the MWP, so I tend to view
their evaluations as starting out from a somewhat biased perspective, i.e.
the cup is not only “half-empty”; it is demonstrably “broken”. I come
more from the “cup half-full” camp when it comes to the MWP, maybe
yes, maybe no, but it is too early to say what it is. Being a natural skeptic,
I guess you might lean more towards the MBH camp, which is fine as
long as one is honest and open about evaluating the evidence (I have my
doubts about the MBH camp). We can always politely(?) disagree given
the same admittedly equivocal evidence. I should say that Jan should at
least be made aware of this reanalysis of his data. Admittedly, all of the
Schweingruber data are in the public domain I believe, so that should not
be an issue with those data. I just don’t want to get into an open critique
of the Esper data because it would just add fuel to the MBH attack squad.
They tend to work in their own somewhat agenda-filled ways. We should
also work on this stuff on our own, but I do not think that we have an
agenda per se, other than trying to objectively understand what is going
on. Cheers, Ed

From: Keith Briffa [CRU]
To: Edward Cook [Columbia University]
April 29, 2003
Subject: Re: Review- confidential

Thanks Ed
Can I just say that I am not in the MBH [Mann Bradley Hughes] camp—if
that be characterized by an unshakable “belief” one way or the other,
regarding the absolute magnitude of the global MWP [Medieval Warm
Period]. I certainly believe the “medieval” period was warmer than the
18th century—the equivalence of the warmth in the post 1900 period, and
the post 1980s, compared to the circa Medieval times is very much still
an area for much better resolution. I think that the geographic/seasonal
biases and dating/response time issues still cloud the picture of when and
how warm the Medieval period was . On present evidence, even with
such uncertainties I would still come out favouring the “likely
unprecedented recent warmth” opinion—but our motivation is to further
explore the degree of certainty in this belief—based on the realistic
interpretation of available data. Point re Jan well taken and I will inform
him

From: Keith Briffa [CRU]
To: Michael E. Mann [University of Virginia]; Tom Wigley [University
Corporation of Atmospheric Research]; Phil Jones [CRU]; Raymond
Bradley [University of Massachusetts, Amherst]
Cc: Jerry Meehl [University Corporation of Atmospheric Research];
Caspar Ammann [University Corporation of Atmospheric Research]
May 20, 2003
Subject: Re: Soon et al. paper
Mike and Tom and others
… As Tom W. states, there are uncertainties and “difficulties” with our
current knowledge of Hemispheric temperature histories and valid
criticisms or shortcomings in much of our work. This is the nature of the
beast—and I have been loathe to become embroiled in polarised debates
that force too simplistic a presentation of the state of the art or
“consensus view”… . The one additional point I would make that seems
to have been overlooked in the discussions up to now, is the invalidity of
assuming that the existence of a global Medieval Warm period, even if
shown to be as warm as the current climate, somehow negates the
possibility of enhanced greenhouse warming… . The various papers
apparently in production, regardless of their individual emphasis or

approaches, will find their way in to the literature and the next IPCC can
sift and present their message(s) as it wishes., but in the meantime, why
not a simple statement of the shortcomings of the BS paper as they have
been listed in these messages and why not in Climate Research? Keith

From: Tom Wigley [University Corporation of Atmospheric Research]
To: Phil Jones [CRU]
Note: Ben Santer [Lawrence Livermore National Laboratory] may have
been Cc’d.
October 21, 2004
[Subject: MBH]
Phil,
I have just read the M&M stuff critcizing MBH [Mann Bradley Hughes].
A lot of it seems valid to me. At the very least MBH is a very sloppy
piece of work—an opinion I have held for some time. Presumably what
you have done with Keith is better?—or is it? I get asked about this a lot.
Can you give me a brief heads up? Mike is too deep into this to be
helpful.
Tom.
From: Phil Jones [CRU]
To: Tom Wigley [University Corporation of Atmospheric Research]
Cc: Ben Santer [Lawrence Livermore National Laboratory]
October 22, 2004
Subject: Re: MBH
Tom,
… A lot of people criticise MBH [Mann Bradley Hughes] and other
papers Mike has been involved in, but how many people read them fully
—or just read bits like the attached. The attached is a complete distortion
of the facts. M&M are completely wrong in virtually everything they say
or do… . Mike’s may have slightly less variability on decadal scales than
the others (especially cf Esper et al), but he is using a lot more data than
the others. I reckon they are all biased a little to the summer and none are
truly annual—I say all this in the Reviews of Geophysics paper ! Bottom
line—their is no way the MWP [Medieval Warm Period] (whenever it
was) was as warm globally as the last 20 years. There is also no way a
whole decade in the LIA [Little Ice Age] period was more than 1 deg C

on a global basis cooler than the 1961-90 mean. This is all gut feeling, no
science, but years of experience of dealing with global scales and
varaibility. Must got to Florence now. Back in Nov 1.
Cheers
Phil

From: Phil Jones [CRU]
To: Kevin Trenberth [University Corporation of Atmospheric Research];
et al.
December 20, 2004
Subject: Re: [Fwd: Re: [Fwd: Re: “Model Mean Climate” for AR4
[IPCC Fourth Assessment Report] ]]
… I would like to stick with 1961-90. I don’t want to change this until
1981-2010 is complete, for 3 reasons : 1) We need 30 years and 81-10
will get all the MSU in nicely, and 2) I will be near retirement !! 3) is
one of perception. As climatologists we are often changing base periods
and have done for years. I remember getting a number of comments when
I changed from 1951-80 to 1961-90. If we go to a more recent one the
anomalies will seem less warm—I know this makes no sense
scientifically, but it gives the skeptics something to go on about ! If we do
the simple way, they will say we aren’t doing it properly… .
From: Keith Briffa [CRU]
To: Jonathan Overpeck [University of Arizona]
February ??, 2006
[Subject: bullet debate #3]
Third
I suggest this should be[:]
Taken together, the sparse evidence of Southern Hemisphere temperatures
prior to the period of instrumental records indicates that overall warming
has occurred during the last 350 years, but the even fewer longer regional
records indicate earlier periods that are as warm, or warmer than, 20th
century means….
Peck, you have to consider that since the TAR [IPCC Third Assessment
Report], there has been a lot of argument re “hockey stick” and the real
independence of the inputs to most subsequent analyses is minimal. True,
there have been many different techniques used to aggregate and scale

data—but the efficacy of these is still far from established. We should be
careful not to push the conclusions beyond what we can securely justify
—and this is not much other than a confirmation of the general
conclusions of the TAR . We must resist being pushed to present the
results such that we will be accused of bias—hence no need to attack
Moberg . Just need to show the “most likely” course of temperatures over
the last 1300 years—which we do well I think. Strong confirmation of
TAR is a good result, given that we discuss uncertainty and base it on
more data. Let us not try to over egg the pudding. For what it worth, the
above comments are my (honestly long considered) views—and I would
not be happy to go further . Of course this discussion now needs to go to
the wider Chapter authorship, but do not let Susan [Solomon of NOAA]
(or Mike [Michael Mann]) push you (us) beyond where we know is right.

From: Jonathan Overpeck [University of Arizona]
To: Keith Briffa [CRU]
September 13, 2006
… I think the second sentence could be more controversial—I don’t think
our team feels it is valid to say, as they did in TAR [IPCC Third
Assessment Report], that “It is also likely that, in the Northern
Hemisphere,… 1998 was the warmest year” in the last 1000 years. But,
it you think about it for a while, Keith has come up with a clever 2nd
sentence (when you insert “Northern Hemisphere” language as I suggest
below). At first, my reaction was leave it out, but it grows on you,
especially if you acknowledge that many readers will want more explicit
prose on the 1998 (2005) issue… .
From: David Rind [NASA Goddard Institute for Space Studies]
To: Jonathan Overpeck [University of Arizona]
Cc: Keith Briffa [CRU]; et al.
September 13, 2006
Now getting back to the resolution issue: given what we know about the
ability to reconstruct global or NH temperatures in the past—could we
really in good conscience say we have the precision from tree rings and
the very sparse other data to make any definitive statement of this nature
(let alone accuracy)? While I appreciate the cleverness of the second
sentence, the problem is everybody will recognize that we are ‘being

clever’—at what point does one come out looking aggressively
defensive? I agree that leaving the first sentence as the only sentence
suggests that one is somehow doubting the significance of the recent
warm years, which is probably not something we want to do.

From: Jonathan Overpeck [University of Arizona]
To: Keith Briffa [CRU]
Cc: Eystein Jansen [Bjerknes Centre for Climate Research]
January 5, 2005
Subject: Fwd: Re: the Arctic paper and IPCC
… I’m still not convinced about the AO recon [Arctic Oscillation
reconstruction], and am worried about the late 20th century “coolness” in
the proxy recon that’s not in the instrumental, but it’s a nice piece of work
in any case… .

From: David Parker [UK Met Office]
To: Neil Plummer [Bureau of Meteorology, Australia]
January 5, 2005
Neil
There is a preference in the atmospheric observations chapter of IPCC
AR4 [IPCC Fourth Assessment Report] to stay with the 1961-1990
normals. This is partly because a change of normals confuses users, e.g.
anomalies will seem less positive than before if we change to newer
normals, so the impression of global warming will be muted… .

From: David Rind [NASA Goddard Institute for Space Studies]
To: Keith Briffa [CRU]
January 10, 2005
… Well, yes and no. If the mismatch between suggested forcing, model
sensitivity, and suggested response for the LIA suggests the forcing is
overestimated (in particular the solar forcing), then it makes an earlier
warm period less likely, with little implication for future warming. If it
suggests climate sensitivity is really much lower, then it says nothing
about the earlier warm period (could still have been driven by solar
forcing), but suggests future warming is overestimated. If however it

implies the reconstructions are underestimating past climate changes, then
it suggests the earlier warm period may well have been warmer than
indicated (driven by variability, if nothing else) while suggesting future
climate changes will be large. This is the essence of the problem. David

From: Phil Jones [CRU]
To: John Christy [University of Alabama, Huntsville]
July 5, 2005
Subject: This and that
John,
There has been some email traffic in the last few days to a week—quite a
bit really, only a small part about MSU. The main part has been one of
your House subcommittees wanting Mike Mann and others and IPCC to
respond on how they produced their reconstructions and how IPCC
produced their report. In case you want to look at this see later in the
email ! Also this load of rubbish ! This is from an Australian at BMRC
[Bureau of Meteorology Research Centre] (not Neville Nicholls). It
began from the attached article. What an idiot. The scientific community
would come down on me in no uncertain terms if I said the world had
cooled from 1998. OK it has but it is only 7 years of data and it isn’t
statistically significant.

… The Hadley Centre are working on the day/night issue with sondes,
but there are a lot of problems as there are very few sites in the tropics
with both and where both can be distinguished. My own view if that the
sondes are overdoing the cooling wrt MSU4 in the lower stratosphere,
and some of this likely (IPCC definition) affects the upper troposphere as
well. Sondes are a mess and the fact you get agreement with some of
them is miraculous. Have you looked at individual sondes, rather than
averages—particularly tropical ones? LKS is good, but the RATPAC
update less so.

… What will be interesting is to see how IPCC pans out, as we’ve been
told we can’t use any article that hasn’t been submitted by May 31. This
date isn’t binding, but Aug 12 is a little more as this is when we must
submit our next draft—the one everybody will be able to get access to
and comment upon. The science isn’t going to stop from now until AR4

[IPCC Fourth Assessment Report] comes out in early 2007, so we are
going to have to add in relevant new and important papers. I hope it is up
to us to decide what is important and new. So, unless you get something
to me soon, it won’t be in this version. It shouldn’t matter though, as it
will be ridiculous to keep later drafts without it. We will be open to
criticism though with what we do add in subsequent drafts. Someone is
going to check the final version and the Aug 12 draft. This is partly why
I’ve sent you the rest of this email. IPCC, me and whoever will get
accused of being political, whatever we do. As you know, I’m not
political. If anything, I would like to see the climate change happen, so
the science could be proved right, regardless of the consequences. This
isn’t being political, it is being selfish.
Cheers
Phil

From: Phil Jones [CRU]
To: Neville Nichols [Bureau of Meteorology, Australia]
July 6, 2005
Subject: Fwd: Misc
Neville,
Here’s an email from John, with the trend from his latest version in. Also
has trends for RATPAC and HadAT2. If you can stress in your talks that it
is more likely the sondes are wrong—at least as a group. Some may be
OK individually. The tropical ones are the key, but it is these that least is
know about except for a few regions. The sondes clearly show too much
cooling in the stratosphere (when compared to MSU4), and I reckon this
must also affect their upper troposphere trends as well. So, John may be
putting too much faith in them wrt agreement with UAH. Happy for you to
use the figure, if you don’t pass on to anyone else. Watch out for Science
though and the Mears/Wentz paper if it ever comes out. Also, do point out
that looking at surface trends from 1998 isn’t very clever.
Cheers
Phil

From: Neville Nichols [Bureau of Meteorology, Australia]

To: Phil Jones [CRU]
July 6, 2005
[Subject: RE: Misc]
… I thought Mike Mann’s draft response was pretty good—I had
expected something more vigorous, but I think he has got the “tone” pretty
right. Do you expect to get a call from Congress?
Neville Nicholls

From: Phil Jones [CRU]
To: Neville Nichols [Bureau of Meteorology, Australia]
July 6th, 2005
Subject: RE: Misc
Neville,
Mike’s response could do with a little work, but as you say he’s got the
tone almost dead on. I hope I don’t get a call from congress ! I’m hoping
that no-one there realizes I have a US DoE grant and have had this (with
Tom W.) for the last 25 years. I’ll send on one other email received for
interest.
Cheers
Phil

From: Mike MacCracken [Climate Institute]
To: Phil Jones [CRU]; Chris Folland [UK Met Office]
Cc: John Holdren; Rosina Bierbaum
January 3, 2009
Subject: Temperatures in 2009
Dear Phil and Chris—
… In any case, if the sulfate hypothesis is right, then your prediction of
warming might end up being wrong. I think we have been too readily
explaining the slow changes over past decade as a result of variability—
that explanation is wearing thin. I would just suggest, as a backup to your
prediction, that you also do some checking on the sulfate issue, just so
you might have a quantified explanation in case the [warming] prediction
is wrong. Otherwise, the Skeptics will be all over us—the world is
really cooling, the models are no good, etc. And all this just as the US is

about ready to get serious on the issue. We all, and you all in particular,
need to be prepared.
Best, Mike MacCracken

From: Tim Johns [UK Met Office]
To: Chris Folland [CRU]
Cc: Doug Smith [UK Met Office]
January 5, 2009
… The impact of the two alternative SO2 emissions trajectories is quite
marked though in terms of global temperature response in the first few
decades of the 21st C (at least in our HadGEM2-AO simulations,
reflecting actual aerosol forcings in that model plus some divergence in
GHG forcing). Ironically, the E1-IMAGE scenario runs, although much
cooler in the long term of course, are considerably warmer than A1B-
AR4 for several decades! Also—relevant to your statement— A1B-AR4
runs show potential for a distinct lack of warming in the early 21st C,
which I’m sure skeptics would love to see replicated in the real world…
(See the attached plot for illustration but please don’t circulate this any
further as these are results in progress, not yet shared with other
ENSEMBLES partners let alone published). We think the different short
term warming responses are largely attributable to the different SO2
emissions trajectories… .

From: Phil Jones [CRU]
To: Tim Johns [UK Met Office]; Chris Folland [UK Met Office]
Cc: Doug Smith [UK Met Office]
January 5, 2009
Subject: Re: FW: Temperatures in 2009
Tim, Chris,
I hope you’re not right about the lack of warming lasting till about 2020.
I’d rather hoped to see the earlier Met Office press release with Doug’s
paper that said something like—half the years to 2014 would exceed the
warmest year currently on record, 1998! Still a way to go before 2014. I
seem to be getting an email a week from skeptics saying where’s the
warming gone. I know the warming is on the decadal scale, but it would
be nice to wear their smug grins away. Chris—I presume the Met Office

continually monitor the weather forecasts. Maybe because I’m in my 50s,
but the language used in the forecasts seems a bit over the top re the cold.
Where I’ve been for the last 20 days (in Norfolk) it doesn’t seem to have
been as cold as the forecasts… .

From: Kevin Trenberth [University Corporation of Atmospheric
Research]
To: Michael Mann [Penn State University]
Cc: Stephen Schneider [Stanford University]; Myles Allen [University of
Oxford]; Peter Stott [UK Met Office]; Phil Jones [CRU]; Ben Santer
[Lawrence Livermore National Laboratory]; Tom Wigley [University
Corporation of Atmospheric Research]; Thomas R Karl [NOAA]; Gavin
Schmidt [NASA Goddard Institute for Space Studies]; James Hansen
[NASA Goddard Institute for Space Studies]; Michael Oppenheimer
[Princeton University]
October 12, 2009
Subject: Re: BBC U-turn on climate
Hi all. Well I have my own article on where the heck is global warming?
We are asking that here in Boulder where we have broken records the
past two days for the coldest days on record. We had 4 inches of snow.
The high the last 2 days was below 30F and the normal is 69F, and it
smashed the previous records for these days by 10F. The low was about
18F and also a record low, well below the previous record low. This is
January weather (see the Rockies baseball playoff game was canceled on
saturday and then played last night in below freezing weather). The fact
is that we can’t account for the lack of warming at the moment and it is a
travesty that we can’t. The CERES data published in the August BAMS
09 supplement on 2008 shows there should be even more warming: but
the data are surely wrong. Our observing system is inadequate…

From: Tom Wigley [University Corporation of Atmospheric Research]
To: Phil Jones [CRU]
November 6, 2009
Subject: LAND vs OCEAN
We probably need to say more about this. Land warming since 1980 has
been twice the ocean warming—and skeptics might claim that this proves

that urban warming is real and important. See attached note. Comments?
Tom

From: Michael E. Mann [ University of Virginia]
To: Keith Briffa [CRU]; Tom Wigley [University Corporation of
Atmospheric Research]; Phil Jones [CRU]; Raymond Bradley
[University of Massachusetts, Amherst]
May 16, 2003
[Subject: Soon et al. paper]
Tom,
Thanks for your response, which I will maintain as confidential within
the small group of the original recipients (other than Ray whom I’ve
included in as well), given the sensitivity of some of the comments
made… . In my view, it is the responsibility of our entire community to
fight this intentional disinformation campaign, which represents an affront
to everything we do and believe in. I’m doing everything I can to do so,
but I can’t do it alone—and if I’m left to, we’ll lose this battle, mike

From: Michael E. Mann [University of Virginia]
To: Phil Jones [CRU]; Raymond Bradley [University of Massachusetts,
Amherst]; Tom Wigley [University Corporation of Atmospheric
Research]; Tom Crowley [Duke University]; Keith Briffa [CRU]; Kevin
Trenberth [University Corporation of Atmospheric Research]; Michael
Oppenheimer [Princeton University]; Jonathan Overpeck [University of
Arizona]
Cc: Scott Rutherford [University of Rhode Island]
June 3, 2003
[Subject: Prospective Eos piece?]
Dear Colleagues,
… Phil, Ray, and Peck have already indicated tentative interest in being
co-authors. I’m sending this to the rest of you (Tom C, Keith, Tom W,
Kevin) in the hopes of broadening the list of co-authors. I strongly
believe that a piece of this sort co-authored by 9 or so prominent
members of the climate research community (with background and/or
interest in paleoclimate) will go a long way ih helping to counter these
attacks, which are being used, in turn, to launch attacks against IPCC….

From: Michael E. Mann [University of Virginia]
To: Phil Jones [CRU]; et al.
June 4, 2003
Subject: Re: Prospective Eos piece?
Phil and I have recently submitted a paper using about a dozen NH
[Northern Hemisphere] records that fit this category, and many of which
are available nearly 2K [2 thousand years] back—I think that trying to
adopt a timeframe of 2K, rather than the usual 1K, addresses a good
earlier point that Peck [Jonathan Overpeck—University of Arizona]
made w/regard to the memo, that it would be nice to try to “contain” the
putative “MWP” [Medieval Warm Period], even if we don’t yet have a
hemispheric mean reconstruction available that far back [Phil and I have
one in review—not sure it is kosher to show that yet though—I’ve put in
an inquiry to Judy Jacobs at AGU about this]… .

From: Phil Jones [CRU]
To: Michael E. Mann [University of Virginia]
June 4, 2003
[Subject: Prospective Eos piece?]
… EOS would get to most fellow scientists. As I said to you the other
day, it is amazing how far and wide the SB pieces have managed to
percolate. When it comes out I would hope that AGU/EOS ‘publicity
machine’ will shout the message from rooftops everywhere. As many of
us need to be available when it comes out. There is still no firm news on
what Climate Research will do, although they will likely have two
editors for potentially controversial papers, and the editors will consult
when papers get different reviews. All standard practice I’d have
thought. At present the editors get no guidance whatsoever. It would seem
that if they don’t know what standard practice is then they shouldn’t be
doing the job !
Cheers
Phil

From: Phil Jones [CRU]
To: Janice Lough [Australian Institute of Marine Science]

August 6th, 2004
Subject: Re: liked the paper
… PS Do you want to get involved in IPCC this time? I’m the CLA
[Coordinating Lead Author] of the atmospheric obs. [observations]
chapter with Kevin Trenberth and we’ll be looking for Contributing
Authors to help the Lead Authors we have. Paleo[climatology] is in a
different section this time led by Peck and Eystein Janssen. Keith is a
lead author as well.

From: Phil Jones [CRU]
To: Michael E. Mann [Penn State University]
May 19, 2009
[Subject: nomination: materials needed!]
… Apart from my meetings I have skeptics on my back—still, can’t seem
to get rid of them. Also the new UK climate scenarios are giving govt
ministers the jitters as they don’t want to appear stupid when they
introduce them (late June?)… .

From: Narsimha D. Rao [Stanford University]
To: Stephen H. Schneider [Stanford University]
October 11, 2009
Subject: BBC U-turn on climate
Steve, You may be aware of this already. Paul Hudson, BBCs reporter on
climate change, on Friday wrote that theres been no warming since 1998,
and that pacific oscillations will force cooling for the next 20-30 years. It
is not outrageously biased in presentation as are other skeptics views… .
BBC has significant influence on public opinion outside the US. Do you
think this merits an op-ed response in the BBC from a scientist?

From: Michael E. Mann [Penn State University]
To: Stephen H. Schneider [Stanford University]
Cc: Myles Allen [University of Oxford]; Peter Stott [UK Met Office];
Phil Jones [CRU]; Ben Santer [Lawrence Livermore National
Laboratory]; Tom Wigley [University Corporation of Atmospheric
Research]; Thomas R Karl [NOAA]; Gavin Schmidt [NASA Goddard

Institute for Space Studies]; James Hansen [NASA Goddard Institute for
Space Studies]; Kevin Trenberth [University Corporation of Atmospheric
Research]; Michael Oppenheimer [Princeton University]
October 12, 2009
Subject: Re: BBC U-turn on climate
extremely disappointing to see something like this appear on BBC. its
particularly odd, since climate is usually Richard Black’s beat at BBC
(and he does a great job). from what I can tell, this guy was formerly a
weather person at the Met Office. We may do something about this on
RealClimate [website], but meanwhile it might be appropriate for the
Met Office [UK’s National Weather Service] to have a say about this, I
might ask Richard Black [BBC environment correspondent] what’s up
here?

From: Phil Jones [CRU]
To: Gavin Schmidt [NASA Goddard Institute for Space Studies];
Michael E. Mann [Penn State University]; Andy Revkin [New York
Times]
October 27, 2009
[Subject: The web page is up about the Yamal tree-ring chronology]
Gavin, Mike, Andy,
It has taken Keith longer than he would have liked, but it is up. There is a
lot to read and understand. It is structured for different levels. The link
goes to the top level. There is more detail below this and then there are
the data below that… . I’ll let you make up you own minds! It seems to
me as though McIntyre cherry picked for effect. There is an additional
part that shows how many series from Ch 6 of AR4 [IPCC Fourth
Assessment Report] used Yamal—most didn’t!

From: Michael E. Mann [Penn State University]
To: Phil Jones [CRU]
Note: Gavin Schmidt [NASA Goddard Institute for Space Studies] may
have been cc’d.
October 27, 2009
[Subject: The web page is up about the Yamal tree-ring chronology]
thanks Phil,

Perhaps we’ll do a simple update to the Yamal post, e.g. linking Keith/s
new page—Gavin t? As to the issues of robustness, particularly w.r.t.
inclusion of the Yamal series, we actually emphasized that (including the
Osborn and Briffa ‘06 sensitivity test) in our original post! As we all
know, this isn’t about truth at all, its about plausibly deniable
accusations, m

From: Michael E. Mann [Penn State University]
To: Phil Jones [CRU]
Note: Gavin Schmidt [NASA Goddard Institute for Space Studies] may
have been cc’d.
October 27, 2009
[Subject: The web page is up about the Yamal tree-ring chronology]
Hi Phil,
Thanks—we know that. The point is simply that if we want to talk about
about a meaningful “2009” anomaly, every additional month that is
available from which to calculate an annual mean makes the number more
credible. We already have this for GISTEMP, but have been awaiting
HadCRU to be able to do a more decisive update of the status of the
disingenuous “globe is cooling” contrarian talking point, mike
p.s. be a bit careful about what information you send to Andy [Revkin
with the New York Times] and what emails you copy him in on. He’s not
as predictable as we’d like

“HARRY READ ME” FILE
Among CRU’s exposed documents is the so-called “HARRY_READ_ME”
file, which served as a detailed note keeping file from 2006 through 2009 for
CRU researcher and programmer Ian “Harry” Harris. As he worked to
update and modify CRU TS2.1 to create the new CRU TS3.1dataset, the
HARRY_READ_ME.txt details Harris’s frustration with the dubious nature
of CRU’s meteorological datasets. As demonstrated through a handful of
excerpts below, the 93,000-word HARRY_READ_ME file may raise
several serious questions as to the reliability and integrity of CRU’s data
compilation and quality assurance protocols.

I am very sorry to report that the rest of the databases seem to be in nearly as
poor a state as Australia was. There are hundreds if not thousands of pairs of
dummy stations, one with no WMO and one with, usually overlapping and
with the same station name and very similar coordinates. I know it could be
old and new stations, but why such large overlaps if that’s the case?
Aarrggghhh! There truly is no end in sight.
----
One thing that’s unsettling is that many of the assigned WMo codes for
Canadian stations do not return any hits with a web search. Usually the
country’s met office, or at least the Weather Underground, show up—but for
these stations, nothing at all. Makes me wonder if these are long-
discontinued, or were even invented somewhere other than Canada!
----
OH F**K THIS. It’s Sunday evening, I’ve worked all weekend, and just
when I thought it was done I’m hitting yet another problem that’s based on the
hopeless state of our databases. There is no uniform data integrity, it’s just a
catalogue of issues that continues to grow as they’re found.
------
Here, the expected 1990-2003 period is MISSING—so the correlations
aren’t so hot! Yet the WMO codes and station names/locations are identical
(or close). What the hell is supposed to happen here? Oh yeah—there is no
‘supposed’, I can make it up. So I have :-)
------
You can’t imagine what this has cost me—to actually allow the operator to
assign false WMO codes!! But what else is there in such situations?
Especially when dealing with a ‘Master’ database of dubious provenance
(which, er, they all are and always will be).
False codes will be obtained by multiplying the legitimate code (5 digits) by
100, then adding 1 at a time until a number is found with no matches in the
database. THIS IS NOT PERFECT but as there is no central repository for
WMO codes—especially made-up ones—we’ll have to chance duplicating
one that’s present in one of the other databases. In any case, anyone
comparing WMO codes between databases—something I’ve studiously

avoided doing except for tmin/tmax where I had to—will be treating the false
codes with suspicion anyway. Hopefully.
Of course, option 3 cannot be offered for CLIMAT bulletins, there being no
metadata with which to form a new station.
This still meant an awful lot of encounters with naughty Master stations,
when really I suspect nobody else gives a hoot about. So with a somewhat
cynical shrug, I added the nuclear option— to match every WMO possible,
and turn the rest into new stations (er, CLIMAT excepted). In other words,
what CRU usually do. It will allow bad databases to pass unnoticed, and
good databases to become bad, but I really don’t think people care enough to
fix ‘em, and it’s the main reason the project is nearly a year late.
------
This whole project is SUCH A MESS. No wonder I needed therapy!!
-----
So.. we don’t have the coefficients files (just .eps plots of something). But
what are all those monthly files? DON’T KNOW, UNDOCUMENTED.
Wherever I look, there are data files, no info about what they are other than
their names. And that’s useless.. take the above example, the filenames in the
_mon and _ann directories are identical, but the contents are not. And the
only difference is that one directory is apparently ‘monthly’ and the other
‘annual’—yet both contain monthly files.
------
I find that they are broadly similar, except the normals lines (which both start
with ‘6190’) are very different. I was expecting that maybe the latter
contained 94-00 normals, what I wasn’t expecting was that are in % x10 not
%! Unbelievable—even here the conventions have not been followed. It’s
botch after botch after botch. Modified the conversion program to process
either kind of normals line.
------
The biggest immediate problem was the loss of an hour’s edits to the
program, when the network died.. no explanations from anyone, I hope it’s
not a return to last year’s troubles.

(some weeks later)
well, it compiles OK, and even runs enthusiastically. However there are
loads of bugs that I now have to fix. Eeeeek.
Timesrunningouttimesrunningout.
(even later)
Getting there.. still ironing out glitches and poor programming.
25. Wahey! It’s halfway through April and I’m still working on it. This surely
is the worst project I’ve ever attempted. Eeeek.
------
So the ‘duplicated’ figure is slightly lower.. but what’s this error with the
‘.ann’ file?! Never seen before. Oh GOD if I could start this project again
and actually argue the case for junking the inherited program suite!!
-------
Wrote ‘makedtr.for’ to tackle the thorny problem of the tmin and tmax
databases not being kept in step. Sounds familiar, if worrying. am I the first
person to attempt to get the CRU databases in working order?!! The program
pulls no punches.
---------
Back to the gridding. I am seriously worried that our flagship gridded data
product is produced by Delaunay triangulation—apparently linear as well.
As far as I can see, this renders the station counts totally meaningless. It also
means that we cannot say exactly how the gridded data is arrived at from a
statistical perspective—since we’re using an off-the-shelf product that isn’t
documented sufficiently to say that. Why this wasn’t coded up in Fortran I
don’t know—time pressures perhaps? Was too much effort expended on
homogenisation, that there wasn’t enough time to write a gridding procedure?
Of course, it’s too late for me to fix it too. Meh.
------
Now looking at the dates.. something bad has happened, hasn’t it. COBAR
AIRPORT AWS cannot start in 1962, it didn’t open until 1993! Looking at
the data—the COBAR station 1962-2004 seems to be an exact copy of the
COBAR AIRPORT AWS station 1962-2004, except that the latter has more

missing values. Now, COBAR AIRPORT AWS has 15 months of missing
value codes beginning Oct 1993.. coincidence?
--------
I am seriously close to giving up, again. The history of this is so complex that
I can’t get far enough into it before by head hurts and I have to stop. Each
parameter has a tortuous history of manual and semi-automated interventions
that I simply cannot just go back to early versions and run the update prog. I
could be throwing away all kinds of corrections—to lat/lons, to WMOs
(yes!), and more.
So what the hell can I do about all these duplicate stations? Well, how about
fixdupes.for? That would be perfect—except that I never finished it, I was
diverted off to fight some other fire. Aarrgghhh.
I—need—a—database—cleaner.
What about the ones I used for the CRUTEM3 work with Phil Brohan? Can’t
find the bugger!! Looked everywhere, Matlab scripts aplenty but not the one
that produced the plots I used in my CRU presentation in 2005. Oh, F**K IT.
Sorry. I will have to WRITE a program to find potential duplicates. It can
show me pairs of headers, and correlations between the data, and I can say
‘yay’ or ‘nay’. There is the finddupes.for program, though I think the
comment for *this* program sums it up nicely:
‘program postprocdupes2

c Further post-processing of the duplicates file—just to show how crap
the c program that produced it was! Well—not so much that but that once it
was

c running, it took 2 days to finish so I couldn’t really reset it to improve
c things. Anyway, *this* version does the following useful stuff:
c (1) Removes and squirrels away all segments where dates don’t match;
c (2) Marks segments >5 where dates don’t match;
c (3) Groups segments from the same pair of stations;
c (4) Sorts based on total segment length for each station pair’

You see how messy it gets when you actually examine the problem?
-------

Well, dtr2cld is not the world’s most complicated program. Wheras cloudreg
is, and I immediately found a mistake! Scanning forward to 1951 was done
with a loop that, for completely unfathomable reasons, didn’t include months!
So we read 50 grids instead of 600!!! That may have had something to do
with it. I also noticed, as I was correcting THAT, that I reopened the DTR
and CLD data files when I should have been opening the bloody station
files!! I can only assume that I was being interrupted continually when I was
writing this thing. Running with those bits fixed improved matters somewhat,
though now there’s a problem in that one 5-degree band (10S to 5S) has no
stations! This will be due to low station counts in that region, plus removal
of duplicate values.

WHY?

THE FORCES ARE STILL OUT THERE and, with or without President Obama, they
have the commitment and resources to achieve their Crown Jewel.
As I said in the beginning… Why?
When the United Nations is totally refuted…
When Al Gore is totally discredited…
When man-made catastrophic global warming is totally debunked… When
passing global warming cap and trade is totally futile…
Why is this book necessary?
Now you know.

 

FOREWORD

What  a  timely  book!  Larry  Bell’s  insightful  overview  of  global
warming  hysteria  will  open  the  eyes  of  many  who  still  believe  in  the
science as propagated by the United Nations’ Intergovernmental Panel on
Climate  Change  (IPCC)  and  in  the  promises  of  politicians  to  save  them
from climate disasters. After two decades of corrupt science, we are finally
able  to  learn  the  truth  about  the  politics  behind  the  conspiracy  among  a
small group of influential scientists to manufacture a global warming scare
from data that showed none.

Many  would  place  the  beginning  of  the  global  warming  hoax  on  the
Senate testimony delivered by James Hansen of NASA during the summer
of 1988. More than anything else, this exhibition of hyped alarm triggered
my active skepticism about the man-made warming scare. This skepticism
was  further  amplified  when  I  acted  as  a  reviewer  of  the  first  three  IPCC
reports, in 1990, 1996, and 2001. Increasingly, claims were made for which
there  was  no  evidence;  in  some  cases  the  “evidence”  was  clearly
manufactured.  For  example,  the  1996  report  used  selective  data  and
doctored graphs. It also featured changes in the text that were made after
the  scientists  had  approved  it  and  before  it  was  printed.  It  caused  Dr.
Frederick  Seitz,  a  worldfamous  physicist  and  former  president  of  the  US
National  Academy  of  Sciences,  the  American  Physical  Society,  and  the
Rockefeller University, to write in the Wall Street Journal:  “I  have  never
witnessed  a  more  disturbing  corruption  of  the  peer  review  process  than
events that led to this IPCC report.”

All throughout, politicians loudly proclaimed that “the science is settled”
and  proceeded  to  construct  regulatory  schemes  to  limit  the  emission  of
greenhouse  gases  (GHGs)  and—in  the  process—to  control  energy.  The
capstone to all this fraudulent behavior must surely be the “cap-and-trade”
legislation  passed  by  the  US  House  of  Representatives  in  2009.  It  has
nothing to do with climate; instead, it is a giant tax scheme that redistributes

income  from  citizens  who  use  energy,  whether  electricity  heat  or  motor
fuel,  to  the  favored  few.  It  has  been  estimated  that  this  legislation  has
provided  a  livelihood  to  some  two  thousand  or  more  lobbyists  and  has
placed a corresponding burden on the rest of the population.

By now, the international climate business has degenerated into a scheme
to transfer resources from developed to developing nations. Or as cynics put
it, “from the poor in rich countries to the rich in poor countries.”

The truth is that there is no evidence for any significant human impact on
global climate, and that there is nothing in a practical sense we can do to
affect  global  climate.  And,  as  Larry  Bell  points  out,  a  somewhat  warmer
climate with increased levels of carbon dioxide in the atmosphere would be
beneficial overall to Earth’s inhabitants, especially to those in developing
nations  who  depend  on  agriculture  for  a  living.  Climate  of  Corruption
brings a breath of fresh, cool air to the overheated climate debate.

—S. Fred Singer
Former director of the US National Weather Satellite Service, professor
emeritus at the University of Virginia, and coauthor of Unstoppable Global
Warming

 

PREFACE

Regarding climate science there is at least one certainty: There is
absolutely no reason to believe that Earth is any warmer now than it was
during  past  periods  when  life  flourished—times  when  agriculture  was
abundant,  pyramids  and  cities  were  built,  and  world  citizens  became
connected in trade and culture.

The  March  2006  Time  magazine  cover  story  “Global  Warming:  Be
Worried, Be Very Worried” warned of impending climate doom that would
result  in  melting  polar  caps,  rising  oceans,  and  other  catastrophes.  If  any
worry  is  warranted,  think  about  the  next  overdue  Ice  Age  that  scientific
“experts” predicted only a few decades earlier. Then hope that the cooling
period we are currently experiencing will only be brief. Understand that the
real  impetus  behind  the  cooked  numbers  and  doomspeak  of  the  global
warmers has little to do with the state of the environment and much to do
with shackling capitalism and transforming the American way of life in the
interests of global wealth redistribution (“social justice”).

Is this all a conspiracy? It really isn’t in the conventional sense, where a
diabolical network of people and organizations unite to hatch intentionally
malevolent  plans.  Let’s  assume  that  most  of  the  entities  and  individuals
discussed in this book truly believe they are pursuing righteous causes, even
when we happen to strongly disagree with their viewpoints and priorities.
Maybe we can hope that some of them will cut us the same slack.

But then, what about when those people and institutions we rely upon for
important public information knowingly violate our trust? For example, by
perpetrating  unwarranted  fear  campaigns  and  by  politically  attacking  and
marginalizing  those  who  challenge  and  expose  factual  errors,  omissions,
and  uncertainties  we  need  to  know  about.  Should  we  excuse  them  even
when  they  believe  such  actions  are  guided  by  superior  moral  authority?
Absolutely not! These are clear acts of deception and corruption.

Obviously, this book addresses controversial topics, and readers have a
right to know something about the person who wrote it. First, I am not a
climate  scientist  and  have  never  even  played  one  in  the  movies.  And
although Houston is my chosen home, I have never been associated with
“Big Oil”—or “little oil” either, for that matter. Nor am I connected with
scientific funding, business organizations, or lobbies on either side of the
issues. Few people within any such camps will know who I am, nor do they
have any real reason to.

I have written some articles about climate, energy, and technology that
were published in the Energy Tribune, an international magazine. This was
done by invitation, and for small stipends, yet never was I influenced in any
way  regarding  what  I  would  write  about  or  say.  I  would  have  cheerfully
written them free of charge, but please don’t tell the magazine.

2

In short, I am a space guy. My field is space architecture, which deals
with planning and designing space stations and habitats for future lunar and
Mars  missions.  I  also  undertake  research  and  planning  for  extreme
environments  on  Earth,  such  as  polar,  desert,  underwater,  and  disaster
facilities. This interest extends to working to prevent our entire planet from
becoming an extreme environment.

My background and interests emphasize a holistic perspective regarding
basic principles that govern how natural and technical systems work, how
they  are  connected,  and  how  they  can  be  managed  to  support  the  most
complex systems of all—us humans. This, in fact, is how this project really
got  started.  I  was  innocently  exploring  some  research  and  notions  about
“Spaceship Earth”—considering what we might possibly learn from nature
about how to design artificial, closed climate and energy systems operating
beyond our planet. Some space guys think about those kinds of things. In
any case, that inquiry revealed much more than I bargained for.

Quite early in my investigation, I recalled a comment offered by S. Fred
Singer when he visited my office several years ago to exchange ideas on a
totally different space-related matter. During our meeting, he observed that
satellite  temperature  recordings  of  the  Earth’s  lower  atmosphere  were
cooling  more  rapidly,  relative  to  the  surface,  than  greenhouse  theory
predicts. It would be expected that carbon dioxide (CO
) would warm the
lower atmosphere first, which would then radiate heat back to the surface,
the reverse of what was being observed. I certainly had no reason to doubt
him.  Fred  is  an  internationally  recognized  climate  physicist  and  former

Distinguished Research Professor at George Mason University. He served
as the first director of the US National Weather Satellite Service and also as
vice  chairman  of  the  US  National  Advisory  Committee  on  Oceans  and
Atmospheres.  In  addition,  he  has  written  numerous  publications  about
climate,  energy,  and  environmental  issues,  including  a  recent  New  York
Times best seller, Unstoppable Global Warming, coauthored with Dennis T.
Avery.

Although I didn’t think all that much about Fred’s casual observation at
the  time,  it  piqued  a  mild  interest,  and  my  later  investigations  have
amplified  questions  regarding  numerous  climate  change  hypotheses,  most
particularly  in  regard  to  alarmist  assertions,  which  have  no  real  basis  in
science.  My  subsequent  conclusions  will  now  qualify  me  as  a  global
warming skeptic and doomsday denier. This is not intended to suggest that I
don’t believe that climate change occurs or that it isn’t abnormally warm
right now. Compared with Ice Ages that have historically dominated Earth’s
climate about 90 percent of the time, we can be very grateful we are blessed
with conditions more favorable for the lives we enjoy.

Yes, climate change is real, occurring with regular and irregular cycles
and for lots of reasons. Scientists know about many of them, but much less
about  how  these  dynamic  causes  and  effects  interact  or  what  combined
results will occur at any given time. No one, not anyone, can even begin to
reliably  predict  what  Earth’s  global  climate  will  be  a  decade  or  multiple
decades hence, much less whether the impacts will be positive or negative
with  regard  to  all  God’s  creatures—us  included.  Nor  has  anyone  or  any
science conclusively demonstrated that human activities have caused or are
causing climate change for better or worse, or if so, which activities, and
with  how  much  influence.  Any  claims  of  certainty  to  the  contrary  are
bogus. Accordingly, and specifically, assertions that human CO
 emissions
are the root of climate crisis, or that such a threat exists, are challenged as
factually unsupportable alarmism.

One  unfortunate  fact  we  can  count  on  is  that  we  are  facing  a  global
energy supply dilemma that has no simple solution. Answers do not lie with
much-touted  “renewable”  energy  sources,  because  they  all  lack  sufficient
potential  capacities  to  make  much  overall  difference.  And  for  all  their
advertised  “greenness,”  absolutely  no  options  are 
from
environmental activist opposition. Yet fossil fuels (coal, oil, and natural gas
—particularly  coal)  are  regarded  as  real  villains,  and  a  predicted  global

2

immune 

2

2

warming apocalypse will be their curse. (The term “fossil fuels” often will
be referred to hereafter simply as “fossils.”) Even resistance to the scourge
of  disaster  risks  previously  associated  with  nuclear  power  now  pales  in
comparison.

Global  warming  hysteria  centered  upon  fossil-fuel  CO

  emissions  is
being advanced and exploited by powerful alternative energy marketers and
carbon-trading  interests.  Aided  and  abetted  by  climate  science  hijackers,
their  aggressive  lobbying  campaigns  have  been  extremely  consequential.
CO
, which sustains plants that nourish us with oxygen and food, has come
to be popularly characterized as a polluting menace. Initiatives to develop
vital oil and natural gas reserves are being delayed, while options with scant
potential dominate public media and legislative attention.

Inescapable evidence shows that human activities are impacting Earth’s
environment, typically not for the better. Air, water, and land pollution are
an  expanding  global  reality.  Environmental  scientists  who  study  such
matters play important roles in pointing such things out and helping us to do
better. That purpose is not well served, however, by exaggerated statements
calibrated to get maximum attention. Alarmism is not conducive to sound
judgment or worthy of public respect, whatever the motives.

There can also be no doubt that fossil fuel depletion is a very real and
serious  problem,  and  while  nuclear  development  is  essential,  it  is  most
unfortunate that there are presently no complete or perfect remedies. Yet,
given our proven history of human innovation, progress, and resilience, we
have every reason to believe that solutions will ultimately be realized.

I had never planned to write this book—or any book. First, after family
and  friends  witnessed  the  amount  of  research  I  had  compiled—and  were
relentlessly exposed to my ever-deepening passion about the topics—they
insisted that I do so. Eventually, I realized that I had to, like it or not. This
decision was motivated by the fact that, like many of you, I am a parent
who  cares  about  the  future  of  my  children  and  the  generations  who  will
follow. I want them to inherit a clean, healthy planet, along with means to
obtain energy sufficiency essential for comfortable lifestyles and economic
opportunities. Conservation must be a big part of all solutions.

We clearly need to develop better alternatives, and to begin doing so now.
In the meantime, we must also develop and expand access to resources that
will enable those transitions; we must not be misled by hyperbole regarding
sustainable replacement options that can only serve as supplements at best.

Misguided, climate hysteria–induced, knee-jerk energy policies won’t help
get us where we need to go.

Each of us must determine whether or not we regard ourselves to be true
environmentalists. I believe that environmentalism is not so much defined
by what we are against as by what we are for, and neither fear nor guilt are
prerequisites.  Environmentalism  need  not  be  strident  or  perpetually
confrontational. An environmentalist identity cannot really be owned, only
practiced.

SOME DESERVED WORDS OF APPRECATION

 

supported 

its 

The  preparation  and  production  of  this  book  turned  out  to  be  a
much larger enterprise than I originally expected, and I am grateful to many
people  who 
realization.  My  wife  Nancy’s  early
encouragement to undertake the project and continued belief in its value has
been essential throughout the process. Major typing assistance afforded by
my sons, Aaron and Ian, has transcribed my hand-printed draft jottings into
legible text.

My esteemed colleague Professor Olga Bannova has been an ever-willing
sounding board for ideas and a constructive critic for narrative. Two of our
graduate  students,  Harmon  Everett  and  Michael  Fehlinger,  contributed  to
manuscript  production  as  author-compensated  consultants.  It  should  be
noted that neither the book nor any perspectives it presents are in any way
implied to represent publications or views of my employers: the University
of Houston, the Gerald D. Hines College of Architecture, or the research
center that I direct within the university. Nor is the book used as a text or
designated information resource for any courses that I teach or supervise.

The wonderfully competent, dedicated, and enthusiastic Greenleaf Book
Group team has contributed in all ways imaginable to make this project a
successful  and  pleasurable  experience.  Editors  Bill  Crawford  and  Linda
O’Doughda offered innumerable structural and literary suggestions to make
it a greatly improved product. Graphic designer Brian Phillips produced the
attractive  and  engaging  jacket  artwork,  also  collaborating  with  Design
Manager Sheila Parr on internal book layout. Others, including Production
Manager  Chris  McRay,  Marketing  Associate  Katelynn  Knutson,  and
Distribution  Manager  Kristen  Sears,  planned  and  coordinated  numerous
aspects of production and market promotion.

Thank you all.

 
ACEEE American Council for an Energy-Effcient Economy
Alliance for Climate Protection
ACP
AMO
Atlantic Multidecadal Oscillation
ANWR Arctic National Wildlife Refuge
AWEA American Wind Energy Association
bbl/d
CAFE
CAP
CBO
CCS
CCSP
CCX
CDA
CFCs
CFL

barrels per day
corporate average fuel economy
Center for American Progress
Congressional Budget Offce
carbon capture and storage
Climate Change Science Program
Chicago Climate Exchange
Center for Data Analysis
chlorofuorocarbons
compact fuorescent lighting
carbon dioxide
CRU
Climate Research Unit
CSP
concentrating solar power
CSR
corporate social responsibility
DOE
Department of Energy (US)
DOI
Department of the Interior (US)
DSCOVR Deep Space Climate Observatory
ECX
EGS
EIA
EPA
ESA

ABBREVATIONS AND ACRONYMS

European Carbon Exchange
enhanced geothermal system
Energy Information Administration
Environmental Protection Agency (US)
Endangered Species Act

European Union
EU
extreme ultraviolet
EUV
Framework Convention on Climate Change
FCCC
Fish and Wildlife Service (US)
FWS
galactic cosmic ray
GCR
general circulation model
GCM
gross domestic product
GDP
greenhouse gas
GHG
Generation Investment Management
GIM
GISS
Goddard Institute for Space Studies
GSAM Goldman Sachs Asset Management
GW
ICAP
ICE
IEA
IGCC
IMF
IPE
IPCC
IPPR
kW
kWh
LIA
MW
MWP
N
NASA
NCEE
NGO
NIEO
NOAA National Oceanographic and Atmospheric Administration
NRDC

gigawatt
International Carbon Action Partnership
Intercontinental Exchange, Inc.
Institute of Economic Analysis
integrated gasifcation combined cycle
International Monetary Fund
International Petroleum Exchange
Intergovernmental Panel on Climate Change
Institute for Public Policy Research
kilowatt
kilowatt-hour
Little Ice Age
megawatt
Medieval Warm Period
nitrous oxide
National Aeronautics and Space Administration
National Center for Environmental Economics
nongovernmental organization
New International Economic Order

Natural Resources Defense Council

O

2

Offce of Management and Budget
Open Society Institute
Pacifc Decadal Oscillation
parts per million
production tax credit
Royal Canadian Mounted Police
renewable portfolio standard
sulfur dioxide
Statistical Assessment Services
Union of Concerned Scientists

OMB
OSI
PDO
ppm
PTC
RCMP
RPS
SO,
STATS
UCS
UNCED United Nations Conference on Economic Development
UNEP
United Nations Environment Programme
USCAP United States Climate Action Partnership
USGS
UV
VAT
WCED World Commission on Environment and Development
WMO World Meteorological Organization
WTO
WWF World Wildlife Fund

United States Geological Survey
ultraviolet
value added tax

World Trade Organization

Introduction

THE BIG CLIMATE CRISIS LIE

 

Spaceship  Earth  reporting  …  all  systems  functioning…  thermal

controls optimum. Thank you, God.

Conscientious environmentalism does not require or benefit from
subscription  to  hysterical  guilt  over  man-made  climate  crisis  claims.
Perhaps  some  may  argue  that  unfounded  alarmism  is  justifiable,  even
necessary, to get our attention to do what we should be doing anyway: for
example,  conserve  energy  and  not  pollute  the  planet.  Hey,  who  wants  to
challenge those important purposes?

But what about examining motives? For example, when those who are
twanging our guilt strings falsely portray polar bears as endangered climate
victims  to  block  drilling  in  Alaska’s  Arctic  Natural  Wildlife  Reserve
 as an endangering pollutant to
(ANWR), and when alarmists classify CO

2

promote  lucrative  cap-and-trade  legislation  and  otherwise  unwarranted
alternative energy subsidies. What if these representations lack any sound
scientific basis? Is that okay?

The Hot Spin Cycle

 

Cyclical,  abrupt,  and  dramatic  global  and  regional 

temperature
fluctuations  have  occurred  over  millions  of  years,  long  before  humans
invented agriculture, industries, automobiles, and carbon-trading schemes.
Many  natural  factors  are  known  to  contribute  to  these  changes,  although
even  our  most  sophisticated  climate  models  have  failed  to  predict  the
timing,  scale  (either  up  or  down),  impacts,  or  human  influences.  While
theories abound, there is no consensus, as claimed, that “science is settled”
on  any  of  those  theories—much  less  is  there  consensus  about  the  human
influences upon or threat implications of climate change.

Among these hypotheses, man-made global warming caused by burning
fossils has been trumpeted as an epic crisis. CO
, a “greenhouse gas,” has
been  identified  as  a  primary  culprit  and  branded  as  an  endangering
“pollutant.”  This,  despite  the  fact  that  throughout  Earth’s  history  the
increases  in  the  atmospheric  CO
  level  have  tended  to  follow,  not  lead,
rising temperatures. It should also be understood that CO
 accounts for only
0.04 of 1 percent of the atmosphere, and about 97 percent of that tiny trace
amount  comes  from  naturally  occurring  sources  that  humans  haven’t
influenced.

The big lie is that we are living in a known climate change crisis. Climate
warming  and  cooling  have  occurred  throughout  the  ages.  Is  the  Earth
warming right now? Probably not, but what if it is? It might be cooling next
year. The models that predict a crisis are speculative at best, and two recent
events  have  cast  even  more  doubt  on  their  accuracy.  One  relates  to
undisputable  evidence  that  influential  members  of  the  climate  science
community  have  cooked  the  books  to  advance  their  theories  and
marginalize  contrary  findings.  The  other  problem  is  evidence  provided
directly by Mother Nature herself that the global climate appears to have
entered a new cooling cycle.

Public  exposure  of  hacked  e-mail  files  retrieved  from  the  Climate
Research  Unit  (CRU)  at  Britain’s  University  of  East  Anglia  revealed

2

2

2

scandalous communications among researchers who have fomented global
warming  hysteria.  Their  exchanges  confirm  long-standing  and  broadly
suspected manipulations of climate data. Included are conspiracies to falsify
and  withhold  information,  to  suppress  contrary  findings  in  scholarly
publications,  and  to  exaggerate  the  existence  and  threats  of  man-made
global warming. Many of these individuals have had major influence over
summary  report  findings  issued  by  the  United  Nations’  IPCC.  This
organization  has  been  recognized  as  the  world  authority  on  such  matters,
and  it  shares  a  Nobel  Prize  with  Al  Gore  for  advancing  climate  change
awareness.

Among the more than three thousand purloined CRU documents is an e-
mail from its director, Philip Jones, regarding a way to fudge the data to
hide evidence of temperature declines: “I’ve just completed Mike’s Nature
[journal] trick of adding the real temperatures to each series for the past 20
years [i.e., from 1981 onward] and from 1961 for Keith’s to hide the decline
[emphasis mine].” “Mike,” in this instance, refers to climatologist Michael
Mann,  who  created  the  now  infamous  “hockey  stick”  chart  that  has
repeatedly appeared in IPCC reports, as well as in Al Gore promotions, to
portray  accelerated  global  warming  beginning  with 
the  Industrial
Revolution—hence,  caused  by  humans.  The  chart  has  been  thoroughly
debunked  thanks  to  careful  analyses  by  two  Canadian  researchers  who
uncovered  a  variety  of  serious  problems.  Included  are  calculation  errors,
data used twice, and a computer program that produced a hockey stick out
of whatever data was fed into it.1

Some  of  the  e-mails  reveal  less  than  full  public  candor  about  what
scientists  don’t  know  about  past  temperatures.  For  example,  one  from
Edward Cook, director of tree ring research at the Lamont-Doherty Earth
Laboratory, to CRU’s deputy director Keith Briffa on September 3, 2003,
admitted that little could be deduced regarding past Northern Hemisphere
temperatures from the tree ring proxy data Mann used: “We can probably
say  a  fair  bit  about  [less  than]  100-year  extra-tropical  NH  temperature
variability … but honestly know f**k-all [expletive deleted] about what the
[more than] 100-year variability was like with any certainty.”

Correspondence leaves no doubt that the members of the network were
concerned  the  cooling  since  1998  they  had  observed  would  be  publicly
exposed. In an October 26, 2008, note from CRU’s Mick Kelly to Jones, he
comments,  “Yeah,  it  wasn’t  so  much  1998  and  all  that  I  was  concerned

Another  e-mail  to  Michael  Mann  (which  James  Hansen  at  NASA  was
copied on), sent by Kevin Trenberth, head of the Climate Analysis Section
of the US National Center for Atmospheric Research, reflected exasperation
concerning  a  lack  of  global  warming  evidence:  “Well,  I  have  my  own
article on where the heck is global warming. We are asking here in Boulder
where  we  have  broken  records  the  past  two  days  for  the  coldest  days  on
record. We had four inches of snow.” He continued, “The fact is that we
can’t account for the lack of warming at the moment, and it is a travesty that
we can’t … the data is surely wrong. Our observing system is inadequate.”2
Trenberth, an advisory IPCC high priest and man-made global warming
spokesperson,  didn’t  waste  a  publicity  opportunity  to  link  a  devastating
2005 US hurricane season to this cause. After ignoring admonitions from
top expert Christopher Landsea that this assumption was not supported by
known  research,  Trenberth  proceeded  with  the  unfounded  claim  that
dominated world headlines.

Clearly,  members  of  the  CRU  e-mail  network  used  their  considerable
influence  to  block  the  publication  of  research  by  climate  crisis  skeptics,
thus  preventing  inclusion  of  contrary  findings  in  IPCC  reports.  In  one  e-
mail, Tom Wigley, a senior scientist and Trenberth associate at the National
Center  for  Atmospheric  Research,  shared  his  disdain  for  global  warming
challengers, common among global warming proponents: “If you think that
[Yale professor James] Saiers is in the greenhouse skeptics camp, then, if
we  can  find  documentary  evidence  of  this,  we  could  go  through  official
[American Geophysical Union] channels to get him ousted.”3

about, used to dealing with that, but the possibility that we might be going
through a longer 10-year period of relatively stable temperatures . . .” He
added,  “Speculation  but  if  I  see  this  possibility,  then  others  might  also.
Anyway, I’ll maybe cut the last few points off the filtered curve before I
give the talk again as that’s trending down as a result of the effects and the
recent cold-ish years.”

Possibly  one  of  the  most  serious  and  legally  hazardous  breaches  of
professional  accountability  is  seen  in  an  e-mail  from  Jones  to  Mann
concerning withholding of taxpayer-supported scientific data: “If they ever
hear  there  is  a  Freedom  of  Information  Act  now  in  the  UK,  I  think  I’ll
delete the file rather than send it to anyone.” He then asks Mann to join him
in deleting official IPCC-related files: “Can you delete any e-mails you may
have had with Keith re: AR4 [the IPCC’s Fourth Assessment Report]?” A

different  e-mail  from  Jones  assures  Mann  of  the  way  some  troublesome
contrarian research will be handled: “I can’t see either of these papers being
in the next IPCC report. Kevin and I will keep them out somehow, even if
we have to redefine what the peer-reviewed process is!”

A Jones letter to his colleagues instructed them, “Don’t any of you three
tell anyone that the UK has a Freedom of Information Act.” Still another
stated, “We also have a data platform act, which I will hide behind.”

The CRU fallout is spreading: It now includes broader allegations by a
Russian  scientific  group  that  climate-change  data  obtained  from  that
country has been cherry-picked to overstate a rise in temperatures. Russia
accounts  for  a  large  portion  of  the  world’s  landmass,  and  incorrect  data
there would affect overall global temperature analyses.

Two  things  are  clear  from  the  CRU  emails:  (1)  Perpetrators  of  climate
science fraud have routinely conspired to exaggerate temperature increases
since  the  Industrial  Revolution,  and  (2)  these  same  perpetrators  virtually
ignored  comparable  and  even  warmer  times  that  preceded  this  period,  as
well  as  prolonged  temperature  declines  since  this  period,  that  contradict
greenhouse theory and model predictions. Other explanations that conform
much  more  closely  to  observed  fluctuations  have  been  dismissed  or
aggressively  attacked.  These  practices  have  produced  unsupportable
alarmist statements trumpeted in the world press that continue to influence
multitrillion-dollar US and international policy decisions—decisions based
upon a contrived crisis of hysteria … a climate of corruption.

Chilling News for “Warm-Mongers”

 

The climate is always changing, in long and short cycles, and mankind
has survived and thrived in conditions that have varied greatly from what
they are right now.

It is apparent that our planet is once again experiencing a global cooling
trend, just as it did quite recently between 1940 and 1975, when warnings
of  a  coming  new  ice  age  received  front-page  coverage  in  the  New  York
Times  and  other  major  publications.  NASA  satellite  measurements  of  the
lower  atmosphere,  where  warming  greenhouse  models  predicted  effects
would  be  greatest,  stopped  rising  as  a  decadal  trend  after  1998  despite
.  Measurements  recorded  by  four  major
increased 

levels  of  CO

 

2

If  ordinary  citizens  don’t  receive  or  heed  scientific  reports,  many  may
legitimately  question  global  warming  assertions  from  direct  experience.
Take  the  year  2007,  for  example.  North  America  had  the  most  snow  it’s
recorded  in  the  past  50  years.  A  Boston  storm  in  December  dumped  10
inches of snow, more than the city typically receives in that entire month,
and  Madison,  Wisconsin,  had  the  highest  seasonal  snowfall  since  record
keeping  began.5  Record  cold  temperatures  were  recorded  in  Minnesota,
Texas, Florida, and Mexico.

Those trends continued into the following 2 years. During October 2008,
Oregon temperatures mid-month dipped to record lows, and Boise, Idaho,
received its earliest-ever recorded snowfall. December 2008 witnessed 3.6
inches of snow in the Las Vegas Valley, the most to have fallen at that time
of  year  since  1938,  when  record  keeping  began.  Houston  witnessed  its
earliest-ever recorded snowfall on December 4, 2009.6

temperature-tracking outlets showed that world temperatures plummeted by
more than 1 degree Fahrenheit (1ºF) during 2007. This cooling approached
the total of all the warming that had occurred over that past 100 years. In
other words, temperatures worldwide and collectively never rose more than
1ºF  in  a  century.  2008  was  significantly  colder  than  2007  had  been.
Although models predicted that the year 2008 would be one of the warmest
on  record,  it  actually  ranked  fourteenth  coldest  since  satellite  records
commenced in 1979, and the coldest since 2000.4

A blizzard on February 20, 2010, broke a Washington, DC, 110-year-old
annual  snowfall  record  of  55  inches  as  well  as  seasonal  records  in
Baltimore and Philadelphia.7 Then, on February 26 and 27, another storm
that pummeled New York City for 2 days broke a monthly snowfall record
(37 inches) in Central Park that had stood for 114 years; the previous record
for February was 28 inches in 1934, and the largest for any month was 30.5
inches in March 1896.8

Most people’s perceptions about warming and cooling trends depend on
where they happen to reside and the time range they have experienced for
reference.  During  July  2010,  those  throughout  New  England  witnessed
temperatures among the ten warmest recorded during that month in about a
century,  while  temperatures  in  southeastern  US  states  registered  below
normal. Simultaneously, Los Angeles broke a coldest July day record set in
1926, Australia since 1966, and the southern cone of South America saw
the coldest July in half a century.9 Freezing temperatures in eastern Bolivia

(normally  above  68ºF)  killed  millions  of  fish  in  three  major  rivers,
characterized there as an environmental catastrophe.10

Going back to 2007, Baghdad saw its first snowfall ever recorded, and
China experienced its coldest winter in 100 years. Record cold temperatures
were also recorded in Argentina, Chile, and yes, even Greenland. The end
of 2007 set a record for the largest Southern Hemisphere sea ice expanse
since satellite altimeter monitoring began in 1979, it was about 1 million
square kilometers more than the previous 28-year average. In 2008, Durban,
South Africa, had its coldest September night in history, and parts of that
country  experienced  an  unusual  late-winter  snow.  A  month  earlier,  New
Zealand officials at Mount Ruapehu reported the largest snow accumulation
ever.11

2

According  to  records  collected  by  NASA,  the  National  Oceanographic
and  Atmospheric  Administration  (NOAA),  and  the  Hadley  Centre  for
Climate Change, 2008 was cooler than 2007, making it the coldest year thus
far of the 21st century. And this has occurred while atmospheric CO
 levels
have continued to rise.12

This picture is far different from much of the information presented in the
media. As a case in point, a 2008 Associated Press report claimed that the
10  warmest  days  recorded  have  occurred  since  the  time  of  President  Bill
Clinton’s  second  inaugural  in  January  1997.  The  report  quoted  James
Hansen,  who  heads  NASA’s  Godard  Institute  for  Space  Studies  (GISS);
Hansen is a principal adviser to Al Gore and has been a primary source of
much global warming alarmism. NASA later issued corrections. In reality,
the warmest recorded days—in descending order— occurred in 1934, 1998,
1921, 2006, 1931, 1934, 1953, 1990, 1938, and 1939. As Jay Lehr, a senior
fellow and science director at the Heartland Institute, stated on CNN’s Lou
Dobbs  Tonight  program  in  December  2008,  “If  we  go  back  in  really
recorded  human  history,  in  the  13th  century  we  were  probably  7  degrees
Fahrenheit warmer than we are now.”13

Bear in mind that monthly, annual, decadal and much longer temperature
fluctuations  are  fundamental  aspects  of  Earth’s  dynamic  climate  history.
Also  remember  that  incredibly  complex  and  interactive  mechanisms  and
effects  of  those  changes  are  geographically  distributed  in  ways  that
confound global generalization. Most recently, NOAA’s National Climatic
Data Center reported that March, April, May and June of 2010 set records
for  the  warmest  year  worldwide  since  record-keeping  began  in  1880.

However,  June  was  actually  cooler  than  average  across  Scandinavia,
southeastern China, and the northwestern US according to the same report.14
NOAA  ground  stations  reported  the  June  average  to  be  1.22ºF  higher
than  normal,  while  NASA  satellite  data  showed  the  average  to  be  only
0.79ºF above a 20-year average. This made June 2010 the second warmest
in the short 32-year satellite temperature record, and the first six months of
2010 were also the second warmest. So what can we really deduce from all
of this to predict a trend? Not much of anything, and certainly nothing to be
alarmed about.

Climate, Carbon, and Conspirators

 

So, who stands to gain from climate science corruption? There are many
culprits, and they are becoming ever more powerful. Principal among these
are  certain  agenda-driven 
regulatory  agencies,
alternative  energy  and  environmental  lobbies,  and  yes,  the  UN  and  other
organizations that seek global resource and wealth redistribution. Many of
these organs of misinformation are joined at a common colon.

federal  government 

The IPCC has long served as the authoritative source of alarmist climate
change predictions cited in media and activist warm-mongering campaigns.
A  richly  funded  example  is  Al  Gore’s  Alliance  for  Climate  Protection
(ACP),  which  has  routinely  enlisted  celebrities  in  advertising  for  united
action  against  a  “climate  crisis.”  In  reality,  the  IPCC  only  conducts
literature reviews, although many of the publications it selectively cites are
produced by the same influential people that author its reports. Moreover,
illuminating  CRU  e-mails  revealed  that  a  small  group  within  that
organization actively worked to prevent research findings that contradicted
their  biases  from  being  published  in  leading  journals,  hence  blocking
dissenting views from being reviewed and cited in IPCC reports.

Global  warming  doom-speakers  and  promoters  of  fossil  energy
alternatives  are  united  behind  carbon-capping  politics.  Climate  change
alarm  drives  the  development  and  marketing  of  technologies  that  are
otherwise uncompetitive without major government support. Unwarranted
climate  fear,  combined  with  legitimate  public  concern  about  fossil-fuel
depletion  and  dependence  upon  foreign  oil,  is  promoted  to  justify  to
taxpayers  and  consumers  the  use  of  more  costly  energy  options.  Media

campaigns  portray  images  of  dying  polar  bears  as  fossil  fuel–generated
carbon casualties to support arguments against drilling in ANWR and, by
association,  other  national  oil  and  natural  gas  reserves.  Fossil-fuel  prices
rise higher, assisted by massive CO
 sequestration costs and de facto cap-
and-trade taxes, so consumers pay more, making alternatives seem all the
more attractive.

2

Does it seem remarkable that the US Environmental Protection Agency
(EPA) applied a global warming argument to declare that CO
, the natural
molecule  essential  for  all  plant  life,  is  a  “pollutant”?  Might  that  possibly
have  to  do  with  a  larger  agenda  supported  by  the  EPA  and  other
organizations, such as wind and solar power lobbies and prospective carbon
brokers, to limit fossil fuel use by requiring costly carbon sequestration, in
turn making alternatives more price competitive, justifying subsidies, and
supporting cap-and-trade schemes? But of course, those purposes wouldn’t
fall within EPA responsibilities, would they? And they wouldn’t make any
sense at all if man-made carbon emissions didn’t pose a dire climate threat.
Yet consider the implications of the suppressed EPA “Internal Study on
Climate”  report  that  was  kept  under  wraps,  its  author  silenced,  due  to
pressure  to  support  the  agency’s  agenda  to  regulate  CO
.  Alan  Carlin,  a
senior  research  analyst  at  the  EPA’s  National  Center  for  Environmental
Economics (NCEE), had stated in thatreport that after examining numerous
global warming studies, his research showed the available observable data
to invalidate the hypothesis that humans cause dangerous global warming.
He  concluded,  “Given  the  downward  trend  in  temperatures  since  1998
(which some think will continue until at least 2030), there is no particular
reason to rush into decisions based upon a scientific hypothesis that does
not appear to explain most of the available data.”15

After  serving  with  the  EPA  for  38  years,  Alan  Carlin  was  taken  off
climate-related  work  and  was  forbidden  from  speaking  to  anyone  outside
the  organization  on  endangerment  issues  such  as  those  in  his  then-
suppressed  report.  A  then-proposed  “endangerment  finding”  under  the
Clean Air Act would enable the EPA to establish limits on CO
 and other
GHG  concentrations  as  threats  to  public  health,  directly  supporting  cap-
and-trade carbon regulations. That finding is now in force.

Bowing to pressure from global warming alarmists, the US Department
of the Interior (DOI) placed polar bears on its Endangered Species Act list
in 2008. Reported threats of massive melting in their habitats prompted this

2

2

2

action. While the act’s purview doesn’t extend to actually regulating GHGs,
there is little doubt that the classification establishes the species as poster
cubs  for  the  man-made  global  warming  movement.  It  also  supports
environmentalist opposition to oil and gas drilling in ANWR.

But are polar bear populations really declining, as tragically depicted in
Al  Gore’s  film,  An  Inconvenient  Truth?  Apparently  not,  according  to
Mitchell Taylor, manager of Wildlife Research for the Government of the
Canadian Territory of Nunavut, which monitors these conditions: “Of the
thirteen  populations  of  polar  bears  in  Canada,  eleven  are  stable  or
increasing in number. They are not going extinct [nor do they] even appear
to be affected at present … [It is] silly to present the demise of polar bears
based on media-assisted hysteria.”16

2

Cap-and-trade legislation, a major priority of President Barack Obama’s
administration,  has  no  defensible  purpose  without  a  supporting  global
warming rationale. It also makes no sense from an economic standpoint. It
will place onerous cost burdens upon energy consumers, continue to drive
businesses  overseas,  and  offer  no  real  climate  or  environmental  benefits
whatsoever.  Such  legislation  will  multiply  the  price  of  electricity  by
dramatically increasing coal plant construction and operating costs for CO
sequestration. While intended to make such “renewables” as wind and solar
more attractive, even this legislation won’t make them competitive without
large tax-supported subsidies. A new stock exchange would then be created
that  treats  (“bad”)  carbon  as  a  valuable  (“good”)  commodity,  providing
billions of profits for operators.

Al Gore, now a very wealthy “green energy” proponent, strongly lobbies
for  carbon-emission  trading  through  a  London-based  hedge  fund  called
Generation  Investment.  He  cofounded  the  company  with  David  Blood,
former head of investment management at Goldman Sachs, which in turn is
a  large  shareholder  in  the  Chicago  Climate  Exchange,  a  “voluntary  pilot
agency”  established  in  2003  to  advance  trading  in  US  carbon  emissions.
Both organizations are working hard to persuade governments to block new
power  plants  that  use  fossils.  Gore  exuberantly  told  members  at  a  March
2007 Joint House Hearing of the Energy and Science Committee: “As soon
as carbon has a price, you’re going to see a wave [of investment] in it …
There will be unchained investment.”17

Perhaps  the  most  serious  public  deception  perpetrated  by  this  “war
against climate change” (e.g., the carbon enemy) is the notion that cleaner,

sustainable  options  are  available  in  sufficient  abundance  to  replace
dependence upon fossil resources that currently provide about 85 percent of
all US energy. Regrettably, this is broadly recognized not to be the case at
all. Ironically, many of the same groups that champion environmental and
human causes are inhibiting progress toward vital solutions.

Extravagantly funded media campaigns continue to advertise a “climate
change crisis,” despite obvious evidence that the Earth began cooling once
more  at  least  a  decade  ago.  Meanwhile,  America’s  energy  and  industrial
progress is being held hostage by political and legal pressures applied by
groups  that  no  one  elected  to  represent  us,  and  industries  and  other
businesses that provide jobs and revenues are being driven overseas. And,
as  artificially  manipulated  energy  costs  continue  to  add  unsustainable
burdens  to  already  out-of-control  government  borrowing  and  spending
deficits,  those  impacts  will  fall  hardest  upon  people  who  can  least  afford
them.

Section One

Setting the Records Straight

Chapter 1

THE CHICKEN LITTLE SYNDROME

Former  vice  president  (now  Nobel  laureate)  Al  Gore,  Time
magazine,  and  numerous  other  sources  have  proclaimed  that  the  climate
debate  has  officially  ended.  The  Earth  is  warming,  the  consequences  are

dire,  and  humans—  along  with  our  technologies  of  destruction—are  the
cause.  Our  only  hope,  it  seems,  is  to  implement  extremely  stringent  cap-
and-trade  legislation  to  drastically  reduce  horrifically  polluting  carbon
dioxide  (CO
)  emissions  spewing  from  fossil-fueled  smokestacks  and  to
switch  to  clean  energy  alternatives  that  are  claimed  to  be  most  assuredly
abundant.

As  I  mentioned  earlier,  Time’s  March  2006  cover  proclaimed,  “Global
Warming: Be Worried, Be Very Worried.” Polar ice caps are melting faster
than  ever;  rising  waters  will  flood  coastal  communities;  more  and  more
areas are being devastated by droughts; and by any measure, Earth is at the
tipping point—all because of us.

Fear and guilt are powerful motivators for those who care, and most of us
really  do.  Some  who  purport  to  be  even  more  caring  and  knowledgeable
about basic concerns we all share have become adept at pulling our guilt
strings.  Their  tactics  are  often  most  effective  when,  with  a  pretense  of
superior moral authority, they project really horrific consequences onto that
guilt. This brands doubters as deniers of inconvenient truths recognized by
all truly smart and informed experts. Maybe you have heard some of that.

2

A  basic  tactic  used  by  calculating  “hysteria  hypesters”  is  to  treat
propaganda  as  obvious  fact.  The  Institute  for  Public  Policy  Research,  a
British think tank, has advocated a way to induce “mass behavior change”
to combat global warming by nurturing a new “common sense”: “[We] need
to work in a more shrewd and contemporary way, using subtle techniques of
engagement . . . The facts need to be treated as being so taken for granted
that  they  need  not  be  spoken  .  .  .  It  amounts  to  treating  climate-friendly
activity as a brand that can be sold. This is, we believe, the route to mass
behavior changes.”1

Just how well are those guilt and scare tactics succeeding? An emerging
market  for  “eco-therapists”  who  specialize  in  treating  “eco-anxiety”
suggests that those tactics are working quite well. For instance, a February
16,  2008,  New  York  Times  article  reported  that  more  than  120  of  these
specialists  are  now  listed  in  the  field  of  “ecopsychology”  to  help  people
who  are  excessively  worried  that  their  own  carbon  emissions  are  causing
global  warming.  The  International  Community  for  Ecopsychology’s
definition refers to that term as “a synergetic relationship between planetary
and personal well-being” and states that “the needs of one are relevant to
the  other.”  Some  schools,  including  Lewis  &  Clark  College  in  Portland,

Oregon, have created courses on counseling such patients. Sarah Edwards
explained to Fox News in April of that year that eco-anxiety (manifested in
feelings of fear, grief, anger, confusion, and depression) caused her shoulder
pain, fibromyalgia, and fatigue. Her reasoning may go pandemic: A British
independent news source has reported that eco-anxiety has been blamed for
symptoms  ranging  from  overeating  and  bulimia  to  depression  and  even
alcoholism.2

This seems to raise another threat for Mr. Gore to consider: Is it possible

that rising CO

 levels are making people crazy?

2

All this might seem comical if not for the fact that a number of people are
deeply  troubled  with  alarm  and  guilt  about  human  impacts  upon  climate
change. A particularly tragic case involved an Argentine family. In March
2010, Francisco Lotero and Miriam Coletti shot two of their children before
killing themselves after making an apparent suicide pact over fears about
effects of global warming. Although their 2-year-old son, Francisco, died
instantly, their unnamed 7-monthold infant daughter remarkably survived.

Galloping Glaciers

 

This  isn’t  the  first  time  that  prominent  news  publishers,  supported  by
scientific  experts,  have  warned  us  about  perils  of  uncorrected  climate
changes. On October 7, 1912, for example, the Los Angeles Times alerted
readers, “Fifth Ice Age Is on the Way: Human Race Will Have to Fight for
Existence in Cold.” By August 9, 1923, the situation had already become
desperate,  causing  the  Chicago  Tribune  to  declare  on  its  front  page,
“Scientist Says Arctic Ice Will Wipe Out Canada.” A complementary story
posited that huge parts of Asia and Europe were also threatened. The world
soon appeared to be warming again by the 1930s, however, causing some
scientists and news reporters to suggest that CO

 might be the cause.

By  the  1940s,  it  became  apparent  that  global  mean  temperatures  had
begun to fall once again, which through the 1970s led to concerns that the
Earth was once more heading toward a new Ice Age. Advancing glaciers
presented renewed threats to human settlements in Alaska, Iceland, Canada,
China, and the Soviet Union.

In  1973,  Science  Digest  concluded,  “At  this  point  we  do  not  have  the
comfortable distance of tens of thousands of years to prepare for the next

2

Ice Age, and that how carefully we monitor our atmospheric pollution will
have  direct  bearing  on  the  arrival  and  nature  of  this  weather  crisis.”
Consequently, the scientists warned, “Once the freeze starts, it will be too
late.”3

In a June 1974 article titled “Another Ice Age?” Time observed, “When
meteorologists take an average of temperatures around the globe, they find
the atmosphere has been gradually cooler for the past three decades . . . and
the weather aberrations they are studying may be the harbinger of another
Ice Age.”4

The  March  1,  1975,  cover  of  the  respected  Science  News  magazine
depicted the city of New York being swallowed by an approaching glacier
and announced, “The Ice Age Cometh.” The threat was clear and urgent:
“Again,  this  transition  would  induce  only  a  small  change  in  global
temperature—two  or  three  degrees—but  the  impact  on  civilization  would
be catastrophic.” The New York Times followed suit with a headline story:
“Scientists  Ponder  Why  World’s  Climate  Is  Changing;  A  Major  Cooling
Widely Considered to Be Inevitable.”5

The prestigious National Academy of Sciences agreed with this view. In
1975, it issued a warning that there was a “finite possibility that a serious
worldwide  cooling  could  befall  the  Earth  within  the  next  100  years.”6
Popular publications echoed and amplified the alarm. The title of a book by
science writer Lowell Ponte, published that same year, pretty much summed
up the crisis: The Cooling: Has the Next Ice Age Already Begun? Can We
Survive  It?  He  warned  that  “global  cooling  presents  humankind  with  the
most important social, political, and adaptive challenge we have had to deal
with for 110,000 years.”7The Genesis Strategy, published a year later, had a
similar  message.  Noteworthy  is  that  the  author,  Stephen  Schneider,  has
subsequently  changed  his  course  of  concern  180  degrees  and  has  now
become a prominent global warming authority. The same shift of position is
true  for  Crispin  Tickell,  who  wrote  Climate  Change  and  World  Affairs
(published in 1977), an influential book of that period.

A New Crisis Emerges

 

By  the  late  1970s,  observed  rising  world  temperatures  heralded  the
coming  of  new  media  sensations.  Climate  model  calculations,  including

2

some  at  Princeton’s  Geophysical  Fluid  Dynamic  Laboratory,  began  to
predict  that  substantial  global  warming  could  result  from  increasing
atmospheric  CO
  levels.  At  the  time,  those  projections  were  generally
regarded to be an interesting but largely academic exercise, even by many
of  the  scientists  involved.  But  about  10  years  later  the  theory  gained
worldwide attention following testimony in 1988 by NASA’s James Hansen
before  then-Senator  Al  Gore’s  Committee  on  Science,  Technology  and
Space.  When  queried  by  Gore  (D-TN),  Hansen  stated  that  he  was  99
percent certain that temperatures had in fact increased, and that there had
been  some  greenhouse  warming,  although  he  made  no  direct  connection
between  the  two.  This  observation  was  consistent  with  concerns  about  a
particularly warm summer that year in some US regions.

The  scheduling  and  staging  of  Senator  Gore’s  hearings  were  carefully
orchestrated. As later recounted by his co planner Senator Timothy Wirth
(D-CO) in an interview with PBS Frontline: “We called the Weather Bureau
and found out what historically was the hottest day of the summer…so we
scheduled the hearing that day, and bingo, it was the hottest day on record
in Washington, or close to it…we went in the night before and opened all
the windows so that the air conditioning wasn’t working inside the room.”8
Although the general response within the small community of scientists
engaged in large-scale climate research was critical regarding use of highly
uncertain model results as a basis for determining important public policy
decisions,  many  did  agree  that  increased  atmospheric  CO
  levels  could
possibly  have  influenced  the  changes  in  temperature.  Their  agreement,
however,  did  not  warrant  the  greatly  exaggerated  claims  that  began  to
appear  in  the  popular  US  and  European  media  by  early  1989  that  “all
scientists” agreed that warming was real and had catastrophic potential.

2

Scientists  who  took  issue  with  these  “objective  facts”  were  often
subjected to painful consequences. Lester Lave, a professor of economics at
Carnegie Mellon University, reported that he was dismissed from one of the
hearings  for  even  suggesting  the  global  warming  issue  was  controversial.
The late Reginald Newell, a meteorology professor at MIT, believed that he
had lost National Science Foundation funding for data analyses that were
failing  to  show  net  warming  over  the  past  century  because  reviewers
suggested his results were dangerous to humanity.

As  the  Cold  War  ended  in  the  late  1980s,  the  Union  of  Concerned
Scientists,  an  organization  originally  devoted  to  nuclear  disarmament,

actively  turned  its  attention  to  the  new  cause.  In  1989,  they  circulated  a
much-publicized petition that was published in the New York Times, urging
recognition  of  global  warming  as  a  potentially  great  danger  to  mankind.
Seven  hundred  scientists,  including  many  members  of  the  National
Academy  of  Sciences  and  some  Nobel  laureates,  signed  that  petition.
Merely  three  or  four  of  the  signers,  however,  had  any  involvement  with
climatology. The article helped to solidify the desired public perception that
all scientists agree with the global warming disaster scenario.9

In  specific  reference  to  the  petition,  the  president  of  the  National
Academy of Sciences warned its members at their 1990 annual meeting not
to  lend  their  credibility  to  issues  about  which  they  have  no  special
knowledge.  His  warning  came  too  late:  Exaggerated  claims  based  upon
meager  scientific  evidence  had  already  become  the  gospel  for  a  new
religious  fervor.  Claudine  Schneider,  a  US  congresswoman  from  Rhode
Island, expressed the tenets of the new orthodoxy at a 1989 Tufts University
global  warming  symposium  when  she  said,  “Scientists  may  disagree,  but
we can hear Mother Earth, and she is crying.” What caring person would
want that? After all, scientists are people too.

Political pressures on global warming dissidents increased when Senator
Gore admonished skeptics in a featured  New York Times op-ed piece and
associated “true believers” with Galileo. In another article, he compared the
warm summer of 1988 to Kristallnacht, which ushered in the Holocaust.

Well-known entertainment figures joined politicians and activist groups
to rally more followers: In 1989, Robert Redford proclaimed at a meeting
he hosted at his Sundance, Utah, ranch that it was time to stop the research
and  begin  acting  (a  subject  he  was  more  familiar  with).  Barbra  Streisand
financially  supported  the  research  of  Michael  Oppenheimer  (at  the
Environmental  Defense  Fund),  who  was  a  global  warming  activist,  not  a
climatologist.  Meryl  Streep  presented  an  impassioned  public  television
appeal  to  stop  warming.  There  should  be  no  doubt  that  their  pleas  were
truly  sincere  and  caring.  Perhaps  this  applies  to  many  others  termed
“skeptics” (agnostics) and, even worse, “deniers” (atheists) as well? Can’t
they be sincere and caring environmentalists, too?

Wages of War

 

The  April  2008  cover  feature  of  Time  drew  a  direct  and  unseemly
parallel  between  US  involvement  in  World  War  II  against  Nazi  Germany
and Japan and the current battle against climate change. The famous image
of five American Marines raising a flag at Iwo Jima following a terrible 35-
day battle during which sixty-eight hundred American soldiers were killed
was  changed  to  depict  the  Marines  planting  a  tree,  and  the  caption  read,
“How to Win the War on Global Warming.”

Climate war marketing has become a large business, and it’s becoming
much  bigger  with  substantial  help  from  an  organization  founded  by  Al
Gore,  called  the  Alliance  for  Climate  Protection  (ACP).  The  alliance  has
launched a $300 million climate crisis media campaign over a 3-year period
to  promote  GHG  reductions  through  a  new  international  treaty,  US
legislation,  and  other  initiatives.  Advertisements  are  already  appearing  in
nationwide  television,  print,  radio,  and  online  media,  targeted  to  diverse
audiences.  As  Al  Gore  stated,  “NASCAR  fans,  churchgoers,  labor-union
members,  small  businessmen,  engineers,  hunters,  spokesmen,  corporate
leaders,  you  name  it—where  public  opinion  goes,  federal  policy  will
follow.” An example is an early television segment, narrated by William H.
Macy,  showing  footage  of  American  soldiers  storming  beaches  at
Normandy during World War II, a civil rights march, and a Moon landing.
The  message  links  these  critical  points  in  history  to  an  urgent  need  for
action now: “We can’t wait for someone else to solve the climate crisis. We
need  to  act,  and  we  need  to  act  now.  Join  us.  Together  we  can  solve  the
climate crisis.”10

And the solution? Although the message doesn’t quite tell us, it’s actually
very clear: We should all support the war against climate change. And the
answer,  of  course,  is  to  support  carbon  cap-and-trade  legislation  and
alternative energy subsidies.

Although major donor sources are not known, it is understood that large
ACP contributions have been provided by from such billionaire luminaries
as George Soros; CNN founder Ted Turner; Sun Microsystems cofounder
Vinod Khoska; and Apple CEO Steve Jobs. (Mr. Gore sits on Apple’s board
of directors.)11 Al Gore is contributing his salary as a partner in the venture
capital firm of Kleiner Perkins Caufield & Byers to the alliance, along with
his Nobel winnings ($750,000) and proceeds from his movie and his book,
An  Inconvenient  Truth.  This  does  not  include  the  investment  income  he
garners  from  Kleiner  (much  greater  than  salary  and  taxed  at  lower  than

ordinary  income  rates),  yet  his  strong  personal  commitment  to  alliance’s
goals  is  irrefutable.  There  should  be  no  doubt  regarding  his  genuine
dedication to the cause.12

At least you have to give Al Gore credit for putting his money where his
mouth  is,  while  also  feeding  it  from  profits  he  receives  from  the  climate
change war. For example, he has invested $35 million with the Capricorn
Investment Group, a firm that Bloomberg News says puts clients’ assets into
hedge funds and invests in “makers of environmentally friendly products.”
Capricorn  was  founded  by  billionaire  Jeffrey  Skoll,  who  produced  Mr.
Gore’s “documentary,” An Inconvenient Truth. That’s quite a large sum for
someone whose estimated total assets in 2000 were between $800,000 and
$1.9 million.13

Since  his  nonelection  to  the  presidency,  Mr.  Gore  has  been  very
successful as an eco-multimillionaire, with an estimated net worth well in
excess  of  $100  million.  In  addition  to  his  six-figure  speaking  gigs,  he
signed  on  as  an  adviser  to  Google  in  2001—before  it  went  public—and
received  stock  options  now  reportedly  valued  at  more  than  $30  million.
When he joined Apple’s board in 2003, he received stock options believed
to be valued now at about $6 million. In 2004, Mr. Gore and some partners
purchased the Canadian news network News World International (NWI) for
$70  million  and  renamed  it  Current  TV.  His  investment  partners  were
former Goldman Sachs senior director Philip Murphy (Democratic Finance
Committee  chair);  Richard  Blum  (husband  of  California  senator  Dianne
Feinstein); Sun Micro-systems cofounder Bill Joy; and Bill Pittman, former
AOL Time Warner CEO.14

Fast  Money,  in  an  article  titled  “Al  Gore’s  $100  Million  Makeover,”
quotes  Philip  Murphy’s  recollections  of  the  time  in  2003  when  Mr.  Gore
was struggling to launch the Current TV cable network and also starting a
hedge  fund  (now  with  more  than  a  billion  dollars  in  assets)  called
Generation Investment Management (GIM). According to Gore, both were
created with a desire “to incorporate sustainability values into the financial-
services  work  I  was  doing.”  Murphy  had  introduced  Gore  to  his  GIM
partner,  David  Blood,  formerly  with  Goldman  Sachs,  and  “they  were
asking, ‘can this make money? Can this be a business?’” Apparently, the
answer was (and still is) strongly affirmative.15

Good News! It’s Terrifying—and It’s Our Fault!

 

Government,  corporate,  and  private  climate  change  research  funding
depends upon delivering results the sponsors want. What if it turned out that
climate  change  follows  natural  cycles,  and  for  good  or  for  bad,  we  don’t
have a lot to say or do about it? That would qualify as a true climate change
disaster for thousands of scientists, administrators, and their families, whose
work  and  lives  have  come  to  depend  upon  causes  that  are  anthropogenic
(resulting  from  the  influence  of  human  beings  on  nature;  hereafter  used
interchangeably with “man-made”).

Let  there  be  no  mistake:  As  most  of  us  recognize,  environmental
scientists are among Earth’s most dedicated and caring inhabitants—people
who  are  as  principled  and  ethical  as  humans  come.  Few,  if  any,  selected
their  profession  for  its  financial  potential,  and  their  education  and
competencies  warrant  true  respect.  Many  are  associated  with  universities
and others with government agencies. They write scholarly papers; compete
for  publishing  opportunities  in  selective  journals;  present  peer-reviewed
papers at global conferences; spend countless hours writing grant proposals;
and yes, participate in the United Nations’ IPPC scientific working groups.
Okay. Enough pandering.

The stark reality is that climate sciences have become strongly politicized
over the global warming issue. Some of the strongest proponents of human-
caused  climate  change  theories  in  Congress,  for  example,  are  among  the
strongest supporters of those funding programs. Government agencies that
receive  and  distribute  these  funds  find  it  necessary  to  demonstrate  that
threats are real and urgent in order to justify budgets and demonstrate public
benefits.  Philanthropic  organizations  routinely  give  out  large  sums  of
research money on the same alarmist basis. An example is the MacArthur
Foundation,  which  earmarks  $500,000  no-strings-attached  grants  for
climatologists who speak out about global warming threats.16

An  inescapable  fact  is  that  climate  change  politics  has  had  apparent
partisan  leanings.  Climate  Science  Watch  (a  nonprofit  public  interest
education  and  advocacy  project  “dedicated  to  holding  public  officials
accountable for the integrity and effectiveness with which they use climate
science  and  related  research  in  government  policymaking”)  has  been
strongly critical of US Climate Change Science Program (CCSP) cutbacks
under Republican influences since the program peaked in 1995. That was

when Republicans gained control of both houses of Congress. In February
2002,  the  first  year  the  George  W.  Bush  administration  gained  major
influence  over  the  federal  budget,  interagency  climate  research  budgets
were reduced to 1993 levels. Then, after the 2006 election cycle brought a
new Democratic majority in both the House and the Senate, the dynamic
changed  to  restore  more  funding  to  the  “starved”  climate  programs  and
create new ones through twelve different 2008 bills approved by the Senate
Appropriations  Committee.  Various  Office  of  Management  and  Budget
(OMB)  reports  projected  CCSP  increases  of  $155  million  for  February
2007 and $282 million for 2008.17

A  National  Academy  of  Sciences/National  Research  Council  analysis
report  reviewing  CCSP  funding  cutbacks  under 
the  Bush-Cheney
administration wasn’t positive; it concluded that “U.S. capability to monitor
trends, document the impacts of future climate change, and further improve
prediction and assimilation models … will decline even as the urgency of
addressing climate change increases.” This conclusion is not surprising. The
National  Academy  of  Sciences  has  always  enthusiastically  supported
expensive  climate  research  programs,  and  it  has  taken  strong  stances
supporting  both  global  cooling  and  global  warming  models,  shifting
positions  in  concert  with  weather  patterns.  The  academy,  along  with  the
numerous institutions it represents, depends upon billions of federal dollars
to fund thousands of research projects, tens of thousands of PhD degrees
(along  with  many  more  associate  positions),  and  dozens  of  professional
journals that publish the results.18

Democrats  and  Democrat-backed  institutions  are  almost  invariably  the
ones  who  support  the  United  Nations’  efforts  to  use  global  warming  to
promote worldwide wealth redistribution. It is ironic to see IPCC political
officials  challenging  the  objectivity  of  scientists  who  don’t  subscribe  to
their clearly demonstrated and well-documented biases. A case in point is
when  their  “Climate  Change  1995  Summary  for  Policymakers”  report
stated they had found a “human fingerprint” as evidence of anthropogenic
global  warming.  The  author  of  that  particular  science  chapter,  a  US
government  employee, 
to  having  made
“backroom”  changes  under  pressure  from  top  US  governmental  officials.
The  report  had  been  edited  to  remove  five  different  statements—all  of
which  had  been  approved  by  the  panel’s  scientific  consultants—that  had
specifically said no such evidence had been found. The IPCC, to this day,

finally  publicly  admitted 

has  never  offered  real  evidence  to  support  its  assertion  that  humans  are
causing  global  warming.19  Yet  without  humans  as  the  cause  of  global
warming,  there  is  no  way  to  develop  widespread  support  for  the  global
warming funding bonanza.

US Climate Science: Growth of a New Industry

 

Global  warming,  aka  climate  change,  has  been  a  natural  blessing  to
government  agencies  and  researchers  that  undertake  climate  science
research. Growth of US government funding in this new industry has been
phenomenal: It increased from $209 million in 1989, when the subject first
began  to  heat  up  following  then-Senator  Gore’s  1988  Committee  on
Science, Technology and Space hearings, to a proposed $1.446 billion (in
2005 dollars) in 2008. That’s nearly a 200 percent rise. The largest player in
this  arena  in  2008  was  NASA  ($816  million  in  2008),  followed  by  the
National  Science  Foundation  ($145  million),  the  US  Department  of
Commerce and NOAA ($163 million), and the US Department of Energy
(DOE;  $122  million).  Comparatively  paltry  budgets  were  awarded  to  the
US Department of Agriculture ($55 million), the DOE’s Office of Health,
Safety and Security ($47 million), and the EPA ($17 million), plus a few
others.20

Notably,  the  EPA’s  climate  budget  has  since  become  much  more
generous, with $112 million included by the Obama-Biden administration
for FY 2010.

That’s  only  part  of  it.  According  to  a  2007  press  release  by  the  White
House  Office  of  Science  and  Technology,  the  US  was  already  spending
about  $5  billion  per  year  on  climate  research  through  various  programs.
This was more than twice the amount spent on sending humans to the Moon
during the Apollo program (about $2.3 billion per year). But then, the Moon
doesn’t  have  a  climate  to  study,  and  even  if  it  did,  we  probably  couldn’t
blame the Industrial Revolution for changing it.21

to  capture 

Climate  change  began 

the  attention  of  US  security
organizations following the Yom Kippur Arab-Israeli war of 1973 because
of  this  nation’s  increasing  concerns  about  continued  dependence  upon
foreign oil imports. This provided the foundation for expanded government
investment  in  science  programs  that  connected  energy  priorities  with

environmental  issues  that  have  broad  public  appeal  and  support.  “Saving
the planet” has become a popular theme to justify expanding budgets.22

NASA  brought  satellite  Earth  surface  and  atmospheric  sensing
capabilities to climate science research, applying technologies and expertise
from  the  Mercury  and  Gemini  programs  of  the  1960s.  Its  2008  climate
change research budget in constant 2005 dollars was more than thirty-five
times larger than was reported in 1989, but it was reduced in 2006 by about
6 percent below the 2005 level.

One of the casualties of NASA’s climate science cutbacks was the Deep
Space Climate Observatory (DSCOVR) satellite proposed in 1998 by then-
Vice President Gore for Earth observations. Republican critics derided the
$100 million system, nicknamed “GoreSat,” as an “overpriced screensaver”
with unfocused purposes; it has since been held in storage at the cost of $1
million per year.

The  most  outspoken  critic  of  NASA’s  priorities  and  the  Bush-Cheney
administration’s  climate  change  policies  was  James  Hansen  (who,  as  I
mentioned  earlier,  was  Gore’s  star  global  warming  testifier  at  his  1988
Senate hearings). In his capacity as director of GISS, Dr. Hansen expressed
his disagreement very strongly during a 2004 speech at the University of
Iowa,  where  he  also  publicly  announced  his  support  for  the  presidential
campaign  of  Senator  John  Kerry  (D-MA).  In  2001,  Hansen  had  been  the
recipient  of  a  $250,000  award  from  the  Heinz  Foundation,  headed  by
Senator Kerry’s wife, Teresa Heinz, a circumstance that made the political
endorsement  by  a  prominent  NASA  civil  servant  appear  particularly
suspect.

Hansen  was  never  the  least  bit  hesitant  about  speaking  out  against  the
Bush  administration  and  NASA  policies  regarding  global  warming  in
general and their lack of support for aggressive GHG abatement measures
in particular.23 In a January 29, 2006, interview published in the New York
Times, he charged that NASA public relations officials had pressured him to
allow  them  to  review  future  public  lectures,  papers,  and  postings  on  the
GISS website. This followed a December 6, 2005, presentation he gave to
the American Geophysical Union, during which he stated that the Earth’s
climate  is  approaching  a  tipping  point  that  will  result  in  the  loss  of  the
Arctic  as  we  know  it,  with  sea  levels  rising  as  much  as  80  feet  and  thus
flooding  coastal  areas.  He  warned  that  this  could  be  halted  only  if  GHG
emissions are reduced within the next 25 years.24

And  that  was  the  good  news.  In  a  paper  titled  “Is  There  Still  Time  to
Avoid  Disastrous  Effects?”  presented  at  a  Climate  Change  Research
Conference held in Sacramento, California, on September 13, 2006, Hansen
added  hellfire  for  the  damned:  “Melting  ice  caps  will  raise  sea  levels  by
between 32 and 78 feet, forcing millions to seek refuge; increasingly violent
weather patterns will cause major destruction and as the land dries up, bush
fires will be more frequent.”

After  the  master  of  disaster  claimed  to  have  received  threats  of
recrimination  for  continuing  to  speak  out,  he  was  offered  support  by  two
organizations.  One  was 
the  Government  Accountability  Project,  a
Washington, DC, law firm that volunteered legal support in a suit against
NASA.  The  other  was  George  Soros  through  his  Open  Society  Institute
(OSI), as part of its Politicization of Science program. (Yes, that’s what they
really call it!) As reported in a September 24, 2007, editorial published in
the Investor’s Business Daily, titled “The Soros Threat to Democracy,” OSI
may have supported Hansen to the tune of up to $750,000 out of that fund.
Dr. Hansen has denied receiving any money, and there is no proof that he
did. Yet he was listed as an “OSI grantee” in the “2006 Soros Foundation
Network Report.” George Soros, like Al Gore and John Kerry, was not a big
George W. Bush fan.

The Science Unanimity Myth

 

Widely  circulated  statements  that  scientists  unanimously  agree  about
global  warming  and  human  contributions  to  it  or  the  importance  and
consequences of it are patently false. The apparent purpose of such claims
is to discredit those with opposing viewpoints, deriding them with contempt
previously  reserved  for  those  who  deny  the  Holocaust,  the  dangers  of
tobacco,  and  the  achievements  of  NASA’s  Apollo  program.  Al  Gore  has
little  tolerance  for  unbelievers,  as  evidenced  in  this  statement:  “Fifteen
percent of the population believes the Moon landing was staged in a movie
lot in America, and somewhat fewer believe the Earth is flat. I think they
should all get together with the global warming deniers on a Saturday night
and party.”25

“Scientific  consensus” 

to  scary  climate
projections  have  played  well  to  legitimize  highly  speculative  research

representations  attached 

conclusions  useful  to  justify  additional  funding,  sell  newspapers,  and
enhance  television  audience  ratings.  But  several  petitions  and  surveys
involving science communities present a far from unified picture.26

•    In  1992,  a  “Statement  of  Atmospheric  Scientists  on  Greenhouse
Warming”  that  opposed  global  controls  on  GHG  emissions  drew
about 100 signatures, mostly from American Meteorological Society
technical committee members.27

•  In 1992, a “Heidelberg Appeal,” which also expressed skepticism on
the  urgency  of  restraining  GHG  emissions,  drew  more  than  4,000
signatures from scientists worldwide.28

•  In 1996, a “Leipzig Declaration on Climate Change” that emerged
from an international conference addressing the GHG controversy,
was signed by more than 100 scientists in climatology and related
fields.29

•    In  1997,  a  survey  of  American  state  climatologists  (the  official
climate monitors in each of the fifty states) found 90 percent agreed
that “scientific evidence indicates variations in global temperatures
are  likely  to  be  naturally  occurring  and  cyclical  over  very  long
periods of time.”30

that  “climate  prediction 

•    In  2001,  the  American  Association  of  State  Climatologists
concluded 
is  complex,  with  many
uncertainties;  the  AASC  recognizes  climate  prediction  is  an
extremely difficult undertaking. For time scales of a decade or more,
understanding  the  empirical  accuracy  of  such  prediction—called
verification—is simply impossible, since we have to wait a decade
or more to assess the accuracy of the forecasts.”31

•    In  May  2007,  a  survey  of  530  climate  scientists  by  the  Heartland
Institute  revealed  that  only  about  one-half  agreed  that  “climate
change is mostly the result of anthropogenic causes,” and only one-
third  of  those  agreed  that  “climate  models  can  accurately  predict
conditions in the future.”32

•  In April 2008, the results of a survey of 489 scientists, conducted by
the Statistical Assessment Service (STATS), indicated that most (74
percent)  believed  that  some  human-induced  greenhouse  warming
has  occurred,  up  from  41  percent  reported  in  the  1991  Gallup

survey.  Only  41  percent  of  those  polled,  however,  said  they  were
directly involved in any aspect of global climate science.33

•  In 2008, a US Senate minority report issued by Senator James Inhofe
(R-OK) presents the testimony of 650 climate-related scientists from
around  the  world  who  strongly  challenge  global  warming  crisis
claims.  They  include  a  Nobel  laureate  and  former  IPCC  study
participants.

•    In  March  2009,  more  than  600  skeptical  people  attended  a
conference organized by the Heartland Institute in New York City to
protest  cap-and-trade  regulations  favored  by  President  Obama  that
would roll GHG emissions back to 1990s levels. President Vaclav
Klaus  of  Czech  Republic  delivered  the  keynote  speech.  Speaking
again the next day to Columbia University faculty and students, he
reaffirmed his strong opposition to a concept that global warming is
man-made. “The problem is not global warming … by the ideology
which uses or misuses it—it has gradually turned the most efficient
vehicle  for  advocating  extensive  government  intervention  into  all
fields  of  life  and  for  suppressing  human  freedom  and  economic
prosperity.”34

Scientific questions and disputes will never be resolved by opinion poll
tabulations. If that were the case we might now be fleeing in seal-oil fueled
snowmobiles  the  ravages  of  the  miles-thick  glaciers  predicted  a  few
decades ago. Yet it is disingenuous to suggest that the debate is over. Or if it
is, that will come as a big disappointment to those with a few remaining
contrary opinions that they may be required to abandon by majority vote. In
fact,  some  man-made  warming  proponents  are  attempting  to  discredit
skeptical scientific opinions out of existence altogether.

For instance, in a December 2004 article titled “Beyond the Ivory Tower:
The  Scientific  Consensus  on  Climate  Change,”  published  in  the  journal
Science,  Naomi  Oreskes,  a  University  of  California–San  Diego  history
professor, reported that her search of the Internet under the term “climate
change”  turned  up  928  studies,  based  on  which  she  cheerfully  concluded
that there was complete scientific agreement.35

Few,  if  any,  scientific  papers  claim  to  “refute”  the  theory  of  human-
induced warming, and a search under the term “climate cycles” rather than
“climate  change”  would  have  produced  a  different  result.  Hundreds  of

Issues  of  debate  cannot  be  resolved  by  claims  that  a  consensus  among
authorities has settled the matters so long as a minority, even a small one,
believes  otherwise.  Objective  science  and  progress  have  always  been
advanced by those who have proven that simple lesson.

If  global  warming  crisis  skeptics  and  deniers  are  heretics,  they  may
perhaps  take  some  comfort  in  the  fact  that  their  numbers  are  rapidly
growing. This is particularly true in the US. A 2010 Gallup poll indicates
that the percentage of respondents who said they worry “a great deal” about
global warming was only 28 percent, down from 33 percent in 2009 and 41
percent in 2007, when worry peaked. Global warming ranked last of eight
environmental issues listed in the survey.37

studies have been published that discuss potentially important and dominant
natural  forces  that  influence  global  warming  and  cooling  over  both  short
and  very  long  periods,  including  solar  climate-forcing  factors  (hereafter
referred to at times simply as climate forcings).36

Gallup  also  conducted  a  2010  poll  that  asked  the  question  “Thinking
about  what  is  said  in  the  news,  in  your  view  is  the  seriousness  of  global
warming  generally 
correct,  or  generally
underestimated?” In just 4 years the percentage of Americans who believe
global warming has been exaggerated has grown by 60 percent, constituting
48 percent of the respondents.38

exaggerated,  generally 

Chapter 2

COOKING THE CLIMATE BOOKS

Departing from responsible science recipes.

No one can confidently forecast global, national, or even regional
weather  conditions  that  will  occur  months  or  years  into  the  future,  much
less predict climate changes and impacts that will be realized over decadal,
centennial,  and  longer  periods.  Nevertheless,  this  broadly  recognized
limitation  has  not  dissuaded  doomsday  climate  predictions  that  have
captured  worldwide  media  attention.  Such  postulations  attach  great
credence  to  extreme  speculations,  incomplete  data,  and  overly  simplistic
computer  models  that  have  never  demonstrated  accuracy.  Given  the  huge
uncertainties, and under great pressures to produce definitive conclusions,
modelers hedge many projections with probabilistic language that gets them

off  the  hook  of  accountability.  The  pronouncements  are  typically  cast  as
percentile chances that something or other may happen, or simply that such
an  event  is  more  or  less  likely  to  happen  than  not.  Truly  alarming
possibilities are usually treated as most newsworthy.

Fog in the Crystal Ball of Climate Forecasts

 

Scientists who study climate change phenomena generally fall into two
professional camps. Meteorologists tend to recognize the inherent, almost
biological complexity of the overall climate system and view it as resilient
and  “self-healing.”  As  a  group  they  are  usually  more  skeptical  about  the
importance  of  global  warming  and  less  confident  about  climate  model
results.  Physicists,  on  the  other  hand,  are  accustomed  to  reducing  the
behavior  of  a  physical  system  (e.g.,  climate)  to  a  minimum  number  of
mathematical equations in order to study it. They have a simpler view of
climate  forcings,  and  they  tend  to  have  more  confidence  in  models  as
predictive tools.1

Even the most sophisticated climate models must be simplified to run on
present-day computers, which aren’t nearly fast enough to handle all known
processes  with  a  high  level  of  definition.  Not  even  really  big,  three-
dimensional  general  circulation  models  (GCMs),  which  are  capable  of
tracking more than 5 million different variables at any given time, can do
the trick. Those variables include climate influences associated with these
factors,  among  hundreds  of  others:  upper  atmosphere  jet  streams;  deep
ocean currents; variations in radiant energy from the Sun; amounts of solar
radiation  reflected  back  to  space  by  ice  sheets  and  glaciers;  seasonal
vegetation patterns; atmospheric GHG and aerosol changes; eddies in the
oceans  that  transfer  heat  laterally;  and  numbers,  types,  and  altitudes  of
clouds.2

Since  GCMs  must  process  enormous  amounts  of  complex  data,  they
require  very  expensive  supercomputers 
that  only  wealthy  national
governments  can  afford.  Prominent  US  systems  are  located  at  NASA’s
GISS,  the  US  National  Center  for  Atmospheric  Research,  NOAA’s
Geophysical Fluid Dynamics Laboratory, and Britain’s Hadley Centre. Yet
with all their computing power, even the EPA urges caution in taking results
too literally: “These complicated models are able to simulate many features

of  the  climate,  but  they  are  still  not  accurate  enough  to  provide  reliable
forecasts of how the climate may change.”3

Very  small  model  errors  associated  with  reference  data,  or  underlying
assumptions  regarding  climate-forcing  mechanisms  and  interactions,  can
yield very large errors in outputs. Such errors are inescapable and expand
rapidly as a function of the projected forecast period. Because the accuracy
cannot  be  tested  prior  to  that  yet-unrealized  future  time,  results  are
compared  with  those  yielded  by  other  modelers  for  general  validation,
although all these may be very wrong.

Roy Spencer, a principal research scientist at the University of Alabama,
Huntsville,  and  former  senior  scientist  for  climate  studies  at  NASA,
observes  that  results  of  the  one  or  two  dozen  climate  modeling  groups
around the world often reflect a common bias. One reason is that many of
these  modeling  programs  are  based  upon  the  same  “parameterization”
assumptions; consequently, common errors are likely to be systematic, often
missing 
important  processes.  Such  problems  arise  because  basic
components  and  dynamics  of  the  climate  system  aren’t  understood  well
enough on either theoretical or observational grounds to even put into the
models. Instead, the models focus upon those factors and relationships that
are most familiar, ignoring others altogether. As Spencer notes, “Scientists
don’t like to talk about that because we can’t study things we don’t know
about.”4

Joanne Simpson, who recently died at age eighty-six, developed some of
the  first  mathematical  models  of  clouds  in  attempts  to  better  understand
how hurricanes draw power from warm seas. Ranked as one of the world’s
top meteorologists, she believed that global warming theorists place entirely
too much emphasis upon faulty climate models, stating: “We all know the
frailty  of  models  concerning  the  air-surface  system  .  .  .  We  only  need  to
watch the weather forecasts.”5

Another  prominent  scientist,  Syun-Ichi  Akasofu,  is  a  staunch  critic  of
certain parts of the United Nations’ IPCC 2007 AR4 report “Summary for
Policymakers.”  He  determined  that  IPCC  computer  models  could  not
duplicate observed temperature patterns in Arctic regions. Although the CO
forecasts  did  indicate  a  warm  Arctic  condition,  they  were  lower  than
actually  reported,  and  colder  areas  were  absent.  Dr.  Akasofu  stated,  “If
fourteen  GCMs  cannot  reproduce  prominent  warming  in  the  continental

2

Arctic, perhaps much of this warming is not produced by greenhouse effect
at all.”6

Spencer coauthored a report of a scientific study that was published in
Science  Daily  at  the  end  of  2007.  In  the  report  he  asserted  that  IPCC’s
computer models may be wildly overestimating man-made global warming
due to a lack of understanding of the important roles that clouds play: “All
leading climate models forecast that as the atmosphere warms there should
be  an  increase  in  high-altitude  cirrus  clouds,  which  would  amplify  any
warming caused by man-made GHGs . . . To give an idea how strong this
enhanced cooling mechanism is, if it was operating on global warming it
would reduce estimates of future warming by over 75 percent.”7

NASA’s GISS director, James Hansen, seems to have been well aware of
this  model  problem  when  his  organization,  along  with  MIT,  published  a
paper  in  the  February  28,  2001,  issue  of  the  Bulletin  of  the  American
Meteorological Society. The authors of that paper explained that the Pacific
Ocean “may be able to open a ‘vent’ in its heat-trapping cirrus cloud cover
and release enough energy into space to significantly diminish the projected
climate  warming  …  This  newly  discovered  effect—which  is  not  seen  in
current  prediction  models—could  significantly  reduce  estimates  of  future
climate warming.”8

Graeme  Stephens  of  Colorado  State  University’s  Department  of
Atmospheric  Science  warned  in  a  January  2005  paper,  published  in  the
Journal of Climate, that computer models involve simplistic cloud feedback
descriptions:  “Much  more  detail  on  the  system  and  its  assumptions  [is]
needed to judge the value of any study. Thus we are led to conclude that the
diagnostic  tools  currently  in  use  by  the  climate  community  to  study
feedback,  at  least  as  implemented,  are  problematic  and  immature  and
generally cannot be verified using observations.”9

A peer-reviewed climate study that appeared in the July 23, 2009, edition
of Geophysical Research is critical of IPCC modeling tendencies to fudge
climate projections by exaggerating CO
 influences and underestimating the
importance  of  shifts  in  ocean  conditions.  The  research  indicated  that
influences  of  solar  changes  and  intermittent  volcanic  activity  have
accounted for at least 80 percent of observed climate variation over the past
half century. Study coauthor John McLean made this observation:

2

When  climate  models  failed  to  retrospectively  produce  the
temperatures  since  1950, 
the  modelers  added  some  estimated
influences of carbon dioxide to make up the shortfall . . . The IPCC
acknowledges in its fourth Assessment Report that [El Niño-Southern
Oscillation]  ENSO  conditions  cannot  be  predicted  more  than  12
months ahead, so the output of climate models that could not predict
ENSO  conditions  were  being  compared  to  temperatures  during  a
period  that  was  dominated  by  those  influences.  It’s  no  wonder  that
model  outputs  have  been  so  inaccurate,  and  it’s  clear  that  future
modeling must incorporate the ENSO effect if it is to be meaningful.10

Even Kevin Trenberth, an exposed party in the University of East Anglia
CRU  “Climategate”  scandal,  has  admitted  that  the  IPCC  climate  models
failed to duplicate realities. In 2007 he stated, “None of the models used by
the IPCC are initialized to the observed state and none of the climate states
in the models correspond even remotely to the current observed climate.”11

Human CO

 Fingerprints and Footprints

2

 

One reason why anthropogenic influences upon global climate change
are extremely difficult to model is that there is no reliable way to separate
human  sources 
from  natural  ones.  More  generally,  atmospheric
measurement records are very short and exist against a background of other
natural  variables  that  have  other  unknown  effects.  For  example,  satellite
spectral analyses of North Atlantic oscillations reveal a randomly varying
climate  pattern  with  little  evidence  of  a  persistent  long-term  trend
influenced  by  man-made  CO
  contributions.  Models  have  been  shown  to
under-predict  natural  climate  variations  on  decade-long  to  century-long
timescales;  to  incorrectly  predict  variances  over  timescales  in  which
anthropogenic CO
 levels would be expected to rise; and to under-predict
changes  due  to  short-term  natural  influences.  Such  influences  include
volcanic eruptions, stratospheric ozone variations, sulfate aerosol changes,
and solar change events.

It is important to realize that global temperatures and atmospheric CO
levels  have  fluctuated  greatly  over  hundreds,  thousands,  and  millions  of
years,  long  before  humankind  lit  cave  fires  or  Icelandic  Vikings  tended

2

2

2

2

2

 levels—not the other way around.
 levels and temperatures bear this out.

cattle, sheep, and goats on previously warm Greenland grasslands. In most
cases,  the  temperature  changes  led,  rather  than  followed,  changes  in
atmospheric CO
Records of CO
During the 149 years between 1812 and 1961 there were three periods
when  average  CO
those  when
temperatures peaked in 2004. Circa 1820, they reached about 440 parts per
million (ppm). Around 1855, they were about 390 ppm, and they returned
to about 440 ppm in 1940 when man-made CO
 emissions were nearly 30
times higher than they were in 1880 (such emissions are even higher than
that now).12

  concentrations  were  higher 

than 

2

2

2

2

Based  upon  a  variety  of  proxy  indicators,  such  as  ice  core  samples,
atmospheric CO
 levels have remained relatively low over the past 650,000
years,  even  during  the  six  previous  interglacial  periods  when  global
temperatures  were  as  much  as  9ºF  warmer  than  the  temperatures  we
currently  enjoy.  If  this  is  true,  might  we  legitimately  wonder  what
accounted  for  those  nonhuman  greenhouse  influences?  It  would  seem  to
suggest  that  anthropogenic  CO
  contributions  may  have  no  discernible
influence upon climate, or that proxy data is often inaccurate—or both.

Maurine Raymo, an MIT associate professor of earth, atmospheric, and
planetary sciences, published a paper in the April 1988 issue of the journal
Nature,  suggesting  that  the  Earth  has  endured  huge  climate  swings  on  a
number  of  occasions  over  the  past  1.5  million  years.13  Records  from  ice
cores and ocean sediments show that atmospheric CO
 concentrations over
this period have fluctuated greatly throughout Earth’s history due to several
natural  causes.  Levels  of  CO
  rise  and  fall  seasonally  in  response  to
warming  and  cooling  effects  of  plant  growth  cycles.  GHGs  and  aerosols
emitted  from  volcanic  eruptions,  along  with  probable  Earth  orbit,  solar
changes, and other contributors, have combined heating and cooling effects.
In  turn,  these  forcing  factors  affect  ocean  temperatures,  which  influence
evaporation rates (rainfall and plant growth) and the amount of atmospheric
CO
  is  dissolved  at  ocean  surfaces,
particularly in polar regions where water is coldest. Aerobic respiration by
plants  and  animals  breaks  down  glucose  into  CO
  and  water,  while
photosynthesis  reverses  the  process.  Huge  deposits  of  limestone,  marble,
and  chalk,  mainly  composed  of  calcium  carbonate,  are  eroded  by  ocean

  absorbed  and  released.  More  CO

2

2

2

2

2

water  to  produce  CO
natural.14

2

  and  carbonic  acid.  Climate  change  is  thus  very

2

2

2

, or any CO

Looking  back  over  several  million  years  in  Earth’s  history,  it  is
challenging  to  imagine  that  major  global  temperature  swings  can  be
attributed to man-made CO
for that matter. It is apparent that
past CO
 levels have been high at times when global temperatures were low,
and vice versa. During the eras when dinosaurs thrived, global temperatures
ranged between 72ºF and 77ºF, a blistering 20 degrees higher than today’s
average  between  54ºF  and  57ºF.15  So  far  as  we  know,  none  of  those
creatures,  flatulent  as  they  may  have  been,  would  have  been  responsible.
And there is no evidence that they burned coal or drove SUVs.

Around 600 million years ago (during the Cambrian period of the Early
Paleozoic  era),  atmospheric  CO
  levels  were  believed  to  be  about  7,000
ppm,  compared  with  the  379  ppm  in  2005!16  Then,  approximately  480
million  years  ago  (between  the  Ordovician  and  Silurian  periods),  those
levels gradually dropped to 4,000 ppm over about 100 million years, while
average  temperatures  remained  at  a  steady  72ºF.  The  CO
  levels  later
jumped rapidly to 4,500 ppm during the Late Ordovician period, and guess
what!  Temperatures  dove  to  an  estimated  average  similar  to  today,  even
though the CO
 level was around twelve times higher than it is at present.
Yes, as CO

 went up, temperatures plummeted.

2

2

2

2

2

About 438 million years ago, atmospheric CO

 dropped from 4,500 ppm
to  3,000  ppm,  yet  according  to  fossil  records,  world  temperatures  shot
rapidly back up to an average 72ºF. So, regardless of whether the CO
 levels
were 7,000 ppm or 3,000 ppm, temperatures rose and fell independently.17

Also, over the past 600 million years there have been only three periods,
including now, when Earth’s average temperature has been as low as 54ºF.
One was the Late Ordovician period; the other occurred about 315 million
years  ago,  during  a  45-million-year-long  cool  spell  called  the  Late
Carboniferous period. Most of our planet’s coalfields date back to that time.
Both CO
 and temperatures shot back up at the end of it, just as the main
Mesozoic  dinosaur  era  was  beginning.  CO
  levels  rose  to  between  1,200
ppm and 1,800 ppm, and temperatures again returned to the average 72ºF
that Earth seemed to prefer.18

Around 180 million years ago, CO

 rocketed up from about 1,200 ppm to
2,500  ppm.  And  would  you  believe  it?  This  coincided  again  with  a  big

2

2

2

2

2

temperature dive from 72ºF to about 61ºF. Then, at the border between the
Jurassic period when T. rex ruled and the Cretaceous period that followed,
CO
 levels dropped again, while temperatures soared back to 72ºF. Average
temperatures  remained  at  that  high  level  until  long  after  the  dinosaurs
became extinct.19

Perhaps  you’ll  wish  to  ponder  this  question:  Given  that  over  most  of
Earth’s  known  climate  history,  the  atmospheric  CO
  levels  have  been
between  four  and  eighteen  times  higher  than  they  are  now—throughout
many  times  when  life  not  only  survived  but  also  flourished;  times  that
preceded  humans;  times  when  CO
  levels  and  temperatures  moved  in
different directions—how much difference will putting caps on emissions
accomplish? Consider also that about 97 percent of all current atmospheric
CO

 derives from natural sources.

2

2

2

Throwing the Public a Curve

 

2

2

2

The  IPCC’s  2007  AR4  report  “Summary  for  Policymakers”  asserted
that atmospheric GHGs (read as CO
) “now far exceed preindustrial values
over the past 650,000 years.” That report appears to reflect some short-term
memory deficiencies. It is not accurate, as the authors claim, that CO
 in the
preindustrial era was about 25 percent lower than it is now. As evidenced
by  the  more  than  ninety  thousand  direct  CO
  measurements  taken  in
America,  Asia,  and  Europe  between  1812  and  1961,  concentrations  have
been much higher than the approximate 380 ppm level we see today.20

With such weak evidence, how did organizations find a way to convince
the public that atmospheric CO
 levels are skyrocketing? As it turned out,
IPCC  representations  of  postindustrial  influences  on  climate  change  were
heavily  based  upon  a  very  misleading,  unpublished,  non-peer-reviewed
research  report  submitted  by  the  author.  A  now-infamous  “hockey  stick”
graph  that  illustrated  the  report’s  conclusions  has  since  been  thoroughly
debunked,  yet  it  was  repeatedly  highlighted  in  IPCC  summary  reports.  It
was also used as the main visual in the Clinton-Gore administration’s report
“National Assessment of the Potential Consequences of Climate Change”
(2000)  and  again  featured  in  Al  Gore’s  An  Inconvenient  Truth—both  the
movie and the book.

2

The graph’s creator was none other than Michael Mann, later exposed as
a  principal  inner-circle  member  of  the  Climategate  network  revealed
through hacked CRU e-mails. When Mann produced the graph, he was a
young  PhD  at  the  University  of  Massachusetts  and  the  IPCC  report  lead
author,  who  selected  his  own  non-peer-reviewed  paper  for  inclusion.
Arguably, this flawed study has done more to advance the concept of global
warming hype than all others combined.

Mann’s  research  produced  curves  representing  changes  in  atmospheric
CO
 levels over time frames ranging from 300 to 10,000 to 400,000 years.
All  showed  that  low  preindustrial  concentrations  soared  up  to  about  370
ppm at the end of the 20th century, obviously (it appeared) due to human
influences. Temperature change graphs superimposed over the CO
 curves
made that connection clear and dramatic.21

2

2

Mann’s  early  temperature  data  was  taken  from  several  different  proxy
records,  particularly  tree  rings.  More  recent  records  were  based  upon
official surface readings taken since 1980. They included some measured in
what are termed “urban heat islands” influenced by buildings, paving and
other  infrastructure  developments,  which  probably  inflated  the  warming
change.  He  eliminated  substantial  temperature  fluctuations  that  occurred
during the Medieval Warm Period (MWP; about 950 to 1300 AD) and the
Little  Ice  Age  (LIA;  1500s  to  1800s),  reinforcing  the  premise  that
temperatures were quite stable for approximately 900 years prior to 1910,
and then they rocketed upward. The results were impressive.22

Issues arose after two Canadian non–climate scientists trained in statistics
began to wonder if the chart wasn’t maybe too stunning. Steve McIntyre, a
metallurgy  and  data  analysis  expert,  and  Ross  McKitrick,  an  economist
from the University of Guelph, asked Mann for sources of the original data,
and after repeated requests, they finally obtained an incomplete response.
They learned that the Mann temperature proxy studies had given heaviest
weight  to  tree  ring  data  from  fourteen  Sierra  Nevada  mountain  sites  in
California  based  upon  ancient  slow-growing,  high-elevation  bristlecone
pine trees that wouldn’t have reflected a strong 20th-century growth spurt
attributable to warming as had been assumed. On the other hand, early high
CO
 levels could have been a fertilizing growth factor, since the trees can
live 5,000 years. After eliminating the problematic tree data and repeatedly
recalculating, the distinctive hockey stick shape flattened.

2

McKitrick commented on this circumstance regarding issues of corporate
transparency:  “The  failure  of  the  IPCC  to  carry  out  …  independent
verification  or  to  audit  studies  may  be  partly  explained  by  the  lack  of
independence  between  the  chapter  authors  and  the  original  authors.
Professor Mann was lead author of the chapter relying on his own findings,
a  lack  of  independence  that  would  never  be  tolerated  in  ordinary  public
offering of securities.”23

Russia: A New Cold War

 

A  report  titled  “How  Warming  Is  Being  Made:  The  Case  of  Russia”
alleges that the CRU and England’s Hadley Centre for Climate Change, the
UK’s  two  top  climate  research  organizations,  have  improperly  selected
Russian climate data to bolster warming claims. Issued by the Institute of
Economic Analysis (IEA), an independent Moscow-based group, the report
shows that the Russian data used for analyses came from just 25 percent of
the  country’s  meteorological  stations  and  omitted  about  40  percent  of  its
landmass.  Those  chosen  stations  tended  to  be  closer  to  large  population
centers, which tend to be warmer.

According  to  the  IEA’s  president,  Andrei  Illarionov,  “The  IEA  report
concludes that it is necessary to recalculate all global temperature data in
order to assess the real rate of temperature change during the last century.
Global temperature data will have to be modified because the calculations
used  at  Copenhagen  by  the  United  Nations  Climate  Change  Conference
analysts are based upon Hadley-CRU research.”24

McIntyre  notes  that  a  March  2004  CRU  e-mail  tends  to  confirm  an
intentional suppression of Russia’s Siberian climate records that has been
suspected  for  some  time.  A  communication  from  CRU’s  director,  Phil
Jones, to Mann states, “Recently rejected two papers (one for JGR and [one
for] for GRL) from people saying CRU has it wrong for Siberia. Went to
town  in  both  reviews,  hopefully  successfully.  If  either  appears  [in  the
journals] I will be very surprised, but you never know with GRL.”25 (JRL
refers to the Journal of Geophysical Research, and GRL to the Geophysical
Research Letters.)

In  response  to  this  charge  of  data  manipulation,  the  United  Nations’
World  Meteorological  Organization  (WMO;  aka  Met  Office)  offered  a

denial, stating in part, “These [Russian stations] are distributed around the
globe and provide a fair representation of changes in global temperatures
over land. We do not choose these stations and therefore it is impossible for
the Met Office to fix the data.” Dave Britton, spokesman for the Met Office,
stated that while it would publish station data as soon as it could, this may
take a while because that data came from climate centers in many countries,
some of which may not be willing to give up their intellectual property.26

Intellectual property? Is climate change a proprietary investment?

Confused Carbon Conceptions

 

Popular  conceptions  about  anthropogenic  CO

  contributions  to  global
warming  promulgated  by  the  IPCC  for  media  dissemination  are  very
misleading, for several reasons.

2

1.  The actual amount attributable to human sources is extremely tiny
relative  to  the  atmospheric  total,  with  incalculably  small  net
greenhouse influences.

2.    History  shows  that  those  miniscule  amounts  have  fluctuated
between  much  higher  and  lower  relative  concentrations  than  now,
long before industrial societies existed and yet when life on Earth
flourished.

3.  These fluctuations generally followed, rather than preceded, global
temperature  changes,  often  by  hundreds  and  even  thousands  of
years.

4.  No one can say just how warm is “normal,” although interglacial
periods,  such  as  our  present  one,  are  certainly  abnormally  more
wonderful than glacial periods that last about ten times longer.

5.  There is absolutely no evidence to support catastrophic scenarios
projected by those who simultaneously ignore predictable warming
benefits.

Al Gore’s description of GHGs in both his movie and his book titled An
Inconvenient Truth  refers  to  these  gasses  producing  a  “thickening”  of  the
atmosphere.  This  gives  the  impression  that  human  activities  are  really
  that  are  enveloping  the
filling  up  the  air  with  copious  quantities  of  CO

2

planet  in  a  dense,  heavy  insulating  blanket  of  massive  proportions.  A
realistic image is very different.

.  The  vast  majority—about  368—of  these  CO

For every 1 million molecules in the atmosphere, only about 380 of these
are CO
molecules are from
natural land and sea sources, while only about 12 are believed to come from
human activities, which include fossil-fuel burning and cement industries.

2

2

2

2

It is estimated that atmospheric CO

 has been growing at a rate of about
0.4  percent  per  year  since  1974.  Let’s  assume,  for  example,  that  all  this
increase  is  attributed  to  humans.  Now  imagine  that  instead  of  CO
molecules, we are thinking of a medium-size city with a population of about
a hundred thousand people. At this rate, it would require about 5 years to
add  one  new  person.  That  is  how  much  the  city’s  urban  density  would
“thicken.”

Of  all  GHGs,  water  vapor  comprises  about  70  percent  of  the  total  by
 constitutes somewhere between 4.2 and 8.4 percent. If
volume, while CO
we  assume  that  humans  are  responsible  for  about  0.12  percent  of  the
greenhouse  effect,  that  would  probably  amount  to  less  than  0.02ºF  of
warming  over  the  past  hundred  years.  That  includes  all  CO
  emission
sources.  On  a  molecular  basis,  methane,  another  GHG,  is  about  twenty-
three times more efficient at producing atmospheric warming than is CO
; it
is also accumulating in the atmosphere more rapidly.
A common misconception is that all or most CO

 emissions from human
activities  accumulate  steadily  in  the  atmosphere  with  a  proportional
greenhouse effect. Yet, on average, the surface environment absorbs about
half of those CO
 increase generally
produces half the warming effect of the preceding one, and the atmosphere
can become saturated to stop further effects.

 emissions. In addition, each unit of CO

2

2

2

2

2

2

Discerning Influences

 

The  IPCC’s  Second  Assessment  Report,  “Climate  Change  1995,”
asserts that “the balance of evidence suggests a discernible human influence
[on warming],” but it doesn’t really explain which human influence(s) are
discernible.  We  are  left  to  assume  it  is  referring  to  GHG  emissions
(primarily CO
) due to the attention directed toward them, along with some
mention  of  deforestation.  Development  of  cities,  growth  of  agriculture,

2

2

2

2

construction  of  highways,  and  other  possible  influences  are  difficult  to
model. The main suspect, of course, is CO

.

About 80 percent of the recorded atmospheric CO

 rise during the 20th
century that has been attributed to greenhouse warming occurred after  an
initial major rise in global surface temperatures. Temperature increases in
the Northern Hemisphere since the 1970s have occurred mostly during cold
seasons.  Most  of  the  current  warming  occurred  before  1940  and  declined
afterward, until the 1970s, despite a large surge of CO
 during that cooling
period. The Earth has warmed only slightly since the 1940s.

Historical surface measurements, along with “proxy” evidence obtained
from  mountain  glaciers,  tree-growth  rings,  ocean  coral  layers,  and  other
biological  indicators,  suggest  that  global  average  temperatures  had  risen
about 0.9ºF during the early part of the 20th century, before most GHGs had
been  added  to  the  air  by  human  activities.  The  temperatures  peaked  by
around 1940 and then cooled until the 1970s. Since that time, little or no
surface temperature increases have been observed.27

On  the  other  hand,  about  80  percent  of  the  atmospheric  CO

  increases
(those that might be attributed to human sources), entered the air after 1940,
a  time  when  temperatures  were  either  cooling  (prior  to  the  1970s)  or
increasing  very  little  (after  this  period).  If  this  is  accurate,  anthropogenic
contributions since the 1970s would be only about 0.18ºF.

But  what  if  atmospheric  CO

  levels  continue  to  rise,  and  perhaps  even
double the present level of about 380 ppm? Some models predict that they
might, which has led to concerns about disastrous temperature levels in the
future. Typical GCM estimates predict that such a doubling might produce
an  increase  in  the  range  of  2.7ºF  to  8.1ºF,  but  with  very  high  levels  of
uncertainty subject to large errors. These numbers are not at all consistent
with  observed  CO
  doubling  effects  from  natural  causes.  For  example,
influences  of  volcanic  eruptions  suggest  a  “sensitivity  “  (impact)  of  only
0.54ºF  to  0.9ºF  for  a  doubling,  and  a  variety  of  biological  and  other
feedback yields a sensitivity of about 0.72ºF if doubled.28

The  IPCC’s  Third  Assessment  Report,  “Climate  Change  2001,”
acknowledged  that  climate  is  naturally  variable,  and  human  influences
couldn’t be conclusively pinpointed or quantified: “The fact that the global
mean temperature has increased since the late 19th century and that other
trends have been observed does not necessarily mean that an anthropogenic

2

2

2

effect on the climate has been identified. Climate has always varied on all
timescales, so the observed change may be natural.”29

Yet the report still went on to blame people for recent warming in even
stronger terms than before: “There is new and stronger evidence that most
of the warming that occurred over the last 50 years is attributable to human
activities . . . In light of new evidence and taking into account the remaining
uncertainties, most of the observed warming over the past 50 years is likely
to have been due to the increase in greenhouse gas concentrations.”

The selection of “the last 50 years” as a benchmark is interesting because
the  IPCC  report  then  deleted  temperature  records  from  weather  balloons
(past 41 years) and satellites (past 21 years) that did not reveal substantial
warming.  In  addition,  about  half  of  that  period  (from  1950  to  1975)
recorded cooling, not warming. Records also show that areas of Greenland
have  become  colder  during  the  last  half  century,  particularly  in  the
southwestern  coastal  region,  as  have  surface  temperatures  in  the  nearby
Labrador Sea. This information is based upon studies conducted by Edward
Hanna at Britain’s University of Plymouth and John Capellan of the Danish
Meteorological  Institute,  using  data  obtained  at  eight  Greenland  weather
station sites and three stations recording sea surface temperatures.30

The  IPCC’s  Fourth  Assessment  Report,  “Climate  Change  2007,”
specifically asserts that global temperatures during the past 50 years are the
warmest  in  the  past  1,300  years  due  to  fossil  burning,  although  neither
statement  can  be  proven.  The  Medieval  Warm  Period,  which  was  well
documented in earlier IPCC summaries but disappeared in the hockey stick
graph, was warmer than the 20th century, when temperatures peaked about
1940 and changed little from the 1990s on. Again, about 80 percent of that
20th-century increase occurred before the end of that 1940 peak.

If humans are responsible for warming (or cooling) changes during the
20th century, then who or what caused similar changes before we allegedly
messed up the climate balance before that time? And why do temperatures
continue to periodically fluctuate downward at times when our greenhouse
contributions  should  theoretically  cause  steady  increases—as  the  IPCC’s
own  report  “Climate  Change  1990”  observed?  According  to  that  report,
“The  upper  troposphere  shows  there  has  been  a  rather  steady  decline  in
temperatures since the late 1950s and early 1960s in general disagreement
with  model  simulations  that  show  warming  at  those  levels  when  the
concentrations of GHGs is increased.”31

Changing Climate: From What to What?

 

Satellites  can  now  provide  new  information  about  the  dynamics  of
weather  systems,  global  atmospheric  and  surface  temperature  changes,
atmospheric  composition,  cloud  and  rainfall  patterns,  sea  and  ice  level
measurements, and other data that wasn’t available a few decades ago. One
of  the  numerous  reasons  this  is  important  is  that  to  understand  climate
change,  we  must  know  what  benchmarks  it  is  changing  from.  This  is  a
fundamental problem because before the advent of satellites (i.e., prior to
1979),  which  observe  the  entire  Earth  and  its  atmosphere,  all  we  have  to
compare changes to are surface recordings at scattered sites that used now-
antiquated instruments and methods. In addition, some later developments
at  many  of  those  sites  have  produced  localized  heat  island  pockets  of
warming  effects  that  have  been  inflated  and  projected  onto  regional  and
global scales.

We still don’t really know how cool (or warm) it was at the beginning of
the  last  century.  Early  temperatures  were  recorded  using  liquid-in-glass
thermometers that were both difficult to calibrate and read and unreliable
due to glass shrinkage. Most measurements were conducted in the Northern
Hemisphere,  typically  in  or  near  small  towns,  often  with  thermometers
placed  in  direct  sunlight  or  on  walls  of  buildings  that  blocked,  absorbed,
and/or reflected radiation. Not all were continually monitored or recorded at
the same time of day. Over time, large buildings were constructed next to
many  of  these  thermometers  and  cities  grew  up  around  them.  Weather
stations  were  developed—often  located  at  airports  where  aircraft  engines
contributed heat.32

NOAA  operates  a  surface  network  of  1,221  temperature  monitoring
stations  across  the  US.  So  we  might  assume  that  the  government  has  a
pretty  good  handle  on  temperature  changes,  right?  Maybe  not.  Anthony
Watts, a former TV meteorologist who distrusted the data, organized a large
volunteer  effort  to  visit  most  of  those  stations  and  photographically
document compliance with official installation standards. Direct surveys of
1,003  of 
that  only  11  percent  met  basic
requirements.33

those  stations  revealed 

From  early  times  until  today,  many  regions—remote  polar  regions  in
particular—have had spotty surface temperature monitoring. Measurements
of sea temperatures are recorded at floating buoys and on board cooperating

ships. The ship measurements rarely occur at the same locations and often
employ nonstandard procedures, sometimes using buckets or engine intake
water, which is obtained from levels lower than the buoys. US compilers of
global temperatures don’t recognize ship measurements as being acceptably
reliable.

How much exactly has the Earth warmed during the past hundred years
or  so?  Nobody  really  knows  for  sure  because  nobody  knows  what  the
temperature was before satellites entered the scene.

And as far as the future is concerned, scientists know virtually nothing
about  that  either.  Global  climate  forecast  models  are  really  nothing  more
than  informed,  but  highly  speculative,  guesses  produced  by  untested
methods,  which  are  easily  manipulated  to  comply  with  preconceived
expectations.  Even  in  regard  to  local  weather  predictions,  the  ability  to
really forecast beyond about 10 days is unrealistic. This is because of what
is known as a “butterfly effect,” in which very small events, analogous to
the flutter of a butterfly’s wings, mix together with compounding complex
influences  that  become  manifested  a  few  days  or  weeks  later  in
unpredictable ways.34 Climate models are no different, and they have never
demonstrated an ability to predict changes even 10 years ahead, much less
100 years or more.35

Setting the Thermostat: How Cool Do We Want It?

 

Former NASA administrator Michael Griffin got in hot water with some
“environmentalists” for remarks he made in May 2007. During an interview
broadcast on National Public Radio’s Morning Edition, he commented:

I  have  no  doubt  that  global—that  a  trend  of  global  warming
exists . . . I’m not sure it’s fair to say that it is a problem to wrestle
with … to assume that is a problem is to assume that the state of the
Earth’s climate today is the optimal climate, the best climate that we
could have or ever have had, and that we need to take steps to make
sure  it  doesn’t  change  .  .  .  First  of  all,  I  don’t  think  it’s  within  the
power of human beings to assume that the climate does not change, as
millions of years of history have shown. Second, I guess I would ask
which  human  beings—where  or  when—are  to  be  accorded  the

privilege  of  deciding  that  this  particular  climate  that  we  might  have
right here today is the best climate for all other human beings. I think
that’s a rather arrogant position for people to take.36

Mr.  Griffin’s  remarks  were  taken  by  some  to  suggest  that  NASA  was
backsliding  on  its  roles  in  collecting  and  analyzing  satellite  climate  data.
Philip Clapp, president of the National Environmental Trust, a nonpartisan
watchdog  group,  thought  that  the  comments  conflicted  with  his  own
organization’s science findings. “The science performed by NASA, as well
as scientists around the world, shows that global warming is no longer an
environmental  issue.  It’s  a  rapidly  advancing  disaster.  Millions  of  people
across the world will face hunger, flooding from a rise in sea levels, and
water scarcity. To try to hide that by saying we don’t know what the climate
should be is ignoring the science of his own agency.”37

So NASA appears to have a new mandate: Verify that there definitely is a
climate crisis based upon doomsday scenarios projected by your employee,
James  Hansen,  and  other  alarmists,  regardless  of  what  the  actual  data
shows. Forget about large temperature shifts that predated modern human
history, and assume we are responsible. After all, what else do we pay you
for?

It’s Official Now: No More Global Warming

 

The  term  “global  warming”  has  now  been  replaced  in  sophisticated
circles  with  the  more  PC,  acceptable  term  “climate  change.”  Since  the
distinctions  tend  to  be  somewhat  confusing,  please  accept  the  following
clarification as a public education service for those discriminating readers
who actually care to be up to date on such matters.

To begin this short tutorial, global warming—as commonly used by Al
Gore  and  in  the  press—is  a  euphemism  for  unnatural  (man-made)  and
dangerous heating of the Earth that human beings are causing through CO
emissions  we  release  into  the  atmosphere.  It  does  not  generally  refer  to
natural warming periods that have been interspersed with cooling and very
cold  events  throughout  billions  of  years.  It  also  doesn’t  refer  to  those
natural changes plus any additional, yet unknown, influences humans have
contributed. You probably already knew that.

2

So what exactly does “climate change” mean? Perhaps we can find out

by referring to an official definition cooked up by the UN and its IPCC.

2

Article  1  of  the  United  Nations’  Framework  Commission  on  Climate
Change (FCCC) makes it clear that the organization’s definition of the term
is about human, not natural, causes. Here, climate change is a “change of
climate which is attributed directly or indirectly to human activity that alters
the composition of the global atmosphere and which is in addition to natural
climate variability observed over comparable time periods.”

In any case, if some of you haven’t followed all this, you needn’t worry
too  much.  What  it  all  comes  down  to  is  that  humans  are  causing  global
warming,  climate  change,  and  lots  of  variability  through  our  “unnatural
acts.”

There  can  be  little  doubt  that  our  ancestors  and  we  ourselves  have
affected our planet and its ecosystems in many ways that are not entirely for
its  good.  Examples  of  those  ways  include,  but  are  not  limited  to,
deforestation,  and  soil  erosion  caused  by  improper  land  management;
atmospheric  particulates,  sulfates,  and  other  pollutants  produced  from
industry and vehicle exhaust; and the release of toxic wastes into soils and
water. There is, however, no clear scientific evidence that atmospheric CO
resulting from human activities has produced or will produce dangerously
deleterious effects upon global climate or temperatures. Meaningful impact
projections  using  current  climate  models  are  not  yet  possible.  Forecast
methods that attempt to include all suspected influences over exceedingly
short-term  (much  less  inter-annual,  decadal,  and  centennial)  timescales
produce only highly speculative guesses.38

Chapter 3

FORCING FACTORS AND FICTIONS

 

Hot
1 

2 

3 

4 

that 

the
blocks

  Active  sunspot  periods
produce  high  solar  wind
pressures.
  Solar  winds  create 
heliosphere 
cosmic rays from space.
  Some  cosmic  rays  are
blocked  by 
the  Earth’s
magnetosphere.
  Solar 
through 
producing warming.

radiation  passes
the 
atmosphere,

Not
1 

low  sunspot
  Periods  of 
activity  reduce  solar  wind
pressures.

2  More cosmic rays from deep
the

enter 

space 
magnetosphere.

3    The  magnetosphere  blocks
some,  but  not  all,  cosmic
rays.
the
  Cosmic 
atmosphere,  creating  clouds
that reflect radiation.

rays  enter 

4 

 

There is certainly nothing new about cyclical, often abrupt climate
changes,  most  particularly  those  that  occur  during  relatively  brief
interglacial periods, such as the one we currently have the great fortune to
enjoy. To appreciate just how lucky we are to live in the present, consider
climate  cycles  from  a  big-picture  historical  perspective.  Over  the  past
400,000 years, much of the Northern Hemisphere has been covered by ice,
up to 3 miles thick, at regular intervals lasting about 100,000 years each.
Much  shorter  interglacial  cycles,  like  our  current  one,  lasting  anywhere
from  12,000  to  18,000  years,  have  offered  reprieves  from  the  bitter  cold.
From  this  perspective,  there  can  be  no  question  that  current  temperatures
are indeed abnormally warm.

The average temperature of our planet has been gradually increasing on a
fairly constant basis over the past 18,000 years or so since it began thawing
out  of  the  last  Ice  Age.  By  about  12,000  to  15,000  years  ago,  Earth  had
warmed enough to halt the advance of the glaciers and cause sea levels to
rise. About 8,000 years ago, an ice bridge across the Bering Strait became
submerged, cutting off migrations of people and animals to North America.1
A short review of recent history (at least according to Earth’s large-scale
calendar)  may  provide  some  perspective.  Let’s  start  with  a  period  from
about 750 BC to 200 BC, before the founding of Rome, when temperatures
had  dropped  from  a  previously  warmer  time.  A  resulting  cooler,  drier
climate caused river and lake levels to drop in Egypt and Central Africa.
The Tiber River froze, and snow remained on the ground for long periods
almost  unimaginable  now.  European  glaciers  advanced,  and  water  that
became  trapped  in  them  and  other  ice  sheets  caused  sea  levels  to  drop
somewhat also.2

Then the climate warmed up again. Grapes were first reported in Rome
about 150 BC, and soon grapes and olives were being cultivated in large
abundance  further  north  in  Italy  than  would  have  been  possible  during
earlier  centuries.  By  about  350  AD,  the  climate  became  milder  in  the
northern regions, and tropical regions became much wetter. Heavy rains in
Africa caused high-level Nile floods, and Central America and the tropical
Yucatan experienced similar conditions. By the late 4th century, the climate
may have been warmer than it is now.

As  the  glaciers  continued  to  melt,  the  sea  may  have  risen  slightly,
possibly  3  feet  or  less,  evidenced  by  the  remains  of  ancient  harbors  in
Naples and the Adriatic that are now about that depth below water. North
Africa  (now  Tunisia,  Algeria,  and  Morocco)  was  moist  enough  to  grow
grain,  and  the  area  experienced  a  large  population  expansion.  But  those
good times were to end for a while. By around the 9th century, conditions
began to cool again, and ice formed once more on the Nile.3

But  then,  guess  what!  At  about  the  beginning  of  the  10th  century,  it
began to become warmer. A decline in high winds and fierce storms favored
more shipping, and trade fairs began to occur in about 1000 AD. The Norse
colonized Greenland and caught codfish and seals in ice-free seas.

Conditions  during  this  MWP,  also  referred  to  as  the  Medieval  Climate
Optimum, witnessed an estimated 50 percent population growth in Europe
(about 5.5 million by 1300 AD), evidence that year-round food crops were
abundant.  Mountain  passes  stayed  open  longer  during  summers,  enabling
luxury goods—such as spices from Oriental caravans, sugar from Cyprus,
and Venetian glass—to be traded for English wools and Scandinavian furs.
The warm times were good times. Thousands of temples were constructed
in  Southeast  Asia,  including  Angkor  Wat,  which  suggests  very  favorable
weather for agriculture and labor.

Consider  that  as  recently  as  1,000  years  ago  Icelandic  Vikings  on
Greenland’s  southwestern  coast  were  raising  cattle,  sheep,  and  goats  in
grasslands. Then, around 1200, temperatures in Greenland began to drop,
and the settlements were abandoned by approximately 1350. Atlantic pack
ice  began  to  grow  around  1250,  and  shortened  growing  seasons  and
unreliable weather patterns, including torrential rains in Northern Europe,
led to the Great Famine of 1315–1317.4

Beginning about 1300, weather became unstable and unpredictable, with
warm  and  dry  summers  in  some  years,  cold  and  wet  summers  in  others.
Storms and high winds increased in the North Sea and the English Channel,
making the shipping industry hazardous. Grain failures occurred throughout
Europe  in  1315,  and  catastrophic  rains  affected  huge  areas,  ranging  from
Ireland to Germany and north into Scandinavia.5

Then,  starting  around  the  year  1550,  climate  shifts  began  to  turn
increasingly dreadful, and between the years 1690 and 1700, food shortages
claimed millions of lives. Hubert Lamb, founder of the Climate Research
Unit at the University of East Anglia, described the shift this way: “In the

middle of the 16th century, remarkably sharp changes occurred. And over
the  next  150  years  or  more,  the  evidence  points  to  the  coldest  regime—
though accompanied by notably great variations from year to year and from
one group of a few years to the next—at any time since the last major Ice
Age ended 10,000 years or so ago.”6

Although temperatures have been generally mild over the past 500 years,
we  should  remember  that  significant  fluctuations  are  normal.  Remember,
the  Little  Ice  Age  brought  frigid  weather  to  the  Northern  Hemisphere
between  the  16th  and  19th  centuries,  when  Alpine  glaciers  advanced  to
gradually  engulf  farms  and  villages  by  the  mid-17th  century  (the  LIA
mentioned  in  chapter  2).  The  Thames  River  and  New  York  Harbor  froze
over  by  1780,  and  sea  ice  closed  shipping  harbors  in  Iceland,  where  an
estimated one-third of the population perished.

Some widely publicized reports ignore or minimize mention of the MWP
and  LIA,  dismissing  them  as  rather  erratic  regional,  rather  than  global,
phenomena. They are wrong to do so. The first of these was clearly global,
and the second was hemispheric, if not worldwide.7

The current CO

 increases we have witnessed most dramatically during
the first half of the 20th century have followed a 300-year warming trend,
during which surface temperatures have been recovering from chills of the
LIA.  History  shows  us  that  lagging  atmospheric  CO
  increases  can  be
expected due to natural releases of gases from oceans as the temperatures
have risen.8

2

2

The past century witnessed two distinct periods of global warming. The
first occurred between 1900 and 1945, and the second, following a slight
cool-down, began quite abruptly in 1975. That second period rose at quite a
constant  rate  until  1998,  and  then  stopped  and  began  falling  again  after
reaching a high of 1.16ºF above the average mean. Between 2001 and 2008,
the temperature anomaly declined, averaging out at 0.86ºF above the mean.
Although this represents only a short change over a long period of warming,
global anomalies had risen steadily from around 1.10ºF below the mean in
1910. While still not enough to prove a trend, it does tend to throw cold
water  on  notions  of  a  climate  catastrophe  in  progress.  About  half  of  all
estimated  warming  since  1900  occurred  before  the  mid-1940s  despite
continuously rising CO

 levels.

As  Donald  Easterbrook,  a  geology  professor  of  Western  Washington
University, has observed, “Two cycles of global warming and two cycles of

2

global cooling have occurred during the past century, and no matter what
the causes, we cannot escape the conclusion that the Earth is in for global
cooling in the next two to three decades.”9

None other than East Anglia CRU director Phil Jones has admitted that
there  has  been  no  statistically  significant  warming  trend  for  at  least  15
years. He has also admitted that temperatures during the Middle Ages may
have been higher than they are today.10

The Uncertain Nature of Climate

 

As we all must recognize, it is impossible to reliably forecast weather
events over days and weeks, much less accurately predict climate changes
and causes typically measured over multiple decades. Climate forcings are
too  poorly  understood,  the  variables  too  numerous,  and  their  interactions
too dynamic and complex, for confident modeling. But given the fact that
Earth’s  climate  has  been  fluctuating  long  before  mankind  discovered  the
benefits of harnessing fire, it is logical to assume that many influences—
and  clearly 
the  most  dominant  ones—involve  naturally  occurring,
sometimes cyclical, events.11

Key among these natural climate forcings are believed to be changes in
the  Earth’s  orbital  eccentricity  around  the  Sun,  along  with  its  slow  axial
“wobble” over many thousands of years, called Milankovitch cycles. These
conditions influence the amount of sunlight received on the surface of the
Earth,  and  they  seem  to  correspond  with  glacial  and  interglacial  cycles.
Short  fluctuations  within  interglacial  periods  appear  to  be  linked  to  other
influences.  They  include  periodic,  cyclical  variations  in  solar  outputs;
natural  changes  in  ocean  currents;  seasonal  effects  of  cloud  cover,
precipitation,  and  vegetation  growth;  and  occasional  volcanic  eruptions
producing warming GHGs, along with dust and aerosols that block sunlight
to cause cooling.

Variations  in  Earth’s  orbital  path  around  the  Sun  occur  in  21,000-year
cycles as gravitational influences of other planets, the Moon, and the Sun
pull Earth closer or farther away from the Sun. Variations in the direction of
Earth’s axis (the wobble) occur in roughly 26,000-year cycles. Variations in
Earth’s  axial  tilt  between  22.1  degrees  and  24.5  degrees  (currently  23.44
degrees) occur in 41,000-year cycles.

Earth’s orbital path and axial tilt cycles affect climate patterns in the two
hemispheres  significantly.  Current  Southern  Hemisphere  summers  and
winters  are  more  extreme  than  in  the  Northern  Hemisphere  due  to  a  6.8
percent  differential  solar  exposure.  Also,  the  Earth  is  closest  to  the  Sun
during  southern  summer  and  farthest  away  during  southern  winter.
Gradually,  Northern  Hemisphere  winters  will  become  slightly  warmer  as
Earth becomes closer to the Sun during northern winter—southern summer.
About  7,000  years  ago,  when  Earth’s  axis  was  24.14  degrees,  the  Sahara
Desert was a very different place, with lakes, rivers, grasslands, and patches
of jungle. Then, over a 3,400-year period, it became gradually transformed
into what we find today. Better times will eventually return, since massive
amounts of water remain trapped underground.12

Many  scientists  believe  that  Pacific  Decadal  Oscillation  (PDO)  and
Atlantic Multidecadal Oscillation (AMO) ocean cycles associated with El
Niño and La Niña conditions, in combination with solar activity variances,
have had important climate influences during the past century. These factors
may account for much of the observed warming trends of the period from
1910 through the 1930s, cooling from the 1940s to the 1970s, and warming
during the decade between 1980 and 1990. Solar activity cycles of about 11
years and 200 years may modulate the effects of galactic cosmic ray (GCR)
magnetic fields, producing changes in cloud cover with both warming and
cooling results.

Much  scientific  attention  is  recently  being  directed  to  correlations
between periodic changes in sunspot activity and climate change. Reasons
for periodic changes that occur in the Sun’s behavior, when they will occur,
and  their  influences  upon  Earth’s  climate  are  not  well  understood,  and
current climate models cannot predict these factors. However, scientists are
beginning to discern certain trends.

As first recognized in the 1800s, solar activity typically runs in roughly
11-year cycles, often varying between 9 and 14 years long. But sometimes
periods of very low sunspot activity can stretch out for decades. During the
17th  century,  for  instance,  a  70-year-long  period  of  little  or  no  activity,
known as the Maunder Minimum, corresponded closely with the LIA that
extended  into  the  19th  century.  George  Washington’s  famous  winter  at
Valley Forge was associated with this LIA, as was Napoleon’s bitter retreat
from Moscow.13

Sunspot activity correlates with the strength of the solar wind, a plasma
stream of charged particles from the Sun’s upper atmosphere that interacts
with all planets in our Solar System and defines the envelope border with
interstellar space, called the “heliosphere.” This border is a location where
the  wind’s  strength  becomes  insufficient  to  push  back  the  wind  of  other
stars—the boundary of the “heliopause,” a shield of intersteller material that
wards  off  a  significant  portion  of  cosmic  rays  originating  from  the
surrounding  galaxy.  As  our  Sun’s  solar  wind  activity  decreases,  the
heliosphere diminishes in strength, allowing more galactic cosmic rays to
enter  the  inner  part  of  our  Solar  System.  Many  scientists  believe  those
penetrating cosmic rays cause atmospheric water vapor molecules to cluster
into droplets, forming low-level clouds that produce cooling when sunspot
activity is low.

Times  of  high  solar  activity  appear  to  correspond  with  eight  warming
periods noted during the past 12,000 years, including the Medieval Warm
Period and much of the recently past 20th century. It appears that the Sun’s
mood swings may be very consequential.14

Although the IPCC’s 2007 AR4 report claimed that solar influence upon
climate  is  “negligible,”  conclusions  of  a  scientific  study  published  in  the
journal Physics Today strongly take issue with that assessment. The authors,
Nicola Scafetta and Bruce West, reviewed recorded data that the IPCC and
many other climatologists had ignored as just “background noise,” and they
found  that  the  “noise”  perfectly  matched  solar  activity  over  at  least  4
centuries.  They  also  found  that  the  computer  modeling  by  the  IPCC  and
others was not producing accurate information because key solar flare and
other activity data had not been entered. The study concluded the following:
“If  climate  is  sensitive  to  solar  changes  as  the  …  findings  suggest,  the
current  anthropogenic  contribution  to  global  warming  is  significantly
overestimated. We estimate that the Sun could account for as much as 69
percent of the increase in Earth’s average temperature. Furthermore, if the
Sun does cool off, as some solar forecasts predict will happen over the next
few  decades,  that  cooling  could  stabilize  Earth’s  climate  and  avoid
catastrophic consequences predicted in the IPCC report.”15

The  study  of  the  Sun’s  activity  leads  some  to  anticipate  colder,  not
warmer,  temperatures  in  the  future.  Richard  Mewald  of  Caltech  observes
that  “In  2009,  cosmic  ray  intensities  have  increased  19  percent  beyond
anything that we’ve seen in the past 50 years.”16

Based  upon  current  solar  data,  the  Russian  Pulkovo  Observatory  space
research  laboratory  concludes  that  Earth  has  passed  its  latest  warming
cycle, and staff there predict that a fairly cold period will set in by 2012.
Temperatures may drop much lower by 2041 and remain very cold for 50 to
60 years, or longer.17

Kenneth Tapping at Canada’s National Research Council thinks we may
be  in  for  an  even  longer  cold  spell.  He  predicts  that  the  Sun’s  unusually
quiet current 11-year cycle might signal the beginning of a new Maunder
Minimum cold period, which occurs every couple of centuries and can last
a century or more.18

Solar activity peaked at the end of the 1990s, broke with a brief blip in
2002,  and  then  slumped  to  almost  none.  This  coincides  with  observed
cooling,  since  1999,  which  may  well  be  continuing.  There  were  only  six
sunspots during the entire year of 2008, the lowest number in 95 years. Yet
as recently as 2006, NASA predicted that the upcoming solar cycle would
be “a biggie.”19

Scientists agree that the Sun’s output is not constant, although it would
have been considered heresy a couple of centuries ago to suggest this. The
Sun has actually brightened about 30 percent during Earth’s history, while
interestingly  and  curiously  enough,  the  Earth’s  average  temperature  has
remained relatively constant.20 The Sun may have been about 0.25 percent
dimmer  during  the  Maunder  Minimum  and  other  similar  low-activity
periods. A solar radiative energy reduction of about 0.1 percent measured
against  an  average  “solar  constant,”  or  watts  of  solar  energy  per  square
meter as a baseline measurement (about 1,367 W/m2 at Earth’s surface), has
been observed to occur during fairly regular 11-year solar cycles.21

Solar minimum periods recorded over several decades since at least 1500
AD show positive correlations between times of greatest solar intensity and
sunspot  activity.  In  fact,  these  connections  appear  to  be  strong  enough  to
enable  sunspot  frequencies  to  be  used  as  a  proxy  for  levels  of  solar
brightness (or “irradiance”). Variations recorded by satellite measurements
also  demonstrate  correlations  between  the  solar  cycles  and  weather
influences. These observations contradict current climate models that have
ignored  solar  influences  thought  to  be  too  small  to  account  for  climate
changes.

Some  recent  data  indicates  that  variations  in  solar  irradiance,  spectral
irradiance  in  particular,  may  be  much  more  important  than  previously

assumed. When focusing attention upon those short radiation wavelengths
in  the  ultraviolet  (UV)  and  extreme  ultraviolet  (EUV)  thermal  bands,  the
levels vary during 11-year cycles by more than ten times.22

Satellite  data  analyses  conducted  by  Robert  Lee  at  NASA’s  Langley
Research  Center  have  compared  thermal  radiation  and  solar-irradiance
measurements from 1979 to 1989, which captured part of an 11-year solar
cycle. During that period a 0.54ºF–1.08ºF cooling was recorded from 1979
to 1985, followed by a 0.36ºF–0.54ºF warming to 1989. The observations
concluded  that  if  solar  forcing  was  the  only  cause  of  these  temperature
variations,  the  effects  are  five  times  greater  than  climate  models  predict.
However,  that  larger  response  may  have  been  influenced  by  a  1982  El
Chichón volcano eruption, which could have produced some cooling effects
in 1983 and 1984.23

Ongoing  studies,  such  as  the  European  Space  Agency’s  Influence  of
Solar Activity Cycles on Earth’s Climate project, are attempting to gain a
better  understanding  of  the  complex  nature  of  Sun-Earth  weather  and
climate  relationships.  Yet  it  may  be  a  long  time  before  enough  will  be
known to provide a serious basis for 21st-century climate forecasts. And as
wise modelers recognize, a bad forecast is worse than no forecast at all.24

The Infamous Greenhouse Effect

 

GHGs have been getting a very bad rap in the media to the point that
they are commonly accepted by many as something to avoid at all costs. Yet
without them, it is estimated that our planet would be much colder, and life
as we know it would never have existed.25

We can get a good sense of the greenhouse effect when we experience a
dry, chilly desert at night in comparison with a humid, tropical area. Absent
a  desert  cloud  cover,  the  heat  absorbed  by  the  desert  surface  is  radiated
directly back to space in the form of infrared energy. In tropical locations,
water  molecules  in  the  air  overhead  intercept  ground  heat  and  radiate  it
back toward the surface.26

At the present time, scientists don’t really have a clear idea of how even
the  most  prevalent  GHG,  water  vapor,  affects  climate  change.  They  do
know, however, that water in the form of clouds has extremely important
influences  upon  ways  greenhouse  mechanisms  work,
and  complex 

producing both warming and cooling effects. While the ones we see from
below  tend  to  look  gray,  they  would  appear  white  from  above  if  viewed
from an airplane. Upper parts scatter about half of the incoming sunshine,
which would otherwise warm the surface, back into space. Clouds absorb
some of that radiation as well, but since the cloud tops are cooler than the
ground,  the  overall  heat  loss  is  typically  less.  This  creates  a  complicated
exchange budget between the incoming sunlight in a visible spectrum and
outgoing,  invisible  infrared  thermal  energy.  In  general,  though,  when
everything is taken together, clouds—particularly the low ones—tend to be
net  coolers.  Thin  ones,  however,  have  an  overall  warming  effect.  This
presents large accounting problems for climate modelers, particularly since
they don’t know entirely how clouds form, what types will be created, or
how expansive their cover will be at any given place or time.27

The vast majority of all greenhouse warming effects is caused by water
vapor in the air (considered a gas) and water droplets in clouds, with minor
contributions from CO
 and methane. It is estimated that atmospheric water
vapor  may  account  for  about  70  percent  of  this  effect,  compared  with
somewhere between 4.2 and 8.4 percent for CO
, absorbing solar infrared
over much of the same wavelength band range as CO
 and even more. The
thermal  radiation  emitted  from  the  Earth’s  surface  is  mostly  in  the  7–30
micrometer wavelength range, while the radiation that most readily escapes
back to space with least atmospheric absorption is between about 7 and 14
micrometers,  accounting  for  about  70  percent  of  that  which  is  lost  Water
vapor strongly absorbs in a range of about 4 to 17 micrometers, and CO
most strongly between about 13 and 19 micrometers.28

2

Since warming influences of GHGs have not proven to be as strong as
theoretical  climate  models  have  predicted,  some  scientists  attribute  the
discrepancy  to  underestimated  levels  of  anthropogenic  aerosols.  Aerosols
include atmospheric dust particles and sulfate droplets released from such
human  activities  as  fossil  burning,  forest  clearing,  and  agriculture,  in
combination  with  natural  sources,  such  as  volcanos,  sea  spray,  and  land
wind  erosion.  These  elements  block  some  incoming  sunlight  that  would
otherwise  produce  lower-level  atmosphere  and  surface  warming.  Unlike
CO
, however, which becomes more globally distributed, aerosols tend not
to travel nearly as far from their sources or to remain in the atmosphere as
long.

2

2

2

2

The  climate-forcing  influences  of  aerosols  are  highly  uncertain,  partly
because they rapidly fall out or are washed out of the atmosphere by rainfall
in  days  or  weeks.  This  does  not  allow  them  enough  time  to  be  mixed
uniformly around the globe. For sulfate aerosols it is also very difficult to
distinguish  droplets  from  industrial  versus  biogenic  sources,  because  key
modeling parameters are not well understood.29

Unlike  CO

,  which  is  sometimes  associated  with  unwelcome  warming
but is popular with herbaceous plants, sulfate droplets cause cooling but are
unwelcome in general. They are responsible for urban smog that contributes
to respiratory health problems for people; in the 1970s and 1980s the acid
rain  that  was  believed  to  damage  forests  and  cause  acidification  of  lakes
was  attributed  to  this  smog.  Subsequently,  a  large,  10-year-long  study
concluded that the acid rain alarm had been exaggerated.

2

 

2

Greenhouse Bounties: Why Warm Is Cool

2

Global warming hysteria has gotten many people so heated up that they
overlook  the  consequences  of  the  opposite  condition.  And  assuming  that
small  CO
  increases  may  come  with  the  deal,  as  they  have  in  the  past,
what’s so bad about that? You might even consider the possibility that when
Earth finishes warming its way out of the last cold period, the climate might
slide down the temperature graph into another one, and it might possibly be
a doozy. According to glacial and interglacial cycles over the past 400,000
years or so, be afraid … be very afraid (just kidding). And if anthropogenic
CO
  can  really  make  a  difference  to  help  forestall  or  prevent  that,  let’s
maybe reconsider our interests.30

The  United  Nations’  World  Meteorological  Organization  released  a
December  2009  report  claiming  that  the  10-year  period  from  2000–2009
was  the  warmest  since  records  began  in  1850.  This  assertion  is  rather
comical in light of the fact that no one really knows with any real accuracy
or  certainty  what  the  Earth’s  temperatures  were  even  a  few  decades  ago.
While  the  WMO  says  the  data  is  culled  “from  networks  of  land-based
weather and climate stations, ships and buoys, as well as satellites,” it might
be  reasonable  to  ask  just  how  many  satellites,  ships,  and  buoys  were
measuring  temperatures  in  1850.  And  how  many  ground  stations  and
records have existed or can be trusted? Those that currently exist are highly

concentrated in the US and Europe, while few are located in Asia, Africa,
and South America.31

Distributions and placements of the stations is another matter. Since these
stations were first established 150 years ago, many have seen major urban
development in the areas where they are located, and that has created heat
islands.  Are  these  the  same  stations  used  since  1850?  Have  some  been
added  or  dropped?  Have  record-keeping  procedures  and  equipment
standards remained constant? And how can old data be combined with later,
more  accurate  records,  particularly  when  global  satellite  instruments  that
offer  fuller,  more  accurate  readings  were  not  available  during  about  80
percent of the period referenced?

Claims that the Earth is now warmer than at any other time in the past
1,000 years are readily disputable. A National Academy of Sciences review
panel addressed this issue in 2006. It concluded that all that we can really
be certain about is that the Earth was then warmer than it had been over the
last 400 years. Considering that the LIA accounted for 250 of those years,
that shouldn’t be unduly alarming. History has demonstrated that a return to
prolonged cooling would be a much more legitimate worry.

day.

Yes, global warming—and cooling—is real, and is still going on to this

Now let’s just imagine that our currently observed cooling trend is brief,
and consider the alternative the next time someone nervously asks if you
“believe” in global warming. As a student of history, your answer might be
strongly affirmative and go something like this.

Yes,  I  really  believe  in  global  warming.  Evidence  suggests  that
global warming is critical to keep people comfortable during day and
night,  preventing  millions  from  freezing  to  death  and  starving.  It
provides long growing seasons and excellent plant conditions on large
expanses of unfrozen land that allows essential food to be grown for
8–9  billion  people  around  the  world.  It  enables  many  domestic  and
wildlife species to thrive that couldn’t otherwise survive. And if this
includes  an  upward  temperature  shift  that  causes  some  species  from
the  tropics,  where  the  greatest  diversity  exists,  to  extend  to  higher
latitudes, that isn’t necessarily a bad thing at all.

But that may not be what some of those people want to hear. You might

not be invited to their cocktail parties again.

Section Two

Political Hijackers of Science

Chapter 4

FEVERISH CLIMATE CLAIMS

And  then  maurading  bands  of  mosquitos  will  migrate  to  spread

malaria and other plagues to northern lattitudes.

Promoting  global  warming  alarmism  has  become  an  effective
manipulating  tactic  to  advance  a  variety  of  special-interest  agendas  that
often  have  little  to  do  with  the  environmental  goals  and  social  benefits
espoused  by  responsible  individuals  and  organizations.  In  2006,  for
example, the Institute for Public Policy Research (IPPR), a think tank that
actually  supports  CO
  cuts,  provided  an  analysis  of  the  circumstances
surrounding  global  warming  debates  that  were  occurring  in  the  UK:
“Climate  change  is  most  commonly  constructed  through  the  alarmist
repertoire as awesome, terrible, immense, and beyond human control . . . It
is typified by an inflated or extreme lexicon, incorporating an urgent tone
and  cinematic  codes.  It  employs  [a]  quasi-religious  register  of  death  and
doom, and it uses language of acceleration and irreversibility.”1

2

The  IPPR  concluded  that  “alarmism  might  even  become  secretly

thrilling”— effectively a form of what they referred to as “climate porn.”

Mike  Hume,  director  of  the  UK’s  Tyndall  Centre  for  Climate  Change
Research and one of his country’s top climate scientists, spoke out in late
2006 against mounting alarmism. He recognized that climate change is real
and  that  humans  contribute  to  it,  but  he  took  issue  with  such  words  as
“catastrophic”  and  such  claims  as  “climate  change  is  worse  than  we
thought,”  “[we  are  approaching  an]  irreversible  tipping  in  the  Earth’s
climate,” and “[we are] at the point of no return.” He noted that such ideas
are planted as “unguided weapons with which forlornly to threaten society
into behavioral change.”

Recognizing  that  such  language  helps  to  advance  climate  science
funding,  he  concluded,  “We  need  to  take  a  deep  breath  and  pause.  The
language of catastrophe is not the language of science . . . Framing climate
change as an issue which evokes fear and personal stress becomes a self-
fulfilling prophecy. By ‘sexing it up’ we exacerbate, through psychological
amplifiers, the very risks we are trying to ward off.”2

Pounding Our Hot Buttons

 

During the steamy summer of 1988, Senator Al Gore’s Committee on
Science,  Technology  and  Space  hearings  succeeded  in  putting  man-made
global warming at center stage in the national political arena. (Remember,
however,  that  barely  a  decade  earlier  an  alarm,  based  on  the  observed
cooling  trend  during  the  1960s  and  late  1970s,  signaled  the  coming  of  a
new  Ice  Age.)  James  Hansen’s  rather  mild  affirmation  of  that  possibility
was  presented  as  persuasive  and  confident  proof.  This  certainty  was
reinforced  in  a  book  titled  World  on  Fire:  Saving  an  Endangered  Earth,
written  by  Democrat  and  Senate  Majority  Leader  George  Mitchell  and
published 
the  father  of  a  prominent
environmental  activist,  urged  recognition  of  greenhouse  warming  as  a
global  threat.  By  that  time,  the  popular  media  in  Europe  and  the  United
States  had  reached  a  common  conclusion:  They  were  declaring  that  “all
scientists” agreed that warming was real and had catastrophic potential, and
that rising atmospheric CO
 levels released by burning fossils must be the
cause.3

in  1991.  Senator  Mitchell, 

2

Since  the  late  1980s  and  early  1990s,  we  have  been  subjected  to  a
barrage of global warming crisis alerts. Many such alerts find their origin in
doomsday  predictions  repeatedly  rendered  by  alarmist  James  Hansen  and
visually dramatized in Mr. Gore’s print and film productions. These media
present  powerful  statements  and  graphic  images  that  leave  lasting
impressions  of  global  warming  devastation,  present  and  future,  which  is
represented to be based upon science. It’s awesome to imagine a Manhattan
under water, heartbreaking to see polar bears and penguins become extinct,
and terrifying to think of horrible diseases that are spreading—all because
of us.4

Others  have  gotten  into  the  act,  targeting  impressionable  young  minds
and sensitive big hearts with messages of fear and guilt. An example is the
children’s  book  The  North  Pole  Was  Here,  authored  by  New  York  Times
reporter Andrew Revkin. It warns children that some day it may be “easier
to sail than stand on the North Pole in summer.”5 Of course, it’s mostly their
parents’  fault  because  of  the  nasty  CO
  they  produce  driving  the  kids  to
school.

Many  leading  scientists  (who  don’t  work  for  oil  companies)  strongly
disagree  with  these  prognoses  and  the  alleged  science  behind  them.  Let’s
consider some of their reasons.

2

Lower Tides of Despair

 

Although  the  UN  IPCC  is  one  of  the  biggest  purveyors  of  climate
change hysteria, the predictions in its own 2007 AR4 report “Summary for
Policymakers” stating that sea levels will “probably” rise between 7.08 and
23.22  inches  during  the  21st  century  may  at  least  relieve  some  angst
regarding the mother of all scares. After all, that’s about twenty times less
than the 20- to 40-foot levels envisioned in the Academy Award–winning
documentary An Inconvenient Truth.

The  IPCC  AR4  report  confirmed  that  unlike  the  spectacular  scenario
depicted  in  the  film,  Antarctica’s  ice  sheets  will  “remain  too  cold  for
surface melting,” and the continent is “expected to gain [water] mass due to
increased  snowfall.”  It  also  states  that  no  scientific  consensus  exists  that
Greenland’s  ice  caps  are  melting  enough  to  contribute  to  increased  sea
levels.  While  it  acknowledges  unknowns,  including  some  observed

variability and local changes in glaciers that could contribute to increased
sea  levels,  it  concludes  that  overall,  “there  is  no  consensus  on  their
magnitude.”

And  what  about  the  AR4’s  probable  projection  of  the  oceans  rising
anywhere from 7.08 inches to 23.22 inches in the 21st century? A study in
the July 26, 2009, issue of Nature Geoscience, a top journal in the field, had
concluded  that  the  investigation  “strengthens  confidence  with  which  one
may interpret the IPCC results.” The authors later retracted the report, titled
“Constraints in Future Sea-Level Rise from Post Sea-Level Changes,” after
two  other  scientists  pointed  out  technical  research  errors.  One  involved  a
miscalculation, and the other cited incomplete information about ice sheet
melting,  which  caused  the  conclusions  to  be  highly  uncertain.6  The
researchers had used fossil coral data and temperature records derived from
ice core measurements to reconstruct how the sea level has fluctuated with
temperature since the peak of the last Ice Age and to project how it would
rise with predicted warming over the next few decades.

According to a recent study conducted by US and Dutch scientists that
appeared in the journal Nature Geoscience, previous estimates of ice melt
rate losses in Greenland and West Antarctica may have been exaggerated as
double the actual rate. The earlier projections apparently failed to account
for  rebounding  changes  in  the  Earth’s  crust  following  the  last  Ice  Age,
referred 
isostatic  adjustment.”  Lead  researcher  Bert
Vermeersen of Delft Technical University in the Netherlands described the
phenomena  as  being  similar  to  the  way  a  mattress  compressed  by  the
weight of a sleeper recovers its shape when the person gets up. The total
revised annual contributions of the combined ice melts to ocean rise would
amount to only about 1.5 mm (0.1 inch), similar to a 1.8 mm (0.07 inch)
annual rise in the early 1960s.7

to  as  “glacial 

Hendrik Tennekes, former director of research at the Netherlands Royal
National Meteorological Institute, believes that the sea level has flattened
since  2006.  He  also  disputes  claims  that  there  has  been  any  statistically
significant warming of upper ocean surfaces since 2003, pointing out that
Arctic Sea anomalies have actually decreased.8

Dr.  Tennekes  places  much  of  the  blame  for  poor  sea  level  forecasting
upon  a  failure  to  understand  and  include  influences  of  natural  events  in
models. He reported to the well-known climate science blog Climate Depot:
“From  my  perspective  it  is  not  a  little  bit  alarming  that  the  current

generation of climate models cannot simulate such fundamental phenomena
as the Pacific Decadal Oscillation. I will not trust any climate model until it
can  accurately  represent  the  PDO  and  other  slow  features  of  the  world
ocean circulation. Even then I would remain skeptical about the potential
predictive skill of such a model many tens of years into the future.”9

Nils-Axel  Mörner  is  head  of  the  Paleogeophysics  and  Geodynamics
department  at  Stockholm  University  in  Sweden;  past  president  of  the
INQUA Commission on Sea Level Changes and Coastal Evolution; leader
of the Maldives Sea Level Project; and one of the UN’s “expert reviewers”
of the IPCC’s 2001 and 2007 reports. He agrees that concerns about rising
sea  levels  are  totally  unfounded.  His  research  in  this  area  has  taken  him
around  the  world,  from  Greenland  to  Antarctica  and  to  most  coastal
regions.10

Dr. Mörner observes that of the twenty-two IPCC authors, none was a sea
level specialist. He later said, “So all this talk that sea level is rising, this
comes  from  the  computer  modeling,  not  from  observations  .  .  .  The  new
level, which has been stable, has not changed in the last 35 years . . . But
they  [IPCC]  need  a  rise,  because  if  there  is  no  rise,  there  is  no  death
threat  .  .  .  If  you  want  a  grant  for  a  research  project  in  climatology,  it  is
written  into  the  document  that  there  ‘must’  be  a  focus  on  global
warming . . . That is really bad, because then you start asking for the answer
you want to get.”11

According to studies by the INQUA commission, ocean levels have even
fallen  in  recent  decades.  The  Indian  Ocean,  for  example,  was  higher
between  1900  and  1970  than  it  has  been  since.12  The  real  sea  change,  it
appears, has been in the way climatologists have predicted sea levels.

Science on Thin Ice

 

Much of the specter of global warming alarm centers upon Greenland
and  upon  concerns  that  glaciers  will  cause  disastrous  sea  level  rise.  A
December 2005 BBC feature reported that two massive glaciers in eastern
Greenland,  Kangderlugssuaq  and  Helheim,  were  melting,  with  water
“racing to the sea.” It was predicted that continued recession of more than 2
miles  per  year  would  be  catastrophic.  That  prognosis  proved  premature,
however. Only 18 months later, and despite slightly warmer temperatures,

the melting rate of both glaciers not only slowed down and stopped but also
had  actually  reversed,  and  the  glaciers  began  expanding  in  size.  Landsat
images revealed that by August 30, 2006, Helheim had advanced beyond its
1933 boundary.13

Even  though  Greenland  has  been  experiencing  a  slight  warming  trend,
satellite measurements show that the ice cap is accumulating snow growth
at  a  rate  of  about  2.1  inches  per  year.  Also  consider  that  Greenland’s
temperatures  over  the  past  decade  were  no  warmer  than  several  others
recorded during the 20th century; they only recently began to exceed those
of  the  1930s,  and  the  1980s  and  1990s  were  colder  than  the  previous  6
decades.  Temperatures  in  the  late  12th  century  in  Greenland  were
considerably warmer than they have been in recent years.14

Then,  on  March  31,  2010,  scientists  were  once  again  surprised.
According to the National Snow and Ice Data Center, Arctic sea ice reached
the greatest expanse for that late date recorded since 1979, when satellite
records commenced. The growth was largely attributed to cold weather and
winds from the north over the Bering and Barents seas. So, what does this
prove?  Maybe  very  little—just  about  as  little  as  you  probably  heard  it
reported in the news.

As reported by the Greenland glacier study lead author Ian Howat, in a
February  8,  2007,  New  York  Times  interview:  “Greenland  was  about  as
warm or warmer in the 1930s and 1940s, and many glaciers were smaller
than they are now . . . Of course, we didn’t know very much about how the
glacier dynamics changed then, because we didn’t have satellites to observe
it.  However,  it  does  suggest  that  large  variations  in  sheet  dynamics  can
occur from natural variability.” Further on in the interview he concluded,
“Special care must be taken in how these and other mass-loss estimates are
evaluated,  particularly  when  extrapolating  into  the  future,  because  short-
term spikes could yield erroneous long-term trends.”15

The International Arctic Research Center reported a 29 percent expansion
of  Arctic  sea  ice  in  2008  over  2007’s  total.  Alaska  also  experienced  an
unusually large amount of winter ice and snow during 2007–2008, followed
by extremely cold temperatures in June, July, and August. “In June, I was
surprised to see snow still at sea level in Prince William Sound,” reported
US Geological Survey (USGS) glaciologist Bruce Molnia. “On the Juneau
Icefield,  there  was  still  20  feet  of  new  snow  on  the  surface  of  the  Taku
Glacier in late July.”16

Snow Jobs

 

The IPCC has recently admitted that the assertion in its 2007 report that
the Himalayan glaciers would likely melt by 2035 due to man-made global
warming is false. That assertion had prompted great alarm across southern
and eastern Asia, where glaciers feed the major rivers. Even though many
glacier  experts  had  considered  such  a  prediction  to  be  preposterous,  the
IPCC had kept it in its report. As it turned out, the prediction was traced to
a  speculative  magazine  article  authored  by  an  Indian  glaciologist,  Syed
Hasnain, which had no supporting science behind it Mr. Hasnain works for
a research company headed by IPCC’s chairman, Rajendra Pachauri. IPCC
author Marari Lai admitted to the London Daily Mail, “We thought that if
we  can  highlight  it,  it  will  impact  policymakers  and  politicians  and
encourage them to take concrete action.”17

Other  world  climate  alarm  bells  chimed  when  it  was  reported  in  the
media that September 2007 satellite images revealed the Northwest Passage
—a  sea  route  between  the  UK  and  Asia  across  the  top  of  the  Canadian
Arctic Circle—had opened for the first time in recorded history.18 First, it
should  be  pointed  out  that  recorded  history  in  this  regard  began  only  as
recently as 1979, when satellite monitoring began. It should also be noted
that the route froze again just a few months later (winter 2007–2008). In
fact,  the  average  Arctic  sea  ice  extent  for  the  month  of  December  2008
(4.84 million square miles) was actually 54,000 square miles greater than in
December 2007; worldwide at that time, the average sea ice coverage was
about the same as it had been in 1979.19

There is clear evidence that the Northwest Passage in reality has opened
on previous occasions. The sea ice had been thinning ever since the end of
the  LIA  (before  the  Industrial  Revolution),  and  it  had  already  warmed
enough so that Eskimos first began fishing there for newly migrating cod in
the 1920s.

In  diary  entries  he  wrote  in  1903,  sailor  Roald  Amundsen  reported  his
experience on board a ship in those waters: “The Northwest Passage was
done  [had  opened].  My  boyhood  dream—at 
it  was
accomplished. A strange feeling welled up in my throat; I was somewhat
over-strained  and  worn—it  was  weakness  in  me—but  I  felt  tears  in  my
eyes. Vessel in sight . . . Vessel in sight.”20

the  moment 

During  the  early  1940s  a  Royal  Canadian  Mounted  Police  (RCMP)
schooner assigned to Arctic patrol made regular trips through the Northwest
Passage.21 And in 2000 (that is to say, 7 years before the first-ever satellite
records),  another  RCMP  patrol  vessel  was  renamed  the  St.  Roch  II  and
recreated the voyage, making the crossing in only three weeks. The crew
reported  seeing  very  little  ice  except  for  the  occasional  icebergs  they
passed.22

it  was  discovered 

In  February  2009, 

that  scientists  have  been
underestimating the regrowth of Arctic sea ice by an area larger than the
state of California (twice as large as New Zealand). The errors are attributed
to faulty sensors on the ice.23 And although the Arctic ice expanse was still
slightly smaller in 2008 as compared with 1979, the Antarctic expanse was
larger. The University of Illinois Arctic Climate Research Center posted an
analysis in January 2009 concluding that global sea ice coverage in 2008
was nearly the same as satellites revealed in 1979.24

Research  conducted  by  the  Scripps  Institution  of  Oceanography  at  the
University of California–San Diego has turned up evidence that polar ice
caps at least half as large as those we see now existed when Earth was at its
warmest (72ºF to 77ºF), about 91 million years ago. That conclusion was
based upon a study of tiny marine fossils, collected from the ocean floor,
that contained a particular telltale isotope of oxygen molecules (d180). The
study’s  coauthor,  Richard  Norris,  observed:  “Until  now  it  was  formerly
believed  no  glaciers  existed  on  the  poles  prior  to  the  development  of  the
Antarctic ice sheet about 33 million years ago . . . This study demonstrates
that  even  the  superwarm  climates  of  the  Cretaceous  Thermal  Maximum
were not enough to prevent ice growth.”25

To  further  support  their  conclusions,  the  research  team  pointed  to
evidence  that  sea  levels  fell  between  82  and  131  feet  during  the  period
examined.  This  could  be  expected  when  that  much  water  became
landlocked in massive ice volumes.

As broadly advertised, it’s true that famous glaciers at the peak of Mt.
Kilimanjaro are indeed receding. Actually, they have been doing so since
1890, according to research by Kaser et al., published in the International
Journal of Climatology  (2004).  By  1936,  when  Ernest  Hemingway’s  The
Snows  of  Kilimanjaro  was  released,  the  mountain  had  already  lost  more
than  half  of  its  surface  ice  area  over  a  period  of  56  years.  According  to
another report, published in Geophysical Research Letters (2006) by N. J.

Kullen et al., this is being caused by a shift toward drier conditions, not by
weather temperatures, that began around 1800.26

Global Warming “Spokesbears”

 

Are polar bears becoming global warming victims? Mr. Gore says they
are, and that opinion, along with grief-evoking images, has been expressed
by  leading  media  programs  and  commentators.  Even  the  DOI  seems  to
believe  this;  it  has  recently  added  polar  bears  to  the  Endangered  Species
Act (ESA) listing. It must be true, right?

A  January  20,  2008,  global  warming  special  hosted  by  Scott  Pelley
reported  that  polar  bears  “may  be  headed  toward  extinction,”  noting  that
researchers  are  finding  them  to  be  thinner  and  weaker,  with  less  time  to
stock up on fat reserves because ice sheets are melting too fast. Mr. Pelley
has strong convictions about global warming. He is the same reporter who
once compared global warming skeptics to Holocaust deniers. Nick Lunn,
the researcher featured in that special broadcast, somberly observed that the
polar bear population in the Western Hudson Bay has declined during  the
last  decade  from  about  twelve  hundred  in  the  mid-1990s  to  about  a
thousand  now.  What  wasn’t  mentioned  is  that  the  total  population,
estimated  to  be  about  five  thousand  in  the  1970s,  has  increased  to  about
twenty-five thousand today. And though it is true that the Western Hudson
Bay population has been seeing some decline, other groups are stable and
even increasing in number.

ABC’s  Sam  Champion  told  Good  Morning  America  audiences  on
February  8,  2008,  that  a  2-degree  increase  in  global  temperatures  would
make  “polar  bears  struggle  to  survive.”  On  November  6,  2007,  NBC’s
Today Show cohost, Matt Lauer, said the bears “are facing an epic struggle
for survival.” Reporter Kerry Sanders warned, “If the Arctic ice continues
to melt in the next 100 years, the US Wildlife Service says ‘the only place
you’ll find a polar bear on Earth will be at the zoo.’” On a September 9,
2007, Good Morning America broadcast, Kate Snow called polar bears “the
newest victims of global warming.” The same segment featured Dr. Steven
Amstrup,  a  USGS  scientist,  who  stated  that  bears  “could  be  absent  from
almost all their range by the middle of this century.”27

It may be interesting to note that only 5 years earlier, a 2002 study by the
same  USGS  had  reported  that  the  “[polar  bear]  populations  may  now  be
near historic highs.”28

Dr. Mitchell Taylor, manager of wildlife research for the Government of
the Canadian Territory of Nunavut, agreed with the US Geological Survey’s
2002  assessment  and  recently  reported  that  his  organization’s  research
shows  that  the  Canadian  polar  bear  population  has  increased  about  25
percent  during  the  past  decade  (from  about  twelve  thousand  to  fifteen
thousand).29  Even  Polar  Bears  International,  a  nonprofit  organization  that
works to protect the animals, rates only five groups as “declining,” another
five  as  “stable,”  one  as  “increasing,”  and  others  as  “data  deficient”
(impossible to measure) out of nineteen total world populations.30

A problem with such reasoning—one that even Polar Bears International
points out—is that swimming up to a hundred miles is not a big deal for the
animals. The drowned bears that Gore referred to in his film turned out to
be victims of a storm, not a lack of ice.

On March 28, 2008, Paul Milikin, a National Geographic photographer,
stated on ABC’s Good Morning America, “I realize what I need to do is try
and  tell  these  stories  through  National  Geographic  magazine  by  using

We continue to see the polar bear represented as the “spokesanimal” for
global  warming  threats.  Al  Gore’s  An  Inconvenient  Truth  shows  one
apparently drowning because it is too tired from swimming in search of ice
that we have caused to melt.

CBS  reporter  Daniel  Sieberg,  in  an  August  14,  2007,  segment  of  the
Evening  News,  echoed  this  presumption,  explaining  that,  “Less  ice  also
means the polar bears spend more time in the water, sometimes for so long
they  drown.”  An  April  2006  Time  magazine  cover  featured  a  bear
seemingly  “stranded”  on  melting  ice.  The  Defenders  of  Wildlife  website
explains: “Loss of sea ice leads to higher energy requirements to locate prey
and a shortage of food. This causes higher mortality rates among cubs and
reduction in size among first-year adult males.” Such claims appear to be
supported  by  anecdotal  evidence  that  four  polar  bears  drowned  while
swimming in Alaska’s Beaufort Sea, and that three polar bears attacked and
ate others, allegedly due to hunger.31 Some environmentalists also contend
that  human-induced  global  warming,  which  will  cause  most  of  the  North
Pole ice to melt in the next 50 years, will make it impossible for the bears to
hunt seals, their preferred prey.

animals, such as polar bears, to say that if we lose sea ice in the Arctic, and
projections are to lose sea ice in the next twenty to fifty years, we ultimately
are going to lose polar bears as well.”

He  went  on  to  acknowledge  how  the  photograph  featured  on  Time’s
cover in 2006, the seemingly “distressed” polar bear, came about: “It was
just a moment where I was not thinking clearly. I was ten feet away, lying
on my belly, and this bear is shaking water. And he was just … he took a
lunge at me basically, but as [he] lunged up and was coming down on me,
the ice broke and got away. And my first thought was, ‘I know I have the
shot,’ so I was really excited that this shot would help tell the story that I
want to tell about melting ice.”32

Biologist Mitchell Taylor pointed out in his testimony to the US Fish and
Wildlife Service (FWS) that modest warming may actually be beneficial to
polar bears; it could both provide a better habitat for seals and dramatically
boost the growth of blueberries, which the bears feed upon. In those cases
where bear weights and numbers are declining, he thinks the cause is too
many bears are competing for food, not Arctic warming.33

You  might  ask,  what  is  the  real  basis  for  predicting  these  polar  bear
extinctions?  Perhaps  you  may  have  guessed  by  now:  It’s  those  climate
models that predict a dire, and warm, future.

A 2006 US DOI news release stated that it would consider further polar
bear  protection  programs,  and  the  agency  acknowledged  that  “Alaska
populations have not experienced a statistically significant decline, but Fish
and  Wildlife  Service  biologists  are  concerned  that  they  may  face  such
decline in the future.”34

FWS  then  requested  nine  administrative  reports  from  government
agencies to bolster its case for listing the bears as an endangered species.
All  those  reports  were  based  upon  climate  models  that  shared  common
assumptions about sea ice levels during the 21st century—namely, that the
area of the Arctic covered by sea ice in summer would decline by more than
two-thirds, causing seal populations to decline. No ice, no seals; no seals,
no bears; case closed.

Alaska’s former governor Sarah Palin, along with many of the citizens
she  served,  was  and  is  not  happy  about  the  DOI’s  decision  to  add  polar
bears to the Endangered Species Act list. And this isn’t because Alaskans
don’t like bears. In an October 2007 press release, Palin argued, “[Listing] a
currently healthy species based entirely on highly speculative and uncertain

climate and ice modeling and equally uncertain and speculative modeling of
impacts  on  a  species  would  be  unprecedented.  Listing  polar  bears  under
ESA could actually harm many of the existing and highly successful polar
bear conservation measures.”35

And why would anyone want to do that? Some suspicious minds wonder
if maybe the main reason is oil drilling rather than bear welfare. As Ben
Lieberman  wrote  in  a  January  25,  2008,  Web  memo  for  the  Heritage
Foundation, “The first victim of listing would be new oil and natural gas
production throughout [Alaska] and its surrounding waters. It would put an
end  to  any  chances  of  opening  up  a  small  portion  of  the  Arctic  National
Wildlife Refuge (ANWR), estimated to contain 10 billion barrels of oil—
nearly 15 years’ worth of current imports from Saudi Arabia.”36

So  now  that  polar  bears  are  officially  “endangered,”  the  DOI,  working
through  the  Endangered  Species  Act,  has  been  granted  broad  powers  to
work with other federal agencies to “solve the problem” by linking global
warming threats to energy procurement and carbon emissions, two central
agenda priorities under one legislative action. As Myron Ebell, director of
global  warming  policy  at  the  Competitive  Enterprise  Institute,  postulates,
“The larger goal is to compel regulatory controls on energy use that global
warming alarmists have been unable to persuade Congress to enact. “37

The  polar  bear  issue  illustrates  how  interest  groups  have  used  the
pretense of a global warming crisis to advance other agendas. Still, if some
polar  bears  gain  from  the  deception,  it’s  probably  only  fair.  After  all,
consider all of the UN diplomats to whom we grant immunity, and they’re
certainly not an endangered population either.

Penguins: The Emperor Still Needs Warm Clothes

 

So,  what  about  the  overheated  emperor  penguins  we’ve  been  told  to
worry about? Like polar bear cubs (but not necessarily the big, ferocious
adults), they’re cute too! Think about all those noble creatures we fell in
love  with  in  the  big  hit  movie  March  of  the  Penguins  as  they  battled  to
survive  the  coldest  weather  conditions  on  Earth.  And  now  we’re  killing
them by making the Earth too warm. It seems like we’re always messing
things up. Fortunately, however, it appears that the Antarctic climate has a
changing mind of its own.38

2

Much  of  the  media  attention  to  climate  change  impacts  upon  penguin
populations  draws  heavily  upon  press  reports  released  by  the  World
Wildlife Fund (WWF), which advocates large and immediate CO
 emission
restrictions.  Those  reports  invariably  emphasize  connections  between
climate  change  and  penguin  declines,  focusing  upon  carefully  selected
colonies that have experienced diminished populations during the past 10 to
20 years or so. Other colonies that show stable or expanding populations
aren’t deemed to be as interesting.

Antarctica is a huge place that exhibits a variety of climate fluctuations
and trends at various temporal and spatial scales. The Antarctic Peninsula,
which  gets  a  lot  of  media  attention  for  the  study  of  periodic  warming,
comprises  only  about  2  percent  of  the  continent;  over  the  rest  of  the
continent, temperature changes over the past 30 to 40 years have been slight
or  undetectable.  And  while  sea  ice  extent  may  be  declining  off  the
peninsula,  it  has  changed  little,  on  average—and  in  some  areas  even  has
increased—around the continent in total.

Records show that overall, the continent of Antarctica has warmed about
1ºF  since  1957,  yet  average  temperatures  still  remain  about  50  degrees
below  zero.  West  Antarctica,  which  is  most  heavily  influenced  by
atmospheric and ocean changes occurring thousands of miles to the north, is
about  20  degrees  warmer  than  East  Antarctica  and  has  warmed  twice  as
fast. But temperature changes in the area rose up to five times more rapidly
in  the  1940s,  and  then  they  fell  by  the  same  amount  after  the  warming
effects of a major El Niño cycle were depleted.39

The  1997  El  Niño  was  one  of  the  most  severe  during  the  entire  last
century,  and  1998  was  an  exceptionally  warm  year.  Still,  according  to
Australia’s Commonwealth Scientific and Industrial Research Organisation,
satellite  measurements  indicate  that  East  Antarctica  north  of  latitude  81°
south gained up to 500 billion tons of ice over the last decade—one that Al
Gore claimed to be the warmest in 100 years.40

Due  to  unique  atmosphere-related  weather  influences,  warming  in
Antarctica  can  actually  bring  more  snowfall  to  the  continent.  Unlike
conditions  over  most  of  the  globe,  where  the  stratosphere  begins  at  an
altitude  of  8  to  10  miles  above  the  Earth’s  surface,  the  stratosphere  over
Antarctica  begins  at  an  altitude  of  about  5  miles,  or  roughly  25,000  feet
above  sea  level.  As  airstreams  over  the  flat  ocean  encounter  the  rougher
landmass, they are slowed, and they have nowhere to go but up. In doing

so, the air becomes compressed between the surface and the stratosphere (a
phenomenon called “convergence”). This forms shallow-height clouds that
both  reflect  the  Sun’s  energy  up  toward  space  (net  cooling)  and  cause
precipitation.  Ocean  warming  produces  more  humid  air  currents  (more
clouds and snow).

There  is  no  clear  connection  between  Antarctica’s  climate  and  average
surface  temperatures  elsewhere  around  the  globe,  and  certainly  none  that
can be linked to human influences. But count on it to remain very cold.

The  only  long-term  emperor  penguin  studies  have  taken  place  in  East
Antarctica: at Terre Adélie, on the Mawson Coast, and on the Prince Olav
Coast/Riiser-Larsen  Peninsula.  Although  their  numbers  have  dwindled
around the Antarctic Peninsula near Palmer Station, on Anvers Island, the
Terre  Adélie  population  has  tripled  since  the  1950s  at  Marguerite  Bay,
about  400  kilometers  to  the  south.  Nevertheless,  the  population  at  Terre
Adélie, which experienced a significant decline in the 1970s, had begun to
stabilize  until  recently,  and  it  then  declined  again.  Similarly,  emperor
colonies at Taylor Glacier and Auster, along the Mawson Coast, seemed to
be  stable  while  monitored  from  1988  to  1999,  but  the  Prince  Olav
Coast/Riiser-Larsen Peninsula populations recently declined in 2000.

Conditions  at  Palmer  Station  are  warming,  and  therefore  the  area  is
readily  accessible  for  observation.  Researchers  are  witnessing  a  large
proliferation  of  southern  fur  seals  and  elephant  seals  that  were  present
during  the  1990s  only  as  small  colonies.  One  population  formerly  of  six
seals  now  numbers  about  five  thousand.  Such  species  that  prefer  open
water, which was limited to the northern and eastern parts of the peninsula
where the ocean didn’t freeze in winter, are expanding their ranges. As the
“polar”  ecosystem  has  shifted  southward,  so  have  the  emperor  penguins
migrated from Terre Adélie. While they seem to like lots of ice, they don’t
like too much of it; its greater expanse makes it too strenuous to reach open
water for foraging. David Ainley of H. T. Harvey & Associates, who studies
these penguins in the southern Ross Sea area, observes, “As ice breaks up,
there should be more habitat, and we should see more penguins.”41

About 25 percent of all emperor penguins worldwide are believed to live
near  the  Ross  Sea,  an  area  of  Antarctica  subject  to  changing  climate
patterns.  Researchers  have  not  yet  found  evidence  to  suggest  either  an
overall increase or a decrease in the emperor population between 1983 and
2005.

To  sum  up  the  data,  local  and  regional  climate  variations,  which  have
always occurred along with fluctuations in aquatic food abundance, impact
various penguin species and colonies differently. Some are expanding their
ranges, some groups are declining in numbers while others are growing, and
most appear to be doing pretty well. Because of their remote habitat, which
makes them so difficult to observe, a lot remains to be known about many
aquatic  mammals  and  birds,  including  emperor  penguin  populations.
Satellite  imaging  is  used,  yet  many  studies  have  concluded  that  data
remains  insufficient  for  broad  analysis  of  impacts,  such  as  any  related  to
climate change.

Coral Catastrophes: Taking Claims with a Pinch of Salt

 

2

Such organizations as WWF and the Pew Charitable Trusts have raised
the issue of global warming and CO
 impacts upon the bleaching (killing) of
coral reefs as a key environmental concern. Such influences are not to be
taken  lightly,  because  ocean  reefs,  like  the  world’s  rain  forests,  are  vital
habitats  for  wide  varieties  of  life  and  thus  deserve  protection.  But  as  Dr.
Gary Sharp, a marine biologist who is the scientific director of the Center
for Climate/Ocean Resources Study in Salinas, California, points out, “We
need to look closely at what is most likely to affect the reefs, and what is
not.”42 (According to its website, the center is linked with the International
Oceanographic  Data  &  Information  Exchange  of  the  Intergovernmental
Oceanographic Commission of UNESCO.) Dr. Sharp cautions about being
too  alarmed  regarding  influences  of  anthropogenic  greenhouse  emissions
for several reasons.

He observes that conjectures that global warming will kill reefs are based
upon predictions that sea temperatures may increase about 3.6ºF over the
next  hundred  years  and  that  rising  CO
  levels  are  making  oceans  more
acidic. Yet coral reefs currently exist in waters with temperature gradients
of 10.8ºF–12.6ºF, so all reefs aren’t likely to die even if that increase were
to occur. It’s also not very probable that such an increase will happen. The
Earth’s ocean circulation pattern maintains a relatively narrow temperature
boundary according to natural cycles, and it would be extremely unusual for
sea surface temperatures in the open ocean to change that much.

2

The oceans appear to now be heading into one of their periodic cooling
phases  in  accordance  with  a  typical  55-to-70-year  dipolar  warm/cool
pattern.  Whether  ocean  waters  warm  or  cool  depends  upon  where  you
happen  to  be  within  these  large-scale  processes.  The  current  trend  is
ongoing and is expected to dominate global circulation between 2008 and
2012. The effect of this cycle can be witnessed in recent long, cold winters
with  near-record  low  temperatures  caused  by  highly  mobile  polar  cold
fronts measured as cold high-pressure regions in various places. This cold
phase  may  be  expected  to  continue  for  about  20  to  25  years  before  a
transition into another epoch of generally warmer, remedial climate.

And what about claims made by the Pew Charitable Trust that CO

 from
burning  fossils  is  “acidifying”  the  oceans?  This  alarm  is  primarily  based
upon  a  June  2006  release  of  data  from  a  NOAA  study  showing  that  the
water sampled from our oceans had an average pH of approximately 8.175
(0.025  units),  which  had  declined  from  8.2;  this  indicates  the  water  had
become  more  acidic  over  the  last  15  years.  However,  recent  studies  also
show  that  the  pH  difference  was  twelve  times  that  miniscule  change  (8.5
units) at the time of the last glaciation period, and the reefs thrived under
that  falling  pH.  It  would  require  a  drop  forty-seven  times  more  than  that
recorded by NOAA to reach a pH level of 7—the point when acid/alkaline
neutrality would occur and the coral would die. That would not only require
that oceans absorb billions more tons of CO
 than mankind is ever going to
emit; it would also require that its buffering agents—carbonate, nitrate, and
other  radicals  that  minimize  ocean  acidity  by  accepting  and  expelling
hydrogen  ions—disappear.  In  fact,  CO
  is  a  fundamental  building  block
necessary for coral to exist.

2

2

2

Pandemic Pestilence: The Political Variety

 

What is it, exactly, that we are supposed to be alarmed about regarding
global  warming?  It  seems  that  Mr.  Gore’s  predicted  20-  to  40-foot  ocean
rise isn’t very credible, even to the IPCC, so Palm Beach property owners
can  relax.  Polar  bears  can  carry  on  their  normal  business  of  merrily
multiplying, except for the invasions of privacy posed by polar paparazzi.
Penguins  are  moving  south,  a  trend  paralleling  that  of  snowbirds  on  this

continent. And bleached coral reefs aren’t either likely or sexy enough to
compete with bleached blondes for popular centerfold attention.

Okay,  let’s  try  examining  the  threat  of  global  warming  causing  really
nasty tropical diseases to spread, just as An Inconvenient Truth dramatically
warns.  That  should  warrant  some  fear.  Well,  maybe  not.  At  least  Paul
Reiter,  a  medical  entomologist  and  professor  at  the  Pasteur  Institute  in
Paris, doesn’t think so. He is one of the scientists featured in the film The
Greatest Global Warming Swindle, produced by WAG-TV in Great Britain
in response to the Gore movie. Dr. Reiter was also a contributory author of
the IPCC’s 2001 report who resigned because he regarded the processes to
be driven by agenda rather than science. He later threatened to sue the IPCC
if  they  didn’t  remove  his  name  from  the  report  he  didn’t  wish  to  be
associated with.43

Professor  Reiter’s  career  has  been  devoted  primarily  to  studying  such
mosquito-borne  diseases  as  malaria,  dengue,  yellow  fever,  and  West  Nile
virus,  among  others.  He  takes  special  issue  with  any  notion  that  global
warming is spreading such illnesses by extending the carriers to formerly
colder locales where they didn’t previously exist. In reference to statements
in An Inconvenient Truth that the African cities of Nairobi and Harare were
founded  above  the  mosquito  line  to  avoid  malaria,  and  that  now  the
mosquitoes  are  moving  to  those  higher  altitudes,  Dr.  Reiter  comments,
“Gore  is  completely  wrong  here—malaria  has  been  documented  at  an
altitude of 8,200 feet—Nairobi and Harare are at altitudes of about 4,920
feet. The new altitudes of malaria are lower than those recorded 100 years
ago. None of the 30 so-called new diseases Gore references are attributable
to global warming. None.”44

Although  few  people  seem  to  realize  it,  malaria  was  once  rampant
throughout  cold  parts  of  Europe,  the  US,  and  Canada,  extending  into  the
20th century. It was one of the major causes of troop morbidity during the
Russian/Finnish War of the 1940s, and an earlier massive epidemic in the
1920s went up through Siberia and into Archangel on the White Sea near
the Arctic Circle. Still, many continue to regard malaria and dengue as top
climate change dangers—far more dangerous than sea level rise.

Dr.  Reiter  submitted  written  testimony  to  the  British  House  of  Lords
Select Committee on Economic Affairs on March 31, 2005. His testimony
included the following critique of the chapter written by Working Group II

—much  of  which  was  devoted  to  mosquito-borne  diseases,  principally
malaria—for the IPCC's Second Assessment Report:

The scientific literature on mosquito-borne diseases is voluminous,
yet  the  text  references  in  the  chapter  were  restricted  to  a  handful  of
articles, many of them relatively obscure, and nearly all suggesting an
increase in prevalence of disease in a warmer climate. The paucity of
information  was  hardly  surprising:  Not  one  of  the  lead  authors  had
ever  written  a  research  paper  on  the  subject!  Moreover,  two  of  the
authors,  both  physicians,  had  spent 
their  entire  careers  as
environmental  activists.  One  of 
these  activists  has  published
“professional”  articles  as  an  “expert”  on  32  subjects,  ranging  from
mercury poisoning to land mines, globalization to allergies, and West
Nile virus to AIDS.45

Hurricane Hullabaloo

 

Despite  large  modeling  uncertainties  with  undemonstrated  reliability
even over short forecast periods, the IPCC’s “Climate Change 2007” AR4
report “Summary for Policymakers” predicts (with greater than 66 percent
confidence) that the next century will experience an increase in droughts,
tropical cyclones, and extreme high tides. Yet as John Christy, a professor
of  atmospheric  science  at  the  University  of  Alabama,  points  out  in  an
October  20,  2000,  article  published  in  NASA  Science,  “The  fact  that
different  computer  models  often  produce  different  forecasts  doesn’t  offer
much reassurance. For example, one model predicted that the Southeastern
US  would  become  more  jungle-like  in  the  next  century,  while  another
model predicted the same region would become a dried-out savanna.”46

An event preceding the release of the 2007 AR4 summary report offers
reasons to be even less confident about some of the IPCC’s conclusions. It
occurred following the summer of 2004, a year when a deadly storm season
brought  five  devastating  hurricanes  that  made  landfall  in  Florida.  The
terrible  destruction  made  headlines  throughout  the  world,  and  many
conjectured the hurricanes were linked to global warming.

Opportunities to capitalize on the unusual and terrifying hurricane pattern
to validate man-made global warming threats were not lost on some IPCC

officials,  who  rapidly  responded.  In  October  2004,  the  IPCC’s  Kevin
Trenberth participated in a press conference that announced, “Experts warn
global  warming  likely  to  continue  spurring  more  outbreaks  of  intense
activity.”  But  there  was  a  serious  problem.  The  IPCC  studies  released  in
1995 and 2001 had found no evidence of a global warming–hurricane link,
and there was no new analysis to suggest otherwise.

this  subject  at 

Christopher  Landsea,  an  expert  on 

the  Atlantic
Oceanographic  and  Meteorological  Laboratory,  was  astounded  and
perplexed  when  he  was  informed  that  the  press  conference  was  to  take
place.  As  a  contributing  author  to  both  of  the  previous  reports  and  an
invited  author  for  the  2007  AR4  report,  he  believed  there  must  be  some
huge mistake. He had not done any work to substantiate the claim. Nobody
had.  There  were  no  studies  that  revealed  an  upward  trend  of  hurricane
frequency or intensity. Not in the Atlantic basin or in any other basin.47

Landsea  wrote  to  top  IPCC  officials,  imploring,  “What  scientific,
refereed  publications  substantiate  these  pronouncements?  What  studies
being  alluded  to  have  shown  a  connection  between  observed  warming
trends  on  Earth  and  long-term  trends  of  tropical  cyclone  activity?”
Receiving  no  replies,  he  then  requested  the  IPCC  leadership’s  assurance
that  the  2007  report  would  present  true  science,  saying,  “[Dr.  Trenberth]
seems  to  have  come  to  a  conclusion  that  global  warming  has  altered
hurricane activity, and has already stated so. This does not reflect consensus
within the hurricane research community … Thus, I would like assurance
that what will be included in the IPCC report will reflect the best available
information consensus within the scientific community most expert on the
specific topic.”48

After the assurance didn’t come, he resigned from the 2007 AR4 report
activities  and  issued  an  open  letter  presenting  his  reasons.  And  while  the
IPCC press conference proclaiming that global warming caused hurricanes
received tumultuous responses in the world press, Mother Nature didn’t pay
much  attention.  Hurricane  seasons  since  then  have  returned  to  average
patterns noted historically over the past 150 years.

Feverish Concerns, Cold Sweats

 

Potentially  scary  global  warming  predictions  originate  from  other
sources besides the IPCC. For example, a really good one came from the
US Pentagon, a place that has to worry about a lot of frightening scenarios.
This one concerns a hypothesis that global warming could cause parts of the
world to become colder, a problem for some who like it hot.

It  seems  that  the  Pentagon,  which  had  been  studying  possible  national
security issues associated with climate change for many years, contracted
with a US think tank called Global Business Network to research potential
global  warming  consequences.  The  resulting  research  report  titled  “An
Abrupt  Climate  Change  Scenario  and  Its  Implications  for  United  States
National  Security,”  which  was  released  in  October  2003,  produced  more
than most Pentagon officials expected.

Andrew Marshall, director of the Pentagon’s Office of Net Assessment
(which is responsible for identifying long-term threats to the United States),
was  not  pleased  with  then-President  George  W.  Bush’s  lack  of  anxiety
regarding global warming. Through a decision to bypass the White House,
he presented the summary information and voiced his concerns to Fortune
magazine, which published an article on the subject on February 9, 2004. In
that  article,  Mr.  Marshall  explained  how  melting  at  the  North  and  South
Poles, and from glaciers around the world, presented an impending global
weather disaster.49

Briefly summarized, the theory entails the following scenario, which was
prominently featured in An Inconvenient Truth—both the documentary and
the book— in the discussion involving “thermohaline convection.”

The  Gulf  Stream,  or  “North  Atlantic  thermohaline  conveyor,”  is  a
roughly figure eight–shaped stream of water that transfers heat from south
of  the  equator  as  it  flows  over  the  ocean  surface  toward  the  north  and
warms northern parts of America and Western Europe. It is a primary force
in driving the world’s weather patterns. After the Gulf Stream transfers heat
to the air through convection and cools down, it drops to the bottom of the
ocean and returns as an underwater river that flows back toward the equator,
warms  again,  rises  to  the  surface,  and  returns  north  again,  like  a  huge
thermal conveyor.

The  motor  that  drives  the  conveyor  to  keep  the  water  moving  is
purported by the theory to be located in the north, where the ocean’s salt
density  causes  the  Gulf  Stream  to  drop,  pulling  warm  water  up  from  the
south.  But  if  the  poles  were  to  melt,  large  amounts  of  added  fresh  water

might excessively dilute the Atlantic Ocean’s salt density, causing the Gulf
Stream not to drop as far, and also (in theory) causing it to slow down. This
would  cause  less  warmth  to  be  transferred  to  the  North  Atlantic  region,
affecting  the  climate-heat  balance-driven  weather  patterns.  Northern  parts
of Western Europe would be particularly affected due to prevailing winds
that  move  heat  in  that  direction,  possibly  producing  another  interglacial
cold spell like the LIA.

This scenario is extremely unlikely to happen for several reasons. First of
all, the Gulf Stream disruption theory is based upon different circumstances
that  occurred  about  12,500  years  ago  during  the  Younger  Dryas  episode,
when a giant ice dam burst in North America, causing two enormous lakes
to drain rapidly into the sea. The previous Ice Age had created an ice sheet
up  to  9,000  feet  thick  over  large  northern  regions  of  Europe  and  North
America.  For  example,  the  Laurentide  Ice  Sheet  extended  over  all  of  the
Great Lakes, west into Iowa, and south into Indiana and Ohio. When the ice
melted,  more  than  100,000  cubic  kilometers  of  freshwater  were  rapidly
discharged into the sea, and the Gulf Stream really was overwhelmed. This
can  be  compared  with  freshwater  injections  from  recent  Greenland  ice
melts  amounting  to  only  a  few  hundred  kilometers  per  year,  which  show
signs of stabilizing, at least currently. Since the trillions of tons of ice that
existed prior to our interglacial period melted more than 10,000 years ago,
there simply isn’t enough left to trigger a repeat performance.50

There  is  also  no  evidence  that  recent  warming  is  slowing  the  Gulf
Stream, and the thermohaline conveyor is actually observed to be producing
increased flow rates of deep Atlantic currents. Thermohaline circulation is
now believed to be primarily a wind-driven system energized by the Earth’s
spin and lunar tides, rather than by Gulf Stream salinity differences and sea
temperatures.

Global circulation models based upon real-world data also don’t indicate
any  danger.  A 
the  Lamont-Doherty  Earth
Observatory ran several versions of the Gulf Stream Collapse Theory on a
global climate model at NASA’s GISS and found no evidence of a “tipping
point” that would produce a Gulf Stream shutdown.51

team  of  researchers  at 

While  the  National  Research  Council’s  Committee  on  Abrupt  Climate
Change  previously  warned  about  “large  abrupt  climate  changes”  of  “as
much  as  10°C  (50°F)  in  10  years,”  which  were  claimed  to  be  “not  only
possible but likely in the future,”52 the Lamont-Doherty team found no basis

for  such  dramatic  thresholds  in  their  model  runs.  Instead,  they  concluded
that  the  Atlantic  conveyor  “decreases  linearly  with  the  volume  of
freshwater  added  through  the  St.  Lawrence”  and  that  it  does  so  “without
any threshold effects.”53

Another  team,  at  the  UK’s  Hadley  Centre  for  Climate  Change,  used  a
different model to test the same hypothesis regarding a meltwater shutdown
of the ocean’s circulation, and they found just the opposite: “Accompanying
the freshening trend, the [thermohaline circulation] unexpectedly shows an
upward trend, rather than a downward trend.” This agrees with real-world
evidence  that  deep  ocean  currents  are  becoming  stronger  with  increased
warming and precipitation.54

Rain Forest Rebuttal

 

An  ultimately  embarrassing  assertion  in  the  IPCC’s  2007  AR4  report
was  that  40  percent  of  the  Amazon  rain  forest  in  South  America  is
endangered by global warming Those  findings  were  based  upon  numbers
taken from a non-peer-reviewed paper written by a freelance green activist
journalist  and  published  by  the  WWF.  The  paper  warned  that  “up  to  40
percent  of  the  Amazon  forests  could  react  drastically  to  even  a  slight
reduction  of  precipitation  …  It  is  more  probable  that  forests  will  be
replaced by ecosystems … such as tropical savannas.” The disaster would
be  triggered,  according  to  the  IPCC’s  assessment,  by  a  slight  drop  in  the
rainfall rates expected for a warming world.

The  original  claim  was  based  upon  a  WWF  study,  “Global  Review  of
Forest  Fires,”  written  “to  secure  essential  policy  reform  at  national  and
international  levels  to  provide  a  legislative  and  economic  base  for
controlling harmful anthropogenic forest fires.” The 40 percent figure was
taken from a letter published in the journal Nature, which related to harmful
logging activities.55

Although  the  global  warming–rain  forest  endangerment  connection  has
been debunked by serious scientists, the IPCC has yet to retract or amend
the  claim.  NASA-funded  analyses  of  satellite  imagery  over  past  decades
indicate  that  in  fact  the  rain  forests  are  remarkably  resilient  to  droughts.
Even  during  a  100-year  dry-season  peak  in  2007,  the  jungles  appeared
basically unaffected. Arindam Samanta of Boston University, lead author of

a  recent  study  based  on  satellite  data  from  NASA’s  Moderate  Resolution
Imaging  Spectroradiometer,  or  MODIS,  remarked,  “We  found  no  big
differences in the greenness level of these forests between drought and non-
drought  years.”  Sangram  Ganguly,  author  of  another  study  at  the  NASA-
affiliated Bay Area Environmental Research Institute, added, “Our results
certainly do not indicate such extreme sensitivity to reductions in rainfall.”56

Following Earth’s Runaway Twin

 

One of the best ways to cause man-made greenhouse warming theory
believers’  knees  to  tremble  is  to  assert  that  the  world  is  at  the  cusp  of  a
“tipping  point.”  As  the  Worldwatch  Institute’s  “State  of  the  World  2009”
report defines that term, it is when “climate change begins to feed on itself
and  becomes  essentially  irreversible  for  centuries  into  the  future.”57  A
“really-bad-case” scenario suggests that this can lead to conditions similar
to those on our “sister planet,” Venus.

As  reported  by  Mark  Bullock  at  the  Laboratory  for  Atmospheric  and
Space  Physics  at  University  of  Colorado–Boulder,  in  a  1999  Scientific
American  article,  “Since  Venus  and  Earth  have  a  number  of  similarities,
there are implications here for our own future.”58 Venus is Earth’s twin in
the sense that it’s made up of a similar composition, and it is believed to
once have had a similar atmosphere. But when it comes to current climate,
it couldn’t be much more different. Venus is the hottest planet in the solar
system,  with  an  average  temperature  of  more  than  400  and  a  surface
pressure  nearly  one  hundred  times  greater  than  Earth’s.  And  while  both
planets have clouds, those on Venus contain sulfuric acid and CO
.Venus is
slightly smaller than Earth, is closer to the Sun, and has no plate tectonics.
Its continents simply tip up every 500 million years or so like the lid on a
boiling pot and slide down into a molten core, spewing huge amounts of
heat into the atmosphere as they do so. That is a real tipping point!

At  some  point  in  Venus’s  past,  its  global  magnetosphere  shut  down.
Without this force field the Sun’s solar wind was able to reach the planet
and  tear  away  at  its  atmosphere,  stripping  away  the  lighter  atoms.  The
lightest  atom  is  hydrogen,  a  constituent  of  water,  which  is  a  major
component of Earth’s atmosphere (and GHG).59

2

Another really big difference between the two planets—one that accounts
for  substantially  different  climate  features—is  that  Venus  doesn’t  rapidly
rotate on its axis, creating short day/night cycles as Earth does. In fact, a
Venusian day is slightly longer than a Venusian year. This means that the
same  surface  area  is  exposed  to  radiant  solar  heat  without  relief  for  very
long  periods  of  time.  And  according  to  our  best  reports,  there  are  no
sunbathing  humans  to  enjoy  these  conditions  or  thus  contribute  to
greenhouse emissions. So don’t sweat sibling relationships!

Also, according to MIT scientist Richard Lindzen, “There is no physical
basis for suggesting ‘tipping points’ … especially given that the impact of
each added

amount  of  CO

  [in  the  atmosphere]  is  less  than  the  impact  of  its

predecessor (i.e., “we have diminishing returns) “60.

2

Convenient Illusions

 

Al  Gore  has  represented  himself  as  a  learned  authority  on  the
mechanisms  and  threats  of  a  global  warning  crisis.  And  if  drama  trumps
real facts in his pronouncements, doubts regarding his convictions may be
unwarranted. Still, as someone who frequently quotes a wise observation by
Mark  Twain,  a  person  he  greatly  admires,  Mr.  Gore  might  carefully
consider that advice: “It ain’t what you don’t know that gets you in trouble.
It’s what you know for sure that just ain’t so.”

Another reported “hero” Gore has credited as an important influence is
his  former  Harvard  professor  Dr.  Roger  Revelle,  a  distinguished
oceanographer.  Dr.  Revelle,  however,  has  expressly  disagreed  with
frightening  global  warming  scenarios  that  Mr.  Gore  has  promulgated,
saying, “Evidence of global warming does not justify drastic measures so
far,  unless  they  were  justified  by  reasons  having  nothing  to  do  with  the
climate change issue.”61

Gore  obviously  wasn’t  very  happy  about  having  his  previously  touted
authority challenge the urgency and rationale of his mission. He countered
by accusing his former professor of having become senile when he made
those remarks shortly before his fatal heart attack in 1992. Not a very nice
way to treat a hero.

Dr.  Revelle  was  not  alone  in  his  strong  disagreement  with  factually
impaired  “Gore  lore.”  Yet  most  informed  scientists  who  know  better  are
reluctant  to  publicly  speak  out  on  the  matter.  Such  reticence  may  be
attributable to a widespread “emperor’s clothes” syndrome associated with
multiple causes. Some might be hesitant to say anything that would reflect
poorly upon the sanctity of the IPCC, the UN-sanctioned tribunal of truth
endowed  with  the  Nobel  Prize  distinction  it  shares  with  fellow  Nobel
laureate  Al  Gore.  Many  may  also  recognize  that  contradicting  alarmist
statements are not helpful in gaining public support essential to sustain an
exploding climate science industry.62

Lord  Christopher  Monckton,  a  policy  adviser  to  former  prime  minister
Margaret  Thatcher  who  also  participated  at  the  New  York  conference,

However, more and more concerned people are speaking out to correct
scientifically  unsupportable  and  misleading  statements.  For  example,  a
judge  in  London’s  High  Court  ruled  in  October  2007  that  the  film  An
Inconvenient Truth can be shown only in secondary schools if accompanied
by guidance notes for teachers to balance Mr. Gore’s “one-sided” views.63 In
comments  regarding  his  ruling,  Sir  Michael  Burton  pointed  out  that  the
“apocalyptical vision” presented in the film was politically partisan, and not
an impartial analysis of the science of climate change: “It is built around the
charismatic presence of the ex-vice president Al Gore, whose crusade is to
persuade  the  world  of  the  dangers  of  climate  change  caused  by  global
warming . . . It is now common ground that it is not simply a science film—
although it is clear that it is based substantially on scientific research and
opinion—but it is [clearly] a political film.”64

John Coleman, founder of the Weather Channel in 1982, expressed strong
opinions  about  Gore’s  promotions  of  warming  hysteria  and  his  cap-and-
trade  agenda  at  an  International  Conference  on  Climate  Change  that  was
held  in  New  York  March  2–4,  2008.  The  event  was  sponsored  by  the
Heartland Institute and was attended by more than two hundred scientists
from several countries. Coleman told the audience his strategy for exposing
what he called “the fraud of global warming”: “[I] have a feeling this is the
opening. If the lawyers will take the case—sue the people who sell carbon
credits. That includes Al Gore. That lawsuit would get so much publicity, so
much  media  attention.  And  as  the  experts  went  to  the  [media]  stand  to
testify, I feel that could become the vehicle to finally put some light on the
fraud of global warming.”65

agreed with Coleman that the courts are a good avenue through which to
show  real  climate  science.  He  also  expressed  a  belief  that  science  will
eventually  prevail,  and  that  the  “scare”  of  global  warming  will  go  away.
Anthony  Watts,  another  of  the  conference  speakers,  commented,  “I  was
surprised to learn that Al Gore had been offered an opportunity to address
this  conference,  and  his  usual  $200,000  speaking  fee  and  expenses  were
met, but he declined. I also know that invitations went out to NASA GISS
principal scientists Dr. James Hansen and Dr. Gavin Schmidt weeks ago as
evidenced by their write-up of the issue on their blog, RealClimate.org, a
week or so ago.”66

It’s a shame that Mr. Gore was unable to attend the conference when the
weather  was  nice  and  warm.  It  would  have  been  so  much  better  for  his
message  than  the  timing  of  a  global  warming  speech  he  presented  in  the
same city in January 2004, one of the coldest days ever recorded in New
York. But then, you can never be certain about the weather.

Chapter 5

UN POLITICAL SCIENCE LESSONS

International  experts  agree 

globalclimate peril.

that  US  capitalism 

is  causing

As  I  first  mentioned  in  the  introduction,  most  of  what  we  hear
daily about global warming—the really scary stuff that gets media headline
coverage, wins Academy Awards, and earns Nobel Prizes—originates from
reports 
the
Intergovernmental Panel on Climate Change. Given the IPCC’s tremendous
influence  in  shaping  international  public  opinions,  economic  policies,
environmental  and  energy  legislation,  and  the  political  landscapes  that
determine  huge  science  budget  allocations,  its  background  and  workings
warrant special attention.1

issued  by  a  United  Nations–sponsored  corporation, 

The IPCC’s genesis is linked in large measure to some converging forces
and  events  that  occurred  in  the  US  and  Europe  during  the  late  1980s.
Following the phenomenal growth in environmental movements that began
about a decade earlier, green parties in Europe and private special-interest
groups in the US gained even greater momentum under a global warming
banner. Well-organized lobbying campaigns, backed by large budgets and
voting  blocs,  appealed  to  the  interests  of  prominent  political  figures,  and
leading  captains  of  the  media  were  enlisted  in  the  call  for  action.2  Fund-
raisers and pundits recognized that “saving the planet” sells well, and what
sells even better is the underlying message “pay now or fry.”

Global  warm-mongering  got  a  boost  in  1997  when  Washington,  DC,
group,  Ozone  Action  sent  a  “Scientists’  Statement  on  Global  Climactic
Disruption” to then-President Clinton, which they claimed had been signed
by 2,611 scientists from the US and abroad. The document was offered to
endorse  “conclusive”  evidence  of  man-made  global  warming.3  But
according to Citizens for a Sound Economy, a group that opposed climate
alarmism, only about 10 percent of those signers had experience in fields
associated with climate science. Others included two landscape architects,
ten  psychologists,  a 
trained  Chinese  doctor,  and  a
gynecologist.4

traditionally 

The  UN  has  sponsored  and  organized  a  variety  of  environmental
programs that led up to the IPCC’s creation and activities. One, termed the
Montreal Protocol on Substances That Deplete the Ozone Layer, responded
to  concerns  that  human  activities  were  responsible  for  causing  an  ozone
hole  in  the  stratosphere  over  Antarctica.  The  source  of  the  problem  was
attributed  to  releases  of  chlorofluorocarbons  (CFCs)  used  as  refrigerants,
aerosol  propellants,  and  cleaning  solvents.  The  treaty,  which  took  effect
January 1, 1989, has since undergone seven revisions: 1990 (London); 1991
(Nairobi);  1992  (Copenhagen);  1993  (Bangkok);  1995  (Vienna),  1997
(Montreal);  and  1999  (Beijing).  By  September  2007,  about  two  hundred
countries  agreed  to  eliminate  CFC  use  by  2020;  developing  nations  were
given until 2030. Some critics have argued that richer countries can afford
CFC  substitutes  whereas  poorer  ones  that  cannot  are  realizing  increased
death rates from food-borne illnesses.

In 1988, the UN turned most of its attention to human GHG emissions
when  members  of  two  of  its  organizations—the  World  Meteorological
Organization  (WMO)  and  the  United  Nations  Environment  Programme

(UNEP)—were  assigned  to  establish  the  IPCC.  The  IPCC  panel  is
composed  of  representatives  appointed  by  governments  and  is  led  by
government  “scientists”  who  meet  about  annually  and  whose  role  is  to
control the organization’s structure and procedures. While the governments
are encouraged to appoint people with appropriate expertise, in practice this
may be the exception rather than the rule. Many are primarily bureaucrats,
and few have credentials as climate scientists.5

Roy  Spencer,  in  his  book  The  Great  Global  Warming  Blunder,  reports
that former IPCC chairman and chief environmental scientist in the Clinton-
Gore  administration  Robert  Watson  (1997-2002)  made  it  his  priority  to
regulate CO
 before much of any climate modeling had ever occurred. This
intent  was  expressed  to  Dr.  Spencer  and  his  colleague  John  Christy  soon
after Watson had acted as a key 1987 Montreal Protocol negotiator for CFC
regulations.

2

Why  did  the  UN  establish  the  IPCC?  Was  it  to  objectively  study  and
determine whether there really was a climate change crisis? Did they wish
to  explore  which  ones  among  a  known  variety  of  climate  forcings  were
dominant? Were they curious as to what extent human activities played into
the mix? The answer to those questions is, not very likely.

Or rather, had the UN already determined that recently observed climate
 through excessive
change was dangerous and that human releases of CO
population  growth,  industry,  and  free-market  capitalist  consumption  in
developed countries was responsible? And did the UN wish to gain the lead
role in straightening everything out through global regulation and resource
redistribution? Let’s explore these possibilities.

2

Working the System

 

First, what the public doesn’t generally realize is that the IPCC doesn’t
actually  carry  out  any  original  climate  research,  nor  does  it  even
continuously  monitor  climate-related  data.  Instead,  it  simply  issues
assessments  based  primarily  upon  other  independent  peer-reviewed  and
published scientific and technical literature. At least, that is what the panel
is  supposed  to  do.  Yet  some  of  the  most  influential  conclusions  that  are
summarized in its reports have been neither based upon truly independent
research nor properly vetted through accepted peer-review processes.6

Most  of  the  IPCC’s  actual  work  is  conducted  by  separate  “working
groups”  and  a  “task  force”  that  generally  produce  quite  thorough  and
objective  lengthy  technical  reports.  These  individuals  are  selected  on  the
basis  of  their  special  expertise  to  address  designated  topics.  It  should  be
assumed  that  most  take  these  responsibilities  very  seriously.  Yet  in  the
interest of international parity, and not in the interest of science, each of the
working groups has two cochairs: one from a developing country, and one
from  the  developed  world.  As  might  be  imagined,  this  does  not  reflect  a
balance of the most qualified expertise.

What each of the working group’s reviewers learns goes into a report. If
they aren’t sure what they have learned about an issue, or they can’t agree,
they vote among themselves regarding what they think they are most sure
about, and their levels of confidence. Some voting members may have little
or no real experience in dealing with the particular subjects; they are there
to  ensure  international  representation.  From  the  vantage  point  of  political
correctness, this process may be fair; scientific correctness, however, is an
entirely different matter.

You may have seen references to a “network of thousands of international
scientists”  involved  with  IPCC  studies.  One  such  source  was  Time
magazine’s statement that “thousands of scientists from around the world
contribute  to  IPCC  reports,”  as  represented  by  official  US  government
organizations  such  as  the  Department  of  Energy’s  Pacific  Northwest
National Laboratory.7

How can that many experts be wrong? For starters, let’s begin with that
wildly  exaggerated  number  of  experts.  Dr.  Vincent  Gray  reports  a  far
different circumstance based on his firsthand experience as a reviewer for
the  IPCC’s  2007  AR4  report  “Summary  for  Policymakers”:  “Forget  any
illusion  of  hundreds  of  experts  diligently  poring  over  the  report  and
providing extensive feedback to the editing teams. The true picture is closer
to 65 reviewers for any one chapter, with about half not commenting on any
other chapter and one quarter commenting on just one other.”8

Unfortunately,  very  few  people  ever  read  those  full  reports.  What
happens  to  those  big  compilations  is  that  they  go  through  international
bureaucratic reviews, where political appointees dissect them line by line to
glean the best stuff in support of what IPCC wanted to say in the first place.
These  cherry-picked  items  are  then  assembled  and  spun  into  highly
condensed reports calibrated to get prime-time and front-page attention.9

Political summary editing processes usually progress through a series of
drafts  that  become  increasingly  media  worthy.  For  example,  the  original
text of an April 2000 Third Assessment Report (TAR) draft stated, “There
has  been  a  discernible  human  influence  on  global  climate.”10  This  was
followed by an October version that concluded, “It is likely that increasing
concentrations  of  anthropogenic  greenhouse  gases  have  contributed
significantly to the observed warming over the past 50 years.”11 In the final
official summary, the language was toughened up even more: “Most of the
observed warming over the past 50 years is likely to have been due to the
increase in greenhouse gas contributions.”12

When the UN Environment Programme’s spokesman Tim Higham was
asked by New Scientist about the scientific background for this change, his
answer was honest: “There was no new science, but the scientists wanted to
present a clear and strong message to policymakers.”13

Summary  revisions  also  play  down  or  totally  ignore  findings  that
appointed  IPCC  bureaucrats  don’t  want  the  public  to  consider.  An  earlier
TAR draft stated, “In many developing countries, net economic gains are
projected for mean temperature increases up to roughly 2°C (36°F). Mixed
or neutral net effects are projected in developed countries for temperature
increases in the approximate range of 2°C–3°C (36°F–38°F), and net losses
for  larger  temperature  increases.”14  Because  any  mention  of  net  benefits
from  even  moderate  global  warming  would  have  been  unacceptable,  the
statement in the final summary was changed to “An increase in global mean
temperature of up to a few degrees C would produce a mixture of economic
gains  and  losses  in  developed  countries,  with  economic  losses  for  larger
temperature  increases.”  As  Bjorn  Lomborg  points  out  in  his  book  The
Skeptical  Environmentalist,  this  “political  decision  stopped  IPCC  from
looking  at  the  total  cost-benefit  of  global  warming  and  made  it  focus
instead on how to curb further greenhouse gas emissions.”15

Any  scientific  objectivity  behind  IPCC  summary  reports  is  illusory.
Referring  to  bureaucratic  influence,  Keith  Shine,  a  leading  IPCC  author,
described the editing process as follows: “We produce a draft, and then the
policymakers  go  through  it  line  by  line  and  change  the  way  it’s
presented . . . They don’t change the data, but the way it’s presented. It is
peculiar that they have the final say in what goes into a scientist’s report.”16
And  how  objective  are  the  scientific  editors  who  participate?  Recently
deceased  Stephen  Schneider,  a  prominent  man-made  warming  theory

proponent, served as a lead author of the IPCC Working Group I (1994–
1996) and Working Group II (1997–2001) reports. He was also lead author
for the IPCC “Guidance Paper on Uncertainties” and coauthor of the “Key
Vulnerabilities Cross-Cutting Theme” parts for the 2007 AR4 report. The
Stanford University professor obviously became a global warming convert
sometime  after  he  had  written  The  Genesis  Strategy  (published  in  1976),
which  addressed  global  cooling  risks.  In  a  1989  Discover  magazine
interview,  he  candidly  expresses  a  professional-versus-personal  conflict
between  the  side  of  a  scientist  concerned  with  seeking  truth  and  the  side
concerned  with  being  a  citizen  who  must  take  an  interest  in  political
efficacy.

On  the  one  hand,  as  scientists  we  are  ethically  bound  to  the
scientific  method.  On  the  other  hand,  we  are  not  just  scientists,  but
human  beings  as  well.  And  like  most  people,  we’d  like  to  see  the
world a better place, which in this context translates into our working
to reduce the risk of potentially disastrous climatic change. To do that
we  need  to  get  some  broad-based  support,  to  capture  the  public’s
imagination. That, of course, entails getting loads of media coverage.
So  we  have  to  offer  up  scary  scenarios,  make  simplified,  dramatic
statements, and make little mention of any doubts we might have.17

In other words, trust not what we tell you, but believe that we have your
best  interests  in  mind  because  our  personal  intentions  are  ethical.  Accept
what  we  tell  you  for  that  reason.  If  we  have  to  exaggerate  the  truth  and
frighten you to get your attention, it’s for a righteous cause.

The Confidence Game

 

To  lend  scientific  authority,  IPCC  summary  reports  must  above  all
sound confident. Global forecasts tell us within a comically precise decimal
point range that this or that is “virtually certain,” “very likely,” or “likely”
to occur within such and such a time. If we should happen to remember that
past  predictions  have  been  wrong  or  significantly  revised,  the  IPCC
reassures us that climate models are better now. They are confident about
that, and we should be too!

Which might lead us to ask, better than what? If these models are still
wrong,  how  wrong  must  they  be  to  qualify  as  totally  misleading  and
useless?

Climate  models  contain  huge  uncertainty  factors  that  are  broadly
recognized  by  IPCC  scientists.  They  don’t  (can’t)  incorporate  important
unknown  and  poorly  understood  variables  and  relationships,  and  the
parameters  can  be  adjusted  (“tuned”)  to  fit  almost  any  climate,  including
one with no warming or one that cools. Despite the fact that no model has
yet  successfully  predicted  any  future  climate  sequence,  IPCC  summaries
confidently present “projections” looking forward hundreds of years based
upon creative storylines and untested theories. Those theories are primarily
directed 
to  anthropogenic  greenhouse  forcing  factors  and  warming
consequences. Other possible causes and effects are virtually ignored. Yet,
although  the  IPCC  has  provided  an  abundance  of  information  about
atmospheric GHG concentrations and changes, no evidence of past or future
harmful  effects,  or  relationships  to  “unusual”  weather,  has  yet  been
produced.18

Science researcher Roy Spencer observes that “what scientists claim to
know about man-made global warming is based as much upon faith as it is
upon  knowledge.”  He  regards  probabilistic  language  applied  in  IPCC
summary reports as misleading and inappropriate, stating that “its use is a
pseudoscientific  way  of  conveying  the  level  of  faith  a  scientist  has  in
his/her beliefs.”19

Chapter  8  of  the  2000  IPCC  TAR  report,  titled  “Model  Evaluation,”
contains this confession: “We fully recognize that many of the evaluation
statements we make contain a degree of subjective scientific perception and
may contain much ‘community’ or ‘personal’ knowledge. For example, the
very  choice  of  model  variables  and  model  processes  that  are  investigated
are  often  based  upon  the  subjective  judgment  and  experience  of  the
modeling community.”20

In  that  same  report,  the  IPCC  further  admits,  “In  climate  research  and
modeling,  we  should  recognize  that  we  are  dealing  with  a  coupled  non-
linear chaotic system, and therefore that the long-term prediction of future
climate states is not possible.”21 Here, the IPCC openly acknowledges that
its models are not accurate. Yet it obviously needs to apply them to justify
its  ever-growing  budget  and  influence.  Without  unreliable  data  from  the
models, the IPCC might be out of business.

Politics: The Ultimate Scientific Authority

 

As confirmed by evidence exposed in the purloined CRU e-mail files,
scientists with contrary views regarding anthropogenic greenhouse warming
or the efficacy of present-day climate models aren’t likely to be prominently
represented in IPCC processes or summary reports. Prior to release, drafts
are circulated to “expert reviewers” throughout the world for comment, and
unwelcome statements are deleted. Statements that are not consistent with
the views of designated main authors stand little chance of being seriously
considered. The first 1990 report, which was particularly influential as the
basis  for  negotiating  the  United  Nations’  FCCC  (the  Kyoto  Protocol),
included the following statement: “Whilst every attempt was made by the
lead authors to incorporate their comments, in some cases those formed a
minority opinion which could not be reconciled with the larger consensus.”22
In  the  IPCC’s  1995  Second  Assessment  Report  (SAR),  the  crucial
Chapter  8  of  the  final  draft  denied  any  evidence  connecting  observed
climate  changes  to  anthropogenic  greenhouse  causes  (the  “fingerprint”
factor). In fact, Figure 8.10b of that report showed the pattern correlation of
measured  observations  and  climate  models  actually  decreasing  during  a
major  surface  warming  surge  between  1916  and  1940.  The  consulting
scientists approved the draft, along with the full report, in December 1995.23
Its conclusions, based upon reviews of 130 peer-reviewed science studies,
were these:

•  ”None of the studies cited above has shown clear evidence that we
can attribute the observed [climate] changes to the specific cause of
increases in greenhouse gases.”

•    ”While  some  of  the  pattern-based  studies  discussed  here  have
claimed detection of a significant climate change, no study to date
has positively attributed all or part [of the climate change observed]
to [man-made] causes. Nor has any study quantified the magnitude
of a greenhouse gas effect in the observed data—an issue of primary
relevance to policy makers.”

•    ”Any  claims  of  positive  detection  and  attribution  of  significant
climate change are likely to remain controversial until uncertainties
in the total natural variability of the climate system are reduced.”

•    ”While  none  of  these  studies  has  specifically  considered  the
attribution issue, they often draw some attribution conclusions, for
which there is little justification.”

•  ”When will an anthropogenic effect on climate be identified? It is
not surprising that the best answer to this question is, ‘We do not
know.’”24

This all was to change. Sir John Houghton, chairman of Working Group
I, received a letter from the US State Department dated November 15, 1995,
and signed by then–Acting Deputy Assistant Secretary Day Olin Mount. He
reported  to  Undersecretary  of  State  for  Global  Affairs  Timothy  Wirth,  a
former senator (D-CO), close political ally of then-Vice President Gore and
ardent believer in man-made global warming. The letter said, “It is essential
that the chapters not be finalized prior to the completion of the discussions
at the IPCC Working Group I Plenary in Madrid, and that chapter authors
be prevailed upon to modify their text in an appropriate manner following
the discussion in Madrid.”25

The following year, Mount was appointed by President Bill Clinton to the
prestigious position of Ambassador to Iceland. Wirth was appointed to head
the United Nations Foundation. Both knew on which side their toast was
buttered.

The  Madrid  Plenary,  which  took  place  in  November,  was  a  political
meeting  involving  appointed  representatives  from  ninety-six  nations  and
fourteen  nongovernmental  organizations  (NGOs).  Participants  went  over
the “accepted” Chapter 8 text, line by line. That chapter, which should have
governed  the  entire  IPCC  report,  was  then  substantially  rewritten  to
advance a global warming campaign being waged by the UN, the NGOs,
and the White House.

In  May  1996,  after  the  report  was  released,  the  Chapter  8  conclusions
were startlingly different from the scientists’ accepted version. The Chapter
8 lead author, Ben Santer, from the US government’s Lawrence Livermore
National Laboratory, had excised denials of any scientific evidence of man-
made warming, replacing them with statements asserting just the opposite:
“The  body  of  statistical  evidence  in  Chapter  8,  when  examined  in  the
context of our physical understanding of the climate system, now points to a
discernible human influence on the global climate.”26

Mr.  Santer’s  changes  appeared  to  be  based  primarily  upon  two
unpublished  papers  he  himself  had  submitted,  which  had  not  been  peer-
reviewed at the time. However, a published paper he coauthored at about
the same time contradicted his Chapter 8 IPCC report insertions. That paper
concludes  that  different  estimates  of  three  natural  climate  variability
influences are inconsistent, and until that question is resolved, “it will be
hard to say, with confidence, that an anthropogenic climate signal has or has
not  been  detected.”  That,  in  fact,  is  very  much  in  line  with  the  original
Chapter 8 science conclusions.27

But  that’s  not  what  most  people  remember.  The  “discernible  human
influence” insertion, which reversed the entire IPCC climate science report,
purportedly ended all debate on this matter, providing an official foundation
for the UN-sponsored Kyoto Protocol to follow in 1997.28

The  revision  epitomizes  the  reality  of  political  intrusions  into  science.
The Wall Street Journal condemned the 1995 SAR revision in a July 1996
editorial, “Coverup in the Greenhouse.”29 The journal Nature, which unlike
the Wall Street Journal tended to favor a Kyoto Protocol rebuffed the IPCC
for  rewriting  Chapter  8  to  “ensure  that  it  conformed”  with  political
correctness. Former National Academy of Sciences president Dr. Frederick
Seitz  detailed  his  objections  to  the  illegitimate  rewrite  in  a  Wall  Street
Journal article titled “A Major Deception on Global Warming” on June 12,
1996.30

Challenging Processes

 

The  IPCC’s  activities  concentrate  on  tasks  assigned  by  the  WMO’s
Executive  Council  and  the  UNEP’s  Governing  Council  resolutions  and
decisions, along with priorities that support the UN Framework Convention
on  Climate  Change  process  guidelines.  Prominent  critics  have  called  into
question  both  those  activities  and  those  priorities.  As  noted  by  Dr.  Fred
Singer in the foreword of this book, one of these critics is Dr. Seitz, who
publicly denounced the SAR report, stating, “I have never witnessed a more
disturbing corruption of the peer-review process than the events that led to
this IPCC report.”31

Referring to the third report (TAR), Sir John Maddox, a former editor of
the journal Nature, observed, “The IPCC is monolithic and complacent, and

A  study  conducted  by  the  National  Center  for  Policy  Analysis,  a
nonprofit,  nonpartisan  policy  research  organization,  determined  that  the
IPCC’s  2007  AR4  report  clearly  violated  60  of  127  principles  associated
with sound forecasting methods, and only really followed 17 of those. As
reported by H. Sterling Burnett, author of the Washington Times story that
reported the study, “A good example of a principle clearly violated is ‘Make
sure forecasts are independent of politics’ … Politics shapes the IPCC from
beginning  to  end.  Legislators,  policymakers  and/or  diplomatic  appointees
select (or approve) the scientists—at least the lead scientists— who make
up  the  IPCC.  In  addition,  the  summary  and  the  final  draft  of  the  IPCC’s
Fourth  Assessment  Report  was  written  in  collaboration  with  political
appointees and [is] subject to their approval.”34

Commenting upon observations by Kesten Green and J. Scott Armstrong,
who  conducted  the  IPCC  audit,  Burnett  said,  “Sadly,  Mr.  Green  and  Mr.
Armstrong  found  no  evidence  that  the  IPCC  was  even  aware  of  the  vast
literature  on  scientific  forecasting  methods,  much  less  applied  the
principles.”35

it is conceivable that they are exaggerating the speed of [climate] change.”32
The UK House of Lords’ “Scientific and Economic Analysis Report” on
the IPCC for the G-8 Summit, July 2005, stated, “We have some concerns
about  the  objectivity  of  the  IPCC  process,  with  some  of  its  emissions
scenarios  and  summary  documentation  apparently  influenced  by  political
considerations.”33

Edward J. Wegman, a George Mason University professor who chaired
the  panel  of  audit  investigators,  concluded  that  based  upon  the  IPCC’s
flawed  statistical  analyses  and  procedures,  the  idea  that  the  planet  is
experiencing  unprecedented  warming  “cannot  be  supported.”  He  warned
that  policy  makers  should  take  this  into  account  before  enacting  laws  to
counter global warming that would have severe economic consequences.36

Some of the East Anglia CRU e-mails confirm pressures to provide clear
and  politically  compelling  IPCC  report  conclusions,  whether  or  not  they
were  supportable  by  solid  science.  Keith  Briffa  commented  in  one
exchange, “I know there is pressure to present a nice tidy story as regards
apparent unprecedented warming in a thousand years or more.” In another
he stated, “In reality the situation is not quite so simple” (based upon tree
ring research involving the illegitimate hockey stick charts he coproduced
with Michael Mann).37

In  September  2000,  Fillipo  Giorgi  of  the  International  Centre  for
Theoretical  Physics  in  Trieste,  Italy,  wrote  an  e-mail  stating  that  he  felt
pressure to cite model simulations that hadn’t yet been peer-reviewed. He
worried  that  this  demonstrated  an  unacceptable  relaxation  of  scientific
standards in which the IPCC rules “have been softened to the point that in
this  way  the  IPCC  is  not  any  more  an  assessment  of  published  science
which is not its proclaimed goal.” He added: “At this point there are very
little  rules  and  almost  anything  goes.  I  think  this  will  set  a  dangerous
precedent.”38

Roger  Pielke,  a  University  of  Colorado  political  science  professor,
believes that many IPCC participants want to compel action instead of “just
summarize science.”39 Andrew Weaver, a senior Canadian climate scientist
at the University of Victoria, agrees that IPCC leadership has allowed the
panel  to  advocate  for  action  on  global  warming  rather  than  serve  as  a
neutral science advisory body.

In  a  January  2010  interview  with  the  Canwest  News  Service,  Weaver
echoed published sentiments of other top climate scientists in the US and
Europe:  “There’s  been  some  dangerous  crossing  of  the  line.  Some  might
argue we need a change in some of the upper leadership of the IPCC, who
are  perceived  as  becoming  advocates.  I  think  that  is  a  very  legitimate
question.”40

Weaver  specifically  urged  that  IPCC’s  chairman,  Rajendra  Pachauri,
should  resign  and  that  other  officials  should  cease  being  “overly
enthusiastic”  in  pushing  policy  changes.  Even  the  activist  organization
Greenpeace  has  joined  a  push  for  Pachauri’s  ousting  to  benefit  IPCC
credibility. Greenpeace director John Sauven argued that “we need someone
held in high regard who has extremely good judgment and is seen by the
global public as someone on their side.”41

The  InterAcademy  Council,  an  Amsterdam-based  association  of  the
world’s  leading  academic  national  science  academies,  agreed  that  a
“fundamental  reform”  of  IPCC’s  management  structure  is  needed.  In  a
report released on August 30, 2010 following a review of IPCC practices
and  methodologies  leading  to  their  2007  report,  the  Council  found  two
types of errors. Its chairman, Harold T. Shapiro, stated that, “One is the kind
where  they  place  high  confidence  in  something  where  there  is  little
evidence. The other is the kind where you make a statement . . . with no
substantive  value.”  The  Council  also  found  the  IPCC  guilty  of  making  a

fraudulent claim that the Himalayan glaciers will be gone by 2035, stating
that, “IPCC was not paying close enough attention to what reviewers said
about this example.”42

There are also those who point to some apparent conflicts of interest on
the part of the IPCC’s chairman. Since Dr. Pachauri took control of the Tata
Energy Research Institute (TERI) in the 1980s, that large Indian company
has  vastly  extended  its  interests  in  virtually  every  kind  of  renewable  and
sustainable  energy  technology.  For  example,  its  Tata  Group  has  invested
$1.5 billion in a huge wind farm project. Another project, cofinanced by the
UK Department of Environment, Food and Rural Affairs and the German
Insurance  firm  Munich  Re,  is  studying  how  India’s  insurance  industry,
including Tata, can benefit from exploiting the supposed risks of exposure
to climate change.43

Some believe that Pachauri’s obstinate refusal to quit despite the heavy
weight of condemnation by even his own panel members may be having an
effect upon his mental stability. Responding to critics in a Financial Times
interview, he characterized global warming skeptics as “people who say that
asbestos is as good as talcum powder.” He also expressed hope that such
people would “apply it to their faces every day.”44

Ross McKitrick, who challenged and exposed the now-infamous hockey
stick  temperature  data,  believes  that  the  IPCC’s  scientific  failings  and  its
willingness to cross the line into advocacy will eventually percolate into the
public policy arena. He claims, “The halo has come off of the IPCC. At the
time of the 2007 report, there were very few politicians willing to question
statements from the IPCC. Now, as this plays out, people will start to be
embarrassed to cite the IPCC.”45

The Big Heist: Political Hijacking of Science

 

In essence, then, we’ve seen that the IPCC, the primary source of much
of what we hear about global warming, is not a scientific body. It is a UN-
sponsored  political  advocacy  mouthpiece  for  its  own  special  interests.  It
does  not  conduct  science;  it  conducts  politics.  It  performs  or  supports  no
original  research.  Rather,  it  invites  and  appoints  people  who  do  research,
along with others who don’t, to review reports that agree with fixed views
and agendas—and to ignore or even block findings that do not.

The global warming rubric has served as an ideal platform to enable the
UN to advance large philosophical visions, wealth distribution agendas, and
world  governance  goals  under  a  banner  of  global  environmentalism.
Dangerous  climate  change  and  attributing  its  cause  to  human  activities
serve  as  pretenses  for  a  much  broader  global  environmentalism  doctrine
aimed at defeating capitalism and free market choices.

If  this  sounds  a  bit  like  conspiratorial  paranoia,  let’s  review  the  words
spoken  by  then-President  Jacques  Chirac  of  France  in  a  2000  speech
supporting a key Western European Kyoto Protocol objective: “For the first
time,  humanity  is  instituting  a  genuine  instrument  of  global  governance,
one that should find a place within the World Environmental Organization
which France and the European Union would like to see established.”46

IPCC  Working  Group  II  “Summary  for  Policymakers”  reports  go  far
beyond  science,  offering  prescriptions  for  a  better,  more  equitable
distribution of wealth and resources. They explicitly point out that due to
environmental scarcity, cars and trains should be restricted to lower, more
efficient  top  speeds;  sails  should  be  emphasized  for  ships  to  save  fuel;
biomass should become the primary fuel source; and bicycle use should be
encouraged. Regionalized (smaller) economies should be created to reduce
transportation  demand; 
lifestyles  should  be  reoriented  away  from
consumption; and sharing resources should be emphasized, such as through
co-ownership.  Citizens  should  be  encouraged  to  pursue  free  time  over
wealth,  to  choose  quality  rather  than  quantity,  and  to  “increase  freedom
while containing consumption.” People should resist indoctrination by the
media  to  want  things  that  shape  their  values  and  identities.  The  media
should direct our paths toward a more sustainable world, raising awareness
among media professionals of the need for GHG mitigation and the role of
the media in shaping lifestyles and aspirations to encourage a wider cultural
shift.47

Irresponsible claims within IPCC summary reports have led to legal ones
in the form of junk lawsuits. In one, the plaintiffs asked defendant utility
plants to reduce their CO
 emissions throughout a wide area of the US. The
New  York  Federal  Appellate  Court  ruled  in  September  2009  that  this
regulation-by-judge could go forward even though the EPA is considering
such regulations as well.

In a second case, the plaintiffs alleged that global warming caused by the
 emissions released by fossil fuel–burning utility companies increased

2

CO

2

the ferocity of Hurricane Katrina. The class action suit seeks payment for
all storm area damages, and the Federal Appellate Court in New Orleans
ruled that the case can go forward.

An  Alaskan  Indian  tribe  filed  a  suit  in  San  Francisco  alleging  that  its
village will be destroyed by rising sea levels as glaciers melt due to global
warming. The tribe is seeking reimbursement costs from energy and power
company defendants because of its need to relocate. In this instance the trial
court dismissed the case.48

2

Finally, some US federal and state legislators are beginning to combat the
IPCC’s  scientific  abuses.  Senator  John  Barrasso  (R-WY)  has  called  for
Rajendra  Pachauri  to  resign,  stating  that  “new  scandals”  emerge  “every
day” about the “socalled facts” in the panel’s reports and that “the integrity
of the data and the integrity of the science [have] been compromised . . .
The scientific data behind these policies must be independently verified.”49

Unsurprisingly,  Senator  Inhofe,  the  ranking  member  of  the  Senate
Committee on Environment and Public Works, is joining Senator Barrasso
in calling for an investigation of the IPCC. Senator Inhofe has been a strong
and  vocal  critic  of  climate  fearmongering,  calling  man-made  global
warming “the greatest hoax ever perpetrated on the American people.” Prior
to  the  Climategate  scandal,  he  released  a  committee  report  titled  “More
Than 700 International Scientists Dissent Over Man-made Global Warming
Claims—Scientists Continue to Debunk ‘Consensus’ in 2008 and 2009.”50

Representative Blaine Luetkemeyer (R-MO) has introduced a House bill
that would cut US funding for the IPCC. He characterized the organization
as one “which is nothing more than a group of UN bureaucrats that supports
man-made claims on global warming that many scientists disagree with.”51

Texas  authorities  have  announced  that  the  state  is  taking  legal  action
against the EPA’s efforts to curb GHG emissions under the Clean Air Act.
In its filing, the state argued that the EPA based its decision on IPCC data.

According to the Associated Press, Virginia attorney general Kenneth T.
Cuccinelli  has  asked  the  EPA  to  delay  final  consideration  of  the
endangerment  finding  regarding  CO
  emissions  so  that  “newly  available
information” can be reviewed.52

In total, what has the IPCC’s review of climate change science reports
determined?  It  has  concluded  that  policy  makers  rather  than  the  free
markets should determine our economic desires and lifestyles. Since global
warming—the  basis  for  this  justification—has  no  boundaries,  the  UN

would  be  the  logical  world  seat  of  governance.  There,  politicians  from
around the world can jointly determine what is proper and fair for all of us.
And they received a Nobel prize for this?

Chapter 6

CONTRAILS OVER COPENHAGEN

Earth’s last chance before the next last chance.

The  Copenhagen  Summit  of  2009  was  billed  as  “the  Earth’s  last
chance.”  The  real  agenda,  however,  was  to  pressure  the  US  and  other
industrialized  countries  to  pay  retributions  to  less  fortunate  nations  for
excessive  energy  consumption.  This  was  not  the  first  time  that  an
international forum gathered to promote the cause of global warming. The
buildup to this ultimately hapless event began more than 20 years earlier.

A  period  of  global  cooling  that  ended  in  the  1970s  was  followed  by  a
warming surge. Losing no time, the UN, through its WMO and UNEP, not
only had established by 1988 that the warming was due to a “greenhouse
effect” but also, even more remarkably, had already determined that human
activities were substantially to blame. They pronounced that immediate and
drastic  reductions  were  needed  to  stabilize  conditions.  That  ominous

2

conclusion, in turn, provided the rationale for the UN to sponsor the huge
Conference on Environment and Development (UNCED), or Earth Summit,
in  Rio  de  Janeiro  in  June  1992,  where  participating  countries  began  to
negotiate 
“dangerous”
anthropogenic GHGs (principally CO
) at 1990 levels. The original deadline
for accomplishing this was 2000, and 154 nations agreed to sign on.

international 

agreements 

stabilizing 

for 

Yet no scientific data existed to serve as a sound basis for either those
danger  assertions  or  the  UN’s  motivation  to  establish  its  Framework
Convention on Climate Change in 1992, which stated that “human activities
have  been  substantially  increasing  the  atmospheric  concentrations  of
greenhouse  gases,  that  these  increases  enhance  the  natural  greenhouse
effect, and that this will result on average in the additional warming of the
Earth’s  surface  and  atmosphere  and  may  adversely  affect  natural
ecosystems and mankind.”1

No,  there  was  no  scientific  evidence  to  back  up  those  statements  or  to
justify the Kyoto Protocol, which the FCCC spawned to cap CO
 emissions
in developed countries while giving China and India a pass. There wasn’t
any  evidence  to  support  those  assumptions  in  February  2005  when  the
agreement went into force—and there still isn’t today.

2

The Road from Rio: Politics in the Fast Lane

 

Heads of state from dozens of countries that were concerned that global
warming  was  a  real  and  dangerous  threat  to  mankind  attended  the  Earth
Summit. The event, chaired by billionaire Canadian businessman Maurice
Strong, attracted an estimated forty thousand participants. Mr. Strong had
previously  served  as  secretary  general  for  a  1972  UN  Conference  on  the
Human Environment, and in 1992, he was executive director of UNEP. He
later  became  a  key  person  in  bringing  together  the  thousands  of
international bureaucrats, diplomats, and politicians who participated in the
Kyoto Protocol deliberations.

Mr.  Strong  has  an  interesting  and  active  background.  He  and  his  wife,
Hanne,  an  occultist,  had  earlier  established  a  global  headquarters  in  San
Luis Valley, Colorado, for the New Age movement called “Baca,” after a
mystic  informed  them  it  “would  become  the  center  for  a  new  planetary
order which would evolve from the economic collapse and environmental

catastrophes that would sweep the globe in the years to come.” Together,
the  Strongs  created  the  Manitou  Foundation,  which  brought  together
devotees  of  diverse  religious  sects,  both  traditional  and  mystical.  Actress
Shirley MacLaine’s astrologer told her to move there, and she did.2

The  Strongs’  Baca  Grande  ranch  sat  on  one  of  the  North  American
continent’s largest freshwater aquifers, from which Mr. Strong intended to
pipe water to the desert Southwest. The plan was abandoned due to protests
from environmental groups. Strong ended up with a $1.2 million settlement
from the local water company, yet retained rights to the water.

A 2005 inquiry into the corrupt UN “Oil-for-Food” program revealed that
nearly  $1  million  was  funneled  into  a  Strong-owned  family  company
account  by  Iraq  Foreign  Minister  Tariq  Aziz  through  a  North  Korean
contact.  His  purpose  was  to  persuade  the  UN  to  grant  Saddam  Hussein’s
government  certain  exemptions  from  an  export  ban.  Since  Kyoto,  Strong
had acted as a personal intermediary for UN Secretary General Kofi Annan
for  various  missions,  including  contacts  with  North  Korea’s  communist
regime.  He  had  also  maintained  close  friendships  with  top  Chinese
government  leaders  going  back  in  time  to  the  Cultural  Revolution  under
Mao Tse-tung. After his role in the scandal was revealed 8 years later, he
took up residence in a penthouse flat of a building occupied by UN agencies
in the Chinese capital.3

Strong was a major contributor to a 1987 Brundtland Commission report
titled “Our Common Future,” which had been sponsored by the UN’s World
Commission  on  Environment  and  Development.  The  WCED  is  broadly
credited  with  igniting  the  “green  movement”  and  popularizing  the  term
“sustainable  environment.”  Its  purpose  was  to  address  growing  concern
“about the accelerating deterioration of the human environment and natural
resources and the consequences of that deterioration for human and social
development.”

It  is  no  secret  where  Strong  placed  most  of  the  blame  for  that
deterioration.  He  has  complained  that  “the  United  States  is  clearly  the
greatest  risk  to  the  world’s  ecological  health.”  Furthermore,  he  clearly
stated  in  the  UNCED  August  28,  1991,  report:  “It  is  clear  that  current
lifestyles  and  consumption  patterns  of  the  affluent  middle  class  …
involving  high  meat  intake,  consumption  of  large  amounts  of  frozen  and
convenience foods, ownership of motor vehicles, small electric appliances,
home  and  work  place  air-conditioning,  and  suburban  housing  are  not

sustainable  .  .  .  A  shift  is  necessary  toward  lifestyles  less  geared  to
environmentally damaging consumption patterns.”4

Strong wrote the introduction to Beyond Interdependence: The Meshing
of the World’s Economy and the Earth’s Ecology, which was published in
1992  by  the  Trilateral  Commission,  a  private  organization  founded  by
David  Rockefeller,  chairman  of  the  UN  Council  on  Human  Relations  in
1973. In that introduction, Strong boasts, “This book couldn’t appear at a
better time, with the preparation for the Earth Summit moving into gear …
it  will  help  guide  decisions  that  will  literally  determine  the  fate  of  the
Earth . . . Rio will have the political capacity to produce the basic changes
needed  in  our  international  economic  agendas  and  in  our  institutions  of
governance.”5

And Rio later attempted to accomplish just that. Chairman Strong made it
quite clear to the Rio audience that he was an environmentalist at all costs:
“We may get to the point where the only way of saving the world will be for
industrial  civilization  to  collapse.”  This  was  a  Strong  beginning  in  that
direction for certain.

Timothy Wirth, then serving as undersecretary of state for global affairs
in the Clinton-Gore administration, seconded Strong’s statement. He left no
doubt  regarding  his  indifference  toward  protecting  scientific  integrity,
which became evident later in the 1995 SAR rewrite events: “We have got
to ride the global warming issue. Even if the theory of global warming is
wrong, we will be doing the right thing in terms of economic policy and
environmental policy.”6

Also speaking at the Rio conference, Deputy Assistant of State Richard
Benedick,  who  was  then  head  of  the  policy  divisions  of  the  US  State
Department, agreed: “A global warming treaty must be implemented even if
there is no scientific evidence to back the [enhanced] greenhouse effect.”7
Doesn’t that pretty much say it all with regard to agenda?

Wealth of Nations: An Unfair Advantage

 

The  UN’s  central  Kyoto  Protocol  theme,  codified  through  its  Rio
meeting agenda framework, revolved around a “common but differentiated
responsibilities” rationale that has remained eternally clear and constant. In
Rio, all the parties were in agreement about the following:

•    ”The  largest  share  of  historical  and  current  global  emissions  of

greenhouse gases has originated in developed countries.”

•  ”Per capita emissions in developing countries are relatively low [this

was before China and India changed that picture].”

•  ”The share of global emissions originating in developing countries

will grow to meet their social and development needs.”

The  treaty  that  was  negotiated  in  Kyoto,  Japan,  in  December  1997
opened for signature on March 16, 1998, and went into effect on February
16,  2005,  following  ratification  by  Russia  on  November  16,  2004.  As
discussed later, Russia’s decision to sign on had nothing to do with climate
issues. As of April 2008, a total of 178 countries and other governmental
entities had signed on and ratified its terms. Actual compliance, however,
has fallen far short of that number.

The  agreement  placed  virtually  all  responsibility  for  GHG  reductions
upon  “Annex  I”  (industrialized  countries),  particularly  those  among  them
listed as “Annex II” (developed countries)—a subset made up of members
of 
the  UN-sponsored  Organization  for  Economic  Cooperation  and
Development  established  in  1960.  The  Annex  I  signatories  agreed  to  (1)
reduce GHG emissions (particularly CO
) to targets below their respective
1990 levels by 2012 when the Kyoto Treaty expires; (2) purchase emission
credits  from  other  nations;  and  (3)  invest  in  sanctioned  conservation
measures. Annex II ratifiers are required to pay costs to assist developing
(exempt)  countries  in  reducing  emissions.  The  US  and  Australia,  both
prospective  Annex  II  members,  have  refrained  from  ratifying 
the
agreement.8

2

Kyoto Protocol: Annex I and Annex II Categories Industrialized

and Developed Countries

 

°Australia,  *Austria,  Belarus,  Belgium,  Bulgaria,  *Canada,  Croatia,
Czech  Republic,  *Denmark,  Estonia,  *Finland,  *France,*Germany,
*Greece, Hungary, *Iceland, *Ireland, *Italy, *Japan, Latvia, Liechtenstein,
Lithuania, *Luxembourg, Monaco, *Netherlands, *New Zealand, *Norway,
Poland,  *Portugal,  Romania,  Russian  Federation,  Slovakia,  Slovenia,
*Spain,  *Sweden,  *Switzerland,  Turkey,  Ukraine,  *United  Kingdom,
°United States, and separately, the *European Union

Annex I Industrialized Countries
*Annex II Developed Countries (Ratifying)
°Annex II Developed Countries (Non-Ratifying)

Although  it  sounds  very  official,  the  Kyoto  Protocol  agreement  terms
were drafted and approved by a global warming alliance involving NGOs
appointed  by  functionaries  of  the  United  Nations.  Neither  the  UN  nor  its
NGO appointees actually control any people or territories, or are headed by
publicly elected representatives. NGOs are much more prominent in Europe
than they are in this country. For example, the Climate Action Network in
Europe  is  a  group  of  more  than  365  NGOs  funded  by  the  European
Commission along with the Dutch and Belgian governments; the Climate
Action  Network  in  the  United  States  consists  of  about  forty  NGOs.  Still,
such  organizations  collectively  managed  to  mobilize  nearly  twenty
thousand attendees who traveled to Rio for the Earth Summit from all over
the  world.  Most  of  them  actually  attended  a  parallel  “cheerleaders”
conference  (an  NGO  forum  held  nearby),  and  only  about  twenty-four
hundred attended the actual summit as delegates.9

The  Kyoto  Protocol  built  upon  the  success  of  another  UN-sponsored
agreement, the Montreal Protocol of 1989, which focused world attention
on  reducing  manufactured  ozone-depleting  chemicals  (“CFCs”),  which
were  posited  as  causing  an  “ozone  hole”  over  Antarctica  that  had  led  to
higher incidents of skin cancer.10 Since that scare, there now appears to be a
trend  of  recovery  in  the  ozone  layer,  and  the  hole  may  soon  close  more
rapidly than reduced CFC levels would produce. Some research indicates
that this reversal may be caused, or at least assisted by, shifts in atmospheric
wind patterns.11

Green parties in Western European nations have expanded greatly since
the  1970s,  and  they  have  become  an  important  force  within  fragile
government political coalitions. Accordingly, the participation of thousands
of  environmental  activists  at  Rio  captured  great  political  attention  in  the
UK. Primary themes continue to be that cheap energy is the root cause of
technological  abundance  that  has  created  modern  “throw-away”  societies,
and  that  the  answer  is  to  turn  away  from  fossils,  replacing  them  with
cleaner solar and wind alternatives. Organic farming is also emphasized to
avoid  evils  posed  by  artificial  chemicals.  Paul  Ehrlich,  a  prominent
environmental scientist at Stanford University and author of the best-selling

book  The  Population  Bomb  (1968),  clearly  espouses  these  views,  and  he
attributes many of the world’s problems to “too many rich people.”12

The  late  Aaron  Wildavsky,  a  professor  of  political  science  at  the
University  of  California–Berkeley,  identified  a  close  connection  between
the  proclamation  that  global  warming  is  the  mother  of  all  environmental
scares and the ultimate goals of some  green  coalition  activists:  advocacy.
He  said,”Warming  (and  warming  alone),  through  its  primary  antidote  of
withdrawing  carbon  from  production  and  consumption,  is  capable  of
realizing  the  environmentalist’s  dream  of  an  egalitarian  society  based  on
rejection of economic growth in favor of smaller populations eating lower
on the food chain, consuming a lot less, and sharing a much lower level of
resources more equally.”13

Fossils and their CO

 emission progeny then become important targets,
due  to  their  central  connections  to  industrial  growth,  transportation,  and
modern life in general. This thinking also serves the interests of the United
Nations.  Applying  a  convenient  greenhouse  theory,  the  global  warming
scare  provides  an  ideal  way  to  expand  influence  and  power  through  an
ability to impose de facto rationing of scarce and vital resources.14

Of course, this is intended for the public good. As the FCCC’s Article 2
states, the objective is to “achieve stabilization of GHG concentrations in
the  atmosphere  at  a  level  that  would  prevent  dangerous  anthropogenic
interference with the climate system.” Yet nowhere in either the FCCC or
the Kyoto Protocol is it ever clarified what GHG levels are “dangerous,”
either  to  humans  or  to  the  ecosystems,  or  how  the  danger  claim  is  truly
justifiable.15

2

The US Senate Sends a Message: No Way!

 

The  US  Senate  recognized  that  proposed  Kyoto  Protocol  regulations
would  bring  disastrous  consequences  to  America’s  economy  and  took
action  to  kill  that  threat.  In  a  rare  spirit  of  solidarity,  the  Senate
unanimously passed (95-0) the bipartisan Byrd-Hagle US Senate Resolution
(S  Res  98),  which  made  it  clear  that  the  United  States  would  not  be  a
signatory  to  any  agreement  that  “would  result  in  serious  harm  to  the
economy  of  the  United  States.”  The  Senate  was  particularly  antagonistic

toward any agreement that didn’t include binding targets and timetables for
both developing and industrialized nations.

Then-President  Bill  Clinton,  no  stranger  to  political  pragmatism,
immediately got the message and never submitted a US approval request for
congressional  ratification.  You  can  bet  that  his  vice  president,  who  had
participated  in  Kyoto  Protocol  negotiations  on  behalf  of  Clinton’s
administration in 1997, wasn’t one bit happy about these developments.

Al  Gore  was  gearing  up  for  his  ultimately  unsuccessful  run  for  the
presidency in 2000, featuring environmental priorities as a big pitch point.
Then,  as  now,  he  promoted  global  warming  as  a  threat  to  humanity,
hawking cap-and-trade legislation as the road to salvation. The US Senate
rebuff of Kyoto, along with Gore’s own administration’s unwillingness to
pursue  ratification,  was  an  obvious  and  embarrassing  setback.  Yet  in
subsequent speeches, Gore never seems to clarify those real circumstances
during  his  global  publicity  forays,  suggesting  instead  that  Kyoto  was
“Bushwacked.” Perhaps he counts on the likelihood that his audiences are
too young to know differently, too old and senile to remember, or just too
indifferent to have been paying attention. In any case, the political tide—if
not the polar seas—had truly risen, and for a time had turned against him.

On June 11, 2001, a few months after taking office, President George W.
Bush  commented  publicly  regarding  his  views  of  the  Kyoto  Protocol,
calling  it  “fatally  flawed  in  fundamental  ways.”  He  reported  that  his
cabinet-level  Working  Group  on  Global  Warming  had  “asked  the  highly
respected National Academy of Sciences to provide us with the most up-to-
date information [on] what is known about and what is not known about the
science of climate change.”16  He  then  summarized  the  conclusions  of  that
working group:

•  ”First, we know the surface temperature of the Earth is warming. It
has  risen  by  0.6°C  [1.08°F]  over  the  past  100  years.  There  was  a
warming trend from the 1890s to the 1940s; cooling from the 1940s
to the 1970s; and then sharply rising temperatures from the 1970s to
today.”

•  ”There is a natural greenhouse effect that contributes to warming . . .
Concentrations of greenhouse gases, especially CO
, have increased
substantially since the beginning of the Industrial Revolution. And

2

the  NAS  indicates  that  the  increase  is  due  in  large  part  to  human
activity.”

•  ”Yet the Academy’s report tells us that we do not know how much
effect natural fluctuations in climate may have had on warming. We
do  not  know  how  much  our  climate  could  or  will  change  in  the
future.  We  do  not  know  how  fast  change  will  occur,  or  even  how
some of our actions could impact it. For example, our useful efforts
to  reduce  sulfur  emissions  may  have  actually  increased  warming
because  sulfate  particles  reflect  sunlight,  bouncing  it  back  into
space.  And,  finally  no  one  can  say  with  any  certainty  what
constitutes a dangerous level of warming, and therefore what level
must  be  avoided.  The  policy  challenge  is  to  act  in  a  serious  and
sensible way, given the limits of our knowledge.”17

From  the  beginning,  prominent  US  scientists  and  economists  have
opposed the Kyoto Protocol provisions. Some of those critics are from Al
Gore’s alma mater; a strongly critical review article titled “Problems with
the Protocol,” for instance, was published in the November/December 2002
issue  of  Harvard  Magazine.  Common  observations  were  that  the  Kyoto
Protocol  was  economically  inefficient,  nonobjective,  inequitable,  and
ineffective. One of the major failures cited is the exclusion of China, “the
largest  future  source  of  CO
  emissions”  (and  now  the  current  largest
source). Critics also argued that the agreement gave Europeans a massive
advantage  over  other  countries  in  reducing  CO
  emissions  below  1990
levels.18

2

2

The 2008 completion date mandated by the Protocol was recognized to
present  major  problems  for  the  US.  Economists  argued  that  the  typical
lifetime  of  a  power  plant  is  approximately  30  years,  and  the  average  US
automobile is on the road for about 11 years. Changing the energy economy
too rapidly by retiring equipment would be economically unproductive.

Other  disagreements  with  the  protocol  argued  against  the  postulated
environmental  advantages  that  would  result  from  GHG  restrictions.  For
example, it awards credit for planting forests to sequester carbon but does
so  in  a  way  that  provides  economic  incentives  to  destroy  wetlands,
potentially  creating  net  excess  CO
  releases.  It  also  doesn’t  set  long-term
goals for reduction in atmospheric CO
  concentrations.  Many  believe  that
the real effects upon climate change would be virtually nonexistent in any

2

2

case, because an estimated 2 to 3 percent emission reduction by 2050 would
be well within the margin of error, not to mention being trivial compared
with natural sequestration by the marine and terrestrial biosphere.

European Agendas: Hot Air and Smoke Screens

 

The  terms  and  conditions  put  forth  in  the  Kyoto  Protocol  have  been
strongly influenced and advocated by Western European governments that
have been greatly displeased with the United States’ unwillingness to buy
in. They have argued that the American refusal to ratify the protocol gives
this  country  an  unfair  economic  advantage  in  competition  with  other
industrialized countries and is unreasonable because of the high US GHG
emission levels as compared with theirs. Considering their own lack of any
real  progress  toward  meeting  those  emission  reduction  targets,  one  might
wonder  why  they  persist  in  championing  those  elusive  and  terrifically
costly  goals.  Many  contentious  disagreements  are  rooted  in  a  political
history and socialist philosophy that differ from ours in notable ways.19

One important difference between the US and many Western European
countries is the way political systems are structured and operate. Unlike this
country’s  two-party  system  with  winner-take-all  elections,  European
governments are most typically coalitions where minority parties, such as
“greens,”  can  wield  important  and  deciding  leverage  regardless  of  who
wins.  This  gives  minority  groups  with  special  agendas  real,  often
determinate power in the political arena.

Europe’s strongly socialist leanings saddle its populations with high tax
burdens  essential  to  support  large  welfare  programs  that  stunt  economic
investment and profitability essential for competition in global markets. In
contrast,  US  emphasis  upon  economic  growth  through  lower  taxes,  high
productivity, and strong employment levels is designed to support consumer
purchase  power.  It  is  only  reasonable,  at  least  from  a  Western  European
political perspective, to want to saddle the US with the high energy costs
associated  with  Protocol  compliance  that  will  help  level  the  field  of
international commerce.20

Selecting 1990 as the base year from which Kyoto emission reductions
are  to  be  measured  suspiciously  favors  several  European  countries  at  the
expense  of  US  interests,  placing  most  of  the  economic  burdens  upon

industries,  consumers,  and 

American 
taxpayers.  This  would  occur
regardless of whether any climate benefits were realistically achievable or
not.

While  US  emissions  today  are  higher  than  they  were  in  1990,  the
emissions  in  some  European  nations  are  actually  lower.  By  the  time  the
Kyoto Protocol was negotiated, German and British GHG emissions were
both  already  about  9  percent  below  1990  levels.  The  reunification  of
Germany  has  led  to  the  elimination  of  many  East  German  industries  that
were  huge  polluters,  lowering  their  emission  levels  below  the  1990
benchmark date. And the discovery of large natural gas fields in the North
Sea enabled Prime Minister Margaret Thatcher to break up the British Coal
Union in the 1980s and move the energy system to gas, phasing out large
segments of its coal industry. Yet, although Prime Minister Tony Blair later
proposed  to  reduce  CO
  emissions  60  percent  by  2050,  British  CO
emissions have actually increased more than 3 percent since 1997.21

The Kyoto Protocol provides a good excuse for European governments to
levy even higher taxes on oil for consumers in the laudable cause of saving
the  planet—  taxes  that  are  already  several  times  the  actual  cost  of  each
barrel.  Neither  current  lack  of  compliance  nor  prospects  for  even  higher
taxes,  however,  appears  to  have  substantially  dampened  general  Kyoto
popularity  in  Britain,  particularly  not  in  segments  of  its  scientific
community.22  For 
and
Commonwealth’s  national  academy  of  science)  recently  wrote  an  open
letter to the US oil company ExxonMobil demanding that it stop funding
global  warming  skeptics.  The  letter  particularly  mentions  the  negative
effects  of  such  skepticism  on  the  implementation  of  Kyoto  Protocol  CO
emission reductions.23

the  Royal  Society 

(the  UK 

example, 

2

2

2

Miraculous Conversion: Russia Gets Religion

 

The  Kyoto  Protocol  was  stalled  between  1997  and  2005  for  lack  of
sufficient signatories. Both the US and Australia had refused to ratify, for
somewhat  different  reasons.  The  US  objected 
the  unwarranted
destructive economic impacts and to the compliance exemptions extended
to China and India. Australia primarily objected to a condition that linked
GHG  emission  reduction  targets  to  per  capita  population  ratios,  which

to 

2

2

penalized them as an industrial country with relatively few people. Russia
had been another holdout, and the Europeans badly needed them to get on
board.

Originally,  Russian  president  Vladimir  Putin  had  announced  on
December 2, 2003, that his country would not ratify the protocol for reasons
similar  to  those  stated  by  President  George  W.  Bush  and  prominent
scientists  in  the  United  States.  Putin  observed  that  the  treaty  was
“scientifically  flawed”  and  that  “even  100  percent  compliance  with  the
Kyoto Protocol won’t reverse climate change.”24

The Russian Academy of Sciences presented scientific arguments against
signing  Kyoto  in  a  statement  issued  on  July  1,  2005.  It  noted  that  the
world’s  temperatures  do  not  follow  CO
  levels.  Instead,  the  academy
observed a much closer correlation between world temperatures and solar
activity than with CO
 levels. The Russian scientists had determined that sea
levels were not rising faster with warming; rather, they had been increasing
steadily about 6 inches per century since the Little Ice Age ended in about
1850.  They  discounted  one  of  the  most  significant  danger  claims  about
global warming—that tropical diseases would spread—noting that malaria
is  a  disease  encouraged  by  sunlit  pools  of  water  where  mosquitoes  can
breed,  not  by  climate  warmth.  They  also  pointed  out  the  lack  of  a
correlation between global warming and extreme weather, which a British
government  scientific  delegation  admitted  it  could  find  no  evidence  to
support.25

What,  then,  ultimately  caused  Putin  and  the  Russian  Duma  to  change
their position and ratify the protocol? It is widely speculated that Europeans
were  instrumental  in  getting  Russia  admitted  to  the  World  Trade
Organization  (WTO)  and  thus  categorized  as  a  developing  country  rather
than  a  developed  one  in  applying  the  protocol’s  regulations.  Russia  also
received  an  opportunity  to  sell  to  European  countries  billions  of  dollars’
worth of its former Soviet-era emission credits associated with former dirty
industries that had been casualties of economic meltdown. This would also
help Europe meet Kyoto’s first-phase requirements without actually cutting
emissions or energy use.

Europe’s 1990 CO

 emissions of 4,245 million tons fell to 4,123 million
tons in 2002 due to reductions in burning coal in both Britain and former
East Germany. Yet Kyoto Protocol requirements stipulated further European
Union (EU) cutbacks, to 3,906 million tons before 2012. A December 2003

2

UN report predicted that the EU would miss that reduction target by even
more than that amount, namely, by dropping an additional 311 million tons.
Since Russia’s 1990 emissions were 2,405 million tons and had fallen by
2001 to 1,614 million tons, they could sell up to 800 million tons of credits
to the Europeans at an “auction” price. This would be cheaper for Europe
than  shutting  down  fossil-fired  power  plants  or  removing  trucks  from  its
vital  transportation  infrastructure  by  escalating  already  high  diesel  fuel
taxes.

Incidentally,  the  United  States  would  not  be  given  comparable  breaks
such  as  those  accorded  to  the  Europeans  and  Russians.  First,  unlike
European  and  former  Soviet  countries  that  were  treated  as  separate
emission credit–trading entities, the US was treated as a single nation (no
credit exchanging between states to meet quotas). Second, the US emissions
in 1990 were not inflated to high target allowance levels as was the case in
Germany, Britain, and Russia, making compliance much more difficult to
achieve.26

Protocol Progress: Detours along the Road

 

2

Since  early  2005,  many  key  signatory  countries  have  found  that  their
CO
 emissions are increasing rather than diminishing. Problems in meeting
targets have become clear to many—some of whom realized this probability
before the Kyoto Protocol was even enacted.

Canada, which ratified the treaty on December 17, 2002, had agreed to
reduce emissions to 6 percent below 1990 levels between 2008 and 2012.
The  country  was  influenced  by  numerous  polls  indicating  high  levels  of
public  support  (about  70  percent  approval).  By  2003,  Canada’s  federal
government had already claimed to have spent or committed $3.7 billion on
climate change programs. Yet by 2004, its CO
 emission levels had risen to
27 percent above 1990 levels (compared with an increase by 16 percent in
the  US  during  that  time).  On  April  25,  2006,  Canadian  environment
minister Rona Ambrose announced that the country would have no chance
of meeting its targets.27

By  May  31,  2002,  all  fifteen  then-members  of  the  EU  had  deposited
ratification paperwork at the UN, accounting for about 22 percent of global
greenhouse emissions at the time. This called for a cut, on average, to about

2

8  percent  below  1990  levels.  In  response,  the  EU  created  an  emissions
trading  system  that  introduced  reduction  targets  in  six  key  industries:
energy,  steel,  cement,  glass,  brick  making,  and  paper/cardboard.  It  also
imposed fines on member nations that failed to meet obligations. There was
some  criticism  among  other  developed  nations  that  the  EU’s  target  level
was unfair because it enabled reductions in East Germany to cover nearly
the entire 15 percent goal. On June 28, 2006, the German government then
announced that it would exempt its coal industry from compliance.28

Between 1990 and 2004, greenhouse emissions reported by the UN had
increased in more countries than had experienced reductions. Included were
Greece  (+27  percent),  Ireland  (+23  percent),  Japan  (+6.5  percent),  and
Portugal (+41 percent). As of 2005, Japan was nearly 8 percent above its
1990  levels  and  considered  seeking  to  purchase  emission  rights  from
Russia.  Attempting  to  meet  its  obligation  any  other  way  might  have
reversed its decade-long recovery from an economic recession, thrusting the
nation back into a full-scale depression.29

Why  have  these  increases  occurred?  A  key  reason  is  because  fossils
continue  to  be  the  lifeblood  of  industry  and  commerce,  and  alternative
sources have not significantly offset growing net energy demands.

2

2

In the meantime, China has overtaken all other countries as the world’s
  emitter.  Exemptions  of  China  and  India  were  granted  on  the
largest  CO
rationale  that  even  large  developing  countries  have  not  historically
contributed  as  much  to  atmospheric  CO
  levels  as  have  developed,
industrialized nations; that emission levels should be calculated on a per-
capita  population  basis;  and  that  stringent  restrictions  will  handicap
economic  development  critical  for  those  nations’  social  well-being.  At  a
June  2005  G8  meeting,  Indian  prime  minister  Manmohan  Singh  repeated
the argument that per-capita emission rates are but a tiny fraction of those in
the  developed  world.  Adopting  the  Kyoto  principle  of  “common  but
differentiated  responsibility,”  India  agrees  that  the  major  responsibility  of
curbing  emissions  rests  with 
that  have
accumulated emissions over a long period of time.30

those  developed  countries 

By  2005,  China  and  India,  both  experiencing  rapid  industrial  and
economic growth, were making up for that lost time. China’s huge economy
had been expanding more than 8 percent per year, and India’s more than 5
percent—compared  with  about  3  to  4  percent  annual  US  growth  and
lagging economies and high unemployment in the EU.31

Neither  China  nor  India  welcomed  restrictions  that  would  limit  their
progress.  As  Lu  Xuedu,  deputy  director  of  China’s  Office  of  Global
Environmental  Affairs,  pointed  out,  “You  cannot  tell  people  who  are
struggling to earn enough to eat that they need to reduce their emissions.”32
A  gloom  regarding  Kyoto  Protocol  progress  descended  even  before
Russia’s ratification carried it into full force in February 2005. During the
tenth  Conference  of  Parties  that  was  held  in  Buenos  Aries  only  a  few
months earlier, science writer Ron Bailey believed it was already hopeless.

The  Kyoto  Protocol  is  dead—there  will  be  no  further  global
treaties  that  set  binding  limits  on  the  emissions  of  greenhouse  gases
after Kyoto runs out in 2012 . . . The conventional wisdom, that it’s the
United  States  against  the  rest  of  the  world  in  climate  change
diplomacy, has been turned on its head. Instead, it turns out that it is
the Europeans who are isolated. China, India, and most of the rest of
the developing countries have joined forces with the United States to
completely reject the idea of future binding greenhouse gas emission
limits.33

Italian environment minister Altero Matteoli had stated in Buenos Aires,
“The  first  phase  of  the  protocol  ends  in  2012;  after  that  it  would  be
unthinkable  to  go  ahead  without  the  United  States,  China,  and  India  .  .  .
Seeing as these countries do not wish to talk about binding agreements, we
must  proceed  with  voluntary  accords,  bilateral  pacts,  and  commercial
partnerships.”34

It had also become clear after the Kyoto Protocol was officially enacted
that  a  second  phase  of  the  treaty  beyond  2012  would  require  even  more
aggressive  steps  over  and  above  the  original  5.3  percent  cut  (from  1990
levels)  to  stabilize  atmospheric  CO
  concentrations.  For  the  most  part,
global industrial economies were growing, most particularly in much of the
Third World. Yet it was already apparent by 2005 that most industrialized
members would not even meet those first-phase emissions reduction targets
—not by a long shot. European businesses and their customers were already
experiencing rising energy and production costs that resulted from attempts
at  compliance.  Power  outages  were  also  beginning  to  occur,  and  many
bureaucrats were feeling heat of an unnatural kind.

2

Hot Economic Disputes: Stern Warnings

 

The global warming crisis has been promulgated as not only what I call
a  “warmaggedon” 
terms,  but  one  with  epic  economic
consequences as well. This prophetic view gained a great deal of traction,
particularly  in  the  UK,  thanks  to  a  government-sponsored  “study”  that
produced the politically intended alarmist results.

in  human 

In  July  2005,  then-Chancellor  Gordon  Brown  of  the  UK  (later  prime
minister,  and  strong  global  warming  theory  advocate)  asked  Sir  Nicholas
Stern, a former World Bank vice president, to lead a major review on the
nature of economic challenges associated with climate change. The seven-
hundred-page  report,  the  “Stern  Review  on  the  Economics  of  Climate
Change”  (hereafter  referred  to  as  the  Stern  Review)  was  released  on
October  14,  2006,  and  gave  Brown  just  what  he  seemed  to  want—
something that would really attract public attention to the matter. The report
also  got  its  author  a  lot  of  attention.  As  British  environmental  secretary
David Miliband observed, “Nick Stern is now an international rock star in
the climate change world.”35

The Stern Review was not inhibited by facts or caution in presenting its
conclusions.  It  warned  that  inaction  on  climate  change  will  result  in  a
depressed UK economy worse than the Great Depression of the 1930s, and
that the financial cost would be higher than that depression combined with
the  subsequent  two  world  wars.  In  human  terms,  resulting  droughts  and
flooding would displace 200 million people from their homes, creating the
largest  migration  in  history.  Natural  disasters  would  also  result  in  the
extinction of up to 40 percent of the world’s known species. To avert this
tragedy  we  would  collectively  need  to  spend  1  percent  of  global  gross
domestic  product  (GDP),  which  was  equated  with  about  half  of  what  the
World Bank estimates would be the cost of a full-blown flu pandemic.36

The grim, urgent news made headlines around the world. As summed up
by  the  New  York  Times,  “[It]  predicted  apocalyptic  effects  from  climate
change,  including  droughts,  flooding,  famine,  skyrocketing  malaria  rates,
and  the  extinction  of  many  animal  species.  This  will  happen  during  the
current generation if changes are not made soon.”37

From an economic standpoint, that news was mixed. The bad news was
that the overall costs and risks from climate change are equivalent to losing
at least 5 percent of global GDP now and forever, and possibly up to 20

percent.  But  alternatively,  strong  action  to  combat  these  losses  will  cost
only about 1 percent of the GDP—a real bargain!

As Upton Sinclair once observed, “It’s hard to get a man to understand
something when his job depends upon not understanding it.” Sir Nicholas
did his job superbly, a fact that has been recognized by climate authorities
more knowledgeable about the subject than he. But many other authors of
academic  papers  characterize  his  report  as  a  “political  document,”  often
applying  such  terms  as  “preposterous,”  “incompetent,”  “deeply  flawed,”
and “neither balanced nor credible” to the report’s conclusions.38 Among the
variety  of  criticisms  that  have  been  levied  against  the  Stern  Review  are
these:

•    ”The  report  fails  to  present  an  accurate  picture  of  scientific
understanding  of  science  change  issues  and  massively  exaggerates
prospective  impacts  of  global  warming  that  are  tilted  toward
unwarranted  alarm.”  (Stern’s  background 
is  economics,  not
science.)

•    ”Dangers  from  climate  change  and  benefits  of  action  are  vastly
inflated.  As  several  peer-reviewed  papers  point  out,  the  Stern
Review does not present new data, or even a new model. There is no
way to justify conclusions outside the normal range. Damages are
counted several times and sometimes arbitrarily increased eightfold
or more according to new and conjectured cost strategies that have
never been peer-reviewed.”

•    ”Costs  of  actions  are  vastly  underestimated;  implausibly,  costs  of
renewable fuels are projected to drop sixfold by 2050; and costs of
action beyond 2050 are not included, although they will continue to
escalate far into the 23rd century.”39

Mike Hume, a professor in the School of Environmental Sciences at the
University of East Anglia, commented that the “Stern Review is not the last
word of scientists and economists, it’s the last word of civil servants.”40

But then, who can really blame them? As Sir John Houghton, lead author
on the first three IPCC “Summary for Policymakers” reports, wrote in his
book,  Global  Warming:  The  Complete  Briefing  (published  in  1994),
“Unless we announce disasters, no one will listen.”

Warm Remedies: Comparing Pains and Gains

 

2

For  the  sake  of  examination,  let’s  make  three  assumptions:  (1)  that
dangerous levels of global warming are likely to occur; (2) that human CO
emissions are responsible for global warming: and (3) that Kyoto emission
reduction countermeasures are strictly adhered to by all ratifying countries.
Now let’s ask the follow-on question: How much difference will it make if
those  assumptions  are  true?  Scientific  studies  indicate  that  the  benefits
would  be  negligible  and  probably  too  small  to  even  measure.  Even  if  all
developed, industrialized nations that signed the Kyoto Protocol were able
to not only reduce their overall emissions by 20 percent below what they
would otherwise have been between 2008 and 2012, but also stick to those
reductions until 2050, the estimated temperature-lowering benefit would be
only about 0.1ºF. Then, by 2100, it would still be only about 0.3 degrees
lower. This would only postpone the projected temperature increase of 4.7
degrees by 5 years—to 2105 rather than by 2100.41

Even  those  tiny  delays  are  extremely  unrealistic.  First,  most  of  those
signatory  nations  are  not  coming  close  to  meeting  their  reduction  targets
presently,  and  it  would  become  even  more  difficult  for  them  to  do  so  as
populations and industrial production levels continue to grow. Second, as
countries  that  can’t  reach  their  reduction  targets  through  cutbacks  turn  to
purchases of excess emission rights from Russia and other countries, actual
reductions  are  largely  fictitious.  Effective  net  outcomes  will  probably  be
very tiny indeed. And if no other treaty replaces Kyoto after 2012, the total
effect will be to postpone global temperature increases about a week or less
by 2100. These estimates, based upon IPCC’s models, are why a November
6,  2004,  editorial  in  the  Washington  Post  refers  to  Kyoto  as  a  “mostly
symbolic treaty.”

Computer  models  that  estimate  cost/benefit  correlations  between  CO
cuts and climate changes have been around since the early 1990s. Most are
quite similar, and they have a couple of big problems in common. One is
that  even  current  models  can’t  begin  to  accurately  predict  temperature
changes associated with added or reduced atmospheric CO
 concentrations,
because  there  are  many  other  forcing  influences  and  interactions  that  are
poorly understood. A second is that benefits of higher temperatures, which
can  also  be  very  significant,  don’t  fit  into  the  preconceived  policy
strategies. Still, let’s continue with the economic projections anyway.

2

2

It  has  been  estimated  that  for  full  Kyoto  Protocol  implementation
(including US participation), the total cost over the coming century would
be  more  than  $5  trillion,  and  for  this  investment  any  influences  upon
climate  would  be  tiny  at  best  and  highly  speculative  altogether.  The  US
would bear most of this cost, about four times as much as Europe—not a
very good deal for us. That money can otherwise be spent on lots of other
things that have measurable consequences, such as education, public health
services, roads, Social Security benefits, tax relief, and yes, even foreign aid
to underdeveloped and developing nations.42

The scheme would, however, be a good deal for the Russians, who might
sell their old Soviet-era emissions credits to the US and Europe at a high
price, nearly $3 trillion. That’s a lot of money for hot air, and much of that
burden would fall on the backs of American taxpayers. Politicians, be afraid
—be very afraid—of the repercussions. All this for a theoretical and highly
unlikely lowering effect on global temperature of about 0.7ºF by 2100. This
assumes,  of  course,  that  the  cooling  trend  we  are  currently  experiencing
doesn’t accomplish this, and do so very naturally.

Contentions in Copenhagen

 

The Copenhagen Summit of December 2009 got off to a chilly start, but
that was only the beginning. Not even the GHG emissions spewed by more
than 1,200 limousines and 140 private jets that delivered 110 heads of state
and other distinguished participants seemed adequate to comfortably warm
the  political  atmosphere.  Called  “the  Earth’s  last  chance,”  this  fifteenth
United  Nations  FCCC  gathering  ultimately  proved  to  be  a  real,  not  a
mythic, disaster for the fifteen thousand attendees and their global warming
boosters.

Some  inauspicious  events  leading  up  to  the  meeting  may  have
contributed to that disaster. The CRU scandal had been exposed on global
media  outlets  just  weeks  before.  A  December  defeat  of  Australian  prime
minister  Kevin  Rudd’s  proposed  capand-trade  legislation  as  a  job-killing
bill was undoubtedly another disappointment.

While  developing  countries  called  for  a  demand  that  the  rich  ones
commit many billions of dollars to them and accept sharper emission cuts,
US  and  European  representatives  stated  that  their  nations  were  willing  to

provide their “fair share,” amounting to $10 billion per year from 2010 to
2012. This, according to Sudan’s UN ambassador Lumumba Stanislaus Di-
Aping,  would  not  be  nearly  adequate:  “[It]  would  not  buy  developing
countries’ citizens enough coffins.”43

George Soros agreed. During a press conference at the Copenhagen talks,
he said that the $10 billion proposal is “not sufficient,” and that the gaps
between what developing countries want and what developed countries are
willing to give “could actually wreck the conference.” Instead, he suggested
moving $100 billion from the International Monetary Fund (IMF), which is
being used for financial systems that have been bitten by a global economic
downturn, to help countries mitigate and adapt to climate change.

Discussions  were  temporarily  interrupted  as  representatives  of  several
undeveloped countries walked out of the meetings and angry riots broke out
in the streets over the social injustice of such paltry penance. Secretary of
State Hillary Clinton then came to the rescue, offering to up the ante with a
$100  billion  annual  contribution  from  the  United  States  and  our  more
prosperous friends to the “poorest and most vulnerable [nations] among us”
by  2020.  She  said  that  the  money  would  come  from  “a  wide  variety  of
sources, public and private, bilateral and multilateral, including alternative
sources  of  finance.”  Where  it  would  actually  come  from  no  one  knew,
including Hillary and her boss.

Judging  from  the  tumultuous  standing  ovation  following  a  speech  by
Venezuelan president Hugo Chavez, there was general agreement regarding
where to lay blame for the world’s social, economic, and climate problems.

•  ”If the climate was a bank, [the West] would have already saved it.”
•  ”The destructive model of capitalism is eradicating life.”
•    ”Our  revolution  seeks  to  help  all  people  …  Socialism,  the  other
ghost that is probably wandering around this room, that’s the way to
save the planet; capitalism is the road to hell … Let’s fight against
capitalism and make it obey us.”44

It is possible that an almighty force signaled approval of Chavez’s vision
by dumping 4 inches of snow on the Copenhagen delegation there to fight
global warming. Denmark’s maritime climate and winters are warmer than
those  of  its  Scandinavian  neighbors.  According  to  Henning  Gisseloe,  an
official at Denmark’s Meteorological Institute, there was “a good chance of

a white Christmas.” This hadn’t occurred in 14 years and has happened only
seven times during the last century. 45 But with all that hot air at the podium,
most of the attendees may not have noticed the blizzard outside.

China  offered  merely  to  reduce  its  “carbon  intensity  per  unit  of
production.”  Given  that  country’s  rate  of  growth,  total  emissions  will
nevertheless  double  over  the  next  decade  under  even  the  most  optimistic
scenario.46

to  stop  global  warming.  The 

Nevertheless, China did add to the dialogue by introducing the topic of
population control, one of the real agenda items hidden beneath the UN’s
movement 
in
Copenhagen  by  Chinese  delegate  Zhao  Baige:  “Population  and  climate
change are intertwined, but the population issue has remained a blind spot
when  countries  discuss  ways  to  mitigate  climate  change  and  slow  down
global  warming.”  She  did  not  mention,  however,  that  her  country  faces
what some have called a looming demographic crisis resulting from “family
planning practices,” with an aging population, a reduced workforce, and a
severe nationwide gender imbalance from sex-selective abortions.47

topic  was 

introduced 

Some  delegates,  such  as  Diane  Francis,  who  authored  a  broadly
circulated December 8 opinion article in the Canadian newspaper National
Post,  expressed  her  belief  that  imposing  China’s  one-child  policy  on  all
nations  is  just  what  is  needed.  This  would  reduce  the  current  world
population  of  6.5  billion  to  3.5  billion  by  2075.  And  just  prior  to  the
summit,  Britain’s  Optimum  Population  Trust  launched  a  carbon-offset
scheme. Participants who attended would be able to offset the 1.1 tons of
carbon  emissions  spewed  into  the  atmosphere  from  their  trans-Atlantic
flights  by  donating  $7  to  a  family  planning  program.  Apparently,  no
benefits  were  offered  to  those  who  traveled  by  bicycle  or  sailboat,  the
preferred travel modes recommended by the UN’s IPCC.48

One  week  prior  to  the  summit,  Jairam  Ramesh  told  India’s  Parliament
that the country would plan to reduce the ratio of pollution to production by
20 to 25 percent compared with 2005 levels, but like China, they would not
accept  a  legally  binding  emissions  reduction  target.  India  currently  ranks
fifth in the world in CO
 emissions, accounting for 4.7 percent of the total.49
President Obama arrived near the end of the meetings and confirmed that
global warming is real and the time for talking is over. Then, after talking
with  leaders  from  China  and  India  and  announcing  a  “breakthrough”  in
understanding,  he  returned  to  Washington  into  a  raging  record-breaking

2

snowstorm  that  covered  most  of  the  Eastern  Seaboard.  So,  on  second
thought, maybe that effort to stop global warming achieved some temporary
influence after all.

Where  does  the  road  from  Copenhagen  lead?  The  next  stop  is  a  2010
climate summit that will take place in Mexico City. It is, once again, the
Earth’s only chance.

involved 

2

A Different Copenhagen Consensus

 

first  panel 

In  2004,  Bjorn  Lomborg,  then-director  of  the  Danish  government’s
Environmental  Assessment  Institute,  conducted  a  project  cosponsored  by
his  government  and  the  Economist  newspaper.  The  project,  called  the
Copenhagen Consensus, invited some very smart people to indicate where
best  to  put  resources  to  solve  the  world’s  most  urgent  challenges,  based
upon  “rational  prioritization.”  The 
top-level
economists, including four Nobel laureates who were asked to suggest the
best solutions for a series of problems. For example, with global warming,
the solution might be CO
 taxes or the Kyoto Protocol; for malnutrition, it
might be agricultural research; and for malaria, it might be mosquito nets.
The experts were not just asked which solutions would be desirable; they
were also required to determine the dollar values and costs. They estimated
benefits of Kyoto for each of the positive impacts upon agriculture, forestry,
fisheries,  water  supply,  human  damage,  etc.,  and  they  estimated  costs
through losses of production. In the case of malaria solutions, the beneficial
aspects would be measured in terms of the assigned value of fewer dead,
fewer sick, fewer work absences, more robust populations with respect to
other diseases, and increased production. Malaria intervention costs would
be equated to dollars spent to purchase, distribute, and use mosquito nets.

The study asked the experts to prepare a summary global priority list of
challenges and opportunities divided into “very good,” “good,” and “fair”
categories according to the relative amount of benefit for each dollar spent.
“Bad”  opportunities—those  that  would  cost  more  than  their  value—were
also  listed.  Some  of  the  top  priorities  the  experts  listed  correspond  with
primary  risk  factors  that  have  been  identified  by  the  World  Health
Organization. Preventing HIV/AIDS turned out to be best. Each dollar spent
on  condoms  and  information  was  estimated  to  produce  about  40  dollars’

worth of social good (fewer dead, fewer sick, less social disruption, etc.),
with $27 billion saving 28 million lives over the coming years.50

The panel placed climate change opportunities at the bottom of the list
under the “bad” category. Of these bad opportunities, Kyoto ranked second
to the last, just below an “optimal carbon tax ($25–$300).” In other words,
Kyoto would end up doing very little good for the world relative to costs.

The  Copenhagen  Consensus  study  then  invited  eighty  college  students
from  all  over  the  world  to  assess  top  global  priorities  through  a  5-day
workshop  discussion.  This  group  included  representatives  from  the  arts,
physical sciences, and social sciences, with an equal number of young men
and women and with 70 percent from developing countries. After meeting
with  world-class  experts  on  each  of  the  major  challenge  and  opportunity
categories,  the  students  arrived  at  conclusions  that  were  very  similar  to
those of the first group. Malnutrition and communicable diseases ranked as
top priorities, and climate change was next to last.

It didn’t end there. In 2006, the project was conducted again, this time
involving  a  wide  range  of  UN  ambassadors  in  the  poll.  In  addition  to
participants from the three largest countries—China, India, and the United
States—representatives  of  nations  as  diverse  as  Angola,  Australia,  and
Azerbaijan  participated,  along  with  Canada,  Chile,  Egypt,  Iraq,  Mexico,
Nigeria, Poland, Somalia, South Korea, Tanzania, Vietnam, Zimbabwe, and
many others. This political group was considered to be more difficult to pin
down to firm conclusions because they tended to prefer treating all issues as
equal priorities—wanting to solve everything with unlimited financing. But
they ultimately did make choices. And those choices closely matched those
arrived  at  by  the  other  groups  of  2004.  Communicable  diseases,  clean
drinking  water,  and  malnutrition  ranked  highest.  Again,  climate  change
dragged along near the bottom.51

Climate Change: Politics of Planetary Peril

 

If  the  results  of  the  Copenhagen  Consensus  seem  surprising,  why  is
that? Is it true that most people’s priorities don’t include a global warming
fix? If so, then why is global warming considered to be such a big deal? Is it
because we are being told over and over that we are facing a climate change
crisis,  but  find  it  difficult  sometimes  to  remember  what  it  is  that  is  so

frightening—except that it must be something really, really bad? Didn’t it
have something to do with New York City becoming New York Atlantis …
and exhausted, drowning polar bears … and, oh yeah, the hurricanes?

It is difficult to imagine a time in recent history when so much political
hype has swirled around so little substance. Is it logical to wager trillions of
dollars based upon flawed science practices and suspect agendas?

2

Consider, for example, the momentum of Kyoto and Copenhagen. Was
the UN’s Framework Convention on Climate Change seriously motivated
by  concern  about  global  warming  causing  rising  oceans  due 
to
anthropogenic GHGs, only about a decade after scientific conjectures about
a coming Ice Age? Did IPCC summary reports have solid scientific bases
on which to support their conclusions, or were their assertions based upon
admittedly unreliable climate models that global warming would continue
to present global threats throughout the century unless CO
 emissions were
dramatically reduced? Were Kyoto Protocol emission-cutting terms, based
upon  IPCC  conclusions,  significantly  influenced  by  particular  economic
interests  of  European  Union  members,  along  with  China  and  India?  Did
Russia  have  a  religious  epiphany  concerning  Kyoto  ratification  that
reversed its skepticism about global warming importance, which coincided
with Europe’s invitation to join the World Trade Organization and market
Soviet-era emission credits? And is there a logical basis for the US, or any
country, to subject citizens to the economic burdens that Kyoto compliance
would impose, when any climate benefits would be immeasurably small?

Who  will  pay  the  ultimate  costs  of  fighting  the  unnecessary  and
unwanted  war  against  climate  change?  You  will.  Your  children  and
grandchildren will. People who can least afford them will. And these costs
won’t  be  cheap.  Yale  University  economist  William  Nordhaus,  who  is
probably the best authority on this subject, estimates that the first phase of
Kyoto would cost about $716 billion, with the US, if participating, bearing
two-thirds of the global burden.  52 We can be very certain about one thing
however: That’s only the beginning.

Section Three

Carbon Demonization Scams

Chapter 7

CAP-AND-TAX DAISY CHAIN

Follow the money.

Cap-and-trade  legislation,  a  major  Obama-Biden  administration
priority,  has  no  defensible  purpose  without  a  supporting  global  warming
crisis rationale. It also makes no sense from an economic standpoint. It will
place  onerous  cost  burdens  upon  energy  consumers,  continue  to  drive
businesses  overseas,  and  offer  no  real  climate  or  environmental  benefits
whatsoever. The same consequences apply in the event that CO
 emissions
come to be regulated by the EPA through the auspices of the Clean Air Act.
Such action would serve as a carbon-rationing precedent that paves the way
for cap-and-trade to follow.

Government  restrictions  upon  carbon  emissions  are  being  promoted  on
the basis of three errant and deceptive premises: (1) that they will help to

2

2

protect  our  planet  from  dangerous  climate  change  and  pollution;  (2)  that
they  are  needed  to  wean  the  United  States  and  the  world  away  from
excessive  energy  consumption;  and  (3)  that  they  will  incentivize  energy
technology  and  conservation  innovations  that  will  lead  to  independence
from foreign oil.

The  initial  premise  is  wrong  on  two  accounts.  First,  there  is  no  real
evidence  of  any  human-caused  climate  crisis.  Second,  there  is  no  real
evidence  that  any  attempts  to  reduce  atmospheric  CO
  emissions  would
have any significant climate influence. Simply because the EPA, parroted
by media propaganda, condemns CO
 as a “pollutant,” that does not make it
so. Such a declaration only misleads people and confuses this natural and
essential molecule with real pollutants that truly should be restricted.

2

The  second  premise,  that  carbon  restrictions  are  necessary  for  energy
consumption  control,  follows  the  ideological  agendas  of  the  UN  and  its
IPCC. Specifically targeted at the US and other affluent industrial countries,
the  restrictions  are  intended  to  artificially  drive  up  energy  costs  to  levels
that  curtail  consumption-based  capitalism.  The  burdens  of  this  zero-sum-
gain  strategy  will  fall  heaviest  upon  population  segments  that  can  least
afford them.

The third premise, that carbon penalties attached to fossil-fueled utilities
will incentivize alternative technology innovations, is misleading in several
respects.  Heavily  financed  promotions  fail  to  inform  the  public  of  the
limited-capacity  potentials  afforded  by  “renewable”  energy  sources,  most
particularly  in  regard  to  the  urgent  time  frames  required  to  substantially
offset  demands.  Unfounded  technology  promises  provide  excuses  for
expanding  government  control  and  spending,  unwarranted  mandates,
subsidies,  and  profit-taking  fortunes  for  those  who  play  the  system.  Free
markets built upon delivery of competitive values are compromised when
government is empowered to pick the winners and losers through policies
that  reward  promises  over  performance.  We,  the  taxpayers  and  captive
consumers, cover the costs.

Carbon Cap-Trap

 

In case you’re not very familiar with the way cap-and-trade works, here
is a very basic description. It enables fossil fuel–dependent corporations to

2

promote  themselves  as  being  “carbon  neutral”  by  purchasing  “carbon
offsets” from other entities in the form of emissions reductions elsewhere,
or by claiming that they are achieving CO
 absorption by planting trees to
offset their “carbon footprints.” You might liken the concept to the sale of
indulgences  by  medieval  churches  through  divine  authority.  A  more
contemporary illustration would be to imagine that someone in prison offers
to pay for some of your “good behavior” credits, literally as a “get-out-of–
jail ticket.” The central question would be, how much do you think it would
really  reduce  crime?  As  former  Clinton-Gore  administration  employee
Joseph  Romm  characterized  the  legislative  ploy,  “The  vast  majority  of
offsets are, at some level, just rip-offsets.”1

Unlike futures markets that can be defended as a means to secure long-
term investments essential to help stabilize volatile energy and food prices,
trading of carbon credits involves creation of a market that arbitrarily prices
a  fictitious  commodity  that  has  no  value  whatsoever.2  Such  a  market  can
exist only as long as fear of global warming crises can be perpetuated by
special interest agendas. It requires that government legislation be enacted
to  ration  emissions  at  compliance  levels  that  give  carbon  a  trade  value,
albeit a negative one, so that allowances can be sold by those who don’t
need them to others who have run out of forgiveness coupons. Energy and
product consumers transform the negative carbon commodity into positive
cash benefits for both sides through higher prices.3

Then  there  is  the  matter  of  continued  expansion  of  government
legislative  interference  in  free  market  operations.  How  might  this  impact
volatility? Uncertainties and rumors about government policy changes have
major  market  impacts,  so  just  think  about  the  added  volatility  a  new
derivative carbon bureaucracy could create. When that market tanks, who
will bail out the losers? Any guesses?

Before  cap-and-trade  legislation  is  enacted,  consider  some  important

lessons from the Kyoto Protocol.

1.  Most of the signatories have not found it possible to comply, even

though the 1990 benchmark date gave the EU every advantage.

2.    Trading  conditions  between  corporations  would  be  no  different
from Russia peddling its old Soviet-era credits to other countries—
with no net reductions.

3.  Assuming that full compliance was achieved, which is extremely
unlikely, the climate change impacts would be too small to measure.
And even the most optimistic CO
 reduction goals are founded upon
highly speculative assumptions that climate change is unnatural, that
man-made  GHGs  are  a  principal  cause,  and  that  consequences  of
continued warming (if that happens) are worse than the alternative
—a colder world. History suggests otherwise.

2

It may be interesting to note that the United States has indeed made real
progress  in  energy  economies,  along  with  emissions  reductions  as  a  by-
product. Based upon the amount of energy used to produce a dollar value in
output, this country reduced energy intensity by 20 percent over the period
from  1992  to  2004,  compared  to  only  11.5  percent  in  the  EU  under  a
mandatory  approach.  This  has  also  enabled  economic  growth,  which
averaged  more  than  3  percent  annually  between  1992  and  2005  as
compared with about 1 percent in the EU.4

The  federal  Energy  Information  Administration  (EIA)  reported  in  May
2010 that GHGs fell 7 percent during 2009, the largest-ever percentage and
absolute decline since the EIA began tracking such data in 1949. In fact, the
US  carbon  footprint  has  shrunk  in  three  of  the  last  four  years.  The  2009
decline was particularly dramatic and was attributed to a severe economic
downturn. It took a 3.3 percent drop in per capita GDP and a 4.8 percent
decline  in  overall  energy  consumption  (9  percent  in  industry)  to  produce
most  of  this  circumstance.  Expanded  switching  by  electric  utilities  from
more carbon-heavy coal to natural gas is also believed to have had some
influence.  Yet  it  should  always  be  recognized  that  economic  health  and
energy  use  are  tightly  linked,  as  well  as  that  oil,  coal,  and  other  fossils
continue  to  make  up  about  83  percent  of  America’s  energy  mix.  Cap-
andtrade  and/or  EPA-imposed  restrictions  on  carbon  will  have  costly
economic and social consequences.5

Big Deals

 

Some companies would definitely benefit from cap-and-trade, a policy
that  was  supported  by  one  of  the  strongest  corporate  US  boosters  of  the
Kyoto Protocol. Enron, a major natural gas distributor, recognized that the

approach would kill coal-fired electricity production and provide its energy
traders opportunities to capitalize on big trading commissions. An internal
Enron memorandum initiated by its head, Kenneth Lay, stated that Kyoto
would  “do  more  to  promote  Enron’s  business  than  almost  any  other
regulatory initiative outside the restructuring [of] the energy and natural gas
industries in Europe and the United States.”6

Enron had a major influence on events that led up to proposed cap-and-
trade legislation in the United States. So did Al Gore and some other people
and organizations that collaborated with them.

Back  in  the  1990s,  Enron  was  diversifying  its  energy  business  to
emphasize natural gas. The company had already owned the largest natural
gas  pipeline  that  existed  outside  Russia,  a  colossal  interstate  network.
Natural gas was having difficulties competing with coal, and the company
needed  help  in  Washington  to  tip  the  playing  field.  Hype  about  a  global
warming  crisis  advanced  by  then-Senator  Gore’s  1988  congressional
hearings on the topic provided a dream opportunity, and Enron hired Gore’s
star  witness,  James  Hansen,  as  a  consultant.  They  also  began  direct
discussions with Senator Gore.7

Some  Democrats  in  Congress  were  already  aggressively  pursuing
development of green legislation models. Senators John Heinz (R-PA) and
Timothy  Wirth  had  previously  cosponsored  “Project  88”  to  provide  a
pathway  for  converting  environmental  issues  into  business  opportunities.
Media-fueled alarm about acid rain provided a basis for legislation to create
markets  for  buying  and  selling  excess  sulfur  dioxide  (SO
)  and  nitrogen
dioxide emission credits, and Project 88 became the Clean Air Act of 1990.
Enron was a big SO

 market cap-and-trade player.

So  Enron  and  others  wondered,  why  not  do  the  same  thing  with  CO
?
Because natural gas is a lower CO
 emitter than coal is, that development
would certainly be a profitability game changer. But there was a problem.
CO
 wasn’t a pollutant. At least it wasn’t considered to be then, and the EPA
had no authority to regulate it.

After Senator Wirth became undersecretary of state for global affairs in
the  Clinton-Gore  administration,  he  began  working  closely  with  Enron’s
boss,  Lay,  to  lobby  Congress  to  grant  the  EPA  authority  to  control  CO
.
Between  1994  and  1996,  the  Enron  Foundation  contributed  nearly  $1
million to the Nature Conservancy, and together with the Pew Center and
the Heinz Foundation, they engaged in an energetic and successful global

2

2

2

2

2

2

warming fear campaign that included attacks on scientific dissenters.8 Yes,
that  is  the  exact  same  Heinz  Foundation,  headed  by  Teresa  Heinz  Kerry,
that gave a $250,000 award to James Hansen, who then publicly supported
her husband in his failed presidential bid.

A September 1, 1998, letter from Enron CEO Lay to President Clinton
requested that he “moderate the political aspects” of the climate discussion
by  appointing  a  “Blue  Ribbon  Commission.”  The  intent  of  the  proposed
commission, which was billed as an “educational effort,” was clear: to trash
disbelievers and cut off debate on the matter. Lay had direct contact earlier
with  the  White  House  when  he  met  with  Clinton  and  Gore  on  August  4,
1997,  to  prepare  a  strategy  for  the  upcoming  Kyoto  conference  that
December.  Kyoto  was  the  first  step  toward  creating  a  carbon  market  that
Enron desperately wanted Congress to support.9

Carbon Brokers: Pros and Cons

 

Cap-and-trade  pressure  on  US  legislators  extends  beyond  our  borders
and  also  involves  state  government  proponents.  The  International  Carbon
Action  Partnership  (ICAP),  established  in  October  2007,  operates  as  an
“open forum” comprised of state, regional, and international authorities and
governments that have pursued or are actively pursuing mandatory cap-and-
trade  systems.  Members  include  the  EU,  Australia,  New  Zealand,  and
Norway,  along  with  several  US  state  governments  and  the  Canadian
provinces of British Columbia and Manitoba. ICAP’s organizing purposes
are to enable members to learn from one another how to create a consistent
regulatory  framework  across  national  borders  and  how  to  develop  future
linked markets.10

Several  entities  have  become  well  positioned  to  capitalize  upon  those
markets. Al Gore’s Generation Investment Management LLP, for example,
is  a  London-based  firm  established  in  2004  that  invests  money  from
institutions  and  wealthy  investors  that  are  “going  green.”  GIM  plans  to
purchase CO
 offsets as soon as federal government regulations are passed
to  mandate  cap-and-trade.  Gore’s  cofounding  partners  in  the  venture  are
former chief of Goldman Sachs Asset Management (GSAM) David Blood,
along  with  Mark  Ferguson  and  Peter  Harris,  also  of  Goldman  Sachs.
Bloom-berg  reported  in  March  2008  that  the  investment  fund  had  hit  its

2

hard cap of $5 billion and had been turning away investors.11 Now many of
those  investors  may  be  running  away  after  suffering  big  losses  due  to
shifting political winds in Washington, DC.12

Another  organization  with  friends  in  very  high  places  is  the  Chicago
Climate Exchange (CCX), which was created in 2003 as a “voluntary pilot
agency”  and  aspires  to  be  the  New  York  Stock  Exchange  for  carbon-
emission  trading.  The  CCX  was  initiated  in  2000  with  support  from  a
$347,000  grant  to  Northwestern  University’s  Kellogg  Graduate  School  of
Management from the Chicago-based Joyce Foundation for a study to test
the  viability  of  a  future  carbon-credit  market.  This  transaction  occurred
when a young community organizer, Barack Obama, served on the Joyce
Foundation’s board of directors, along with his mentor and present White
House adviser, Valerie Jarrett. A current CCX board member is none other
than  Al  Gore’s  longtime  pal  and  Rio  de  Janeiro  Earth  Summit  leader
Maurice Strong. Another is Stuart Eizenstat, who led the US delegation to
Kyoto.

The  Joyce  Foundation  has  a  history  of  funding  liberal  causes.  For
example,  it  has  been  a  big  financial  supporter  of  legal  scholarship  to
demonstrate  that  the  Second  Amendment  of  the  US  Constitution  doesn’t
protect  individual  gun  ownership,  and  it  has  made  contributions  to  the
Center for American Progress and George Soros’s Tide Foundation. Total
Joyce Foundation start-up contributions for CCX were about $1.1 million,
and  its  president,  Paula  DiPerna,  later  left  the  organization  to  become
executive vice president of CCX.

CCX was cofounded by Richard Sandor, a former research professor at
Kellogg  when  they  received  the  Joyce  grant,  and  former  Goldman  Sachs
CEO  Hank  Paulson.  Sandor  has  received  8  million  shares  of  CCX  stock,
which  are  now  estimated  to  be  worth  about  $260  million  even  before  a
national  cap-and-trade  system  is  in  place.  GSAM  is  the  biggest  CCX
shareholder  (about  18  percent),  and  Al  Gore’s  GIM—with  his  three
Goldman Sachs cofounders—is fifth largest (about 10 percent). Mr. Sandor
has projected that CCX will become a $10 trillion company by 2050 with
passage  of  cap-and-trade  legislation.  That  would  certainly  afford  a  very
lucrative investment payback.13

Goldman  Sachs  is  also  heavily  invested  in  the  Obama  presidency.
According  to  figures  released  by  the  Federal  Election  Commission  to  the
Center for Responsive Politics, Goldman’s political action committee and

its  individual  contributors  were  the  campaign’s  second  largest  donors
($994,795).14

So far, however, CCX has a long way to go before delivering on investor
expectations. Trading commenced at $1 per cubic metric ton of carbon in
January 2008, reaching a $7 per metric ton peak in May of that year. The
market  (along  with  GIM’s  investment  value)  then  plummeted  to  $0.10  in
October  2009.  The  early  May  2008  speculators  lost  98.6  percent  of  their
investment.15

The  actual  operating  system  for  CCX  trading  has  been  provided  by
deposed  former  Fannie  Mae  head  Franklin  Raines,  who  originally
purchased  the  unpatented  technology  rights  developed  by  the  late  Carlin
Bartells.  Raines,  who  received  $90  million  in  salary  and  bonuses  over  5
years,  had  became  an  expert  in  bundling  worthless  real  estate  mortgages
that  led  to  the  near  collapse  of  the  US  economy.  This  serves  as  an
indispensible talent for bundling worthless air credits. Fortuitously, a patent
was  issued  for  the  technology  on  November  7,  2006,  the  day  after
Democrats swept the congressional elections.

CCX  member  organizations  include,  among  other  companies,  the  Ford
Motor Company, Amtrak, DuPont, Dow Corning, American Electric Power,
International  Paper,  Motorola,  and  Waste  Management,  along  with  the
states  of  Illinois  and  New  Mexico,  seven  cities,  and  a  number  of
universities.  As  planned,  these  members  would  “purchase”  carbon  offsets
on the CCX trading exchange and make contributions to or investments in
organizations that provide “alternative” or “renewable” energy.16

Almost  from  its  inception,  CCX  has  had  a  strong  connection  with  the
Atlanta-based  Intercontinental  Exchange,  Inc.  (ICE),  whose  subsidiary  is
the International Petroleum Exchange (IPE), the world’s largest petroleum
futures options market. In May 2010, ICE agreed to purchase the CCX and
its  parent  company,  Climate  Exchange,  along  with  two  of  its  other
exchanges (Chicago Climate Futures and European Climate Exchange), for
$603 million. While this amount is lower than the more than $1 billion ICE
reportedly paid for the New York Board of Trade in 2007, it still represents
a healthy valuation of fifty-eight times earnings.17

ICEcapades

 

During a May 8, 2006, Senate Democratic Policy meeting, Senator Carl
Levin  (D-MI)  stated  that  futures  speculation  trading  on  ICE  had  been  a
driver  for  adding  $20–$25  to  the  price  of  every  barrel  of  oil,  causing
hardship  to  industry,  households,  and  underdeveloped  nations.18  Senator
Levin’s website reported that in 2007, his Subcommittee on Investigations
released a report titled “Excessive Speculation in the Natural Gas Market,”
which found that a single hedge fund named Amarath, trading through ICE,
had  dominated  the  natural  gas  market  during  the  spring  and  summer  of
2006.  The  report  concluded  that  an  “Enron  Loophole  Act”  had  enabled
unregulated  trading,  which  increased  hedging  costs  that  increased  winter
gas purchases of the Municipal Gas Authority of Georgia by $18 million
alone.  Levin’s  subcommittee  reported:  “Amarath’s  massive  trades  turned
the natural gas market into a giant electronic casino, where all natural gas
buyers  and  sellers  were  forced  to  bet  either  with  or  against  Amarath.
American  businesses  and  consumers  were  socked  with  higher  prices  for
natural  gas  last  winter  as  a  result.  We  cannot  afford  to  let  large  energy
traders  continue  to  play  speculation  and  manipulation  games  with  US
energy prices and supplies. It’s way past time to close the Enron loophole
and put the cop back on the beat in all US energy markets.”19

Richard Sandor—the cofounder and chairman of CCX, now chairman of
the Climate Exchange, and ICE board of director since 2002—is considered
to be one of the fathers of derivatives and futures. He concocted weather
futures,  earthquake  futures,  Ginnie  Mae  futures,  and  others.  He  has  also
served  as  a  director  on  the  board  of  the  London  International  Financial
Futures and Options Exchange, the largest trading market in London.20

On  September  7,  2004,  ICE  released  an  announcement  with  this
headline:  “CCX  and  IPE  Sign  Corporation  and  Licensing  Agreement  for
EU  Emissions  Trading  Scheme/Chicago  Climate  Exchange  Sales  and
Marketing  Subsidiary  to  Be  Based  in  Amsterdam.”  That  arrangement
created  the  European  Carbon  Exchange  (ECX)  as  a  CCX  wholly  owned
subsidiary.  According  to  Sandor,  “This  agreement  positions  CCX  as  a
global leader in emissions trading, and complements IPE’s leadership in the
European energy markets.” Carbon offsets have now been trading on ECX
since 2005.21

As  a  result,  arrangements  are  already  in  place  and  waiting  for  the  US
Congress to create a lucrative and consumer-costly carbon-trading market
in the United States. Investment advocates are ecstatic about the windfall

profit  prospects.  Sandor  predicts  that  CO
  will  become  the  largest
commodity traded in the world market as governments curtail emissions of
GHGs “that scientists say accelerate global warming.”22

2

It’s a Small World After All

 

ShoreBank,  a  small  Chicago  bank  that  nearly  went  bankrupt  from
subprime mortgage fiascos during the depths of the recession, was saved by
$35  million  in  taxpayer  TARP  bailout  money,  along  with  additional
financial  assistance  from  powerful  friends.  Included  were  the  Joyce
Foundation,  some  big  Wall  Street  firms  such  as  Goldman  Sachs,  and
influential private partners. It is now heavily invested in a variety of green
businesses,  including  solar  panel  manufacturing.  CCX  has  designated
ShoreBank as its “banking arm” and holds a big shareholder stake.23

Probably  coincidentally,  one  ShoreBank  cofounder,  Jan  Piercy,  was  a
Wellesley  College  roommate  of  Hillary  Clinton,  and  she  and  former
president  Bill  Clinton  are  small  investors.  A  former  ShoreBank  vice
chairman,  Bob  Nash,  was  the  deputy  campaign  manager  for  Hillary’s
presidential  bid.  Another  cofounder,  Mary  Houghton,  was  a  friend  of
President  Obama’s  mother  who  had  worked  for  Treasury  Secretary  Tim
Geithner’s father at the Ford Foundation. Howard Stanback, a Shore-Bank
board  member,  formerly  served  as  chairman  of  the  Woods  Foundation
where  Barack  Obama  and  terrorist  Bill  Ayres  were  board  members.
Stanback  had  been  previously  employed  by  New  Kenwood,  Inc.,  a  real
estate development company co-owned by Tony Rezko (who had arranged
a great deal with our President on a home purchase). ShoreBank’s director,
Adele  Simmons,  is  a  close  friend  of  Valerie  Jarrett,  and  now-deposed
Obama-Biden  administration  green  czar  Van  Jones  serves  as  their  green
projects marketing director.24

ShoreBank, its partners, and its investors will benefit handsomely if cap-
andtrade  legislation  is  passed.  Assuming  that  an  estimated  $10  trillion
passes through CCX accounts each year, the bank might earn close to $40
billion in interest charges. Al Gore could rake in many billions during the
first year alone, as would GIM, Goldman Sachs, and the Joyce Foundation.
And  even  if  this  money  doesn’t  materialize,  think  of  all  the  many

reminiscences  that  can  be  shared  among  longtime  friends  at  ShoreBank
reunions!

Climate Legislation: Changing the Labels

 

Cap-and-trade  recently  acquired  a  more  lofty-sounding  new  name  to
counteract toxic cap-and-tax derisions. Proponents began referring to it as
the  “Climate  and  Energy  Bill,”  or  simply  the  “Climate  Bill.”  Cosponsors
were senators John Kerry, Joe Lieberman (I-CT), and Lindsey Graham (R-
SC).  Senator  Kerry  explained  that  the  original  term  should  be  dropped
because “we’re talking about setting a target for the reduction of pollution,
which  is  why  we  don’t  call  it  cap-and-trade  anymore.  It’s  a  pollution
reduction  target  with  a  private  investment  incentive  for  companies  to  be
able  to  invest  in  deciding  how  they  want  to  meet  the  pollution  reduction
target.”23

Although the authors claimed that the proposed legislation didn’t include
a  cap-and-trade  system,  critics  argued  that  the  language  allowed  for  such
mechanisms  within  power  generation  and  manufacturing  sectors.  As
Senator Inhofe commented, “The one thing all of the versions [introduced
in recent years] have in common is that they are cap-and-trade.”24

Then, just when we might have thought cap-and-trade legislation labels
couldn’t  get  more  disingenuous,  senators  Kerry  and  Lieberman  upstaged
their  earlier  moniker  with  a  newly  proposed  “American  Power  Act.”  A
more appropriate description might be “American power grab.” In reality, it
has  little  to  do  with  developing  our  nation’s  vast  domestic  fossil  energy
resources. Rather, it emphasizes ways to mitigate their alleged effects upon
climate  and  expand  government  bureaucracy  by  creating  at  least  sixty
expensive new agencies and projects.25

Introduced in May 2010, the American Power Act includes a $7 billion
annual  “linked  fee”  to  be  added  to  gasoline  prices  to  “improve  US
transportation and efficiency.” The way it works is to have producers and
importers  of  gasoline  and  jet  fuel  buy  non-tradable  carbon  allowances
pegged  to  a  fixed  price  established  by  trading  auction  prices.  So,  is  this
actually referred to as a linked fee or tax? Of course not! But you probably
won’t  recognize  any  difference  when  you  pay  the  added  costs  at  the  gas
pump.  In  addition,  $2  billion  has  been  allocated  per  year  for  researching

2

and developing effective carbon capture and sequestration methods—kind
of  like  creating  a  GITMO  for  dangerous  carbon  terrorist  provocateurs.  In
addition,  there’s  a  new  multibillion-dollar  revenue  stream  for  agriculture
through a carbon-offset program. Should someone inform the sponsors that
tilling soil releases deadly CO
 … not to mention the increased flatulence
hazards associated with livestock?

Recognizing  that  bill  enactment  will  send  costs  soaring,  the  legislation
will “provide assistance to those Americans who may be disproportionately
affected by potential increases in energy prices.” Do you suppose they are
referring to taxpayers?

The frenetic efforts of Democrats to pass ObamaCare have drained the
will of many of their congressional minions to fall on their political spears
in another controversial war. The original Climate and Energy Bill has been
plagued by delays caused in part by repeated rewrites attempting to keep
green groups and key industry players on board during a time of growing
reelection anxiety. Cap-and-tax, by any name, also competes for attention
with  other  front-and-center  Obama-Biden  administration  priorities,  most
particularly  immigration  reform.  Another  reform—  namely,  reckless
spending—demands even greater attention.

Corporate Carbon-Capping Collaborators

 

Many  might  be  surprised  to  learn  of  large  corporations  that  would
ordinarily be assumed to be on the economic losing end of GHG emission
regulation actually supporting it. Why would that be? Granted, we should
assume  that  their  leaders,  stockholders,  and  employees  care  about
environmental stewardship regardless of whether or not they subscribe to
the  CO
  demonization  hype.  But  what  about  responding  to  the  fiduciary
bottom  line?  How  does  that  factor  in?  Let’s  consider  some  possible
examples.

Many  of  these  organizations  are  members  of  the  US  Climate  Action
Partnership (USCAP), a lobbying organization with more than thirty large
corporate and nonprofit members that are pushing hard for federal cap-and-
trade  legislation.  Participants  include  Alcoa;  major  automobile,  electrical,
and chemical companies; and oil corporations.26

2

Alcoa, an early USCAP participant, has implemented successful energy
conservation programs, driven by good business planning, which have also
reduced GHG emissions through expanded use of recycled materials. This
is because aluminum produced from recycled metal requires only about 5
percent as much energy to manufacture as the energy required to produce
primary aluminum, and nearly 70 percent of all aluminum ever produced is
still in use today.

Perhaps Alcoa might have originally hoped to be able to receive emission
credits for GHGs they are “not” emitting through their normal and laudable
profit-seeking activity by pushing the time reference baseline back to 1990
(as Kyoto did). In short, the company would have been eligible for windfall
trading  profits  on  top  of  profits  it  has  already  realized  through  efficient
business  practices  since  1990.  This  would  enable  other  companies  that
exceed their allotments to purchase Alcoa’s credits without expensive fixes.
And even if they didn’t succeed, USCAP membership would give them a
seat  at  the  negotiating  table.  If  so,  can  we  really  blame  them?  Yet  at  the
same time, how, exactly, will this reduce total GHH emissions, much less
really influence climate for the better?

After it merged with Cinergy, Duke Energy joined USCAP in May 2005.
Cinergy  was  a  company  that,  much  like  Alcoa,  had  accomplished  97
percent  emission  reductions  as  a  result  of  implementing  major  efficiency
improvements in its overwhelmingly coal-fired electric generating stations.
Its $1.94 million investment in efficiency upgrades reduced CO
 emissions
by 349,882 tons, at a cost of $1.11 per ton. If early action credits provided
by Phase I of the Climate Stewardship Act (originally proposed in 2003 by
senators  John  McCain  (R-AZ)  and  Joe  Lieberman  (I-CT);  a  new  version
was  introduced  in  2005)  were  applied—valuing  the  credits  at  $15/ton  in
2010  and  $45/ton  in  2025—Cinergy  might  reap  windfall  profits  between
1,263 and 3,990 percent. Not a bad investment.27

2

2

2

DuPont  invested  $50  million  in  the  late  1990s  to  reduce  nitrous  oxide
(N
O)  emissions  from  its  production  of  adipic  acid,  a  chemical  used  to
produce  nylon.  N
O  is  a  gas  with  roughly  310  times  the  greenhouse
warming potential of CO
. By 2000, DuPont had reduced these emissions
by 63 percent over a 1990 base year (56.2 million metric tons on a CO
-
equivalent basis). Assuming that DuPont was awarded a tradable allocation
of 90 percent of its 1990 emissions at an average market price of $10/metric
ton,  its  reductions  by  2000  would  yield  more  than  900  percent  return  on

2

2

investment. DuPont sold the nylon business to Invest in 2004, terminating
its ownership of related emission credits, but DuPont has since eliminated
emissions  from  a  refrigerant  with  even  greater  greenhouse  warming
potential as an unintended by-product of another process change that might
compensate for that lost opportunity.28

It is very clear that some companies would richly benefit from laws and
regulations that drive up the price of carbon and that mandate or subsidize
wind and solar power. For example, General Electric would gain expanded
markets  in  such  areas  as  manufacture  of  wind  turbines,  solar  panels,
electricity  grid  modernization,  nuclear  reactors,  natural  gas  turbines,  and
other energy production projects representing hundreds of billions of dollars
in worldwide sales in coming years.

Carbon caps would also help GE market “greener” products through its
ecoimagination  line  of  appliances  as  energy  costs  escalate.  The  company
spent  $7.6  million  to  lobby  for  favorable  legislation  during  the  second
quarter  of  2009  alone.  Again,  GE’s  energy  conservation  innovations  and
profitability are very good things for everyone and are all accomplishable in
free market competition based upon merit, and at lower costs to consumers
and taxpayers.29

PG&E’s  interests  as  a  significant  hydroelectric  and  nuclear  company
would  be  similar  to  GE’s.  They  would  benefit  as  alternative  energy
(including  nuclear)  sources  become  more  cost  competitive  with
skyrocketing fossil-fuel energy prices. Yet those escalating costs will occur
in  any  case.  GHG  legislation  will  only  make  them  higher,  with
disproportionately  heavy  burdens  falling  upon  those  who  can  least  afford
them.

How  would  automotive  companies  benefit  from  cap-and-trade?  Higher
fuel costs will motivate many people to purchase new, more efficient cars
and  trucks.  So  again,  that’s  a  good  thing,  which  an  increasingly  cash-
strapped and conservation-minded public will do in any case. A good case
can be made for government corporate average fuel economy, or CAFE for
short,  standards  aimed  at  boosting  vehicle  mileage  efficiency  without
artificially raising fuel costs based on a bogus climate crisis platform. But
bucking Obama-Biden administration agendas is not an option so long as
the  government  continues  to  assert  unprecedented  control  over  their
businesses.

The former British Petroleum was a USCAP member, but it has recently
dropped out after losing many incentives. BP has been spending millions of
dollars  in  television  ads  featuring  ordinary-looking  folks  wondering  why
the  energy  industry  hasn’t  thought  about  switching  to  cleaner  natural
sources  “because  they  work.”  But  apparently  these  ads  haven’t  been
working  very  well  for  Europe’s  second  largest  fully  publicly  traded
company’s bottom line. In February 2008, BP’s new CEO mentioned  that
the oil giant might off-load part or all of its Green Business Unit, valued at
between $5–$7 billion, and drop its “Beyond Petroleum” slogan, which was
based  primarily  upon  portfolio  investments  rather  than  actual  business
activities. Environmental critics have long been arguing that this had been
nothing more than a marketing gimmick anyway.30

The  London-based  company  has  a  history  of  very  large  environmental
PR problems to overcome. One was an oil spill attributed to poor pipeline
maintenance  that  leaked  more  than  200,000  gallons  of  crude  oil  onto  the
Arctic  tundra  at  BP’s  Prudhoe  Bay,  Alaska,  oil  field  and  led  to  a  partial
shutdown.  Another  was  a  refinery  explosion  at  its  Texas  City,  Texas,
refinery  that  killed  fifteen  workers  and  injured  at  least  170,  also  blamed
upon  cost-cutting-related  maintenance  and  safety  deficiencies.  Most
recently,  the  massive  offshore  oil  spill  disaster  in  the  Gulf  of  Mexico,
beginning in late April 2010, put the entire oil drilling industry in economic
jeopardy.  The  company  was  forced  to  continue  cost-cutting  measures,
primarily through restructuring, to close its business gap with rivals Royal
Dutch Shell and ExxonMobil.31

Conoco  Phillips  and  heavy  equipment  maker  Caterpillar  joined  BP  in
announcing  in  February  2010  that  they  won’t  renew  their  USCAP
memberships. This may be a continuing trend as more and more companies
recognize  that  diminishing  public  alarm  about  global  warming,  coupled
with growing concern about energy costs, makes carbon-capping legislation
less  likely.  This  skepticism  is  growing  at  a  time  when  climate  science
scandal revelations are becoming routine and Republicans are expected to
gain seats in Congress. Although the Obama administration worked hard to
persuade  industry  groups  to  back  cap-and-trade  initiatives  after  the
Democrats’  big  2008  wins,  the  political  value  of  seats  at  the  government
negotiating table appears to have depreciated. Still, according to Whitney
Stanco, a policy analyst for Concept Capital, “The saying in Washington is
that if you’re not at the table, you’re on the menu.”32

Winston  Churchill  advocated  a  similar  political  survival  strategy.  He
described an appeaser as “one who feeds a crocodile, hoping it will eat him
last.”

BP and Conoco Phillips spokespeople maintain that the companies will
continue to support legislation to reduce greenhouse emissions; they’ll just
be working outside USCAP’s umbrella. Caterpillar prudently maintains that
it  will  continue  to  promote  green  technologies,  although  it  is  unlikely  to
abandon its large coal industry equipment business interests anytime soon.
Still, these decisions to withdraw from USCAP may reflect the beginning of
a broader corporate climate change.33

ExxonMobil  has  encountered  enormous  criticism  and  pressure  for  not
jumping  on  the  climate  crisis  bandwagon  early  enough  and  shifting  its
business  emphasis  toward  alternative  energy.  During  a  May  2008  annual
meeting,  the  company’s  chairman  and  CEO,  Rex  Tillerson,  defeated  a
shareholder  effort  led  by  some  of  founder  John  D.  Rockefeller’s
descendants to force the issue through four nonbinding proxy resolutions.
One  called  for  taking  away  one  of  his  job  titles  to  split  the  company’s
leadership. Another, which also failed, urged that more attention be directed
to studying effects of global warming and development of renewable energy
technologies.34

At  a  press  conference  following  the  meeting,  Mr.  Tillerson  was  asked
about  the  global  warming  issue  and  replied,  “My  view  is  that  climate
change policy is so important to the world that to not have a debate on it is
irresponsible. We don’t know everything about it. Nobody has figured this
out. We have to understand that climate change policy, whatever it turns out
to be, is going to hurt some people. But let’s at least have an open debate
about it, so everybody knows what the facts are.”35

It might be noted that while ExxonMobil hadn’t tended to publicize its
actions to advance that understanding until quite recently, it has donated at
least  $100  million  to  Stanford  University’s  Global  Climate  and  Energy
Project. The company also provides funds to support the National Academy
of Sciences.36

Carbon-Capping Costs

 

2

Enactment of cap-and-trade (aka climate and energy) legislation urged
by the Obama administration will have costly consequences. At the time of
this writing, the latest definitive rendition of this initiative was presented in
the  American  Clean  Energy  and  Security  Act  of  2009  (aka  the  Waxman-
Markey  Bill),  which  narrowly  gained  approval 
the  House  of
in 
Representatives on June 27, 2009, by a vote of 219 to 212.

In June 2009, the CBO released its analysis that Waxman-Markey would
cost  citizens  only  $175  per  household  annually.  This  was  based  upon  a
CBO  study  that  projected  a  CO
  emission  price  of  $28  per  ton  in  2020.
Included  were  escalating  projected  allowance  revenues  of  $119.7  billion,
$129.7 billion, $136 billion, $145.6 billion, and $152.9 billion for the years
2015–2019  as  CO
  caps  become  more  stringent.  However,  the  allowance
costs don’t really add up, because they do not account for such economic
costs  as  decreases  in  GDP  that  will  result  from  the  caps.  In  short,  it  was
only an accounting analysis, not an economic analysis—a fact reported in
the CBO report’s footnote.

A study by the Heritage Foundation’s Center for Data Analysis (CDA)
estimates that Waxman-Markey climate change legislation would cost $161
billion (2009 dollars) annually by 2020. For a family of four, this translates
into $1,870. The CDA also found that for all years cited, the average GDP
loss would be about $393 billion … more than double the 2020 hit to family
incomes. By 2035 (the last year analyzed by the CDA), annual GDP losses,
adjusted  for  inflation,  would  be  $6,790  per  family  .  .  .  and  that  is  before
they paid their $4,600 share of carbon taxes.

2

2

A study by the National Association of Manufacturers projects that CO
emission  caps—similar  to  63  percent  of  the  cuts  called  for  by  2020  by
Senate advocates— would reduce US GDP by up to $269 billion and result
in  the  loss  of  850,000  jobs  by  2014.  The  CDA  estimated  that  such
restrictions  would  produce  total  cumulative  GDP  losses  of  up  to  $4.8
trillion and annual employment losses of more than 500,000 jobs by 2030.

Still,  whatever  the  United  States  does,  it’s  going  to  be  exceedingly
difficult  to  keep  up  with  Europe  in  the  carbon  combat  crusade.  The
influential  Environmental  Audit  Committee  of  the  British  Parliament
advocated that every adult be required to use a “carbon ration card” when
he or she pays for petrol, airline tickets, and household energy. Those who
exceed their designated entitlements would then have to pay for what are
called  “top-up  credits”  from  others  who  haven’t  used  up  all  of  their

allowances.  The  amount  charged  would  be  handled  through  a  “specialist
company.”

As Member of Parliament Tim Yeo, a Tory and a leading promoter of the
plan,  stated,  “We  found  that  personal  carbon  trading  has  real  potential  to
engage  the  population  in  the  fight  against  climate  change  and  to  achieve
significant emission reductions in a progressive way.”37

Therein  lies  an  opportunity  for  the  Brits  to  rewrite  a  famous  statement
presented  by  Winston  Churchill  regarding  a  different  war  to  now  say:
“Seldom have so many done so much for so little.” 38

Caps, Crooks, and Cops in the EU

Experiences across Western Europe prove that huge cap-and-tax profits,
combined with vast operational complexities, present enticing temptations
for fraud. Europol, the European criminal intelligence agency, reported that
emission trading system fraud resulted in about €5 billion in lost revenues
during  2009,  as  carbon  traders  schemed  to  avoid  paying  Europe’s  value
added  tax  (VAT)  and  pocket  the  difference.  The  agency  estimated  that  as
much as 90 percent of Europe’s carbon trades involved fraudulent activity.
Oscar  Reyes,  of  a  watchdog  group  called  Carbon  Trade  Watch,  observed
that “carbon markets are highly susceptible to fraud, given their complexity
and the fact that it’s not always clear what is being traded.”39

Twenty-five people were recently arrested in raids by British and German
authorities as part of a crackdown on carbon credit VAT avoidance. Raids
on  eighty-one  offices  and  homes  nabbed  thirteen  people  in  England  and
eight  in  Scotland.  German  officials  raided  230  locations,  including  the
headquarters of Deutsche Bank in Frankfurt and the offices of RWE, one of
Europe’s largest energy firms. Maybe there’s a lesson in this for Congress:
Don’t count on big business enterprises built on hot air to share windfall
profits with taxpayers.

 

 

Endangerment: End Run of Congress

But  who  needs  the  US  Congress  to  protect  the  planet  from  CO
emissions?  The  Obama-Biden  administration  has  warned  that  it  can

2

accomplish the same goal by applying the EPA’s recent CO
 Endangerment
Finding under the Clean Air Act if Congress doesn’t act. Now Lisa Jackson,
the  EPA  administrator  who  seemed  perfectly  willing  to  implement  the
ruling, is discovering that the EPA may need to contend with congressional
warnings  about  doing  so  after  all,  following  an  outcry  from  some
Democratic members along with state regulators.

2

2

Eight  Senate  Democrats  wrote  to  Jackson  expressing  concerns  about
potential  economic  and  energy  impacts  of  the  policy,  and  dozens  of  state
regulators  argued  that  they  didn’t  have  adequate  staffs  to  handle  the
expected  influx  of  new  permits.  Industry  officials  joined  the  resistance
movement, pointing out that the new regulations will be overly burdensome
to many energy-intensive sectors, such as steel production and cement kilns
that rely upon coal-fired energy.40 Senator Lisa Murkowski (R-AK) filed a
“resolution  of  disapproval”  regarding  the  EPA’s  abuse  of  authority  and
prospective  impacts  upon  the  nation’s  economy.  Senator  Jay  Rockefeller
(D-WV)  introduced  a  bill  that  would  put  a  2-year  freeze  on  the  EPA’s
ability to regulate greenhouse GHGs from power plants.41

Bowing to pressure, Jackson agreed to delay subjecting large greenhouse
GHG  emitters,  such  as  power  plants  and  crude  oil  refineries,  to  new
regulations until 2011. She also agreed to raise the threshold for using the
Clean Air Act to regulate CO
 emissions, stating, “I expect the threshold for
permitting  will  be  substantially  higher  than  the  25,000-ton  limit  that  the
EPA originally proposed.”42

Now,  the  EPA  is  once  again  proceeding  with  GHG  regulation  plans.
Beginning  July  of  2011,  permits  issued  for  all  new  facilities  with
greenhouse  gas  emissions  of  at  least  100,000  tons  per  year  and
modifications to existing businesses that will increase emissions by at least
75,000 tons per year must demonstrate use of best available technology to
minimize GHGs. How, exactly, that technology is defined remains unclear,
and that uncertainty discourages essential investment.43

Led  by  the  US  Chamber  of  Commerce,  several  states  and  large
businesses are launching dozens of legal challenges to block the EPA power
grab. Yet these cases are still in their preliminary stages and are unlikely to
bring about any relief before the regulations are enacted, if ever.

Senator Inhofe reported that “Lisa Jackson, Obama’s EPA administrator,
admitted  to  me  publicly  that  the  EPA  based  its  action  today  (issuing  the
findings) in good measure on the findings of the UN’s Intergovernmental

Panel  on  Climate  Change,  or  IPCC.  She  told  me  that  the  EPA  accepted
those findings without any serious independent analysis to see if they were
true.”44

On June 10, 2010, the US Senate voted 53–47 against banning the EPA
from  regulating  carbon  without 
the  consent  of  Congress.  Senator
Rockefeller (WV) sided with the losing GOP voters, along with his fellow
Democrats Mary Landrieu (LA), Evan Bayh (IN), Mark Pryor (AR), Ben
Nelson (NE), and Blanche Lincoln (AR).

While some companies would get very rich through implementation of
capand-trade  or  EPA  endangerment  rules,  millions  of  businesses  and
families would pay dearly. More than thirty states depend upon coal for 35
to  98  percent  of  their  electricity.  All  rely  upon  oil  and  natural  gas.  As
recognized by the US Chamber of Commerce, an organization that is vocal
on  such  matters,  such  regulations  will  have  burdensome  impacts  upon
energy  costs;  will  ship  needed  jobs  overseas;  and  will  shackle  living
standards and civil rights for most American citizens.

Chapter 8

CLIMATE AS RELIGION

Vapors  rise  as  /  Fever  settles  on  an  acid  sea  /  Neptune’s  bones
dissolve  /  Snow  glides  from  the  mountain  /  Ice  fathers  floods  fora
season / A hard rain comes quickly / Then dirt is parched

—Al Gore

Global warming has become a religious mantra, a call to action in
a  crusade  against  larger  evils  we  have  perpetrated  against  nature,  a
punishment for our sins. Author Michael Crichton articulated the essence of
that creed in a 2003 speech that draws a parallel with the Judeo-Christian
belief system: “There’s an initial Eden, a paradise, a state of grace and unity
with Nature; there’s a  fall  from  grace  into  a  state  of  pollution  as  a  result
from eating from the tree of knowledge; and as a result of our actions, there
is a judgment day coming for all of us. We are energy sinners, doomed to
die,  unless  we  seek  salvation,  which  is  now  called  sustainability.

Sustainability is salvation in the church of the environment. Just as organic
food is its communion, that pesticide-free wafer that the right people with
the right beliefs imbibe.”1

Michael Crichton was not arguing against the importance of living more
environmentally responsible lives that apply resources in more sustainable
ways. He was talking about doing this with intelligence that is less clouded
by emotion, which can impair judgment.

The  temple  of  global  warming  is  built  upon  religious  rather  than
scientific foundations. Climate change is not Mother Nature’s punishment
for our human audacity to multiply and survive, any more than a tornado
that  destroys  a  church  is  God’s  retribution  for  belonging  to  the  “wrong”
denomination. Get over it! It’s not all about us! Climate change is the way
nature  balances  itself,  moves  heat  around,  and  produces  motivations  for
species  to  evolve.  CO
  is  a  small  but  nonetheless  important  part  of  that
system. Without it, life would not exist at all. No polar bears, no penguins,
no coral reefs—and certainly no rain forests that directly breathe in lots of
the stuff. Don’t call it “pollution.”2 At least show it a little respect!

2

Fear Merchants

 

Religion  plays  an  important,  if  not  central,  role  in  most  of  our  lives,
whether  we  subscribe  to  a  particular  orthodoxy  or  not.  It  guides  us  to
believe  that  we  are  all  parts  of  something  much  larger  than  ourselves.  It
provides  lessons  that  encourage  us  to  live  cleaner,  use  resources  more
responsibly, and be nicer to all of nature’s creatures. Perhaps it’s really okay
if we need to be a bit frightened about the consequences of things we’re
doing  wrong  to  motivate  us  to  do  better.  But  don’t  we  expect  something
different from science? Isn’t it supposed to tell us real facts about what it
doesn’t  know  as  well  as  teach  us  what  it  has  actually  learned?  When
“science” emulates religion, it oversteps its bounds, and we can no longer
trust  it.  And  in  the  case  of  global  warming,  science  has  overstepped  its
bounds. Nobel Physics laureate Ivar Giaever calls global warming “a new
religion.”3

Many global warming zealots apparently envision life in Earth’s distant
past  as  an  Eden  with  idyllic  conditions.  Those  were  the  good  old  days
before  industrialization  and  modern  technology  wrecked  everything.  Yet

realities going back a hundred years and more reveal a different picture: one
displaying widespread poverty, starvation, disease, and hardship. Yes, and
throughout human history, people have had to adapt to climate changes—
some  long,  some  severe,  and  often  unpredictable.  They  have  blamed
themselves for bad seasons, believing they had invoked the displeasure of
the gods through a large variety of offenses. High priests of doom told them
so,  extracting  oaths  of  fealty  and  offerings  of  penance  for  promised
interventions on their behalf. In this regard, at least for some, it seems little
has changed.

Media networks, politicians, and other headline grabbers readily buy into
doomsday pronouncements, offering them up as packaged sound bite–sized
news  flashes,  and  competition  for  audiences  and  advertising  revenues  is
fierce. Unfortunately, many voices we have previously trusted have become
too busy, biased, or indifferent to check the data. Or sometimes they may be
disinclined to do so because the “facts” are just too juicy to pass up when
facing a ratings war.

Global warming has been effectively marketed as newsworthy because it
provides  really  exciting  visuals:  icebergs  calving,  polar  bears  exhausted
from  swimming,  and  such.  Endless  authorities  will  back  these  images  up
with speculations regarding just how bad things are likely to get. Included
are  wild  projections  of  climate  futures  based  upon  unproven  theories  and
computer models, along with speculative estimates of past temperatures that
are to be accepted as articles of faith.

More from Gore

 

Al  Gore  and  other  perhaps  less  divinely  inspired  modern-day  Noahs
continue to speak out about an imminent threat of floods posed by rising
ocean  levels  resulting  from  melting  ocean  and  glacier  ice.  The  “Goracle”
carried on his prophetic ministry at the Copenhagen conference, declaring
an  impending  Arctic  disaster.  Citing  new  research  undertaken  at  the  US
Navy’s Naval Postgraduate School in Monterey, California, Mr. Gore told
attendees,  “These  figures  are  fresh.  Some  of  the  models  suggest  to  Dr.
[Wieslaw] Maslowski that there is a 75 percent chance the entire north polar
ice  cap,  during  the  summer  months,  could  be  completely  ice-free  within

Predictions of Arctic ice collapse are not new, ranging from warnings in
2008 that the “North Pole may be ice-free for the first time this summer”
and “the entire polar ice cap will disappear this summer.” Much of this was
based  upon  satellite  data  provided  by  the  National  Snow  and  Ice  Data
Center showing that Arctic ice was rapidly disappearing back toward a low
2007 level. Yet the August 2008 ice coverage was approximately 10 percent
greater  than  it  had  been  the  same  month  in  2007.  Fast-paced  freezing  in
November  2007  followed  the  rapid  rate  of  melting  observed  during
September  and  October.  NASA’s  Earth  Observatory  images  reveal  that
some  58,000  square  miles  of  ice  formed  for  10  days  in  late  October  and
early November, a new record. Nevertheless, the extent of sea ice recorded
in  November  was  still  well  shy  of  the  median  observed  over  a  25-year
period between 1979 and 2003.5

According  to  NASA  Marshall  Space  Flight  Center  data,  global  sea  ice
expanses in January 2009 approximately equaled those in 1979, and Arctic
ice  realized  a  substantial  recovery.  Bill  Chapman,  a  researcher  at  the
University  of  Illinois  Arctic  Center,  reported  that  this  was  due  in  part  to
colder  temperatures  and  also  because  wind  patterns  were  weaker.  Strong
winds can slow ice formation and force ice into warmer waters where it will
melt.6

five to seven years . . . It is hard to capture the astonishment that the experts
in the science of ice felt when they saw this.”4

Scientists  were  astonished  by  Gore’s  statements.  One  was  none  other
than Dr. Maslowski himself, who responded, “It’s unclear to me how the
figure was arrived at… I would never try to estimate likelihood at anything
as exact as this.” Mr. Gore’s office later admitted that the 75 percent figure
was one used by Dr. Maslowski as a “ballpark figure” several years ago in a
conversation with Mr. Gore.

In  November  2009,  the  average  rate  of  Arctic  sea  ice  growth  slightly
exceeded  the  1979–2000  average  growth  rate  for  the  month,  although  at
month’s end, some regions, including the Barents Sea and Hudson Bay, had
less  ice  cover  than  normal.  Both  Hudson  Bay  and  the  Barents  Sea
experienced  a  slow  freeze-up  during  fall  2009,  reportedly  caused  by
different and very complex interactions among the sea ice, the atmosphere,
and the ocean.7

Religious  history  is  replete  with  stories  of  floods,  from  Noah  to
Gilgamesh.  But  melting  and  freezing  patterns  are  far  too  complex  and

2

regional  to  be  predicted  by  models.  In  addition,  accurate  satellite  records
only  recently  became  available,  and  those  changes  have  not  been  in
lockstep with either global temperature or CO

 concentration trends.

Knee-jerk responses to alarmist forecasts make for great media, but they
do so at the expense of good science. If claims that continuous Greenland
melting accelerations were correct, even at previously measured advancing
rates it would take thousands of years to significantly affect sea levels. And
a study presented in the July 2008 issue of the journal Science notes that
Greenland’s melt rate may actually be decreasing when viewed over a long
timescale.  This  research,  led  by  Dr.  Roderick  S.  W.  van  de  Wal  of  the
Institute for Marine and Atmospheric Research in Utrecht, is based upon 17
years of satellite measurements. It concludes that speedups in melting rates
are  strictly  short-term,  transient  phenomena,  occurring  primarily  during
summer months.8

Global Warming and Traffic

 

As  noted  in  earlier  chapters,  Dr.  James  Hansen,  a  high  priest  of  the
climate  change  religion,  continues  to  produce  much  media  flood  fodder.
You  will  most  likely  see  his  name  attached  to  any  widely  circulated
headline reports of Greenland ice melting at alarming rates and of sea-level
rise  predictions 
that  make  IPCC’s  seem  extremely  comforting  by
comparison.  Hansen  also  continues  to  stay  very  busy  with  television  and
magazine interviews and public lectures outside his official NASA duties,
obviously unfettered by constraints imposed by any government officials on
his free speech. An April 14, 2008, Newsweek  lead  cover  article,  “Who’s
the  Greenest  of  Them  All?”  (referring  to  presidential  candidates),  quotes
him as having observed, “Anything beyond 350 parts per million of carbon
dioxide threatens widespread global melting and rise of sea levels. We are
at 385 and counting.”9

Hansen  continues  to  be  convinced  that  a  climate  crisis  is  upon  us,
warning that warming during the summer of 2009 has pushed the climate
system toward tipping points that will lead to irreversible and catastrophic
effects.  He  had  previously  gained  headline  media  coverage  when  he
declared  that  October  2008  was  the  warmest  on  record  for  that  month.
NASA  later  corrected  that  record  after  Hansen  was  caught  fudging  the

numbers. NOAA’s registration of 63 snowfall records and 115 lowest-ever
temperatures  for  October  ranked  2008  as  only  the  70th  warmest  in  114
years.  As  Christopher  Booker  wrote  in  the  UK’s  Daily  Telegraph,  “The
reason  for  the  freak  figures  [presented  by  Hansen]  was  that  scores  of
temperature  records  from  Russia  and  elsewhere  weren’t  based  upon
October records at all. Figures from the previous month had been carried
over for two months running.”10

Some recent e-mail messages obtained by Christopher Horner, a senior
fellow  at  the  Competitive  Enterprise  Institute,  through  the  Freedom  of
Information Act call into question the reliability of climate data applied by
NASA’s  GISS,  which  Hansen  heads.  Even  some  top  NASA  scientists
apparently considered the climate dataset produced by GISS to be inferior
to  data  maintained  by  the  University  of  East  Anglia’s  CRU.  In  fact,  they
often  depended  upon  CRU  data.  The  GISS  data  was  also  regarded  to  be
inferior  to  that  provided  by  the  National  Climatic  Data  Center’s  Global
Historical  Climatology  Network,  whose  information  had  been  given
directly  to  a  reporter  from  USA  Today  in  August  2007  but  was  never
published.11

Hansen’s  past  predictions  haven’t  all  proven  to  be  divinely  inspired.
During an interview in 2001, he was asked to predict how global warming
would affect the scene outside his New York City GISS office building 20
years hence. Gazing out of his upper-level window onto the area below, he
said, “Well, there will be more traffic.” Then he went on to say, “The West
Side  Highway  [which  runs  along  the  Hudson  River]  will  be  underwater.
And there will be tape across the windows across the street because of high
winds. And the same birds won’t be there. The trees in the median strip will
change.”  Then  he  added,  “There  will  be  more  police  cars.”  When  asked
why, he explained, “Well, you know what happens to crime when the heat
goes up.”12 At least he was right about the traffic.

Skeptical Judgments

 

The  climate  change  believers  have  launched  what  amounts  to  an
aggressive jihad against those who have differing opinions. Results of that
climate war have brought great injustice to individuals who disagree, have
hampered  sensible  dialogue  and  debate,  and  have  produced  government

policies guided by emotion and fear rather than by balanced reasoning and
sound judgments.

One tactic used to defame those who don’t subscribe to global warming
crisis hysteria is to associate disbelievers with those who have turned their
backs  on  true  villainy.  For  example,  when  television  commentator  Scott
Pelley was asked in a March 23, 2006, CBS PublicEye blog post why he
didn’t interview anyone who didn’t agree that global warming is a threat, he
compared  scientists  who  are  skeptical  about  human-caused  catastrophic
climate change to Holocaust deniers: “If I do an interview with [Holocaust
survivor]  Elie  Wiesel,  am  I  required  as  a  journalist  to  find  a  Holocaust
denier?”13

David Roberts, a regular contributor to Grist, a prominent environmental
news and commentary blog site, carried the denier Holocaust theme even
farther.  Referring  to  the  “denial  industry,”  he  stated  that  we  should  have
“war crime trials for these bastards—some sort of climate Nuremberg.”14

An  Australian  columnist  agrees  with  Roberts,  proposing  that  climate
change denial should be outlawed: “David Irving is under arrest in Australia
for  Holocaust  denial.  Perhaps  there  is  a  case  for  making  climate  change
denial an offence—it is a crime against humanity after all.”15

IPCC’s top scientist and chairman, Rajendra Pachauri, goes beyond the
Holocaust to compare the views of global warming deniers with those of
Hitler himself. Referring to the well-known global warming skeptic Bjorn
Lomborg,  Pachauri  stated,  “Where  is  the  difference  between  Lomborg’s
view on humans and Hitler’s? You cannot treat people like cattle.”16

With regard to IPCC’s scientific objectivity, that pretty much says it all.

Eco-Evangelism

 

In  the  church  of  climate  change,  most  or  all  unfortunate  events  that
occur on our planet are attributed to human causation. Eco-elitist crusaders
argue that economic growth, promulgated by large corporate interests, is the
enemy  of  the  environment.  They  overlook  the  fact  that  global  economic
progress  yields  technological  innovations  and  prosperity  essential  to
support more resourceful, cleaner, and healthier lifestyles. A return to the
small,  self-sufficient,  agrarian  communal  societies  of  our  ancestors  is  no

2

longer practical or desirable, either for us or for the ecosystems we depend
upon.

Environmental  evangelism  has  been  a  unifying  influence  in  a  wide
variety  of  other  non-climate-related  initiatives  with  mixed  cost-benefit
results.17 Do you remember the huge amount of attention during the 1970s
and  1980s  that  surrounded  the  issue  of  acid  rain  damage  to  lakes  and
forests, which was attributed to industrial SO
 emissions from Midwestern
utilities?18  And  did  you  ever  hear  about  the  results  of  a  more-than-half-a-
billion-dollar,  10-year-long  National  Acid  Precipitation  Assessment
Program study that was initiated in 1980 to research the matter?19 Probably
not.

As it turned out, those fears of widespread damage proved to be largely
unfounded, since only one species of tree at a high elevation suffered any
notable  effect,  and  acidity  in  lakes  was  traced  to  natural  causes.  The
scientists  reported  that  they  had  “turned  up  no  smoking  gun”;  that  the
problem was far more complicated than had been thought; that other factors
combine to harm trees; and that sorting out cause and effect is difficult and
in  some  cases  impossible.  As  Robert  Bruck,  a  North  Carolina  State
University  plant  pathologist  who  worked  on  the  project,  observed,  “If
you’re  environmentally  oriented,  you’re  going  to  find  things  to  be
concerned about; if you’re one who finds no reason to get excited, you’ll
find much to support that, too.”20 Is this beginning to sound familiar?

2

Although  the  Reagan-Bush  administration  refused  to  sponsor  any  acid
rain  legislation  before  the  results  were  in,  a  regulatory  groundwork  had
already  been  established  within  the  EPA,  with  many  new  careers  in  the
balance.  So,  although  the  acid  rain  threat  had  been  demonstrably
overblown,  pressures  upon  the  George  H.  W.  Bush  administration  added
 emission restrictions within new Clean Air Act regulations. The
costly SO
IPCC, along with other scientific organizations, now recognize SO
 as a gas
that counteracts atmospheric greenhouse warming effects— though few on
either side of the global warming debate are likely to argue that this is a
worthwhile benefit to be encouraged.21

Knee-jerk  environmental  legislation  likewise  poses  unforeseen  political
and  other  hazards.  An  example  is  a  cap-and-trade  regulatory  mechanism
proposed  under  the  Clean  Air  Act  of  2005  by  the  George  W.  Bush
administration to reduce sources of mercury pollution 29 percent by 2010,
ramping up to 79 percent by 2018. Mercury is universally recognized to be

2

a  highly  toxic  health  hazard,  and  although  the  reduction  cost  to  the
electricity industry (principally transferred to consumers) is estimated to be
$2 billion or more, general public support appears to be strong. Yet even
though  such  regulations  have  not  previously  existed,  some  environmental
groups are critical, demanding that such cuts are not nearly enough, arguing
that the “weak” plan is a conspiracy to hurt our nation’s children.22

In February 2008, the United States Court of Appeals for the District of
Columbia Circuit Court ruled that the EPA-endorsed “Clean Air Mercury
Rule”  was  invalid  because  a  cap-and-trade  program  would  enable  power
plants that fail to meet emission targets to buy credits from plants that did,
rather than having to install their own mercury emission controls. Seventeen
states  argued  that  the  cap-and-trade  approach  would  endanger  children
living near power plants that couldn’t comply by “doing it legally.” If this
were  the  case,  why  would  cap-and-trade  legislation  for  CO
  emission
allowances be any different? That certainly appears to be a problem in most
countries that signed on to Kyoto Protocol emission targets.23

2

Another  ironic  twist  and  turn  in  the  mercury  regulation  saga  revolves
around new US government legislation that will phase out use of traditional
incandescent  bulbs  in  2012,  replacing  them  with  more  energy-efficient
compact fluorescent lighting (CFL) that contains mercury. Many argue that
the  resulting  environmental  and  health  costs  will  cancel  out  the  benefits
realized  through  energy  conservation.  While  advocates  argue  that  the
mercury content in a single CFL bulb is relatively low, comparable to that
in watch batteries and tilt thermostats, a difference is that these items don’t
tend  to  shatter  when  accidentally  dropped.  Critics  argue  that  as  federal
legislation  continues  to  push  CFLs  into  home  use,  exposures  will  add  up
over time, with increased risks to the health of babies, children, pregnant
women, the elderly, and those in poor health.

Disposal of spent bulbs will present large cleanup costs, and those that
end  up  in  landfills  will  leak  mercury  into  the  air  and  groundwater.  A
spokesman for General Electric, a major CFL producer, admitted that even
a little mercury in each bulb will add up to a big problem when sales really
expand. Huge amounts of mercury will also enter the global environment
from  factories  in  China  and  other  nations  where  pollution  control
regulations are much more lax or nonexistent.24

Conscience of an Environmentalist

 

Environmentalists  are  almost  universally  motivated  by  love  and
reverence toward nature, vital concerns about our impacts upon planet Earth
and its ecosystems, and a strong sense of responsibility regarding the legacy
we  leave  for  generations  who  will  follow.  These  basic  priorities  are  not
founded upon specific scientific theories or beliefs; rather, they are founded
upon shared values that fundamentally define and guide our most evolved
human qualities. Included are our abilities to reason objectively, to create
innovative solutions that make things better, and to learn from mistakes that
occur even when we mean well.25

There  can  be  no  doubt  that  environmental  issues  have  captured
mainstream public consciousness, and so they should. Just look at the recent
explosion  of  protective  legislative  initiatives,  government-  and  corporate-
sponsored climate research funding, and tax-supported investments in green
energy  technologies.  People  of  all  backgrounds,  including  scientists,
politicians,  and  regular  folks,  are  passionate  about  such  matters.  Those
passions have yielded urgent and beneficial changes along with some very
costly lessons.

The  beginning  of 

the  popular  “environmental  movement” 

is
conventionally  associated  with  a  virtual  tsunami  of  reaction  to  Rachel
Carson’s Silent Spring after it first appeared in 1962. Endorsed by Supreme
Court Justice William O. Douglas, the book spent several weeks on the New
York  Times  best-seller  list  and  inspired  widespread  public  concerns
regarding human impacts upon the environment. Most particularly, it called
attention  to  a  thinning  of  the  eggshells  of  certain  bird  species  that
threatened  their  existence,  along  with  the  toxic  problems  throughout  the
food chain that resulted from indiscriminate crop spraying of the pesticide
DDT. Although now scientifically challenged, this claim is clearly credited
with a prohibition against DDT use in the United States since 1972, and a
similar ban in Europe.26

Because of threatened European trade restrictions against countries that
used  DDT,  African  nations  terminated  use  of  the  effective  mosquito
pesticide for malaria control. Since that time, death rates from the disease
have increased dramatically. The US Centers for Disease Control estimates
that between 155,000 and 310,000 people have died each year based upon
1997–2002 data tabulated at forty-one African sites. The vast majority of

these  victims  are  desperately  poor,  including  large  numbers  of  young
children  and  the  elderly,  who  are  especially  vulnerable.27  Arguably,  there
would have been millions fewer deaths if African nations had continued to
use  DDT.  How  does  anyone  compute  an  environmental  cost-benefit
assessment that factors in the intrinsic value of those lives?28

The malaria-DDT paradox presents an ironic connection to recent claims
by  some  that  global  warming  is  causing  mosquito  populations  to  expand
into  formerly  cooler  latitudes,  with  the  potential  of  producing  malaria
epidemics  in  various  parts  of  the  world.  Although  many  highly  informed
scientists  strongly  dispute  any  such  influence,  it  again  brings  pesticide
issues to the center stage of environmental debate.

Assuming  that  an  important  goal  of  environmental  policies  is  to  help
underdeveloped  and  developing  nations  gain  the  health  advantages
developed nations enjoy, perhaps a good way is to assist in lifting them out
of  poverty  and  pollution  through  modernization.  Yet  “environmentalists”
continue to block plans to construct hydropower dams in Africa and India
that  can  provide  clean  energy  needed  to  refrigerate  and  safely  preserve
food. Obstructionists who enjoy those essentials cite overriding ecological
concerns.29

Paul  Ehrlich’s  The  Population  Bomb,  published  in  1968,  was  one  of
several books that spread doom-and-gloom predictions attributed to human
pressures  on  the  environment.  It  projected  that  worldwide  crises  in  food
supply  and  natural  resource  availability  would  lead  to  major  famines  and
economic  failures  by  1990.  Ehrlich’s  predictions  were  based  upon  the
premise that while agricultural production was growing linearly, population
was expanding at a much faster and unsustainable geometric rate. What he
failed to consider is that as developing countries modernize, birth rates tend
to  fall,  and  agricultural  output  increases  even  faster  using  less  farmland.
The United States is a prime example.30

John Holdren, the Obama-Biden administration’s science and technology
czar, has been ordained as another high priest of climate calamity. Holdren
was an early protégé of Ehrlich and coauthored a book with him in 1977
titled  Ecoscience:  Population,  Resources,  Environment,  which  explored
measures  a  government  might  take  to  limit  population  growth  if  a
population  crisis  is  to  occur.  The  book  defended  the  constitutionality  of
compulsory abortion and sterilization, a topic reopened at the Copenhagen

climate  change  meeting  in  2009.  In  1986,  Holdren  declared  that  global
warming could cause the deaths of 1 billion people by 2020.

Mr.  Holden  also  holds  several  other  influential  White  House  positions:
assistant to the president for science and technology, director of the White
House  Office  of  Science  and  Technology  Policy,  and  cochair  of  the
President’s  Council  of  Advisors  on  Science  and  Technology.  Fortunately,
during  his  confirmation  hearings,  Holdren  stated  that  the  government
should not determine the optimum US population, and he now believes that
the  figure  of  1  billion  deaths  by  2020  is  “unlikely.”31  We  can  breathe  a
collective sigh of relief!

Holdren  is  a  strong  advocate  of  cap-and-trade  as  a  means  to  control
energy consumption, and he supported this strategy long before Al Gore got
windy on wind. In a paper released in 1995, Holdren explained his model
for sustainable development as one in which “humans are included as just
one  species  and  not  treated  specially.”32  Another  Holdren  sustainability
tenant  is  to  launch  “a  massive  campaign  to  restore  a  high-quality
environment  in  North  America  and  to  de-develop  the  United  States”;  to
achieve this end, he believes that “resources and energy must be diverted
from frivolous and wasteful uses in overdeveloped countries to filling the
genuine needs of underdeveloped countries.”33

Does  any  of  this  sound  familiar?  As  if  it  could  have  come  from  the
United  Nations  and  its  IPCC,  for  example?  Or  maybe  the  Karl  Marx
doctrine? Is it what we would expect the nation’s highest appointed science
and  technology  adviser  to  advocate?  Is  this  where  eco-evangelism  has
driven us? Has our ship of state drifted to a foreign port, or is this still the
United States of America?

Chapter 9

GETTING A REAL GRIP ON “GREEN “ ENERGY

Taking a closer look at alternatives.

The  United  States  is  facing  energy  challenges  that  can  only  get
worse.  It  is  clear  that  we  must  develop  and  exploit  all  reasonable
alternatives and also practice rational conservation measures as we observe
that  global  demand  increases,  readily  accessible  oil  and  gas  deposits
dwindle, competition for world supply accelerates, and costs rise. Equally
important is a need to curb regulatory obstruction of vital and time-critical
energy initiatives based on unwarranted assertions of moral and scientific

authority.  All  these  priorities  demand  trustworthy  public  information  and
leadership. Therein lie the greatest obstacles of all.

In  response  to  man-made  global  warming  and  foreign  oil  dependence
alarms,  many  companies  are  rushing  to  “green  up”  their  investment
portfolios,  advertising  images,  and  lobbying  campaigns.  And  they  are
realizing great successes, cheered on by a hopeful, grateful public and its
representatives.  After  all,  who  can  resist  the  tantalizing  allure  of  cleaner,
perpetually sustainable, unlimited new supplies of power and fuel that will
provide independence from unreliable, often unfriendly foreign sources?

Many are beginning to realize that most green expectations are oversold
and color-blind. Increasing numbers of skeptics and critics are challenging
the  actual  benefits  and  consequences  of  various  energy  choices.  Included
are  growing  numbers  of  taxpayers,  consumers,  and  more  than  a  few
environmental groups.1

Alternative  (aka  “green”)  energy  initiatives  are  receiving  rapidly
expanding  levels  of  arguably  well-justified  encouragement  and  support
through a variety of federal, state, and local incentive programs. Many tens
of billions of dollars have already been provided through such mechanisms
as  subsidies,  production  credits,  accelerated  depreciation  tax  credits,  and
public  funding  for  research.  As  of  February  2008,  twenty-five  states  plus
the  District  of  Columbia  have  instituted  mandatory  renewable  portfolio
standards (RPSs) that set timetables for increasing percentages of legislated
green  power  production.2  It  may  be  interesting  to  note,  however,  that  the
majority  of  government-owned  utilities  in  these  states  have  successfully
lobbied for reprieves from the costly, often unrealistic requirements.3

A  federal  Energy  Policy  Act  of  2005  authorized  by  the  US  Congress
mandates a biofuel RPS phase-in starting at 4 billion gallons in 2006 and
reaching 7.5 billion gallons by 2012.4 In December 2007, the US House of
Representatives  passed  another  RPS  requiring  all  investor-owned  utilities
(but not municipal systems and rural cooperatives) to obtain 2.75 percent of
their power from renewable sources by 2010 and 15 percent by 2020. The
Senate version rejected any RPS provision that would transfer wealth from
already distressed electricity customers to a heavily subsidized wind power
industry.

Consumers generally have no idea how expensive green power actually
is, because so much of the cost is passed on through taxpayer subsidies and
preferential  treatment  that  drives  conventional  power  prices  higher.  Even

with that invisible support, it still costs more than most utility customers are
willing to pay voluntarily. When public polls are taken asking people if they
would pay more for alternatives that are “better for the environment,” the
results  are  usually  overwhelmingly  positive.  Yet,  when  they  are  asked  to
actually sign up and pay more, their euphoria immediately disappears.

In addition to currently uncompetitive costs relative to coal, natural gas,
and nuclear for electricity, and oil for transportation, a major and long-term
green  energy  industry  problem  is  its  very  limited  practical  expansion
capacity.  Consider  that  alternatives  presently  account  for  only  about  6
percent  of  the  total  US  electrical  power  production  (half  of  that  from
hydropower). Wind power (about 0.5 percent of total electricity—less than
0.01  percent  of  total  energy)  is  the  only  alternative  with  a  prospect  for
significant  growth,  and  it  has  a  very  long  way  to  go  in  replacing
dependence upon fossils (about 72 percent of total electricity—75 percent
of  total  energy),  and  nuclear  (about  19  percent  of  total  electricity—11.5
percent of total energy). Hydropower (about 6.5 percent of total electricity
—0.33 percent of total energy) has little expansion capacity. Solar, which
currently  provides  only  about  0.01  percent  of  total  US  electricity—even
much less of total energy—isn’t a contender for a significant share of the
commercial market on the basis of either cost or capacity. Geothermal (less
than 0.5 percent of total electricity—negligible total energy) is even more
restricted, both economically and geographically, in terms of expansion.

In reality, renewable energy development has a long way to go before it
can even begin to significantly offset increasing demands, much less play
dominant supply roles. All combined, these alternatives currently provide
only  about  6  percent  of  US  total  energy,  with  the  vast  majority  of  that
amount split between hydropower and such biofuels as ethanol and wood.

It  is  essential  to  our  national  and  global  future  that  development  and
utilization  of  alternative  energy  sources  and  technologies  continue  and
grow.  This  includes  improvement  and  expansion  of  nuclear  power,  along
with  innovations  to  produce  cleaner  energy  from  coal  and  other  fossil
sources for which there is presently no practical substitute.

It is also vital that the public be made much better informed about the
comparative  advantages  and  disadvantages  of  all  alternatives,  and  that
public policy decisions at all government levels be more fully guided by the
facts.  Currently,  this  is  not  happening.  Performance  benefits  of  unproven
options,  such  as  cellulosic  ethanol,  have  been  asserted  but  not

demonstrated.  Expansion  capacities  of  various  alternative  energy  options
have  been  wildly  exaggerated  by  promoters,  leading  many  in  the  general
public to believe that abundant replacements for fossil sources are available,
but  are  being  neglected  by  the  energy  utilities  in  response  to  “big  oil”
interests.

It  is  not  useful  to  either  overestimate  those  capacities  or  underestimate
their costs and limitations. Both errors are prevalent in media and marketing
hype that tells us what we would really like to believe, namely, that there
are  simple,  Earth-friendly,  sustainable  answers  that  can  make  energy
problems  go  away.  Unfortunately,  this  is  not  the  case.  It  is  difficult  to
comprehend  how  miniscule  the  potential  capacities  of  these  so-called
renewable  sources  are  relative  to  the  colossal  amounts  of  energy  we  will
continue to require.

term  “green  energy”  has  become  meaningless  because 

the
environmental consequences of all alternatives have been ignored by some
and  aggressively  attacked  by  others.  Some  fossil-dependent  energy
resources have been mischaracterized as renewable and nonpolluting. Upon
closer  examination  of  those  green  options,  many  will  appear  decidedly
“browner”  than  advertised,  and  they  will  not  present  a  major  supply-side
solution to our energy challenges.5

The 

Biofuels: Field of Dreams

 

Can we grow our way out of an energy deficit? Federal legislation with
such titles as the Renewable Fuels Act (2005) and the Biofuels Security Act
(2006)  are  both  misleading  with  regard  to  ethanol,  the  primary  biofuel.
First, it really isn’t renewable when you consider that nearly as much fossil
fuel–generated  energy  is  required  to  produce  it  as  it  actually  yields.
Alternatively, if all the energy used to plant, fertilize, harvest, and process
the biofuel came from the ethanol produced, it would displace a gasoline
consumption equivalent to only about 3.5 percent. This is about the same
amount  that  the  Natural  Resources  Defense  Council  (NRDC)  estimates
might be saved by inflating tires properly.6

Regarding  energy  security,  biofuels  suffer  from  some  very  serious
reliability  and  capacity  limitations.  Corn  crops,  the  plant  stock  for  US
ethanol, are vulnerable to periodic drought conditions. On average, a crop

yield  decline  of  nearly  one-third  occurs  about  1  year  out  of  20  due  to
insufficient  rainfall.  And  even  during  good  years,  the  total  offset  on
gasoline  consumption  will  be  very  small,  regardless  of  any  mandates
established by federal and state governments.

Ethanol  refiners  (actually  wood  alcohol  “distillers”)  cite  energy
independence  as  a  compelling  argument  for  the  massive  subsidies  they
receive. Imported oil continues to provide about 60 percent of all petroleum
fuel  we  use,  and  exposes  the  United  States  to  large  economic  risks  and
massive trade imbalances that ethanol will not alleviate.7

But if we were to produce enough ethanol to replace gasoline altogether,
it would require that about 71 percent of all US farmland be dedicated for
energy crops.8 By way of illustration, let’s just think about distilling all of
our present US corn production into that 180-proof grain alcohol—ethanol.
That  would  only  displace,  at  most,  about  14  percent  of  the  gasoline  we
currently  guzzle.  In  2007,  ethanol  consumed  approximately  one-fourth  of
all US corn production. In 2008, that amount grew to about one-third, and
the percent will continue to rise. The 2007 amount was estimated to have
offset  US  gasoline  consumption  by  3.5  percent  while  corn  costs  had
doubled over a 2-year period.9

Assuming that it is possible for the United States to produce a mandated
36 billion gallons of ethanol by 2022, it won’t really make a big difference.
That  would  replace  only  about  1.5  billion  barrels  per  day  (bbl/d)  of  oil,
amounting to only about 7 percent of our needs; that is, providing we hold
consumption to current levels.10

Because  US  farmland  is  scarce  and  expensive,  each  additional  acre  of
corn  used  to  produce  ethanol  is  one  less  that  is  available  for  other  crops
such as soybeans and wheat, which have seen price increases of more than
240 percent since 2006. This, in turn, produces a ripple effect that raises the
costs  of  meat,  milk,  eggs,  and  other  foods  with  international  export
consequences. Since US farmers provide about 70 percent of all global corn
exports, even small diversions for ethanol production have produced high
inflation levels in America and food riots abroad.11

Two professors at the University of Minnesota’s Center for International
Food  and  Agricultural  Policy,  C.  Ford  Runge  and  Benjamin  Senauer,
estimate that filling a 25-gallon tank of an SUV with pure ethanol requires
more than 450 pounds of corn. That would be enough calories to feed one
person for a year.12

Ethanol also competes with people and livestock for water—lots and lots
of  water.  It  requires  about  4  gallons  of  water  to  produce  1  gallon  of  the
alcohol  fuel,  in  addition  to  other  water  that  production  plants  typically
recycle. Many Corn Belt regions where the production facilities are sited,
particularly  in  the  Midwest  and  the  Great  Plains,  are  beginning  to
experience  significant  water  supply  problems.  Beef  and  dairy  cattle  feed
lots located near the plants to take advantage of the co-product distillers’
grain  for  livestock  feed,  add  to  local  water  demands,  as  do  agricultural
irrigation and urban expansion.

About one-half of all ethanol plants use municipal water for some or all
production, and the rest sink their own wells. In many areas, the aquifers
that  supply  the  water  are  being  depleted  faster  than  they  can  recharge,  a
situation that is occurring in the Chicago-Milwaukee region, for example.
By  definition,  this  is  an  unsustainable  condition  that  may,  quite  possibly,
prove  to  be  the  Achilles  heel  of  future  corn-based  ethanol  programs
nationwide.13 Applying Minnesota’s water consumption averages to national
ethanol  production  estimates,  water  consumption  will  have  increased  254
percent  by  volume  between  1998  and  2008.  Lack  of  adequate  water  is
already curtailing some requests for new ethanol plant permits. An example
is  a  planned  Lincoln-Pipestone  Rural  Water  System  application  that
couldn’t meet a 350-milliongallons-per-year water requirement needed for a
proposed 100-million-gallons-peryear ethanol plant.14

Ethanol production is also being linked to water pollution. US farmers,
who planted more corn in 2007 than at any time since World War II, are
tilling more and more land that is not well suited for intensive agriculture,
exacerbating erosion and pesticide runoff that are infiltrating groundwater
and  aquifers.  Rather  than  rotating  corn  planting  with  soybeans  to  replace
soil  nitrogen,  many  farmers  are  planting  corn  year  after  year  and  adding
large  amounts  of  nitrogen  fertilizer.  On  average,  about  30  pounds/acre  of
each 140 pounds/acre of nitrogen fertilizer leaches away and runs off into
creeks, lakes, and aquifers. Some winds up in drinking water, posing special
health  problems  for  children  and  pregnant  women.  More  runoff  occurs
when  corn  isn’t  rotated  with  other  crops  because  the  soil  develops  more
clumps,  which  results  in  the  need  for  more  tilling  and  hence  becomes
looser, which can result in more erosion.15

Then  there  is  the  issue  of  emissions.  Even  though  ethanol  fuel  may
  than  does  gasoline,  it  nevertheless  releases

produce  marginally  less  CO

2

large  quantities  of  nitrogen  oxide  (smog)  that  causes  respiratory  disease.
This can add to an already large problem in many urban areas, such as Los
Angeles and throughout the Northeast.16 Thus, living near ethanol plants can
be  unpleasant.  More  than  two  hundred  such  plants,  located  in  a  swath
extending from Nebraska and Kansas east into Ohio, emit thousands of tons
of CO
 and various pollutants. As Frank O’Donnell, president of Clean Air
Watch, observed, “I think word is getting out that ethanol refineries can be a
heck  of  a  problem  if  you  live  near  them.  You’re  taking  areas  that  are
generally not seeing a lot of pollution now and darkening the skies.”17

2

2

What  about  ethanol’s  prospective  benefits  to  reduce  global  warming?
After all, Al Gore often mentions with pride how, as vice president, he cast
a tie-breaking Senate vote August 4, 1994, that mandated use of ethanol. It
guaranteed  ethanol  and  other  renewable  fuels  a  15  percent  share  of  the
lucrative fuel oxygenate market in 1995, with that amount increasing to 30
percent in following years.18 Mr. Gore also joined the venture capital group
Kleiner Perkins Caufield & Byers (in November 2007), whose key partner,
venture capitalist John Doerr, is pushing for expanded biofuel use. Yet there
is growing evidence that biofuels may actually release more CO
 emissions
than conventional petroleum-based gasoline does. As reported in the journal
Science, “Corn-based ethanol … instead of producing a 20 percent savings,
nearly  doubles  greenhouse  emissions  over  30  years  .  .  .  Biofuels  from
switchgrass, if grown on US corn lands, increase emissions by 50 percent.”
This  is  because  biofuel  markets  encourage  farmers  to  level  forests  and
convert  wilderness  areas  into  farmland,  which  would  otherwise  serve  as
CO

 sinks.19
Still  another  problem  with  ethanol  is  that  it  isn’t  very  efficient  as  an
energy source as compared with petroleum. For one thing, since its energy
density is about one-third less than that of gasoline, more must be burned to
produce  the  same  amount  of  power.  It  is  also  more  energy  intensive  to
produce. On average, an oil company burns energy equivalent to about 1
gallon of oil to process 20 gallons of gasoline, while ethanol yields versus
energy  requirements  are  only  slightly  positive  at  best.  It  takes  burning
almost a gallon of ethanol to produce 1 gallon of ethanol (subject to debate
even  on  that  small  gain).  Sugarcane  ethanol  processing  in  Brazil  is  only
about one-third as energy intensive as corn ethanol processing, but the fuel
is virtually barred from US import by a $0.54/gallon tariff applied to protect
American markets. Since large amounts of fossils are consumed to irrigate,

2

Ethanol  transportation  imposes  additional  energy  costs.  Unlike  oil  and
natural gas, it can’t be moved through existing pipelines because it readily
absorbs  water  and  various  impurities.  Instead,  it  must  be  transported  by
truck  or  rail,  either  of  which  is  much  more  expensive.  Ethanol  produced
from plant cellulose rather than corn is advertised as a promising alternative
to avoid competition with food crops, but it has yet to be demonstrated as a
viable  commercial  option.  It  would  also  impose  even  vastly  larger
transportation requirements in addition to the processing complexities and
difficulties.  Replacing  50  percent  of  current  gasoline  consumption  using
cellulosic ethanol would require about 13 percent of all US land, along with
enormous environmental and economic costs.20

And what about the costs to the consumer? Who comes out ahead on the
deal? You may have guessed by now that we’re being hosed at the pump
along with the fuel.

fertilize,  harvest,  and  process  corn  ethanol,  it  serves  as  little  more  than  a
way to recycle oil and natural gas into a different fuel form, offering no real
advantages and some major liabilities.

During  2006,  US  taxpayers  provided  subsidies,  courtesy  of  our
government, that totaled about $7 billion for 4.9 billion gallons of ethanol
($1.45/gallon of ethanol and $2.21/gallon of gasoline replaced).21 Because
producing that gallon of ethanol cost $0.38 more than making gasoline with
the  same  quantity  of  energy,  that  amounted  to  $1.12  extra  profit.  Of  the
total, $2.5 billion came from subsidies of $0.51/gallon paid out of taxes as a
blender’s  credit,  and  $0.9  billion  was  paid  out  of  tax  money  for  corn
subsidies.  In  addition,  consumers  paid  $3.6  billion  extra  at  the  pump.
Compared  with  subsidies  paid  to  the  oil  industry  based  upon  amount  of
energy produced, ethanol subsidies are more than fifty times higher.22

The  conglomerate  Archer  Daniels  Midland  Company  (ADM)  has  done
well since it first began pushing ethanol in the 1970s. A decade later, ADM
was  producing  175  million  gallons/year;  the  business  has  now  grown  to
produce  more  than  1  billion  gallons/year,  supported  by  more  than  two
hundred different tax breaks and subsidies worth at least $5.5 billion/year.23
Since 2000, ADM has contributed about $3.7 million to state and federal
politicians. For more than a decade, nearly half (or more) of the company’s
profits have come from products the US government has either subsidized
or protected.24

So  just  how  green  is  ethanol?  Its  production  requires  tremendous
amounts  of  fossils,  water,  and  agricultural  land  that  would  be  more
productive if used to grow food crops. At best, ethanol could replace but a
small  percentage  of  fossil-fuel  demands,  and  then  it  could  only  be  cost
competitive through high tax–supported subsidies.

Power Lunches: They’re Not Free After All

 

So,  maybe  ethanol  alcohol  isn’t  the  big  energy  solution  that  many
thought  it  was  brewed  up  to  be.  What  about  all  that  free  wind  and  Sun
power  that’s  unlimited  and  clean?  Also,  there’s  power  from  water  flow,
from Earth’s heat, and from simple hydrogen molecules—all very natural
sources. Why aren’t we taking fuller advantage of them? Has anyone really
thought about that? The answer is yes.

WIND ENERGY: A NATURAL LONG-TERM INVESTMENT
Wind  power—in  contrast  to  ethanol,  for  example—actually  provides
some net energy advantages, although not nearly as much or as cheaply as
we might imagine. Still, given the facts that energy costs will continue to
rise as fossil-fuel resources become scarcer, we will need every source that
can  offer  real  contributions.  It  is  vital  to  begin  expanding  infrastructures
now for future needs, and to begin to amortize investment costs on a long-
term basis. Currently, virtually all these up-front costs are being borne by
taxpayers,  through  subsidies  and  other  development  incentives,  and  by
consumers, through higher electricity bills. Yet there is a precedent for this.
Large hydropower programs were built on public funding too.

Advocates of green energy have grossly exaggerated the capacity of wind
power to make a major impact on US electrical needs. Greenpeace is off the
charts  in  this  regard,  claiming  that  “wind  could  supply  more  than  three
times  the  total  amount  of  electricity  produced  in  the  United  States.”  Any
euphoric  fantasy  that  an  unlimited,  free,  and  clean  alternative  to  carbon-
cursed fossil-fuel sources is blowing by and we’re giving it scant notice is
exceedingly naive and misguided.25

A major point of public confusion regarding wind power potential lies in
a  failure  to  differentiate  maximum  total  capacities,  typically  presented  as
megawatts  (MW),  with  actual  predicted  kilowatt-hours  (kWh)  that  are

determined by annual average wind conditions at a particular site. Wind is
intermittent  and  velocities  constantly  change.  Unfortunately,  wind  isn’t
always  available,  especially  when  it  is  needed  most—such  as  during  hot
summer  days  when  demands  for  air-conditioning  are  highest.  For  this
reason, a backup power system (usually a gas turbine system) or electrical
supplies  from  other  power  grid  sources  must  carry  the  load.  Even  when
wind  is  available,  the  highly  fluctuating  intensity  requires  that  the  power
grid balance the supply on a second-by-second basis, adding complexities
and  costs  to  the  total  network.  The  backup  supplies,  called  “spinning
reserves,” add significant operational costs that are passed on to customers.26
Output  volatility  due  to  wind’s  intermittent  nature  varies  greatly
according to location and time of year. In 2006, wind farms in California, a
major wind state, produced power at only about 10 percent of their rated
capacity. Texas, one of the most promising wind energy states, produced at
about 17 percent capacity. In early 2007, the Electric Reliability Council of
Texas, an independent state operator, determined that only about 8.7 percent
of  installed  wind  capability  could  be  counted  on  as  dependable  capacity
during peak demand periods.27

The  dependable  capacity  of  any  electrical  system  is  referred  to  as  the
“base load” capacity: the minimum amount of proven, consistent, around-
the-clock,  rain-orshine  power  that  utilities  must  supply  to  meet  customer
demands. Here, wind and sunshine have big disadvantages.

Taking  these  volatility  reductions  into  account,  consider  a  May  2007
prediction made by the American Council on Renewable Energy that it is
“technically feasible to increase wind capacity to supply 20 percent of this
nation’s  electricity  by  2030,”  providing  340  gigawatts  (GW),  or  340,000
MW, by that time. What exactly does that mean in terms of real, available
kWh-generating  output?  Actually,  it  means  very  little  if  merely  a  minor
percentage  of  that  technical  feasibility  actually  provides  electricity  when
needed.28

By  the  end  of  2006,  wind  energy  provided  about  0.5  percent  of  US
electricity, with 11.6 GW of installed capacity. The industry has set a new
target of producing 100 GW by 2020, which is suggested to compare with
the capacity of about a hundred nuclear plants. A reality check indicates a
different picture. Unlike for nuclear plants, which produce reliable power
levels continuously, it is necessary to factor in a big discounted equivalency
factor  for  wind.  To  be  extremely  generous,  let’s  assume  that  the  actual

average  output  would  be  30  percent  of  total  installed  capacity.  In  that
optimistic case, the real output would be equivalent to less than 5 percent of
the country’s electricity, and more realistically, about half that amount.29

A  major  limitation  of  individual  wind  farms  is  that  they  simply  don’t
produce power on the massive scales needed in large cities and industrial
areas  where  space  is  at  a  premium  and  land  is  prohibitively  expensive.
Another limitation is that places where wind conditions are most ideal are
often  remote  from  areas  where  demands  are  highest,  requiring  large
investments for power transmission lines and land right-of-way use.30

2

Sites  with  suitable  wind  conditions  for  power  production  are
geographically limited. From an energy-generation point of view, the best
ones are typically along mountain ridges and coastal areas. Unfortunately,
these  same  types  of  locations  are  also  prized  for  scenic  views  and  are
overflown by bird and bat species that become turbine blade casualties. And
even though national environmental organizations such as Greenpeace and
the  Sierra  Club  have  become  wind  power  advocates  in  their  war  against
fossils,  others  who  live  in  proposed  wind  farm  locations  have  launched
strong opposition to turbines. Those areas include the Green Mountains of
Vermont, the Adirondacks in northern New York, the Chesapeake Bay off
the  Atlantic  coast  between  Maryland  and  Virginia,  Cape  Cod 
in
Massachusetts, and the ridges of northern Appalachia. Local residents have
filed  successful  protests.  They  don’t  want  their  own  backyards  cluttered
with towering turbines that would interfere with the spirit-soothing views
they paid a lot of money to enjoy.

Yet  what  true  “environmentalist”  could  possibly  object  to  nonpolluting
wind power that will help save our planet from the dreaded CO
 scourge?
Well,  for  one,  there  was  Robert  F.  Kennedy  Jr.,  nephew  of  a  popular
president  and  prominent  lawyer  for  the  NRDC,  who  fought  against  a
proposed  130-turbine  offshore  development  called  “Cape  Wind”  in
Nantucket  Sound.  Another  uncle,  the  late  Senator  Ted  Kennedy  (D-MA),
along with Senate colleague and fellow Massachusetts resident John Kerry,
didn’t want Cape Wind disturbing his vistas either. Senator Kerry explained
his  reasons  this  way:  “I’ve  always  said  that  I  think  Senator  Kennedy  has
raised  very  legitimate  issues  with  respect  to  the  siting  process  and  with
respect to location. I’ve also suggested that it’s my opinion there may be
even  better  locations  for  it.  I’ve  sat  with  Jim  Gordon  [president  of  Cape
Wind],  I’ve  sat  with  other  folks,  I’ve  met  with  Coast  Guard  people,  I’ve

tried to do due diligence on it, and I’m not sure there aren’t both windier
and, you know, more accessible areas.”31

Yes, areas not off his front beach. And there are others besides members
of the New England elite class who don’t want to live near wind turbines
either. Take Texans, for example.

In  early  2007,  owners  of  the  famous  King  Ranch,  one  of  the  world’s
largest, lobbied the Texas Legislature to pass a law to regulate wind turbine
development.32 They were seeking a bill requiring companies to obtain state
permits  based  upon  studies  to  determine  whether  the  noise  from  turbines
“interferes  with  the  property  rights  of  nearby  landowners.”  This  was
prompted after the King Ranch president, Jack Hunt, heard that managers of
a neighboring Kenedy Ranch were going to allow 240 wind turbines to be
installed on that property. He charged that the smaller ranch (only 400,000
acres) was “sacrificing the long-term value of a rare resource for short-term
revenue,” and that the turbine siting was “a horrific location.” Some folks,
even those with lots of surrounding space, just don’t want to live near wind
farms. Period!

Mortality  impacts  upon  migratory  bird  populations  and  endangered  bat
species  continue  to  rally  anti–wind  farm  activists  who  have  blocked
developments with lawsuits. Studies have shown that as many as forty-four
thousand  birds,  including  golden  and  bald  eagles,  have  been  killed  by
turbines in the Altamont Pass east of San Francisco over 2 decades.33 One
reason appears to be that prey animals tend to take shelter at the turbines
and multiply, serving as attractive bait for raptors.

In May 2006, Superior Renewable Energy received approval to build the
nation’s largest offshore wind farm, with five hundred 400-foot-tall turbines
off the coast of Padre Island, Texas. That site is right in the middle of the
path that an estimated two-thirds of all birds in eastern and central North
America follow as they migrate. One plan that has been considered to help
protect  birds  that  are  forced  to  fly  low  during  bad  weather  is  to  use
technology that turns off the blades at those times. It’s not easy to prevent
all environmental risks.

And what about wind farm influences upon whales? Yes, you read this
right!  On  June  25,  2010,  environmental  groups  filed  a  legal  challenge  to
block the planned Massachusetts offshore Nantucket Sound project because
it  will  endanger  migratory  birds  and  whales.  Are 
they  possibly
underestimating whale intelligence?

There  are  also  costs  and  risks  for  those  who  ultimately  pay  for  wind
energy  projects,  namely,  taxpayers  and  customers.  Without  large  public
subsidies and other incentives, the industry would not be solvent. More than
half  of  all  potential  return  on  investment  for  companies  that  install  and
operate  the  systems  comes  from  federal,  state,  and  local  tax  benefits.
During 2006, federal tax incentives alone were about $2.75 billion.

Special public incentives for wind energy development come in a variety
of  forms.  A  primary  break  involves  tax  avoidance  mechanisms  afforded
through  production  tax  credits  (PTCs),  accelerated  depreciation,  and
reduced or forgiven property and sales taxes.

The  federal  PTC  program,  which  began  in  1992  with  a  1.5¢  per  kWh
subsidy for wind power owners, increases at the rate of inflation (now about
3¢  per  kWh).  These  credits  are  directly  deductible  from  federal  income
taxes  and  are  particularly  valuable  to  large  companies  that  have  lots  of
profits  in  other  areas.  The  PTC  program  is  also  attractive  for  small  wind
farm developers as a way to sell their projects to larger companies for the
tax benefits they provide.34

Federal accelerated depreciation offers a subsidy that enables wind farm
investors  to  take  a  generous  double-declining  5-year  depreciation  tax
shelter. The investors can deduct 40 percent of capital investments the first
year, and 24 percent the second, and can continue until all deductions are
complete before the end of the fifth year. This is a tremendous advantage
that  allows  generators  of  this  energy  to  give  money back to shareholders
rapidly; conventional electricity plant developers, by comparison, must use
20-year depreciations. Some states that “conform” their corporate income
tax programs to the federal system allow otherwise taxable income to carry
through to the wind farm developers’ state income tax returns as well.

Some  federal  and  state  programs  afford  green  energy  providers
guaranteed  markets  at  premium  sales  prices.  States  that  have  legislated
mandatory RPSs require electric utility companies to purchase designated
amounts  of  energy  from  wind,  solar,  and  biofuel  providers,  typically  at
higher-than-conventional costs that are passed on to their customers. Some
states  require  or  encourage  electric  utilities  to  offer  “green  program”
advertising  (“greenwashing”)  to  ask  customers  to  voluntarily  sign  up  for
higher-priced,  more  “environmentally  responsible”  electricity.  Since
subscription levels tend to be low, the extra costs are distributed among all
customers  and  hidden  in  invoices.  There  is  strong  lobbying  by  the

renewable  energy  industry  to  have  RPSs  legislated  by  the  federal
government.35

Federal and state government agencies are mandating that purchases of
the  electricity  they  use  come  from  renewable  sources  at  related  premium
prices. Several state and local governments enable wind energy developers
to  reduce  or  eliminate  property  taxes.  Some  states  authorize  utilities  to
charge  special  public  benefit  taxes  to  customers  who  pay  money  to  wind
farm operators. And some states provide industrial development bonds that
enable  wind  farms  to  be  financed  by  state-backed  loans  that  have  lower-
than-commercial interest rates.

Although wind may never be a big power player in the United States, and
certainly is less of a force than represented by enthusiasts, one fact is clear:
So  long  as  fossil-fuel  costs  are  manipulated  by  government,  emission
regulations  continue  to  rise,  energy  demands  increase,  and  generous
subsidies flow, wind industry forecasts will be at least gusty.

Much to the chagrin of DOE officials and other advocates, a report from
Spain discredited the idea that wind power is a job creator, an idea touted by
the Obama-Biden administration. US officials banded with trade lobbyists
to minimize the exposed facts. The study, released by researchers at King
Juan  Carlos  University,  concluded  that  every  “green  job”  created  by  the
wind industry killed off 2.2 jobs elsewhere in the Spanish economy.36

Research director Gabriel Calzada Alvarez didn’t fundamentally object to
wind  power.  He  did,  however,  find  that  when  a  government  artificially
props up the industry with subsidies, higher electrical costs (31 percent) and
tax hikes (5 percent), along with government debt, follow. Every green job
created was estimated to cost $800,000 per year to create, and 90 percent of
those green jobs were temporary.

Alvarez specifically presented lessons for the United States. He warned
of  potential  “self-inflicted  economic  wounds”  and  forecasted  that  this
country could lose 6.6 million jobs if it followed Spain’s example.

A  few  months  after  the  study  was  released,  researchers  at  the  Danish
Center for Politiske Studier reached similar conclusions about underwriting
wind power based on their country’s experience: “It is fair to assess that no
wind energy would exist if it had to compete on market terms.”37

Both  of  the  reports  have  been  pointedly  ignored  by  the  Obama-Biden
administration,  which  has  declared  an  agenda—with  $2.3  billion  to  be
allocated in tax credits— to create seventeen thousand “high-quality green

jobs.” President Obama has said, “Building a robust clean energy sector is
how we will create the jobs of the future.”38

Then,  in  response  to  a  release  of  the  reports  and  a  critical  Washington
Post column by George Will, bureaucrats at the Energy Department, left-
wing activists, and trade lobbyists went into defensive mode. E-mail files,
obtained  by  Christopher  Horner  at  the  Competitive  Enterprise  Institute,
reveal  concerted  damage  control  efforts.  Their  strategy  was  to  hide  facts,
discredit foreign academic sources, and concoct their own white paper as a
rebuttal.

One  such  e-mail,  sent  by  Elizabeth  Salerno  of  the  American  Wind
Energy  Association  (AWEA)  to  Suzanne  Tegan  of  the  DOE’s  National
Renewable Energy Laboratory, illustrates how the industry lobby and a US
taxpayer-funded  government  entity  can  collaborate.  It  states,  “It’s  critical
we respond (to the Spanish report). This thing won’t die and it’s doing a
good job of undermining our green job message. If we put together a call
with  CAP  [the  Center  for  American  Progress],  can  UCS  [Union  of
Concerned Scientists] participate on a comprehensive response?”39

Tegan  then  called  for  a  telephone  meeting  the  next  day  to  draft  a
response. In an e-mail follow-up, she wrote: “We are working with AWEA
(who is working with UCS and others) to put out a response to this report,
which  is  methodologically  unsound  and  states  that  the  renewable  energy
policy  in  Spain  (and  therefore  the  US)  is  a  waste  of  money  and  actually
costs jobs rather than creates jobs. The report actually addresses the Obama
administration’s  ideas  and  policies.”  Tegan  urged  nine  people  on  her
recipient list to look over the report and provide comments she could pass
up her chain of command. She referred to lobbyists at AWAE and CAP as
“colleagues.”

CAP is a far-left organization funded by George Soros and led by John
Podesta.  According  to  logs,  Podesta  visited  the  White  House  thirty-one
times over 2 months in the fall of 2009, the only period for which records
are available. In an e-mail to the Investor’s Business Daily, which provided
this information, Horner commented:

The  least  revelatory  aspect  of  this  was  the  hollowness  of  the
Obama  administration’s  claims  to  have  driven  lobbyists  from  the
executive  branch.  Providing  an  inside  role  for  politically  favored
industries  in  developing  official  administration  statements  falls  even

further  from  the  rhetoric.  Worse,  with  direct  communications  with
ideological  activists  like  CAP  and  UCS  undoubtedly  the  anticipated
and regular subject of FOIA [Freedom of Information Act] requests,
we  also  see  how  the  Obama  administration  employed  an  industry
lobby to channel the influence of such groups into the administration’s
inner workings to circumvent the expected pathway for security.40

Horner also noted that DOE officials have since misled Congress on such

matters.

The wind industry spent $5 million on lobbying in 2009, up from $1.7
million  the  previous  year.  It  currently  is  reported  to  have  thirty-six
lobbyists, up from two in 2004.

Does the industry create jobs? In all fairness, yes. For example, a new
wind farm in West Texas created twenty-eight hundred jobs. Unfortunately,
twenty-four  hundred  of  those  were  in  China,  with  just  four  hundred
temporary positions in the United States.41

SOLAR POWER: REMOTE POSSIBILITIES
Like wind, solar power is a natural, free source of energy—provided, of
course, that public subsidies and customers of high-price electricity cover
the  large  costs.  Solar  power  is  also  very  versatile.  It  can  provide  thermal
energy  to  heat  water  and  electricity  to  power  spacecraft  above  Earth’s
atmosphere as well as small handheld devices. It can also produce utility-
grade electricity for those who live in a desert and have a habit of going to
bed very early without their television on. But don’t count on solar power to
deliver  us  any  distance  along  the  road  to  energy  independence.  Not  even
during the industry’s sunniest seasons.

This is likely to come as a very unwelcome observation to many people.
When the trade association for the nuclear power industry asked a thousand
Americans in 2007 about what energy source they thought would be used
most in 15 years, the winner was the Sun, the choice selected by 27 percent
of those polled.42 Yet it will be truly remarkable if solar power accounts for
more than even 1 percent in the coming decades. It currently provides only
about  6  percent  of  the  US  electricity  that  is  derived  just  from  renewable
sources.

Thanks almost entirely to a variety of federal and state subsidies the solar
industry is witnessing growth, up about 43 percent during 2007. This added
110  MW  of  new  capacity,  but  onto  a  very  small  previous  base.  At  least
eight large-scale solar power projects are reported to be under development,
comparable to adding six 500 MW coal-fired plants.43

The solar power industry really has two very different types of segments:
One markets photovoltaic panel units to private and commercial customers,
and  another  provides  utility-scale  electricity  from  central  power  stations.
Each segment is realizing progressive technology developments that lower
costs and improve efficiencies.

Panel-type units (typically bolted to roofs) that convert sunlight directly
into electricity continue to be very expensive relative to power benefits. A
small,  4  kW  home  installation  costs  about  $34,000  before  government
rebates and tax breaks. Solar energy conversion efficiencies range from an
average of about 16 percent to as high as 22 percent in direct sunlight. But,
like wind, power is intermittent; often, it isn’t available when you need it or
in  the  amounts  required.  A  lot  depends  upon  both  location  and  the
orientation  of  the  roof  or  other  structure  the  system  is  mounted  to.  An
advantage of solar over wind is that peak power occurs at midday, which is,
correspondingly, a peak demand period.

Nationwide  solar  power  expansion  is  severely  constrained  by  both
geography and the power source’s fundamentally intermittent nature. High
capital cost is also a factor, but let’s assume that increasing fossil-fuel prices
and  technological  advancements  cure  those  problems  over  time—as  well
they might—and that regions are favored with clear skies over much of the
year and that rain, dust, and snow accumulations don’t interfere. And even
under the best conditions, there is a recurring cyclical problem. It is called
“night.”

World  markets  for  silicon,  a  key  ingredient  of  solar  cells,  have
contributed  to  the  high  costs  of  photovoltaic  panel  systems.  Some  newer
technology  markets  are  replacing  the  panels  with  thin-film  photovoltaic
sheets  that  are  less  expensive,  but  presently,  these  are  only  about  half  as
efficient.44 Accordingly, they require more space and are mostly geared for
large  commercial  business  applications.  Traditional  photovoltaic  cell
systems now constitute about 90 percent of the market.

Utility-scale concentrating solar power (CSP) systems, which use lenses
or  mirrors  that  track  the  Sun  to  focus  radiation  on  thermal  collectors  or

photovoltaic cells to produce electricity, are another alternative. They can
range in size from as small as 10 kW to more than 100 MW. Because they
require  direct  sunlight  (not  diffuse  light),  their  use  is  limited  to  Sun  Belt
locations. CSP systems that use lenses to concentrate light onto photocells
are  much  more  efficient  and  require  less  surface  area  than  standard
photovoltaic  panel  approaches  do.  Solar  thermal  CSP  systems  reflect
sunlight  energy  onto  receiver  units  that  contain  a  heat  transfer  fluid  or
molten nitrate salt to drive power turbine generators. Overall, solar-electric
conversion  efficiencies  are  relatively  low,  ranging  from  about  13  to  25
percent.

Energy  storage  continues  to  be  a  big  problem  for  CSP,  just  as  it  is  for
wind. Installations can operate only as part of a larger system network that
provides backups such as fossil-fuel generation or connections to a nuclear-
powered  grid.  New  plants  in  sparsely  populated  desert  locations  also
impose power transmission infrastructure development costs, oftentimes as
much as $1.5 million per mile.

As  with  all  energy  options,  some  environmentalists  don’t  like  CSP
stations either. One complaint is that they take up too much desert land, thus
displacing certain animal and reptile species. A mirrored CSP installation
may require between 5 and 15 acres per MW, compared, for example, with
an equivalent gas-fired generation requirement that can be as little as 1/25
of an acre.45

During 1993 congressional hearings, the Sierra Club and the Wilderness
Society  testified  in  favor  of  preserving  areas  within  California’s  Mojave
Desert  from  commercial  development,  including  solar  and  recreational
uses. The president of the Wilderness Society explained why:

The California desert contains some of the most wild and beautiful
landscapes in America, but these lands are being continually degraded.
The fragile desert soils, scarce water, unique ecosystems, irreplaceable
archaeological  sites,  and  spectacular  scenic  beauty  are  receiving  too
little protection in the face of a variety of development pressures. The
opportunity to experience what remains of the frontier quality of the
region is rapidly disappearing as development spreads. The public has
lost much of this priceless heritage already; it is time to save the best
of what remains as a lasting gift to future generations.46

And  then  there’s  the  problem  with  pollution  from  photovoltaic  solar
panels. Can this be true? Haven’t we been told that solar power is clean?
Sure,  but  making  those  collectors  isn’t,  because  they  are  manufactured
using highly toxic heavy metals, gases, and solvents. Some of the materials
are carcinogenic. Some of the gases are lethal, and some are explosive. Not
all  factories  that  produce  photovoltaic  panels  incorporate  scrubbers  to
protect  against  accidental  releases,  but  workers  in  those  plants  must  be
strictly  protected.  Fires  in  such  facilities  can  be  particularly  hazardous.
After solar panels are decommissioned following about 20 to 30 years of
useful  life,  they  are  supposed  to  be  disposed  of  in  special  toxic  dumps
rather  than  sent  to  waste  incinerators  where  the  heavy  metals  (such  as
cadmium  and  lead-based  solder)  can  vaporize  into  the  atmosphere.  Or,  if
dumped into landfills, the arsenic and lead can leach into the soils and water
tables.48

New designs are lower in toxicity and are required to pass DOE tests to
reduce these hazards. Still, large photovoltaic farms capable of producing
1,000 MW per year would cover 50 square miles or more of land, and their
production  would  yield  substantial  quantities  of  toxic  waste  that  present
costly and difficult disposal challenges.

Based  on  that  argument,  another  environmentalist  even  argued  that  a
nuclear  option  might  be  preferable:  “From  the  standpoint  of  scenic
pollution and destruction of wilderness, there are distinct advantages to the
hard  energy  option  .  .  .  A  nuclear  plant  modifies  a  relatively  small  area
compared to a large-scale solar installation.”47

Solar  power  continues  to  require  subsidies  to  be  competitive  even  in
desert locations where clear skies prevail. The United States Air Force is
taking advantage of these incentives to support development of the largest
North American solar plant to provide electricity for the Nellis Air Force
Base located outside Las Vegas. The power facility covers 140 acres of the
Nevada desert with a massive photovoltaic array of silicon cells that rotate
to  track  the  Sun.  The  facility  is  capable  of  producing  15  MW  of  power,
enough to provide about 30 percent of Nellis’s requirements, where 12,000
people  work  and  7,215  residents  live.  The  Air  Force  expects  to  save  $1
million per year in power costs, thanks to multimillion-dollar federal and
state financial subsidies and incentives. “Without those, prices wouldn’t be
competitive,”  according  to  Daniel  Tomlinson,  editor  of  a  solar  newsletter
for Navigant Consulting. David Edwards, a market analyst of green power

with ThinkEquity Partners of San Francisco, agrees: “The price of solar is
coming down, but today those subsidies are important.”49

The  Nellis  project  was  developed  through  a  complex  arrangement
between the Air Force and three financial partners. The Air Force will not
pay  any  of  the  construction  costs,  but  essentially  guarantees  the  market.
Private  investors  will  pay  more  than  $100  million  in  capital  costs  and
receive  substantial  federal  tax  subsidies.  For  example,  MMA  Renewable
Ventures and its investors enjoy a 30 percent tax credit, have the benefit of
accelerated  capital  depreciation  schedules,  and  sell  solar  energy  credits
generated by the project to NV Energy, which must obtain 20 percent of its
power from renewable sources by 2015.

So  there  you  have  it.  Our  country’s  Air  Force  (that  is,  the  US
government)  is  saving  about  $1  million  a  year  because  that  same  US
government is providing many tens of millions of dollars in tax incentives,
in combination with generous contributions in the form of higher electricity
prices  charged  to  Nevada  customers.  Just  think  of  all  that  money  the
government is saving us! Caution: Don’t attempt this stunt at home.

HYDROPOWER: FEW DAM PROSPECTS
Hydropower is an important and sustainable source of energy that many
environmentalists  don’t  want  to  claim  as  an  alternative.  Although  it
produces  nearly  half  of  all  US  renewable  electricity—about  equal  to
biomass—its impact upon fish and aquatic ecosystems earns it enmity.

Expansion of hydroelectric capacity is not a major option in this country
because most primary sites are already being exploited or are off-limits for
environmental  reasons.  Dams  currently  provide  about  two-thirds  of  all
electricity  in  the  Pacific  Northwest  region  and  are  dominant  sources  in
Idaho and Washington.

Although  hydropower  produces  no  pollutant  emissions  per  se,  dam
construction and operations can have significant ecological repercussions.
As  they  swim  downstream  toward  the  ocean,  many  young  salmon  in  the
Northwest are killed by turbine blades, and adult fish attempting to swim
upstream to reproduce are blocked by dams. After salmon populations were
dramatically reduced in the Northwest Columbia Basin, fish channels and
side channels were built to help alleviate this problem. Also, because water
in the dams tends to be colder and oxygen-poor at the bottom as compared

with  the  surface,  rapid  releases  of  dam  water  can  kill  fish  and  damage
wildlife vegetation food sources downstream.

Pressure from mainstream environmentalists persuaded the Bush-Quayle
administration  to  drop  incentives  to  promote  hydro  development  in  the
Energy Policy Act of 1992, and both the Sierra Club and Trout Unlimited
criticized the Clinton-Gore administration that followed for including it as a
global warming prevention strategy.

Trout Unlimited is also lobbying to remove four major dams that have
been constructed at enormous cost on the Columbia River, due to alarming
trout casualties. Farmers who have come to depend upon the dams for water
irrigation oppose the opposers. Other groups attribute the salmon losses to
overfishing by indigenous Indian residents (the US government purchased a
half  million  dollars’  worth  of  nets  for  their  use)  and  consumption  of
fingerlings by a large colony of terns from a nearby island bird sanctuary
created  by  the  United  States  Army  Corps  of  Engineers.  Trout  Unlimited
also wants to eliminate the bird sanctuary, a proposal that is opposed by the
Audubon Society.

As further evidence of hydropower’s recent politically incorrect status, it
was dropped from renewable category statistical listings in the 1995 edition
of  the  Electric  Power  Annual,  published  by  the  US  Energy  Information
Administration.  Environmental  lobbying  even  prompted  the  US  Export-
Import Bank to deny funding for China’s 18,000 MW Three Gorges Project
when Friends of the Earth and other groups expressed concerns about water
quality, endangered species, and population resettlement impacts.50

Then there’s the possibility of harnessing wave and tidal energy. Those
should be limitless sources, right? After all, oceans are really big, and all
you need to move water around is a Sun-Moon system, and we have one of
those.

But some of the technologies do use turbine blades, presenting potential
dangers  to  fish  and  aquatic  mammals.  Tidal  power  generators,  mostly  in
experimental  and  prototype  testing  stages,  draw  energy  from  underwater
currents.  One  type  uses  the  oscillating  motion  of  water  flowing  past
hydroplane  fins  to  drive  motor  generators,  while  another  operates  with
propeller blades like an underwater wind turbine. Sea-Gen, a system being
tested in Northern Ireland, uses sets of rotor turbines rated at about 1 MW.51
One big limitation of tidal power is that it occurs only twice per day, and
neither of these times coincides with peak power demand periods. Another

constraint  is  that  practical  locations  are  limited  to  places  where  high-
amplitude  tides  exist,  including  river  and  fjord  estuaries  in  the  former
USSR,  Canada,  Korea,  and  the  UK.  Also  problematic  are  relatively  low
output  capacities,  high  maintenance  costs  due  to  corrosion  of  mechanical
parts, and expenses of transmitting power from offshore installations to end
users.

Harnessing power from wave motion may eventually yield more energy
than  from  tides.  New  technologies  are  being  developed  involving
generators  that  can  either  be  coupled  to  floating  devices  or  turned  by  air
displaced by waves. The amount of power produced is determined by wave
height, speed, wavelength, and water density at a given location and time,
and wind conditions are also influential. Northern and southern temperature
zones offer the best wave power locations, with prevailing westerly winds
in winter blowing strongest to produce a “fully developed sea.” Prospective
placements include shoreline, offshore, and deepwater applications. Power
conversion  methods  include  hydraulic  rams,  elastomeric  hose  pumps,
pump-to-shore  systems,  and  hydroelectric  or  air  turbines.  Despite  being
innovative and interesting, potential capacities from tides and waves to be
sufficient  to  seriously  offset  dependencies  upon  other  energy  sources  are
extremely doubtful.

GEOTHERMAL ENERGY: REGIONAL REALITIES
Geothermal  energy  seems  too  good  to  be  true,  and  unfortunately  for
most parts of the United States, it probably is. Think of all that heat, pure
energy,  directly  under  all  of  our  feet—enough  to  provide  countless  times
more  power  than  we  will  ever  need.  But  accessing  it  is  where  the  big
problem arises.

Geothermal  options  fall  into  two  general  categories:  conventional  and
unconventional. The conventional option is hydrothermal, which taps into
hot  water  and  steam  reservoirs  that  exist  only  in  certain  regions  of  the
country.  The  most  economically  feasible  hydrothermal  resources  are
principally  located  west  of  the  Rocky  Mountains,  and  only  California,
Hawaii,  Nevada,  and  Utah  currently  have  operating  power  plants.  The
majority of thermal springs and other surface manifestations of underlying
geothermal  resources  are  also  located  in  the  West,  including  Montana,
North Dakota, and Wyoming.52 Some lower-temperature resources also exist

in central Texas. In total, more than twenty hydrothermal power plants (the
only utility-scale geothermal facilities) produce less than 0.5 percent of all
US electricity.53

More than 90 percent of all installed US geothermal capacity is generated
in  California  and  Nevada,  with  about  half  produced  at  the  Geysers  in
northern  California.  While  that  energy  is  very  clean  with  regard  to
emissions,  it  is  not  technically  renewable  because  the  heat  content  of
reservoirs gradually declines over years of production, often playing out to
unusable temperatures in 50 years or less. Power output at the Geysers, for
example, dropped about 40 percent between 1990 and 2000, due in large
part to reduced reservoir volume.54

As is the case with all energy options, geothermal is not immune from
environmental critics. Because most of the usable reservoirs are located in
remote, scenic wilderness areas, construction of plants, access roads, power
lines, and other infrastructure is perceived as a blighting intrusion on nature.
Geothermal power also consumes large amounts of water, which can impact
aquatic ecosystems and wildlife habitats. Wastewater released from plants
can  potentially  contaminate  surface  and  groundwater  as  well.  For  these
reasons, many prospective sites are restricted from development.55

According to a lawsuit filed against two proposed power plants in the US
District  Court  for  the  Eastern  District  of  California,  the  developments
would  introduce  “highly  toxic  acids”  into  geothermal  wells  in  the  state’s
Medicine Lake Highlands, turning the lands into “an ugly, noisy, stinking
wasteland.” The lawsuit asserted that groundwater pollution would pose a
threat to trout and other wildlife; that the plants would require excavating
750,000-gallon toxic waste sumps; and that trucks and drilling equipment
would  break  the  normal  solitude  of  the  area.  Construction  would  include
unsightly  150-foot-high  drilling  rigs,  nine-story  power  plants  on  15-acre
pads,  seven-story  cooling  towers  capped  by  steam  plumes,  crisscrossing
roads, high-tension transmission lines, and pipelines.56

installations,  should 

Unconventional  geothermal 

into
existence,  aren’t  likely  to  be  popular  with  many  environmental  groups
either.  They  include  (1)  enhanced  geothermal  systems  (EGSs)  that  create
new reservoirs using oil and gas industry technology, and (2) geopressured
and geothermal operations that either drill or take advantage of abandoned
oil  and  gas  “wells  of  opportunity”  to  assess  and  exploit  deep  reservoir
resources.

they  come 

EGSs are proposed to produce energy from geothermal resources that are
otherwise not economical due to lack of water and/or ground permeability.
This  involves  pumping  cold  water  or  water  containing  acids  or  other
chemicals into ground cracks that are too small to allow geothermal fluids
to  flow,  causing  fractures  that  enlarge  them  to  productive  sizes.  An
enthusiastic  report  published  by  MIT  estimates  that  this  approach  could
produce  100  GW  of  US  power  by  2050.  Total  US  electrical  power
consumption is currently about 3,300 GW.57

A study by Jeremy Griggs at Louisiana State University of geopressured
geothermal aquifer potential points out some limitations. A big impediment
is development scale, particularly where commercial aquifers are likely to
be in excess of 10 square miles, causing small-acreage landowners to derail
project  opportunities.  He  concluded  that  medium-term  development
prospects  will  depend  upon  sustainability  of  high  natural  gas  prices;
application  and  acceptance  of  new  technologies;  and  diversification  of
conventional  exploration/production  companies  and  electric  utility
companies to accept the opportunities. The long-term likelihood of large-
scale geopressured aquifer development was predicted to be low.58

HYDROGEN ECONOMY: AN OXYMORON
Imagine the wonderment of a new energy economy based upon vehicles
and  industries  fueled  by  hydrogen  with  no  emissions  other  than  water.
Wouldn’t  that  move  us  away  from  fossil-fuel  dependence  toward  a
blissfully clean environment? Regrettably, only in your dreams! Conserving
energy  through  hydrogen  use  is  the  logical  equivalent  of  converting
processed petroleum back into crude oil in order to stretch oil reserves and
avoid pollutants.

In reality, hydrogen is not characterized by anyone with a background in
physics  or  chemical  engineering  as  an  energy  source  at  all.  Instead,  it  is
defined  as  an  energy  “carrier”—a  way  to  convert  energy  produced  from
another source for storage and use in a different form. This always imposes
energy penalties along with other costs that may sometimes be justified, but
should not be ignored. A free lunch it definitely is not.

Although hydrogen is popularly associated with renewable fuels, most of
the commercial hydrogen in use today is produced as a by-product of fossil-

fuel sources, primarily natural gas. Producing that steam requires energy. It
also consumes natural gas, a nonrenewable fossil-fuel resource.

transfer 

Producing hydrogen through water electrolysis is possible but extremely
energy intensive, with a large net energy loss. If that electricity source is
wind,  the  power  required  must  be  subtracted  from  competing  electricity
uses. If the wind farm is part of a utility grid, then natural gas consumed for
the  spinning  reserve  also  constitutes  a  cost.  If  the  electricity  comes  from
solar  power,  the  penalty  of  water  consumption  in  arid  desert  power  plant
locations becomes a major environmental and practical impediment.59

transportation,  bulk  storage,  and 

In  addition  to  requiring  more  energy  to  create  than  it  yields,  hydrogen
imposes  other  energy  penalties  associated  with  compression,
also 
liquefaction, 
to  end-use
destinations. These problems primarily relate to the fact that hydrogen gas
is the smallest molecule that exists in nature, making it difficult to contain
so that it doesn’t leak out. Unlike natural gas, it can’t be transported through
pipelines.

Hydrogen does not compress easily, requiring energy to increase pressure
sufficiently to compensate for a low energy/volume density. Compressing it
into small containers for transport requires strong and heavy tanks, which
adds  equipment  costs.  Liquid  hydrogen  imposes  even  more  costs  for
liquefaction and tank insulation to prevent the gas from boiling away.

Although hydrogen has a good energy density based upon weight, it has a
poor  energy  density  relative  to  storage  volume.  To  illustrate,  a  gallon  of
liquid  gasoline  weighing  0.9  pounds  actually  contains  about  50  percent
more hydrogen than a gallon of liquid hydrogen weighing 0.6 pounds. This
means that a fuel tank for a hydrogen-fueled car will be much larger and
heavier than a fuel tank for a car that runs on gasoline, unless the driving
range on a tank is reduced substantially.

If  hydrogen  for  automotive  use  is  produced  from  natural  gas,  it  would
make more sense to use the original fuel directly. One reason is for safety.
Hydrogen is highly combustible and will burn in concentrations as low as 4
percent in air. It explodes upon ignition when mixed with air, a fact well
recognized  by  rocket  scientists  and  some  poorly  supervised  high  school
chemistry students.

Hydrogen  does  offer  important  energy  conservation  benefits  in  certain
applications.  A  prime  example  is  for  combined  heat  and  power  fuel  cell
operations that recycle the heat product back into a power system or use it

for  other  purposes,  such  as  air-conditioning.  Hydrogen  technologies  and
applications are most certain to have expanding roles in our energy future
on Earth and in space. But short of some revolutionary and unforeseeable
breakthrough,  they  will  not  present  a  supply-side  solution  to  our  energy
challenges.

Nuclear Power: Elephant in the Closet

 

Nuclear energy must certainly qualify as the world’s least appreciated
and  understood  power  source.  Although  it  provides  some  of  or  all  the
electricity used by more than 1 billion people, and nearly 20 percent of the
US  supply,  it  is  virtually  invisible  even  to  nearby  consumers  who  don’t
realize  it  is  in  their  backyards.  Few  are  aware  that  there  are  104  nuclear
power  plants  distributed  throughout  many  regions  of  this  country,  which
enjoy cleaner air due to their presence.

Nuclear  power  plants  are  environmentally  benign  and  reliable.  They
occupy  very  little  land  area,  produce  only  water  vapor  emissions,  and
require  no  major  transportation  infrastructure.  They  are  extremely  safe,
presenting  no  explosion  or  radiation  contamination  risks,  which  tend  to
worry many people most. And in stark contrast to so-called renewable or
sustainable options, as well as fossil-fuel sources, nuclear power expansion
and longevity capacities are vast.

Nuclear safety and waste hazards primarily arise from regional, national,
and geopolitical issues rather than intrinsically technological problems. In
general, unreasonable and misguided policies are often driven by opposition
to  nuclear  power  based  upon  unwarranted  fear  associated  with  the
Chernobyl  and  Three  Mile  Island  incidents,  which  are  now  readily
preventable.  Licensing  and  construction  of  vital  power  plants  and  waste
storage accommodations are being delayed at a time when other countries
are greatly benefiting from developing these facilities. The United States,
once a leader in nuclear technology development, has fallen behind, both in
science  and 
from
counterproductive  opposition,  a  misinformed  public,  and  bad  policy
decisions,  is  causing  excessive  use  of  precious  fossil-fuel  resources  and
economic burdens that we and future generations can little afford.

infrastructure.  This  circumstance, 

resulting 

in 

The DOE’s EIA estimates that US electricity demands will increase by
about 45 percent by 2030, requiring at least 350,000 MW of new capacity.60
Given that “renewable” sources such as wind, solar, geothermal, and hydro
afford extremely limited expansion potential, the only other non-fossil-fuel
alternative is nuclear. Very fortunately, this option offers large and lasting
growth possibilities. The Nuclear Energy Institute proposes to expand the
nuclear  share  of  US  electricity  production  to  50  percent  by  2050.61  The
World  Nuclear  Association, 
the
International  Atomic  Energy  Agency  believe  that  uranium  resources  are
adequate to meet global power demands for thousands of years.

the  World  Energy  Council,  and 

Uranium is abundant in the United States and in many other parts of the
world, and no other known source can compare in terms of energy density.62
One  ton  of  ore  can  yield  about  4  to  6  pounds  of  yellowcake  uranium,
enough to produce a single pellet of low-enriched uranium oxide weighing
0.24  ounces  (about  7  grams—  slightly  less  than  the  weight  of  three
pennies). That one small pellet contains the same energy as:

•  1,780 pounds of coal
•  149 gallons of oil63
•  157 gallons of regular gasoline64
•  17,000 cubic feet of natural gas

Consider  this  density  in  terms  of  power  plant  land  requirements  for

various options based upon 1,000 MW of installed capacity:

•  According to the AWEA, 1,000 MW of wind farm capacity requires
about  60,000  acres  (94  square  miles);  other  agencies  more  than
double that estimate (intermittent power).65

•    Producing  1,000  MW  from  solar  power  will  require  photovoltaic
arrays  covering  more  than  50  square  miles  (also  intermittent
power).66

•    An  equivalent  1,000  MW  of  energy  from  biofuel  alcohol  could
require about 6,200 square miles of cornfields; about 9,000 square
miles  of  rapeseed  fields  for  bio-oil;  or  burning  of  about  12,000
square miles of wood biomass.

•   A  1,000  MW  nuclear  plant  requires  about  1/3  to  1/2  of  a  square

mile.67

Nuclear  energy  is  finally  gaining  political  traction  from  some  global
warming/carbon-trading  cadres  who  were  previously  staunch  opponents.
For  example,  Patrick  Moore,  a  cofounder  of  Greenpeace  who  rose  to
prominence in the 1960s for his strong stand against nuclear testing, told the
House  Government  Reform  Subcommittee  on  Energy  and  Resources  in
April  2005,  “Nuclear  energy  is  the  only  non-greenhouse-gas-emitting
power source that can effectively replace fossils and satisfy global demand
for energy.”68

Nuclear plant operators are poised to reap billions of dollars in windfall
profits  if  carbon  cap-and-trade  legislation  is  enacted.  Those  who  stand  to
gain most include the Exelon Corp.; FPL Group Inc.; Constellation Energy
Group;  Entergy  Corp.;  FirstEnergy  Corp.;  NRG  Energy,  Inc.;  and  Public
Service Enterprise Group Inc. They will gain in two ways. First, they won’t
have to purchase emission allowances, giving them an advantage over fossil
fuel–dependent  companies.  Cap-andtrade  will  also  push  wholesale
electricity  prices  higher  in  deregulated  markets,  as  coal-  and  natural  gas–
burning utilities jack up charges to their customers, making nuclear energy
even more competitive.69

Yet, notwithstanding its appealing features for dreaded GHG avoidance
and  windfall  cap-and-trade  profits  that  will  raise  energy  costs  to  promote
conservation,  nuclear  power  continues  to  have  powerful  environmental
opponents.  The  Sierra  Club,  for  example,  remains  strictly  antinuclear,
although its position has vacillated over time. During the 1960s, when the
group  was  lobbying  against  big  hydroelectric  projects  it  believed  were
damaging California wilderness areas, its slogan was “Atoms, not Dams.”
In 1970, its board voted in favor of nuclear plants, such as one in Diablo
Canyon, and issued a “crisis report” requesting that irrigation districts build
and manage a nuclear plant for the city of San Francisco rather than another
planned hydropower dam. Then, 5 years later, the board reversed its policy,
began  to  oppose  nuclear  plants,  and  lobbied  to  shut  down  the  Diablo
Canyon plant.70

While  the  United  States  has  refrained  from  reprocessing  spent  fuels,
some other countries are combining the plutonium oxide that is chemically
separated from spent fuels with plutonium from warheads to create a low-
enriched  (about  4.5  percent)  power  reactor  fuel  referred  to  as  “MOX.”
Europeans have been doing this for more than 20 years, reducing stockpiled
weapons-grade  plutonium  in  the  process.  World  stockpiles  of  nuclear

warhead plutonium, now estimated to be about 260 metric tons, offer the
potential to provide 1 year’s worth of global uranium needs for electricity.
MOX  will  eventually  be  produced  from  plutonium  and  highly  enriched
uranium  obtained  from  dismantled  weapons  at  the  DOE’s  Savanna  River
site in South Carolina. Yet antinuclear groups oppose the manufacturing and
use of MOX, even though it converts war materials into energy for essential
and  peaceful  uses.  Turning  weapons  stockpiles  into  low-enriched  and
proliferation-resistant fuel would seem to be a purpose that everyone might
support.

Many Americans tend to be quite unaware of just how much our nation
depends upon nuclear power and of the industry’s long, impressive safety
record  throughout  the  world.  This  ignorance,  based  heavily  upon  widely
circulated media misconceptions and strident opposition campaigns, thwarts
progress in addressing vital energy priorities. Informed education is key to
progress.

Other  countries,  including  France  and  Finland,  educate  children  about
nuclear  power  from  early  years  so  they  grow  up  to  have  realistic
perspectives. It took France only 2 decades to switch over 80 percent of its
electricity generation to nuclear, and now many of that nation’s informed
young  people  have  grown  up  to  become  leaders  in  nuclear  power
technology  development.  The  world’s  most  advanced  experimental
thermonuclear  fusion  reactor  is  under  construction  in  Cadarache,  France,
and the country—with fifty-nine nuclear plants already—is the largest net
exporter  of  electrical  power.  The  French  reprocess  spent  fuels  and  are
developing  a  deep  underground 
repository  at
Meuse/Haute  Marne.  The  DOE  has  purchased  MOX  fuel  fabrication
services  from  France  to  convert  plutonium  from  dismantled  bombs  to
electrical plant fuel, as have other countries.71

radioactive  waste 

Isn’t  it  unfortunate  that  our  nation,  which  pioneered  the  science  and
technology  to  harness  atomic  power  for  war,  must  now  turn  to  another
country for the technology to convert those weapons into fuel for peaceful
use? How politically correct is that?

NUCLEAR WASTE STORAGE—NOT IN MY MOUNTAIN!
After  more  than  $10  billion  spent  so  far  in  developing  the  national
Yucca Mountain nuclear waste repository that Nevada once advocated, the

project has now been canceled by the Obama-Biden administration thanks
to strong political opposition from the state and environmental lobbies. But
this has not always been the case.72 Back in the 1970s, Nevada legislators
actually  authored  a  bid  to  create  a  national  waste  repository  within  the
nuclear test site in order to create thousands of jobs. This would help the
state  recover  from  a  loss  of  federal  money  following  the  termination  of
nuclear device tests and related employee layoffs. By the 1980s, they didn’t
need or want it anymore.

A  rapidly  expanding  gambling  industry  created  about  five  hundred
thousand new jobs, along with Nevada representatives with louder voices
and  national  clout.  In  2007,  Harry  Reid  rose  to  the  position  of  Senate
majority  leader  and  became  a  powerful  project  opponent,  stating,  “It  is
abundantly  clear  that  there  is  no  such  thing  as  ‘sound  science’  at  Yucca
Mountain.”73

State  of  Nevada  representatives,  along  with  various  anti-repository
groups,  accused  the  project  of  fraud,  asserting  that  some  United  States
Geological  Survey  scientists  had  suggested  in  e-mail  exchanges  that  they
had fabricated research data. A US attorney dismissed the charges in 2006,
ruling that the brouhaha had resulted from a misunderstanding.74

It’s 

somewhat  bewildering 

Still,  Nevada  needs  nuclear  power.  Las  Vegas  and  its  millions  of  neon
lights have depended for decades upon hydroelectricity from the Colorado
River. Now a prolonged drought, which may continue, is creating electrical
shortages as well as shrinking the city’s water supply and economic future.75
the  Obama-Biden
administration would announce that the government would guarantee more
than  $8  billion  in  loans  needed  to  build  the  first  two  US  nuclear  power
plants in nearly 3 decades at nearly the same time it announced it would
drop  plans  to  proceed  with  the  Yucca  Mountain  waste  repository  and
consider what it believes are “better options.”76

to  ponder  why 

Termination of the repository will predictably hobble efforts to build “a
new  generation  of  safe,  clean  nuclear  power  plants”  as  presented  in  the
president’s first State of the Union address. Without a permanent solution
for nuclear waste storage, several states, including California, won’t let new
plants  be  built.  And  as  Michael  Morris,  chief  executive  of  American
Electric  Power,  contends,  “There  has  to  be  a  reaction”  to  the  closing
because Yucca is the only site that has been vetted and deemed capable of
storing waste from the nation’s 104 operating reactors.77

Nuclear electrical power expansion is a necessity—not an option. While
nuclear power is not typically characterized as an alternative energy source,
in reality it is the only alternative other than fossils that offers substantial
expansion  potential.  The  Nuclear  Regulatory  Commission  anticipates  that
more than thirty reactor projects will finally be seeking licensing permits.
Following a lengthy approval process, each will require about 4 to 5 years
to build, at a cost of between $6 billion and $10 billion per reactor.78

Although  these  plants  produce  only  water  vapor  emissions,  radioactive
waste containment and releases due to accidents or terrorism are issues of
public  concern.  Safety  records  at  nuclear  facilities  have  been  excellent,
however, and technology advancements are further reducing risks and any
potential  consequences.  Waste  problems  could  be  reduced  by  recycling
spent  materials  using  breeder  reactors,  but  this  approach  was  outlawed
during 
to  concerns  about
proliferation of weapons-grade fuel that might get into enemy hands. Given
that  there  are  much  more  efficient  and  terrorist-accesible  ways  to  obtain
bomb  materials  from  maverick  foreign  government  sources,  there  is  little
current basis for such prohibitions.

the  Carter-Mondale  administration  due 

Alternative Futures

 

It  is  essential  to  our  national  and  global  future  that  development  and
utilization  of  all  alternative  energy  options  continue.  This  includes
improvement and expansion of nuclear power and innovations to produce
cleaner energy from coal and other fossil-fuel sources for which there is no
practical substitute.

It  is  also  vital  that  the  public  be  much  better  informed  about  the
comparative  advantages  and  disadvantages  of  all  alternatives,  and  that
public  policy  decisions  be  more  fully  guided  by  the  facts.  Performance
benefits of unproven options such as cellulosic ethanol have been claimed
but  not  demonstrated.  Expansion  limitations  of  other  options  have  been
obfuscated, leading many to believe that abundant replacements for fossil-
fuel  sources  are  available  but  neglected  by  the  energy  industry.  Green
energy  has  become  a  meaningless  term  because  the  environmental
consequences of most alternatives have been ignored and misrepresented in
marketing  campaigns  and  in  the  media.  Fossil-dependent  fuels  have  been

mischaracterized  as  “sustainable.”  Examples  are  ethanol,  which  requires
fuel for farming and processing, and hydrogen, derived from natural gas.

2

Proposed carbon-emission cap-and-trade legislation predicated on global
warming hysteria and the demonization of CO
 will solve nothing. It will
only  constrain  new  power  plant  development,  making  energy  more
expensive.  Although  resulting  shortages  and  price  escalations  may
eventually  promote  support  for  much-needed  nuclear  power  expansion,
permitting and development of those vital plant infrastructures must begin
immediately to keep the lights burning.

But if any green will be realized through carbon trading, it will primarily
move  from  pockets  of  consumers—through  paying  increased  power,  fuel
processing,  and  product  costs—to  the  bank  accounts  of  hedge  fund
speculators and subsidy recipients. Let our current ethanol experiences be a
lesson.79

Section Four

Retaking America’s Future

Chapter 10

REENERGIZING FREE ENTERPRISE

Bureaucracy protecting the planet from carbon hoofprints

America’s energy progress has been held hostage to political and
legal pressures applied by groups its citizens did not elect to represent the
future best interests of their children and grandchildren. Vilification of CO
as a pollutant, burning fossils as a climate cooker, and oil dependence as a
security threat offers means to justify many ends. All provide the rationale
to promote carbon-offset-trading bonanzas, to subsidize alternative-energy
pricing  advantages,  to  advance  ideological  social-engineering  agendas,  to
validate political and governmental power grabs, and to serve global wealth
distribution agendas. These motives are championed beneath banners whose

2

slogans promise salvation from environmental guilt, essentials for resource
conservation,  energy  independence,  and  most  recently,  creation  of  green
jobs by legislative fiat.

Global warming supporters seem to be loudest in their assault on the use
of fossils, which ironically will necessarily be our most important source of
energy  for  the  foreseeable  future.  Let’s  examine  how  the  climate  change
lobby has acted to keep this key to our energy security buried.

Coal: That Ol’ Black Magic

 

Al Gore calls for a bold new energy policy. All US electricity will soon
be provided by wind, solar, and other renewable sources, and reliance upon
fossils  will  end.  But  beyond  the  superheated  greenhouse  atmosphere  of
Planet  Gore,  real-world  circumstances  are  likely  to  be  dramatically
different. Nuclear power, conspicuously absent in Gore’s vision, is the only
serious  non-fossil  contender  to  add  electricity  capacity,  and  coal,  the
implicitly denigrated fossil, represents the only practical hope to reduce oil
and natural gas dependence for liquid fuel.1

Despite our enormous dependence upon coal for electricity, heating, and
important by-products, coal industries have been unfairly characterized as
“dirty businesses.” Global warming alarmism is now fueling a heightened
wave  of  environmental  activism  that  is  blocking  new  developments,
particularly for electricity generation. Many proposed coal-fired plants are
being  canceled  by  states  from  coast  to  coast  as  stringent  CO
  emission
restrictions deny permits, produce legal challenges, and make construction
and operational costs prohibitively high.

Coal is the largest source of worldwide electrical power generation, and
coke from coal processing is a vital component in the reduction of iron ore.
We  are  fortunate  in  the  US  to  have  relatively  abundant  reserves:  an
estimated 250-year supply, compared with perhaps only a few decades for
oil  and  natural  gas.  The  US  Geological  Survey  projects  that  US  reserves
contain  about  1.7  trillion  tons  of  identified  deposits;  many  geologists
believe that future discoveries may more than double that amount. Not all is
readily  recoverable,  however,  due  to  technology  limitations,  high  access
costs,  and  environmental  restrictions.  Total  recoverable  reserves  are
estimated  to  be  about  472  billion  tons,  but  because  current  mining

2

2

2

2

in 

technologies  leave  substantial  amounts  in  place,  near-term  recoverable
assets are estimated to be about 262 billion tons. This may be more than
one-fourth of the world’s total recoverable supply.2

Uncontrolled coal burning releases many substances that none of us want
in our environment, including sulfur and a variety of heavy metals such as
arsenic  and  lead.  And  there  is  no  question  that  surface  mining  has  had
destructive  impacts  upon  site  land  areas,  has  polluted  waters,  and  has
destroyed natural habitats. These are very real and serious issues that the
coal  industry  must  address.  This  has  begun  to  happen.  Relatively  simple
and inexpensive “clean coal” technologies are available to remove most of
the SO
, smoke-producing oxides of nitrogen oxide (including nitric oxide,
nitrogen  dioxide,  and  nitrous  oxide),  and  particulate  emissions  from  the
coal-burning  process 
through  chemical  washing,
gasification,  and  special  treatments  of  flue  gases  before  they  are  emitted.
Processes to capture, transport, and store CO
  are  much  more  costly  and,
arguably, irrelevant.

is  accomplished 

Just how clean is clean coal? It can be washed and gasified to burn quite
the
cleanly,  with  most  offensive  residual  particulates  captured 
incineration venting process. CO
, which is much more difficult to collect
and contain, has come to be vilified as a polluting coal-burning by-product
solely on the basis of dubiously claimed climate influences.

The  clean  coal  technology  field  is  moving  rapidly  toward  gasification,
which  breaks  coal  down  into  its  basic  chemical  components  rather  than
burning  it  directly.  Modern  gasifiers  expose  the  coal  to  hot  steam  and
carefully controlled amounts of air or oxygen under high temperatures and
pressures.  This  breaks  the  carbon  molecules  apart  through  chemical
reactions  that  produce  carbon  monoxide,  hydrogen,  and  other  gaseous
compounds that burn cleanly. Hydrogen gas, in fact, has been used to drive
electricity-generating turbines, with enhanced fuel-efficiency outputs of up
to 60 percent.

With regard to environmental damage from mining, the largest problem
has been associated with open-pit excavation, which accounts for about 60
percent  of  total  US  recovery.  This  method  is  primarily  used  in  western
regions  where  near-surface  deposits  can  be  up  to  a  hundred  feet  thick;
underground mining, which presents greater risks to human safety, is mostly
applied east of the Mississippi in the Appalachian states. The April 5, 2010,

explosion in the Upper Big Branch mine in Montcoal, West Virginia, which
killed twenty-five workers, serves as a tragic example.

Open-pit  operations  are  now  typically  required  to  post  bonds  for  each
acre of surface mined, and later to restore the soils as nearly as possible to
original  contours  with  native  vegetation  and  trees.  More  than  2  million
acres  of  coal  land  have  been  restored  in  this  manner  over  the  past  2
decades.3

Regardless of growing opposition by the Sierra Club and other activist
combatants  beating  global  warming  war  drums,  the  demand  for  coal  can
only increase. US utilities currently burn more than 9 billion tons annually
in more than six hundred plants, and coal’s share of electricity generation is
projected  to  grow  about  60  percent  over  the  next  2  decades.  While  the
Sierra  Club  alone  continues  to  spend  many  millions  of  dollars  in  legal
actions to block coal-fired plants, the nation has no other significant power
growth  option  other  than  nuclear.  Utilities  certainly  know  this.  The  vast
majority  of  the  American  public  will  ultimately  learn  this  also,  and
hopefully in time to prevent disruptive shortages.4

“Clean  coal”  plants  that  convert  coal  into  a  combustible  gas  for  utility
generation  are  now  being  blocked  and  canceled.  For  example,  a  hearing
judge at the Minnesota Public Utilities Commission urged commissioners to
reject a plan for the Northern States Power Company, a unit of Xcel Energy,
Inc.,  to  purchase  8  percent  of  its  energy  from  a  coal  gasification  plant
proposed by Excelsior Energy Inc. He concluded that it wouldn’t be good
for customers because it would cost an extra $472 million in 2011 dollars to
make the plant capable of capturing about 30 percent of its CO
 emissions,
plus another $635 million to build a pipeline to carry the CO
 to the nearest
available deep geological storage in Alberta, Canada. This would inflate the
cost of power by $50 per megawatt hour, making it twice as costly as older,
exempted plants.5

Potential federal carbon-emission cap-and-trade legislation has added to
coal development miseries by discouraging investment financing. Citigroup
Inc.,  JP  Morgan  Chase  &  Co.,  and  Morgan  Stanley  all  report  that
uncertainties about what these prospective regulations will mandate present
big risks that are forcing conservative bank strategies. Under new “Carbon
Principles,” the banks are requiring companies applying for coal-fired plant
financing  to  show  that  they’ve  first  looked  at  “energy  efficiency”  and
“renewable  energy  options”  and  found  them  to  be  insufficient.  They  also

2

2

2

2

demand  evidence  that  proposed  plants  are  being  designed  to  capture  and
 emissions, while still charging high enough electricity
eventually store CO
rates  to  pay  for  extra  emission  allowances  needed  to  cover  CO
-capping
penalties.6

In July 2007, Citigroup downgraded coal mining company stocks, noting
that “prophesies of a new wave of coal-fired generation have vaporized.”
Steve Leer, CEO of Arch Coal, Inc., said that some of the power plants they
had  expected  to  build  “may  get  stalled  due  to  uncertainty  over  climate
concerns.”7

In  January  2008,  the  DOE  announced  a  $648  million  federal  plan  to
restructure  its  FutureGen  project  aimed  at  demonstrating  cutting-edge
carbon capture and storage (CCS) technology at multiple commercial-scale,
integrated gasification combined cycle (IGCC) clean coal power plants. Its
goal  is  to  at  least  double  the  amount  of  CO
  sequestered  over  a  previous
technology  approach  announced  in  2003.  This  was  the  largest  amount
requested  for  the  DOE’s  coal  program  in  more  than  25  years.  Under  the
plan, the DOE’s investment will provide funding only for the CCS power
plant  component—not  the  entire  construction—beginning  as  plants  are
commissioned  between  2015  and  2016,  subject  to  site  environmental
impact statement approvals.8

2

2

Acting upon a US Supreme Court ruling issued in 2007 that required the
EPA  to  regulate  CO
  emissions  under  the  Clean  Air  Act,  a  Georgia  state
court ruled in 2008 that a planned 1,200 MW coal plant could not be built
until emission standards are met. Bruce Nilles, a Sierra Club lawyer, was
elated:  “This  will  further  accelerate  the  beginning  of  the  end  of  the  coal
push.” Mr. Nilles also lauded the denial of a permit for a planned coal-fired
plant in Kansas, stating, “In 2008 we will really begin to act on stopping the
majority of these plants.”9

Wall  Street  remains  cautious  regarding  any  assumptions  that  advanced
clean  coal  technologies  will  meet  yet-to-be  determined  CO
  cap  emission
allowances,  and  it  is  nervous  about  the  extent  to  which  the  federal
government (that is, we taxpayers) will be willing to pay the differences.
Wall Street is, after all, in business to make money, not to keep our lights on
and businesses open.

During the recent $32 billion private equity purchase of TXU Corp., the
buyers  decided  to  eliminate  eight  of  eleven  planned  coal-fired  plants  in
Texas  following  aggressive  environmental  lobbying  by  actor  Robert

2

Redford  and  others.  Reversals  have  also  occurred  in  Florida,  North
Carolina,  Oregon,  and  other  states;  nearly  two  dozen  projects  have  been
canceled since 2006. Some projects perished because permits were denied
and others because court challenges deterred investor financing.

Coal  has  powerful  opponents  in  the  US  Congress  as  well.  Senate
Majority  Leader  Harry  Reid,  who  has  worked  hard  to  derail  nuclear
development and construction of the Yucca Mountain waste repository in
Nevada,  has  also  fought  against  the  creation  of  three  proposed  coal-fired
plants  in  his  state.  Since  Nevada  doesn’t  have  hydropower,  this  raises
concerns  among  others  about  just  how  Nevada  will  meet  its  future
electricity  requirements—particularly  at  night  and  when  the  wind  isn’t
blowing. As Doug Fischer, a utilities analyst with the investment firm AG
Edwards, observed, coal opponents, including Reid, could “put us in a bind
where we’re not going to have the energy we need.”10

Coal-to-Liquid: Wringing Out Diesel and Jet Fuel

 

Coal  offers  the  potential  to  substantially  reduce  dependence  upon
domestic and foreign oil for automotive and aviation fuel. This isn’t a new
idea, and the technology to do so is well proven. Petroleum poor, Germany
fueled its Nazi World War II war machine primarily on coal-derived diesel.
South  Africa  generates  about  40  percent  of  its  automotive  fuel  from  coal
and natural gas, using processes it developed to meet energy needs during
its isolation under apartheid.11

Coal-to-liquid synthetic fuel development also has a long history in the
US. It began when Congress passed a Synthetic Liquid Fuels Act in 1944
that  authorized  $30  million  to  study  the  construction  and  operation  of
demonstration plants to produce liquid fuels from oil shale, agricultural and
forestry products, and other substances. Soon afterward, some members of
Congress  and  administration  officials  urged  that  the  oil  industry  be
encouraged  to  construct  a  coal-to-liquid  demonstration  plant,  but  without
success. Oil executives resisted, arguing that synthetic fuels would not be
competitive with crude oil.

Undaunted, the US Bureau of Mines proceeded with an initiative of its
own.  With  assistance  from  some  captured  German  scientists,  the  agency
contracted  with  the  Bechtel  Corporation  to  convert  a  synthetic  ammonia

plant into a coal hydrogenation facility. By 1949, the fully operational plant
was processing lignite into 200 bbl/d of diesel fuel. It later used bituminous
coal  as  the  feedstock.  Then,  between  1949  and  1953,  the  demonstration
plant  produced  1.5  million  gallons  of  78-octane  unleaded  coal-derived
gasoline.12

In 1948, the US depended upon foreign oil imports for more than half of
all  its  domestic  needs.  By  the  mid-1950s,  America’s  energy  sights  had
shifted  toward  the  giant  oil  fields  of  the  Middle  East.  National  political
interests followed, driving large deals with Persian oil sheiks. During that
same  period,  the  US  Carbide  and  Carbon  Chemical  Company  (later
renamed  Union  Carbide)  built  and  operated  the  first  private  US  coal
hydrogenation  plant  at  Institute,  West  Virginia,  and  began  to  process  300
tons of coal daily into a variety of chemicals. But US interest in synthetic
fuels was already fading. In 1953, a Republican-led House Appropriations
Committee  killed  funding  for  synthetic  fuel  plants,  claiming  that  the
product  prices  were  too  high.  It  was  widely  believed  that  this  decision
resulted from pressures by oil companies that didn’t want competition from
coal or other alternatives. In any case, the US Bureau of Mines’ operation in
Missouri went back to processing ammonia for military uses.13

Price has been a major stumbling block for coal-to-liquid development
ever since the 1980s, when costs for crude oil stabilized at low levels. This
situation may now be rapidly changing. In 2005, Energy Secretary Samuel
W.  Bodman  contacted  the  National  Coal  Council,  a  federal  advisory
committee, requesting that it draft a report detailing the role coal can play in
the near future. In its response, the council found that “application of coal-
to-liquids technologies would move the United States toward greater energy
security  and  relieve  cost  and  supply  pressures  on  transportation  fuels  by
producing  2.6  MM  bbl/d  [2.6  million  barrels  per  day]  of  liquids.  These
steps would enhance US oil supply by 10 percent and utilize an additional
475 million tons of coal per year.”14

According to a June 5, 2006, MoneyWeek report, “Breakeven for a coal-
toliquids  plant  in  the  US  would  be  in  the  range  of  $39–44  a  barrel,
assuming no tax incentives,” a lot lower than we are likely to see again. Yet
obtaining financing for large projects remains a major obstacle due in large
part 
  emission–capping
legislation.15

regarding  prospective  CO

to  uncertainties 

2

Proposals to convert coal to liquid fuel for automotive use have powerful
congressional  opponents  because  they  compete  with  biofuel  lobbies  and
proponents.  In  February  2007,  House  Republicans  moved  to  recommit  a
biofuels research bill back to committee, with instructions to include coal
synfuels as an “alternative fuel.” The motion failed to carry; voting went
along party lines. Then in March, Senator Inhofe tried to amend a Senate
bill that was calling for implementation of recommendations presented in
the 9-11 Commission by attaching billions of dollars for liquid coal in the
name of national security. Senator Lieberman used his power as the 9-11
bill sponsor to prevent Senator Inhofe from introducing the amendment. A
key  argument  was  that  liquid  fuel  from  coal  would  not  reduce  CO
  over
emission  levels  produced  by  petroleum.16  So  global  warming  trumped
energy priorities once again.

2

The Dirty Side of the Liquid Fuel Debate

 

America, characterized by some as the “Saudi Arabia of coal,” enjoys
an enviable opportunity to fully utilize this natural benefit for liquid fuel as
well as electrical power. Yet powerful environmental organizations such as
the  Natural  Resources  Defense  Council  oppose  all  coal  development
programs, including conversion to liquid fuels. The NRDC states, “Relying
on  liquid  coal  as  an  alternative  fuel  could  nearly  double  global  warming
pollution  per  gallon  of  transportation  fuels  and  increase  the  devastating
effects of coal mining felt by communities and ecosystems stretching from
Appalachia to the Rocky Mountains.”17

Given  that  no  energy  alternative  is  immune  from  strong  environmental
impact  criticisms,  and  that  such  objections  can  be  directed  to  most  all
human activities, let’s put that argument aside for consideration elsewhere.
Let’s also take the “global warming pollution” argument at face value and
assume that CO
 is really dreadful stuff. In that case, how does liquid coal
actually compare with other fuels?

The  NRDC  asserts  that  coal  is  a  “carbon-intensive  fuel,  containing
almost double the amount of carbon per unit of energy compared to natural
gas  and  20  percent  more  than  petroleum.”  It  argues  that  liquid  coal  use
actually  produces  two  different  CO
  streams:  one  from  the  plant  that
produces it and the other from engine exhaust. The NRDC acknowledges

2

2

2

that  vehicle  CO
  emissions  are  comparable  for  liquid  coal  and  other
transportation  fuels  and  that  those  produced  from  liquid  coal  processing
plants  are  much  higher  than  those  that  refine  crude  oil  into  gasoline  and
diesel. It then estimates that if 90 percent of the CO
 from liquid plants is
captured,  instead  of  released  into  the  atmosphere,  “wheel-to-wheel
emissions from coal-derived liquid fuels would [still] be 8 percent higher
than for petroleum.”18

And the NRDC’s solution? A Securing America report jointly published
by the NRDC and the Institute for the Analysis of Global Security found
that “a combination of more efficient cars, trucks, and planes; biofuels; and
smart growth transportation options can cut oil dependence by more than 3
million bbl/d in 10 years and achieve cuts of more than 11 million bbl/d by
2025.”19

2

2

Okay.  So  we  don’t  really  need  that  oil  after  all?  The  Idaho  National
Laboratory  conducted  a  study  of  environmental  aspects  of  coal-to-liquid
technology  in  connection  with  Baard  Energy’s  Ohio  River  Clean  Fuels
project under development in Wellsville, Ohio, that produced conclusions
different from those presented by NRDC. The results showed that coal-to-
liquid fuels would yield 46 percent fewer emissions of CO
 and other GHGs
than conventional low-sulfur diesel transportation fuels when a 30 percent
biomass  co-feed,  CSS,  and  a  combined  cycle  cogeneration  process  was
used. All emission reductions were measured on a wheels-to-wheels basis
using a testing model developed at the Argonne National Laboratory. The
study  also  found  that  the  coal-derived  fuel  was  virtually  sulfur  free,
contained  20  percent  less  nitrogen  oxide  than  did  standard  diesel,  and
reduced both particulate and volatile organic compound emissions by close
to 20 percent.20

The  NRDC  seems  to  have  no  real  problems  concerning  CO

  and  other
emissions  associated  with  the  biofuels,  which  it  and  many  other
environmental activist groups advocate. However, using the same logic it
applies to coal-derived fuels, that isn’t quite fair. Consider the refineries that
process the crude oil into diesel to fuel the tractors that plant and harvest the
corn.  They  release  CO
,  sulfur,  and  particulate  emissions.  And  the  plants
that produce the fertilizer … what about them? Then there is the issue of the
emissions from the natural gas or other fuels that create the heat to process
the corn into alcohol. That should count. And recall that lots of water and
power are needed for crop irrigation and processing—those are resources

2

2

too—not to mention the damage caused to the land and ecosystems from
the farming … all those trees lost that could be absorbing climate-killing
CO

. Where’s the justice?
The  US  is  lagging  behind  in  the  area  of  vital  coal-to-liquid  fuel
development, due to environmental obstructionism. Still, there is hope that
the  US  private  sector  can  begin  to  make  some  progress  against  strong
currents of bureaucratic resistance and perils of uncertain carbon-emission
legislation.  DKRW  Advanced  Fuels  and  Arch  Coal,  Inc.,  plan  to  begin
construction  on  a  coal-to-liquid  plant  in  Wyoming  using  technologies
licensed by General Electric and ExxonMobil. Product costs are estimated
to be about $67–$82/barrel based upon the experiences of Sasol, a South
African coal, oil, and chemicals firm. That range depends upon the costs of
water and coal for the water- and power-intensive process. Unfortunately,
there is no free energy lunch anywhere.

Ironically,  China,  the  world’s  largest  atmospheric  coal-fired  polluter,  is
becoming the leading implementer of cleaner advanced coal-to-liquid fuel
technologies that will advance progress toward oil conservation. Unlike the
US, Australia, and India, all of which are interested in coal but constrained
by environmental lobbies, China is building the world’s largest coal-derived
plant  complex  in  the  grasslands  of  Inner  Mongolia.  The  plant,  which  is
being  developed  through  an  agreement  with  Sasol,  will  annually  convert
about 3.5 million tons of coal into 1 million tons of oil products, such as
diesel fuel for automobiles. This amounts to about 20,000 bbl/d, compared
with the estimated 7.2 million bbl/d of oil that China currently consumes.21

This level of production is tiny compared with China’s demands, but it is
only  the  beginning.  Inner  Mongolia—twice  as  large  as  France,  Germany,
and England combined—contains China’s biggest coalfield, and the region
may be able to yield 50 million tons of liquid fuel per year by 2020. This
equates to about 286,000 barrels, about 4 percent of China’s needs based
upon  present  consumption.  The  state-owned  Shenhua  Group  that,  along
with  Sasol,  oversees  the  project  is  also  conducting  feasibility  studies  for
two more coal-to-liquid plants in the Shaanxi and Ningxia provinces.

2

that 

China  realizes 

its  future  energy  security  demands  reduced
dependence upon foreign oil imports, just as ours does, and is prepared to
invest accordingly. In 2003, China imported about 100 million tons of oil,
and  booming  economic  and  industrial  expansion,  along  with  growth  in
private  automobile  ownership,  is  creating  rapidly  expanding  demand

pressures. While coal-to-liquid processing has long been too expensive to
be price competitive with standard crude oil, this condition is changing. The
Chinese will also market sulfur extracted from the process to offset costs, as
well as hydrogen, another by-product, for fuel cell power applications.

Chinese  and  South  African  partners  will  be  in  positions  to  provide
advanced coal-to-liquid technologies to other nations, rich and poor alike,
willing to forgo presently cheaper crude for enhanced, longer-term energy
security. Similar technologies can also be used to produce liquid fuels from
natural gas, which is even more economical than coal processing is. Sasol is
now switching its liquid fuel feedstock to natural gas in its South African
plant. The process will also compete with evolving technology to extract oil
from tar sands, which is being expanded in Canada and now accounts for
about one-quarter of that country’s automotive fuel.22

Any  predictions  that  coal-to-liquid  technologies  will  offer  a  substantial
solution  to  US  oil  and  natural  gas  dependence  would  be  premature  and
speculative. Likewise, these technologies may ultimately not even prove to
be the best use of valuable coal resources. Advancements in battery design,
for example, may enable clean coal to be used more directly and efficiently
for power to recharge greatly improved electric vehicles. The free market,
not government, will decide. Or will it?

Increasingly,  federal  and  state  governments  are  determining  which
alternatives  will  prevail.  Some  receive  tax-supported  subsidies  and
consumer  cost-burdening  mandates,  while  others  are  penalized  by
regulatory  disincentives 
investment  and  competitive
profitability.  Decisions  that  will  have  critical  impacts  upon  future  energy
security are being predicated on enormously theoretical climate models and
unsupportable  alarmist  premises.  Alternatives  with  broadly  recognized
capacity limitations gain favor in the balance.

that 

inhibit 

Coal isn’t going to win any beauty pageants based upon sex appeal. It
isn’t photogenic like wind turbines filmed against azure blue skies, mirrored
sunbeam  reflections  on  large  solar  panels,  and  verdant  corn  farmlands.
Instead, picture decapitated hills; huge, black rock piles; monstrous earth-
chomping equipment, Caterpillar vehicles; and grimy-looking workers who
have  nothing  better  to  do  with  their  lives  than  keep  your  computer  and
household appliances powered up. Not very glamorous at all.

Now picture your home dark and cold. Imagine that you’re not going to
work  today  because  businesses  are  closed  (but  you  couldn’t  have  gotten

there  anyway  because  your  car’s  fuel  tank  is  empty  and  public  transit
schedules are sporadic). Hope that tomorrow will be windy and sunny; that
corn-fueled  food  prices  will  drop;  and  that  global  warming  is  real  and
doesn’t bypass your neighborhood. Isn’t that dirty coal beginning to look a
lot better?

Oil and Gas: Natural Realities

The  US  is  experiencing  a  two-part  strategic  and  practical  dilemma
regarding oil and natural gas development. On one hand, it is irresponsible
not to conserve use of dwindling global supplies that may be substantially
depleted  before  children  attending  kindergarten  today  reach  typical
retirement  ages.  It  is  unthinkable  to  bankrupt  vital  resources  for  future
generations.  On  the  other  hand,  it  is  foolish  and  perilous  not  to  begin
developing US reserves now. This is necessary to ensure that fuel will be
available  to  sustain  families,  commerce,  and  our  larger  economy  decades
hence.

Conflicting  priorities  regarding  near-term  energy  sufficiency,  long-term
sustainability,  environmental  issues,  and  fossils  versus  alternatives  are
producing  raging  disputes  over  oil  and  natural  gas  drilling  initiatives.
Current  federal  and  state  government  environmental  policies  make  large
natural  reserves  in  ANWR,  the  Rocky  Mountain  basins,  the  Outer
Continental Shelf (OCS) of the East and West coasts, and the eastern Gulf
of Mexico off-limits or severely restricted.

If we can’t drill our way out of a looming energy crisis, we also can’t
wish  our  way  out  with  sunshine  collectors,  moonshine  alcohol,  or
blowhard-style wind power projections. While US politicians dither about
global warming dangers posed by fossil-fuel use, other nations are forging
strategic energy alliances to corner global supplies.

 

 

Regulations: Drilling in Troubled Waters

Since  offshore  drilling  is  particularly  challenging  and  expensive,  it
demands long lead times and large long-term investments before products
and  industry  profits  materialize  a  decade  or  more  later.  Opponents  argue
that  this  delayed  condition  is  a  good  reason  not  to  drill,  because  benefits

won’t  occur  rapidly  enough.  As  Senator  Richard  Durbin  (D-IL)  declared,
“We can’t drill our way to lower prices.”23

About  85  percent  of  America’s  OCS,  a  region  where  big  oil  and  gas
discoveries have been occurring, is currently off-limits to drilling. In June
2008,  then-President  George  W.  Bush  urged  a  Democrat-controlled  US
Congress  to  lift  a  ban  that  would  open  up  twelve  restricted  OCS  areas,
including  two  off  Alaska,  two  off  the  Pacific  coast,  three  in  the  Gulf  of
Mexico,  and  three  along  the  Atlantic  coast—all  subject  to  approval  by
coastal  state  legislators.  This  plan  responded  to  a  public  outcry  about
alarmingly  high  gasoline  prices  at  that  time.  The  following  September,
Congress  allowed  some  restrictive  legislation  to  lapse,  opening  up  8.3
million acres in central and eastern Gulf of Mexico areas. President Obama
indefinitely  postponed  that  plan  soon  after  assuming  office  in  February
2009.

If  some  influential  Democratic  Senate  and  House  members  have  their
way,  much  of  the  OCS  will  be  permanently  off-limits  to  drilling.  On
February 21, 2006, California senators Barbara Boxer and Dianne Feinstein
(both  D-CA) 
in
sponsoring  a  California  Ocean  and  Coastal  Protection  Act  that  would
“provide permanent protection for the California coast from future drilling
and  from  efforts  to  assess  and  inventory  oil  and  gas  reserves  off  the
coastline.”24

joined  with  Congresswoman  Lois  Capps  (D-CA) 

Democratic  coastal  state  resistance  to  offshore  drilling  immediately
stiffened  and  expanded  following  the  disastrous  April  20,  2010,  BP
Deepwater Horizon rig explosion and oil spill in the Gulf of Mexico. About
three  weeks  later,  on  May  13,  Democratic  senators  Maria  Cantwell  and
Patty Murray from Washington, along with senators Ron Wyden and Jeff
Merkley from Oregon, joined with senators Boxer and Feinstein to extend
the  offshore  ban  to  include  their  states.  Their  proposal  would  amend  the
Outer Continental Shelf Lands Act. Senator Cantwell explained, “We must
act to safeguard our precious coastal waters and our dangerous addiction to
fossils . . . It is simply unacceptable to risk irreparable harm to our coastal
communities, economies and ecosystems just to feed our addictions with a
short-term fix—especially when new technologies are emerging that give us
real alternatives.”25

Let’s hope those new, emerging alternatives arrive soon. California crude
oil currently accounts for only about 37 percent of what the state consumes:

roughly  44  million  gallons  of  gasoline  and  10  million  gallons  of  diesel
every day. The state’s crude oil production has decreased 23 percent since
1996.  Beginning  in  1994,  its  refineries  received  more  imported  oil  from
other states than was produced from California reserves.26

Public  commitment  to  protecting  coastal  waters  and  shores  from  all
possible  drilling  calamity  risks  is  a  rational  necessity.  Yet  also  consider
increased  risks  associated  with  protectionist  legislative  policies  that  drive
drilling operations many miles farther out and thousands of feet deeper to
add  hazards,  complexities,  and  costs.  A  similar  technical  failure  to  the
Deepwater  Horizon  event  occurring  in  closer-in,  shallower  waters  would
have been a much quicker and easier fix, with greatly contained impact. It
would also have been much less likely to occur in the first place.

Since 1992, American oil companies have drilled more than twenty-one
hundred wells in the Gulf of Mexico at depths greater than a thousand feet.
These were extremely expensive to construct, each typically costing $100
million or more, and often they haven’t been successful. But sometimes the
oil  companies  did  get  very  lucky.  In  September  2006,  for  example,
Chevron,  Devon  Energy,  and  Norway’s  Statoil  ASA  announced  that  their
Jack  No.  2  deepwater  well  in  the  Gulf  of  Mexico  might  have  opened  up
access to 15 billion barrels of oil, enough to boost US strategic reserves by
50 percent. The find is located in a region called the “lower tertiary trend”
about 270 miles southwest of New Orleans.

Shell, in partnership with BP and Chevron, is building and deploying a
huge  oil  drilling  platform,  known  as  “Perdido,”  in  the  Gulf  of  Mexico,
which  was  originally  set  for  production  in  2010.  Perdido  is  expected  to
yield more than 100,000 bbl/d of crude. The rig is nearly as tall as the Eiffel
Tower, and is secured to the seabed by moorings spanning an area the size
of downtown Houston—an enormous investment.27

It is located in deep water 8 miles north of a maritime boundary defined
by  a  Jimmy  Carter–era  treaty  dividing  the  gulf  for  purposes  of  resource
development  by  the  US,  Mexico,  and  Cuba.  While  the  Shell  partnership
believes the oil to be pooled on the US side, Mexico claims that Perdido
will siphon oil from its side. And although Mexico would like to join the
group,  its  state-owned  oil  company,  Pemex,  is  forbidden  by  law  from
participating with foreign partners in developing its crude.

Ironically,  while  our  oil  companies  are  prohibited  from  drilling  next  to
the US, we drill close to Mexico and turn a blind eye when other nations

drill  next  to  us.  Cuba’s  state-run  company,  Cubapetroleo,  has  forged  an
agreement with China’s Sinopec to explore for oil on its half of the Florida
Strait  using  Chinese  equipment  and  operational  services.  The  USGS
estimates that the North Cuban Basin contains about 4.6 billion barrels of
oil.28

American  oil  companies  are  being  forced 

to  make  spectacular
investments to go farther and deeper offshore because of off-limit drilling
restrictions  in  more  accessible  coastal  reserves.  At  the  same  time,  some
congressional  leaders  continue  to  blame  the  companies,  rather  than  their
own actions, for escalating gasoline prices.

Deepwater Horizon: An Unwasted Crisis

 

Did the Deepwater Horizon disaster provide a rallying event to assert
stranglehold  control  over  drilling  by  opposing  environmental  bureaucrats
and lobbies? Let’s review some developments.

Following the event, Interior Secretary Ken Salazar convened a group of
seven experts identified by the National Academy of Engineers to prepare a
situation assessment with recommendations. The panel later protested that
Salazar  altered  their  report  after  it  was  signed,  misrepresenting  two  key
recommendations:29

•    The  original  report  called  for  a  “temporary  pause  in  all  current
drilling  operations  for  a  sufficient  length  of  time”  to  perform
additional  safety  tests  for  the  thirty-three  exploratory  deepwater
wells  already  working  in  the  Gulf  of  Mexico,  whereas  the  altered
version urged “an immediate halt to drilling operations on the thirty-
three permitted wells, not including the relief wells currently being
drilled by BP, that are currently being drilled using floating rigs in
the  Gulf  of  Mexico.  Drilling  operations  should  cease  as  soon  as
safely practicable for a 6-month period.”

•  The version the experts signed recommended a 6-month moratorium
on  permits  for  new  exploratory  wells  in  water  deeper  than  1,000
feet.  The  altered  version  recommended  a  6-month  moratorium  on
“new wells being drilled using floating rigs.” This included rigs in
water deeper than 500 feet, covering more of them.

President  Obama 

Objecting  to  the  revisions,  the  panelists  argued  that  the  6-month
moratorium on deepwater drilling would make operations less safe, sending
technical experts to foreign locations along with the rigs that will relocate.
then  appointed  a  seven-person  commission 

to
determine  what  caused  the  oil  spill  and  to  take  steps  to  make  offshore
drilling  safer.  Unlike  the  technically  distinguished  presidential  Rogers
Commission  convened  to  investigate  the  NASA  space  shuttle  Challenger
tragedy, 
the  offshore  drilling  commission  had  no  appointees  with
appropriate engineering or petroleum industry backgrounds. In fact, most of
them don’t favor drilling at all.30

•  Commission cochair Senator Bob Graham (D-FL)has fought drilling

off the Florida coast throughout his career.

•    Cochair  William  Reilly  headed  the  EPA  under  President  George
H.W.Bush, but is best known as a former president and chairman of
the  World  Wildlife  Fund,  one  of  the  largest  and  most  aggressive
environmental lobbies.

•    Member  Donald  Boesch,  a  University  of  Maryland  biological
oceanographer and strong opponent of drilling off the Virginia coast,
has previously argued that “the impacts of the oil and gas extraction
industry . . . on the Gulf Coast wetlands represent an environmental
catastrophe of massive and underappreciated proportions.”

•  Member Terry Garcia, an executive vice president of the National
Geographic  Society,  directed  coastal  programs  during  the  Clinton-
Gore  administration  with  particular  emphasis  on  “recovery  of
endangered species, habitat conservation planning, and Clean Water
Act implementation.”

•    Member  Fran  Ulmer,  chancellor  of  the  University  of  Alaska–
Anchorage,is  also  a  member  of  both 
the  Aspen  Institute’s
Commission on Arctic Climate Change and the Union of Concerned
Scientists  board,  which  opposes  nuclear  power  and  more  offshore
drilling,  favoring  government  policies  “that  reduce  vehicle  miles
traveled” (i.e., driving cars).

•    Member  Frances  Beinecke,  president  of  the  Natural  Resources
Defense Council, has called for bans on offshore and Arctic drilling
on at least five occasions since the Deepwater Horizon accident. She
has stated, “We can blame BP for the disaster, and we should. We

can  blame  lack  of  government  oversight  for  the  disaster,  and  we
should.  But  in  the  end,  we  must  place  blame  where  it  originated:
Americans’ addiction to oil.”

•    Harvard’s  Cherry  A.  Murray  has  a  professional  background  in
physics  and  optics,  not  petroleum  engineering,  modern  drilling
techniques,  or  rig  safety,  although  she  has  served  as  dean  of  the
Harvard School of Engineering and Applied Sciences.

Federal  US  District  Court  Judge  Martin  L.  C.  Feldman  temporarily
overturned  the  Obama-Biden  administration’s  6-month  moratorium  on
deepwater  drilling  on  June  22,  2010.  His  twenty-two-page  ruling  in
response to a lawsuit filed by Horn-beck Offshore Services, LLC, made it
clear that even presidents aren’t empowered to impose an “edict” that isn’t
justified  by  science  or  safety.  Feldman’s  findings  expressed  “uneasiness”
over the administration’s claim that its safety report, which recommended
the  ban,  had  been  “peer  reviewed”  by  experts,  because  they  had  since
publicly  disavowed  the  ban.  The  opinion  found  “no  evidence”  that  Mr.
Salazar  “balanced  the  concern  for  environmental  safety”  with  existing
policy and “no suggestion” that he considered any alternatives. The judge
listed  environmental  groups  that  had  joined  the  administration’s  defense
against  the  suit.  One  was  the  NRDC,  headed  by  drilling  commission
member Frances Beinecke.31

Ken  Salazar  promptly  responded  to  Judge  Feldman’s  ruling  with  a
revised  ban  and  a  federal  legal  challenge.  This  time,  instead  of  banning
drilling  deeper  than  500  feet,  he  banned  all  drilling  by  floating  rigs  (the
only equipment that drills in deep water). He also set a firmer November
30th moratorium deadline.

Twenty-two of this country’s total thirty-three deepwater rigs are located
near already-economically ravaged Louisiana. Pending lease sales have also
been  canceled  off  the  Virginia  and  western  Gulf  of  Mexico  coasts,  along
with  a  drilling  program  in  Alaska’s  Chukchi  and  Beaufort  seas  that  had
been  scheduled  to  begin  in  June  2010.  While  of  more  than  50,000  US
offshore  wells  the  Deepwater  Horizon  represents  the  first  significant
accident,  the  ultimate  future  of  offshore  drilling  will  inevitably  be
influenced  by  the  political  party  dominance  results  of  the  2010  and  2012
national elections.

The  International  Association  of  Drilling  Contractors  estimates  that
moratorium  delays  costs  as  much  as  $330  million  per  month  in  direct
wages, not counting lost businesses for servicing the rigs.32

According to the Louisiana Mid-Continent Oil & Gas Association, each
idled deepwater rig can eradicate 1,420 jobs, with salaries averaging $1,804
per week. In the meantime, many of the rig owners are likely to relocate
their platforms to more politically reliable foreign regions, including Brazil,
China, and the North Sea sites. Some may never return. Diamond Offshore
has announced plans to relocate one of its rigs to Egypt and another to the
Republic of the Congo. Scotland’s Stena Drilling is shifting one to Canada.33
Many  of  the  deepwater  rigs  idled  by  the  moratorium  are  likely  to  be
acquired  by  Petrobras  for  Brazil’s  offshore  fields.  The  company  plans  to
drill  at  depths  up  to  14,000  feet.  In  August  2009,  the  US  Export-Import
Bank  issued  a  “preliminary  commitment”  to  loan  Petrobras  $2  billion.
George  Soros  will  benefit  from  his  $900  million  investment  in  the  oil
giant.33

2

It  might  be  noted  that  in  June  2010,  that  very  same  US  Export-Import
Bank (Ex-Im, for short) denied loan guarantees to Reliance Power Ltd., an
Indian utility that is building a coal-fired plant near Sasan, India. The deal
would  have  enabled  Bucyrus  International  Inc.,  based  in  Milwaukee,
Wisconsin,  to  export  about  $600  million  in  mining  equipment  over  three
years.  Although  the  Reliance-Bucyrus  project  met  all  Ex-Im  qualifying
criteria,  including  tougher  CO
  standards  imposed  by  the  Obama  White
House,  the  bank  caved  under  pressure  from  the  Treasury  and  State
departments. Obama-appointed Ex-Im chairman Fred Hochberg explained,
“President  Obama  has  made  clear  his  administration’s  commitment  to
transition away from high-carbon investments and toward a cleaner-energy
future.”34

A  May  2010  NBC  News/Wall  Street  Journal  poll  revealed  that  overall
public  support  for  offshore  drilling  remained  strong,  even  while  BP’s
damaged well continued to gush huge amounts of crude into coastal areas.
Six out of every ten respondents replied that they backed more drilling off
the  US  coast  (34  percent  “strongly  supported”  the  idea),  and  another  26
percent  agreed  “somewhat.”  More  than  half  (53  percent)  agreed  with  the
statement  “The  potential  benefits  to  the  economy  outweigh  the  potential
harm to the environment.” Gulf state respondents (63 percent) were most
inclined to support additional offshore drilling and rigs.35

A  legitimate  concern  cited  against  offshore  drilling  is  that  it  leaks  oil
pollutants  into  ocean  ecosystems,  even  under  safe  operational  conditions.
Yet  putting  this  issue  into  perspective,  a  recent  National  Academy  of
Sciences study estimates that of the 260,000 metric tons of oil seepage that
is thought to occur in waters off North America each year, about 63 percent
of  that  amount  escapes  naturally  from  formations  below  the  seafloor.
Activities associated with oil and gas exploration, on average, are estimated
to  be  about  3,000  metric  tons,  less  than  1  percent.  The  rest  comes  from
petroleum  tanker  transportation  and  releases  from  cars,  boats,  and  other
sources.36

ANWR: Turmoil in the Tundra

 

Particularly intesnse controversy exists over whether or not to lift a 30-
year  government  moratorium  that  prevents  drilling  in  the  Arctic  National
Wildlife  Refuge  area,  which  the  US  Department  of  Interior  estimates  to
contain  between  9  and  16  billion  barrels  of  recoverable  oil;  the  DOE
estimates this figure to be about 10.6 billion barrels (potentially producing
876,000  bbl/d).  John  Cogan,  an  industry  attorney  at  McDermott  Will  &
Emery in Houston, believes there may even be much more— possibly as
much  as  the  16  billion  barrels  suggested  by  the  Interior  Department—
because the DOE based its recovery estimate on outdated methods.

While  top  Alaskan  government  officials  strongly  favor  such  drilling,
environmental  lobbies  aggressively  oppose  it.  Alaska  has  had  little  luck
opening  up  restricted  federal  lands  (65  percent  of  the  state)  to  oil  and
natural gas development. Efforts to release ANWR for drilling have been
stymied for more than a decade.

On May 13, 2008, Senator Chuck Schumer (D-NY) rose on the Senate
floor to demand that arms sales to Saudi Arabia cease unless the kingdom
“increases its oil production by one million barrels per day.” Interestingly,
this  is  nearly  the  same  amount  that  might  be  flowing  from  ANWR  if
President Clinton hadn’t vetoed drilling there in 1995. Yet senator Schumer
doesn’t support drilling in ANWR, or anywhere else in the US.

Alaska’s then-governor Sarah Palin was and still is incensed that drilling
in ANWR is being prevented at a time when the state and nation are facing
energy  shortages  and  skyrocketing  economic  impacts:  “It’s  a  very

nonsensical position that we are in right now, as we send the president and
Secretary [of Energy Sam] Bod-man overseas to ask Saudis to ramp up the
production of oil so that hungry markets in America can be fed, when your
sister state in Alaska has those resources … But these lands are locked up
by Congress and we are not allowed to drill.”37

While  various  polls  show  that  the  majority  of  Alaskans  and  other
Americans still favor expanded drilling on government-controlled lands and
waters,  opponents  of  ANWR  drilling  are  particularly  vocal,  raising  the
specter  of  despoiling  a  vast  wilderness  and  wildlife.  In  reality,  the  actual
operations  would  be  confined  to  a  tiny  fraction  of  the  1.4-million-acre
reserve—namely, a 2,000-acre plot of land smaller than the footprint of the
Los Angeles LAX airport.

Alaska  has  other,  larger  oil  and  natural  gas  resources  that  could  begin
producing even faster than a drilling operation in ANWR. The Chukchi Sea
area has a large number of exploration bids for offshore development, and a
privately  funded,  new  $30  billion  natural  gas  pipeline  project  has  been
approved.  The  pipeline  will  be  the  largest  private  construction  project  in
North  American  history,  consisting  of  a  gas  treatment  plant  on  the  North
Slope  of  the  state  and  approximately  2,000  miles  of  pipe  connecting  to
Alberta, Canada. If required, an additional 1,500-mile-long extension will
deliver natural gas from Alberta to Chicago.

Alaska  has  a  strong  vested  interest  in  maintaining  a  pristine  natural
environment, because tourism is the state’s largest private-sector employer,
accounting  for  one  out  of  eight  jobs  and  growing.  Still  former  governor
Palin  doesn’t  buy  the  notion  that  ANWR  drilling  is  going  to  have  any
significant  impact  upon  the  region’s  wildlife:  “There  are  magnificent
caribou and wolves and bears and porcupines and birds all through Alaska.
You can see them thriving today as you could in the 1960s, before pipelines
were  built.  Talk  about  coexistence;  we’ve  got  grizzlies  roaming  on  the
pipelines and caribou migrations passing beneath them.”38

And  what  about  polar  bears?  The  governor  has  some  opinions  about
them,  too:  “We  have  been  coexisting  with  bears  for  decades  to  no
detrimental  effect;  our  bear  population  is  thriving  .  .  .  This  [Endangered
Species Act] listing is nothing but interference from outsiders who insist on
keeping Alaska from developing.”39

Trading Oil for Carbon

 

In  April  2010,  a  little  over  a  year  after  assuming  office,  President
Obama appeared in the press to have undergone a miraculous energy policy
conversion  by  announcing  that  “in  order  to  sustain  economic  growth  and
produce jobs, and keep our businesses competitive, we are going to need to
harness  traditional  sources  of  fuel.”  Accordingly,  he  proposed  a  plan  to
expand oil and natural gas exploration in the Atlantic, the eastern Gulf of
Mexico, and Alaska.

Did  this  apparent  departure  from  the  traditional  opposition  of  his  very
liberal  base  signal  a  “drill  baby  drill”  epiphany?  Was  it,  in  fact,  a  real
agenda  change  at  all,  or  was  it  possibly  just  a  cap-and-trade  bargaining
strategy? Let’s review some of the circumstances a bit more closely.

Public  anger  over  high  2008  gasoline  prices  prompted  the  Bush  White
House  and  Congress  to  lift  a  long-standing  ban  on  offshore  drilling  and
approve a 5-year plan to open a significant portion of the OCS, including a
lease to begin drilling off the Virginia coast that was to be bid out in 2011.
Interior  Secretary  Ken  Salazar  later  postponed  the  deal  until  2012.  An
assumption held by many is that this would provide more time for activist
environmental groups to fight implementation in courts.

A new moratorium was also placed upon those aspects of the Bush plan
that  would  have  allowed  leasing  along  the  North  Atlantic  and  Pacific
coasts.  Accordingly,  the  big  news  was  that  President  Obama  vowed  to
support development of leased areas off of Alaska’s North Slope, referring
to a $2.6 billion lease sale in the Chukchi Sea that had already been signed
in  2008.  He  also  proposed  to  “study”  drilling  along  the  South  Atlantic
coast.  (These  plans  are  now  on  hold  pending  his  oil  spill  commission’s
conclusions.)

Not  highlighted  was  the  president’s  plan  to  cancel  five  other  Alaskan
leases,  which  include  two  in  Chukchi,  a  location  with  an  estimated  77
billion gallons of oil.

The Obama-Biden administration would now allow drilling along a strip
in the eastern Gulf of Mexico, 125 miles off the coast of Florida, but there’s
a big catch: This will require congressional approval, and ten coastal state
Democrats  have  recently  declared  opposition  to  offshore  drilling.  Such
resistance can present a real obstacle.

Suspicions abound in some circles that President Obama’s proposal has
the  net  effect  of  leveraging  about  13  billion  barrels  of  oil  and  41  trillion
cubic feet of gas controlled by his administration and party through locked-
up  leases  as  a  trade  barter  for  Republican  support  of  his  Comprehensive
Energy and Climate Bill (aka, the newly proposed Power Bill, or cap-and-
trade). Speaking at a solar panel fabrication plant in Fremont, California, on
May 26, Obama used the strategically staged opportunity to say, “Climate
change poses a threat to our way of life. In fact we’re already beginning to
see its profound and costly impact . . . And the spill in the Gulf, which is
heartbreaking,  only  underscores  the  necessity  of  seeking  alternative  fuel
sources.”40

In a June 3, 2010, speech at Pittsburgh’s Carnegie Mellon University, the
president  further  stated,  “I  will  make  the  case  for  a  clean-energy  future
wherever I can, and I will work with anyone from either party to get this
done . . . The next generation will not be held hostage to energy sources
from the last century.”

Again,  on  June  15,  the  president  devoted  a  major  portion  of  an  Oval
Office  speech  regarding  the  spill  to  pitch  comprehensive  energy  reform
legislation as a means to end US dependence on fossil fuels and foreign oil
and to create a clean-energy future: “For decades we have known the days
of cheap and easily accessible oil were numbered . . . For decades, we have
failed to act with the sense of urgency this challenge requires . . . We cannot
consign our children to this future.”

As Daniel Weiss of the Center for American Progress observed, “The oil
disaster adds a new urgency and a new opportunity for connecting with the
public . . . The administration was going to do it anyway, but that gives it a
new way to talk about it.”41

But  successful  bargaining  with  coastal  state  Democratic  congressional
leaders  to  gain  Republican  support  for  energy/climate  legislation  now
appears  to  be  unlikely,  as  attacks  on  offshore  drilling  escalate  in  the
aftermath of the BP oil spill disaster off the Louisiana coast.

Recognizing  a  problem,  bill  cosponsors  senators  Kerry  and  Lieberman
have shifted legislation emphasis away from previous drilling advocacy. In
late 2009, Kerry had called for a bill that included “additional onshore and
offshore oil and gas exploration.” In May 2010, he confessed to Investor’s
Business Daily that changes had to be made to win votes in the wake of the
oil spill. Those changes include a provision that will let any coastal state

ban drilling otherwise permitted by a neighboring state within 75 miles of
its coastline if a mandatory study indicates that an accident could harm that
impacted state’s economy or environment.

Although  it  is  reported  that  Senator  Kerry  and  President  Obama  had
previously  discussed  drilling  leniency  to  woo  Republicans,  it  mostly  just
turned off Democrats. Accordingly, there was little or nothing to barter for
cap-and-trade provisions embodied in the proposed legislation. In any case,
that’s one trade that should be capped without question.42

Oil Prices: Politics and Prognoses

 

President Obama, along with some members of Congress, has declared
war on Big Oil. The proposed 2011 White House budget will kill $4 billion
in long-established accelerated depreciation allowances and other incentives
for oil and gas drilling. Senator Robert Menendez (D-NJ) has introduced a
bill  that  would  remove  another  $20  billion  in  industry  tax  breaks.  The
Independent Petroleum Association maintains that these tax increases will
fall  disproportionately  on  small  drilling  companies,  potentially  reducing
annual  oil  and  gas  production  by  20  to  40  percent.  This  despite  EIA
estimates  that  fossil  fuels  will  still  account  for  79  percent  of  US  energy
demand in 2030, regardless how many tax incentives are hurled at biofuels,
wind and solar. As a result, energy prices will soar, and vital investments
and supplies will dwindle.43

World  competition  for  oil  is  becoming  more  and  more  aggressive  as
developing  countries  such  as  China  and  India  continue  to  increase
consumption through industrial growth and economic prosperity. According
to  the  DOE’s  EIA,  global  demand  is  expected  to  increase  by  60  percent
over the next 2 decades, while demand in developing countries may grow
by  115  percent  during  this  period,  due  in  part  to  increasing  automobile
ownership.  The  US  currently  imports  about  70  percent  of  the  oil  it
consumes  (roughly  13  million  bbl/d),  and  these  imports  are  expected  to
increase to an estimated 17.7 million bbl/d within the next 2 decades.43

Today,  the  US  consumes  more  than  one-fourth  of  the  world’s  oil  and
crude  oil  imports,  amounting  to  about  $700  billion  each  year  and
constituting  about  a  quarter  of  the  nation’s  balance-of-trade  deficit.  More
than  two-thirds  of  this  consumption  is  in  the  transportation  sector,  where

energy  demands  are  growing  rapidly.  Continuously  increasing  crude  oil
prices with major impacts upon gasoline costs for consumers are likely to
impact driving habits and promote purchases of more efficient vehicles that
will moderate consumption rates, but total influences are highly conjectural.
Historic consumption levels have been driven by high per-capita ownership
of  automobiles, 
transportation
preferences,  and  relatively  low  gasoline  costs  compared  with  most  other
countries.  Still,  US  consumers,  on  average,  probably  spend  a  smaller
fraction of their incomes on gasoline now than in previous decades.

large  vehicles,  private  versus  public 

Important Democrat leaders appear to support the idea that high energy
prices  are  useful  to  drive  conservation  and  to  hasten  investment  in  a
transition  to  nonfossil  alternatives.  Speaking  as  a  then-senator  and  a
presidential candidate, Barack Obama stated that he had hoped the rise in
gas  prices  would  have  been  a  “gradual  adjustment”  (not  an  avoidable

According to Bureau of Transportation statistics, the US now has about
251  million  registered  motor  vehicles  (6.6  million  motorcycles  and  135
million passenger cars); more than 8,000 commercial aircraft and 224,000
general aviation aircraft; and 12.7 million recreational boats.44 Some argue
that supply shortages and high fuel prices are just the medicine our country
needs  to  curb  an  “addiction  to  oil”  through  forced  efficiencies  and
development of “sustainable alternatives.” That philosophy would be more
compelling  if  transitioning  to  those  actively  touted  nonfossil  alternatives
offered  any  real  potential  to  replace  oil  and  natural  gas  dependence.  But
they  don’t.  Biofuels  are  little  more  than  an  energy  breakeven  at  best:
Hydrogen  is  a  big  energy  loser,  and  wind,  solar,  and  geothermal  power
options have extremely limited growth prospects.

Here again, global warming alarmists, carbon-trading lobbies, alternative
energy  hypesters,  and  environmental  activist  groups  wield  powerful
influences  over  policies  that  will  determine  America’s  energy  future.
Arguments  that  short  oil  supplies  will  hasten  a  transition  to  biofuels  as  a
solution to transportation needs ignore the unreality of the alleged energy
benefits  those  biofuels  afford,  and  their  broader  economic  impacts  upon
energy, food, and business costs. These burdens will fall most heavily upon
the segments of the population that can least afford them. The result will be
more,  rather  than  less,  dependence  upon  unreliable  foreign  oil  supplies,
while  simultaneously  weakening  US  economic  capacity  to  invest  in  vital
new energy technology development.

adjustment)  so  that  Americans  could  adapt  to  the  reality  of  four-dollar
gasoline. That adjustment is pretty tough on independent truckers who must
pay  $1,500  to  fill  up  their  tanks  and  school  districts  that  are  forced  to
eliminate bus stops—and even entire routes.

Even  though  the  Democrat-controlled  Congress  passed  the  Energy
Independence Act of 2007, the party majority remains steadfastly opposed
to drilling in areas that contain enormous amounts of oil and natural gas. As
a result of large political uncertainties regarding future legislation, nervous
oil  markets  are  driving  pump  prices  even  higher.  Oil  futures  traders,
anticipating tightened supplies, are upping their bids, a condition that will
be  exacerbated  if  cap-and-trade  legislation  is  enacted.  Since  drilling
prohibitions  and  carbon  cap-and-trade  advocacy  tend  to  have  strongest
support  among  Democratic  Party  representatives, 
the  outcomes  of
upcoming  presidential  and  congressional  elections  will  continue  to  have
important  and  enduring  consequences  upon  domestic  oil  and  gas
development and expenses.

Speculators pay close attention to world markets. As China has increased
annual  petroleum  use  by  920  million  barrels  over  a  5-year  period,  the
institutional  investors  (or  “index  speculators”)  upped  their  demand  for
petroleum  futures  by  848  million  barrels  over  the  same  period.  Buying
sizable “long” positions, they have been betting that oil prices, regardless of
how  high  they  may  seem,  will  continue  to  rise.  Because  of  an  “Enron
loophole”  allowed  in  federal  trading  regulations,  the  investors  have  been
able to circumvent typical speculative limits. The resultant lopsided betting
has propelled prices upward.

After  2007,  when  crude  oil  prices  rocketed  to  about  $70  per  barrel,
lawmakers threatened to close the loophole by regulating electronic trading,
and  they  gave  the  Federal  Trade  Commission  more  authority  to  guard
against  market  manipulation.  Trading  experts  are  asking  that  special
attention be directed to pension funds, endowments, and other institutional
investors  who  have  been  big  players,  pouring  billions  of  dollars  into  a
variety of commodities—oil is but one of them—with dramatic price-hiking
results. Other notable examples are corn, soybeans, wheat, and rice.

But  demand-based  speculation  forces  aren’t  the  only  causes  of  the
nervous oil markets that drive up prices. Uncertainties about the steadiness
of  the  supply  sources  are  important  too;  consider  the  potential  political
turmoil in Nigeria or production problems in an unstable Mexico. As John

Felmy, chief economist for the American Petroleum Institute, has observed,
“If  oil  prices  really  were  so  much  higher  than  supply-and-demand  forces
would suggest, then holders of crude oil would be unable to find buyers,
and  inventories  would  build—but  that’s  not  happening.”  As  least,  not  so
long as China’s large needs and deep pockets continue to expand.45

We Shale Overcome?

 

What  if  there  was  a  domestic  source  of  oil  that  could  support  all  of
America’s needs for the next 400 years or so? Well, maybe there is. In that
case,  everyone  would  be  really  excited,  right?  No,  probably  not.  At  least
that’s the current situation.

On  July  22,  2008,  then-President  Bush  announced  that  he  wanted  to
remove  all  barriers  to  extracting  oil  from  enormous  shale  formations
located in a swath of federally owned land encompassing parts of Colorado,
Wyoming,  and  Utah.  This  followed  a  15–14  vote  by  the  Senate
Appropriations Committee that would have ended a 1-year moratorium on
enacting rules for oil shale development on federal lands. Senator Salazar, a
Democrat and leading opponent of oil shale development, had inserted the
moratorium  into  an  omnibus  spending  bill  in  December  2007.  He  then
successfully proposed the new May 2008 bill to extend it for another year.46
Extension  of  the  moratorium  was  a  big  setback  for  Royal  Dutch  Shell
and other oil companies that had already invested many, many millions of
dollars in shale oil development research since the passage of the Energy
Policy Act of 2005. That legislation established the original framework for
commercial leasing of federal oil shale lands, similar to provisions for on-
land and offshore drilling arrangements. Extension of the moratorium came
at a time when Senator Salazar and his fellow Democratic colleagues had
been  blasting  Big  Oil  for  not  reinvesting  enough  of  their  profits  in
developing  new  sources  of  oil,  when  in  2007  the  oil  shale  project
represented Shell’s largest R&D expenditure. And in addition to forbidding
the  DOI  from  leasing  federal  shale  lands,  Democratic  legislators  also
threatened to block imports of oil from Canadian tar sand reserves because
it was considered to be too environmentally “dirty.”

Just how big is that US oil shale reserve? It’s really, really big. Estimates
range from an equivalent of 800 billion barrels of crude up to possibly 2

trillion  barrels.  The  800  billion  estimate  equals  about  three  times  the
amount  of  all  Saudi  Arabia’s  oil—in  fact,  more  than  Saudi  Arabia,  Iran,
Russia,  Venezuela,  Iraq,  and  Mexico  oil  reserves  combined.  One  trillion
barrels  of  crude  equals  all  the  oil  the  world  has  used  since  it  was  first
discovered in Titusville, Pennsylvania, in 1859. When developed, the Green
River  Formation  would  provide  oil  shale  comparable  to  the  extent  of  the
energy  potential  of  Alberta’s  tar  sands  reserves.  Together,  the  US  and
Canada would have the world’s largest oil supply.47

Why hasn’t that huge resource been developed before now? The several

reasons warrant some discussion. The first is cost.

While oil from shale is similar to crude, it is much more complex and
expensive to extract, and it also requires quality upgrading prior to being
used  as  refinery  feedstock.  The  oil  is  contained  in  sedimentary  rock  that
contains solid bituminous materials, called “kerogen,” that are released as
petroleum-like liquids when heated. The kerogen was formed by a natural
process  that  also  created  crude,  but  under  conditions  with  less  heat  and
pressure. The shale contains enough oil to actually burn, and some countries
use it directly for fuel.

Oil  shale  has  seen  limited  development  worldwide  because  many
countries lack large amounts, and those that do tend to rely upon cheaper
crude.  These  circumstances  have  inhibited  technology  advancement,
although  Estonia,  China,  and  Brazil  have  quite  well-established  oil  shale
industries.  High  oil  prices  during  the  1970s  and  1980s  stimulated  US
interest and technology investment, which waned after those prices fell.

Old oil extraction practices that involved mining the shale and processing
it on the surface are now being replaced by in situ methods that leave the
shale rocks in place underground. Shell’s process places electric heaters in
deep vertical holes dug into the shale to gradually raise its temperature over
a  2-to-3-year  period.  At  about  600ºF–700ºF,  the  oil  separates  and  is
gathered in collection wells within the extraction zone.48

Shell’s  current  plan  uses  ground-freezing  technology  to  establish  an
underground barrier (a “freeze wall”) around the extraction zone perimeter,
using pumped refrigeration fluid to block groundwater from entering and to
keep hydrocarbons from  leaving.  This  remains  unproven  at  a  commercial
scale  but  is  regarded  by  the  DOE  to  be  promising.  These  processes  are
energy and water intensive, yet Shell believes that as fuel prices continue to
rise, shale oil processing will be competitive.

Water  use  in  the  Green  River  Formation  region  within  the  Colorado
River drainage basin is a major oil shale concern. Each barrel of oil will
require about 3 barrels of water. Senator Orrin Hatch (R-UT), a strong oil
shale proponent, compares this requirement with ethanol processing: “Let’s
compare  it  to  ethanol.  Corn  needs  about  1,000  barrels  of  water  for  the
energy equivalent of a barrel of oil. That’s a crazy amount of water, but it’s
worked out alright so far because corn is grown in rainy areas, for the most
part.  But  if  you  want  to  increase  the  amount  of  ethanol,  you’re  going  to
have to go to irrigation, and then there will be major water limits on how
much we can afford to grow.”49

Senator Hatch observed that even though water is a lot scarcer in western
Colorado than in Iowa, the oil companies would recycle much of the water
they  would  use.  And  in  regard  to  other  environmental  impacts,  he  said,
“Let’s talk about land use and wildlife habitat. One acre of corn produces
the  equivalent  of  5  to  7  barrels  of  oil.  One  acre  of  oil  shale  produces
100,000 to 1 million barrels … That’s 1 million barrels that we would not
be  importing  from  Russia  and  the  Middle  East.  People  are  going  to  go
berserk  when  they  find  out  that  all  along  the  way  we  had  the  capacity,
within our own borders, to alleviate our dependency in an environmentally
friendly way.”50

American Processing Picture: Crude, Unrefined

 

The  US  faces  a  dangerous  lack  of  adequate  oil  refinery  capacity.  The
last  facility  to  be  constructed  was  the  Ashland  Refinery,  near  Garyville,
Louisiana, completed in 1976. Fewer than half of the number of plants that
existed  in  1981  still  remain,  down  from  324  to  149.  Disruptions  of
production  due  to  maintenance,  accidents,  and  natural  disasters  at  one  or
more of the plants can create supply shortfalls and price hikes with regional
and national impacts.51

Although demand for refined products—transportation fuels in particular
— continues to rise steadily, refineries are high on the list of least-wanted
industries in many locales. In California, where ten plants representing 20
percent  of  that  state’s  refining  capacity  were  closed  between  1985  and
1995,  it  is  unlikely  that  more  will  be  built  due  to  concerns  about  smog,

2

truck traffic carrying hazardous materials, and potential leaks in the event of
earthquakes.

Despite a reduction in plant numbers, refinery capacity has managed to
keep  up  with  the  national  demand  so  far  through  expansion  of  existing
facilities. This has also been less expensive than building new plants due to
stringent  environmental  restrictions.  Now,  even  that  approach  to  bring
capacities more in line with growing demands is being challenged.

The NRDC asked a federal judge to stop a $3.8-billion expansion of a
Whiting, Indiana, refinery owned by BP because it will discharge more CO
than  it  has  been  approved  for.  The  NRDC  has  been  actively  working  to
block  virtually  all  attempts  to  create  more  energy  from  fossils,  using  the
Clean  Air  Act  as  a  weapon.  Another  example  is  a  proposed  refinery  in
Arizona that has now been blocked for more than 10 years.

The National Center for Policy Analysis estimates that nearly 25 percent
of all capital investment that went into refineries during the 1990s was used
to  comply  with  environmental  regulations.  Between  1992  and  2001  this
amounted  to  more  than  $100  billion  in  costs  to  bring  refineries  into
compliance with environmental rules.

As  with  restrictions  on  drilling,  regulatory  constraints  on  refinery
development  and  expansion  will  predictably 
increase  our  energy
dependence upon foreign sources. Huge plants now under construction in
India,  Asia,  and  the  Middle  East,  which  will  produce  tanker  loads  of
gasoline, diesel, and jet fuel, will most likely meet the demand for refined
oil  products  in  the  US  and  other  countries.  By  2012,  India’s  refining
capacity  is  projected  to  double  to  about  4.8  million  bbl/d  through  $70
billion  in  investment.  And  they  won’t  be  worrying  about  CO
  emission
lawsuits.

2

The Political Environment: Time for a Climate Change

 

Unless and until better options are available, there is no alternative but
to  develop  and  optimize  real  resource  possibilities.  This  must  involve
exploring and developing untapped domestic fossil-fuel reserves; improving
and  applying  clean  coal  and  coal-to-liquid  fuel  technologies;  exploiting
opportunities  to  benefit  from  vast  US  oil  shale  deposits;  upgrading  and

expanding  refinery  infrastructures;  and  becoming  a  world  leader  in  the
development and beneficiary of safe nuclear power opportunities.

These priorities do not in any way preclude the importance of developing
solar power, geothermal power, and hydropower for electricity, or biofuels
for  automotive  applications.  And  they  are  certainly  no  substitute  for
essential conservation practices that will stretch all energy resources. Yes,
of course, energy conservation is essential, but it is quite a different matter
to  think  that  we  can  conserve  our  way  to  future  prosperity  without  also
expanding  the  supply  side.  Energy  fuels  the  economic  and  technological
progress  needed  to  advance  conservation  goals  we  all  share.  These
economies are essential to lower our energy consumption rates, our living
costs, and the impacts on our environment.

America  is  blessed  with  a  great  abundance  of  resources.  Key  among
these are the advantages afforded by a free market system and a proactive,
entrepreneurial spirit that enables innovation to flourish. These time-proven
strengths  are  now  under  assault.  The  radical  environmentalism  that  grew
out  of  movements  during  the  1960s  gained  new  purpose  and  traction
through  apocalyptic  climate  visions  in  the  late  1980s,  and  it  has  grown
exponentially  since.  As  a  result,  obstructionist  groups  have  increasingly
gained license to shut down vital energy initiatives that are recognized and
supported by the general public.52

2

Chief  villains  on  the  environmental  opposition  “hit”  parade,  listed  in
order of both vitriolic reaction and the amount we depend upon them for
total  US  electricity,  are  coal,  nuclear,  and  hydropower.  Coal  is  also  the
primary  target  that  carbon  cap-and-trade  promoters  have  in  their  sights.
Global  warming  GHG  hysteria  has  been  fixated  upon  coal-burning  CO
emissions  as  “pollution”  that  threatens  the  natural  world,  whereas  nature
regards CO
 as a fundamental part of all carbon-based life. The real fossil
combustion pollutants, sulfur and various particulates, can be more readily
removed, while CO
 sequestration is more costly—and also nonsensical—
particularly if the Earth continues to cool as scheduled.53

If Americans are concerned about costly energy now, they should realize
that  we  haven’t  seen  anything  yet.  Strategies  to  address  these  problems
through shortage-forced conservation measures and CO
 footprint–reducing
miracle cures aren’t going to be popular for very long. Such approaches will
simply  be  unsustainable,  along  with  the  alternatives  that  have  captured
media prominence.

2

2

2

Another sobering reality is that energy production will probably never be
entirely risk free. For example, there is no guarantee that, rare as they are,
future oil spills won’t occur. If we don’t drill at home, more oil is certain to
arrive by tankers and barges, with even greater accidental spillage threats
nearer  to  our  shores.  Meanwhile,  other  countries  less  careful  than  we  are
will continue to exploit oil and gas resources in nearby waters. Such risks
are  very  real  and  serious,  and  the  costs  of  doing  business  must  include
ample allowances for prevention and cleanup. Those responsibilities come
along with the benefits of the diverse energy resources we most fortunately
enjoy.

Chapter 11

DEMANDING TRUTH AND ACCOUNTABILITY

Climate  models  reveal  dramatic  warming 

Industrial Revolution.

trends  since 

the

We  have  been  grossly  deceived  regarding  purported  scientific
evidence of a man-made climate crisis. Anyone who claims to know what
climate changes will occur a year, a decade, or even longer ahead is either a
fraud  or  a  fool.  Speculations  are  a  different  matter,  and  variant  theories
abound.  That’s  what  moves  science  forward  and  helps  keep  it  honest
through authentic discourse and objective examination.

2

The corruption of science that was publicly exposed through the release
of purloined CRU e-mails came as no surprise to many who have witnessed
these  travesties  or  dared  to  challenge  the  claims.  Timothy  Ball,  a  former
climatology professor at the University of Winnipeg, received death threats
for presenting his beliefs. As he puts it, “CO
 was never a problem, and all
the machinations and deceptions exposed by these files prove that it was the
greatest deception in history, but nobody is laughing.” He adds that he has
“watched  climate  science  hijacked  and  corrupted  by  this  small  group  of
scientists … Surely this is the death knell for the CRU, the IPCC, Kyoto
and Copenhagen, and the carbon credits shell game.”1

Has  the  importance  of  the  CRU  scandal  been  overblown?  This  isn’t
simply  a  matter  involving  a  few  random  researchers  who  lost  their
professional compasses and made some inconsequential mistakes. Rather, it
involves several of the most influential representatives of a climate science
community that has received more than $30 billion from US taxpayers over
the past 20 years … and one that guides many trillions of dollars in policy
decisions. These are key people who have

•  controlled central processes and findings of the UN’s IPCC climate

science reviews and leading publications in that field;

•  issued alarming predictions that have dominated world headlines;
•    mobilized  and  presided  over  international  climate  change  crisis

summits attended by thousands upon thousands of delegates;

•    provided  the  rationale  for  draconian  environmental  and  energy
regulations  that  significantly  impact  local,  regional,  national,  and
global economies;

•  dictated moral imperatives to justify massive transfers of wealth and
power  between  population  segments  and  nations,  and  between
citizens and their governments;
  established  the  basis  for  government  agencies  rather  than
competitive  free  market  processes  to  pick  energy  and  technology
winners and losers; and

•  afforded a good cover story for blatant and unproductive cap-and-

• 

trade profiteering.

Is  this  all  connected  to  a  diabolical,  centrally  organized  conspiracy?
Probably not. More likely, it reflects a confluence of separate agendas that

are  well  served  by,  if  not  totally  dependent  upon,  a  man-made  global
warming premise in general and demonization of carbon in particular. The
vast  majority  of  the  proponents  should  be  assumed  to  be  very  good,
honorable, and sincere people who believe in their causes and the scientific
claims  that  support  them.  The  same  goes  for  competent  and  dedicated
scientists on all sides of climate debates who are deserving of public trust
and support.

Silent Hearings in the Senate

 

Senator Inhofe, an outspoken critic of global warming disaster theories,
has  worked  very  hard  to  bring  solid  science  into  the  debate.  As  ranking
member of the Senate Committee on Environment and Public Works, he has
convened  hearings  to  question  authorities  on  the  matter  with  scant  media
interest. This number of “skeptics” now exceeds six hundred fifty, including
Japanese  chemist  Kiminori  Itoh,  who  calls  global  warming  alarmism  the
“worst  scientific  scandal.”  Dr.  Itoh  is  one  of  many  who  formerly  worked
with IPCC and have since come to oppose the UN’s positions. His group is
growing rapidly, far outnumbering the fifty-two UN scientists who authored
IPCC’s 2007 AR-4 “Summary for Policymakers” report.2

Senator Inhofe is demanding an investigation of scientific improprieties
revealed in the CRU e-mails, calling the affair a wake-up call for America.
“The  notion  that  these  scientists  tried  to  declare  the  science  settled  for
personal  reasons  is  disgraceful,”  says  Inhofe.  “They  were  purposefully
misrepresenting  the  facts.  They  tried  to  make  America  believe,  and  it
worked, for a time. Even my grandkids came home filled with this stuff,
saying that ‘anthropogenic gases cause global warming’! I reminded them
that  these  things  go  in  cycles.  We’ve  had  warming,  then  cooling,  then
warming and cooling again. I’m delighted that people are discovering that
the science has been cooked for a long period of time.”3

Senator  Inhofe  noted  that  the  CRU  data  used  in  the  IPCC’s  2007
summary was subsequently used by the EPA in preparing its guidelines on
carbon  emissions,  connections  that  are  very  worrisome  for  the  American
taxpayer: “There are tremendous economic ramifications to what these guys
were trying to do … The IPCC for years has been costing the government

so much money, and now, wasted time in trying to pass faulty legislation
based on bad data.”4

Hans  von  Storch,  the  former  editor  of  Climate  Research,  said  on
November 23, 2009, that the behavior outlined in the hacked CRU e-mails
went too far, and that the East Anglia researchers “violated a fundamental
principle of science” (by refusing to share data): “They built a group to do
gate-keeping, which is totally unacceptable.” Von Storch is now a professor
at the University of Hamburg’s Meteorological Institute.5 Lord Monckton,
at  Britain’s  Science  and  Public  Policy  Institute,  went  even  farther  in  his
condemnation, saying that these researchers “are not merely bad scientists
—they  are  crooks.  And  crooks  who  have  perpetrated  their  crimes  at  the
expense of British and US taxpayers.”6

Unsustainable Energy Claims

 

Arguably  the  most  serious  public  deception  perpetrated  by  the  war
against climate change is the notion that cleaner, sustainable energy options
are available in sufficient abundance to replace dependence upon dwindling
fossils that currently provide about 85 percent of all US energy. Regrettably,
this is broadly recognized not to be the case at all, and this circumstance,
not global warming, presents epic challenges. Ironically, many of the same
groups  that  champion  environmental  and  human  causes  are  inhibiting
progress toward vital solutions.

Perhaps  the  least  publicly  understood  aspect  of  various  carbon-free
“sustainable” energy alternatives such as wind, solar, and geothermal power
are  their  anemic  capacities  to  contribute  in  significant  ways  to  achieving
higher levels of independence from foreign sources as world demands and
prices continue to escalate. Pervasive green advertising, which suggests that
sustainable fuel and power are virtually unlimited, is extremely misleading
in this regard and does the public great disservice. Such pretense obfuscates
the importance of exploring and exploiting untapped fossil fields, including
oil  shale  deposits;  strengthening  production  infrastructures;  and  greatly
expanding nuclear potentials that have no near-term substitutes at any costs.
Even as those other alternatives become more market competitive by virtue
of technology and processing economies, government incentives, and rising

oil prices, the prospects for real growth or environmental benefits fall far
short of popular illusions.

There is much we can learn from European experiences. The EU, which
like  the  US  relies  heavily  upon  fossils  and  imported  oil,  has  found  little
salvation through its enormous investments in renewable alternatives. This
realization  is  now  causing  pain  accompanied  by  second  thoughts  as
members  witness  their  economies  threatened  by  rampant  fuel  hikes  and
menacing  power  shortages.  A  contributing  factor  can  be  attributed  to  an
emphasis upon unsuccessful CO
 emission–reduction efforts that have also
failed to fill large energy gaps. As a result, the European Commission has
concluded that nuclear power offers the only clean-energy solution that can
avert a rapidly approaching crisis. This, despite the commission’s own polls
indicating that although France, the UK, the Czech Republic, Poland, and a
handful  of  other  European  states  are  strongly  pro-nuclear,  only  about  20
percent of Europe’s total citizenry support its use since Chernobyl.7

2

Yet even formerly antinuclear Italy now plans to begin building nuclear
plants within 5 years. The Italians seem to have become weary of paying
the highest electricity prices in Europe.

Massive European subsidies for expensive wind, solar, and hydropower
projects have delivered poor investment/return ratios, and the food-versus-
fuel  debate  has  deflated  the  biofuels  bubble.  Loud  public  protests  have
influenced  the  Scottish  Parliament  to  deny  permission  to  develop  a  huge
Isle of Lewis wind farm, and other proposals have drawn similar opposition
because of the large land tracts required. Offshore wind farms have proven
to be extremely costly to build and maintain. Still, other than nuclear power,
the EU regards wind to be its most viable nonfossil alternative hope.

Europe, like America, sits upon vast quantities of coal that are becoming
appreciated more and more. Yet Germany’s recently announced plans for a
new generation of coal-fired power plants have hit a wall of environmental
resistance over carbon emissions that have already caused some proposals
to be canceled. This has built a stronger case for more nuclear development
in Germany, reversing previous intentions to phase nuclear out. France and
Britain have hatched a joint plan to construct a new generation of nuclear
power stations and export technology around the world, yet EU leadership
remains  bogged  down  in  trying  to  win  a  public  debate  over  its  energy
future.  As  Czech  prime  minister  Mirek  Topolanek  recently  warned,  “We

must  do  more  than  talk  about  nuclear  energy.  It  is  really  five  minutes  to
midnight.”8

The energy security alarm clock is ticking for America, too, and nuclear
development  is  an  inevitable  priority.  Vital  aspects  of  long-overdue
infrastructure expansion are necessary in order to (1) reassert technological
science and plant construction; (2) ensure leadership; (3) establish an active
spent fuel reprocessing program; and (4) provide safe, effective storage for
radioactive waste products.

The Energy Independence Myth

 

2

2

2

Political  slogans  that  offer  visions  of  an  energy-independent  America
powered and fueled by abundant sunshine, wind, and corn aren’t likely to
come to fruition.

Never mind that CO

 reduction efforts applying alternative energy in EU
countries have accomplished nothing, and that the level of CO
 emissions
between  the  years  2000  and  2004  grew  2.1  percent,  compared  with  1.3
percent for the US. Forget that global temperatures haven’t risen since 1998
despite higher CO
 emissions worldwide; that the 2008 Antarctic summer
ice melt was the smallest on record; and that cooling is expected to continue
at least through 2015. But don’t forget that hot or cold, flood or drought,
energy, not climate, is a global problem that won’t go away.9

The  terms  “energy  independence”  and  “energy  security”  should  be
recognized to mean very different things. Energy independence is an empty
but appealing political slogan. It offers a fantasy illusion of an autonomous
America,  powered  and  fueled  by  limitless  sunjuice,  friendly  breezes,  and
amber waves of grain that sever our reliance upon dirty smokestacks and
greedy  tyrants,  both  foreign  and  domestic.  Forget  it!  It’s  not  going  to
happen.

Energy  security,  however,  is  an  urgent  goal  that  should  be  taken  very
seriously. That goal is to secure sustainable supplies of energy to ensure an
uninterrupted  high  quality  of  life  for  the  citizens  of  this  country,  and  to
advance continuing social and economic progress for generations to come
as members of a larger world community. This does not imply subscribing
to  a  “new  world  order”  or  abandoning  independent  national  interests.  It

does imply that we must realize that globalized markets, including energy,
are an unavoidable reality and an indispensable necessity.10

America’s  future  prosperity  will  hinge  upon  its  ability  to  produce  and
exchange value in an oil-fueled world market, which includes countries that
don’t like us. Oil is the new currency that defines bargaining power, both
for those who provide it and for those who can afford the price. The quest
for that currency is producing some very disturbing alliances among large
emerging consumer nations and unsavory adversarial regimes. They draw
upon huge national budgets, share technologies and operational costs, and
function with environmental impunity in our coastal waters.11

Oil-hungry  China  has  been  putting  together  oil  and  gas  deals  with
Argentina,  Brazil,  Cuba,  Peru,  Ecuador,  and  most  troubling,  Venezuela—
the  fourth-largest  US  oil  supplier.  A  series  of  agreements  will  enable
Chinese companies to explore for new resources and set up refineries in that
South  American  country,  which  currently  is  headed  by  an  anti-American
dictator. A similar circumstance is developing in Cuban waters in the Gulf
of  Mexico  near  Florida.  China’s  continued  penetration  into  the  Western
Hemisphere, including ambitious efforts to secure agreements with Canada,
will  have  profound  economic  and  political  implications  for  America’s
future energy security.

India’s booming economy, which has been growing at a remarkable rate
of  8  percent  per  year  throughout  the  past  decade,  is  driving  escalating
competition  with  other  countries  for  oil  imports.  Presently,  70  percent  of
India’s  energy  needs  are  supplied  by  domestic  coal  reserves  and  only  30
percent by oil, 70 percent of which is imported. At current growth rates, the
IEA predicts that India will continue to increase energy consumption by at
least 3.6 percent annually, causing it to double by 2025 when the nation will
import more than 90 percent of its petroleum supply.12

Looking  to  the  West,  India  is  working  to  pursue  relationships  with
Venezuela,  which  the  Indian  petroleum  minister  has  referred  to  as  “our
arrowhead  in  Latin  America”  that  can  be  used  to  open  up  other  South
American markets. Closer relationships between India and Venezuela will
run  counter  to  US  attempts  to  isolate  the  regime  of  Hugo  Chavez.
Venezuela,  the  fifth-largest  exporter  of  oil  worldwide,  is  seeking  to
diversify  its  markets  in  order  to  reduce  dependence  upon  the  US,  which
buys more than 60 percent of its crude.

Russia’s economy, like that of China and India, has been growing at an
amazing rate. In 2007, its real GDP rose by more than 8 percent, surpassing
the  growth  rates  in  all  other  G8  countries  and  marking  the  seventh
consecutive  year  of  economic  expansion.  Most  of  that  growth  has  been
driven by energy exports due to aggressive oil production and high world
oil  prices.  The  country  has  been  meeting  more  than  half  of  its  domestic
energy needs from enormous natural gas reserves, while energy consumed
from oil has actually decreased from 27 percent in 1992 to about 19 percent
currently. According to IMF and World Bank estimates, Russia’s oil and gas
sector generated more than 60 percent of its export revenues in 2007.13

Russia is actively pursuing cooperative energy deals with other countries,
including  China,  Cuba,  and  Venezuela.  In  2008,  the  Kremlin  dispatched
Deputy  Prime  Minister  Igor  Sechin  to  negotiate  a  bilateral  China-Russia
agreement  involving  crude  oil  trade,  joint  development  of  new  deposits,
construction  of  oil  and  gas  pipelines,  and  development  of  refining  and
chemical  production  facilities.  Both  countries  have  agreed  to  build  an
Eastern  Siberia–Pacific  oil  pipeline  to  China,  along  with  a  $10  billion
pipeline to transport natural gas to China from eastern and western regions
of Siberia.14

Energy Minister Sergey Shmatko announced in August 2008 that Russian
to  develop
oil  companies  have  “sufficiently  promising  prospects” 
cooperation  with  Cuban  partners  for  joint  oil  prospecting  in  the  Gulf  of
Mexico. He further commented, “I think that working groups will soon be
set up as part of an agreement signed by the Russian Energy Ministry and
the Cuban Ministry of Basic Industry at the commission session to examine
the  issue.”  Russia  may  provide  assistance  in  overhauling  Cuba’s  oil
production  infrastructure,  sharing  oil  transportation  technologies,  helping
repair Cuba’s crude oil storage facilities, and inspecting pipelines.15

Shmatko  further  indicated  that  the  Venezuelan  state-owned  petroleum
company Petróleos de Venezuela will also be involved in Russo-Cuban oil
processing  cooperation,  seeing  as  Venezuela  is  Cuba’s  strategic  partner.
Given  China’s  and  India’s  interest  too,  it  looks  as  though  the  Gulf  of
Mexico off America’s coast is going to be a very popular place for drilling
—with everyone except US Congress Democrats.

As other countries continue to exploit readily accessible oil and natural
gas  reservoirs,  including  some  in  North  America’s  coastal  waters,  US
companies  are  being  constrained  by  regulations  that  preclude  similar

opportunities.  Accordingly,  costs  associated  with  finding  and  developing
US  resources  are  rising  sharply;  they  nearly  doubled  between  2004  and
2006.  These  high  costs  are  motivating  American  companies  to  limit
capacities 
to
approaches used in manufacturing, power generation, and consumer goods.
This  involves  cutting  back  on  operations  of  oil  wells  and  platforms  that
aren’t essential to meet immediate demands in order to avoid unnecessary
expenses.16

through  a  “just-in-time”  production  strategy  similar 

If  the  US  really  wants  to  make  progress  toward  energy  independence,
why not begin by opening up offshore access to domestic supplies before
state-owned  national  companies  controlled  by  Cuba,  China,  and  Mexico
siphon off the shared reservoirs?

cap-and-trade 

carbon-emission 

It is safe to bet that days of cheap oil are over without requiring any help
from added government fuel taxes, price-hiking alternative fuel incentives,
and 
shenanigans.  Growing  global
competition for dwindling supplies will guarantee that happens. Continued
federal restrictions upon domestic drilling will also maintain petroleum and
natural gas prices at high levels for American consumers relative to world
levels, placing US exploration companies at even greater cost disadvantages
when  up  against  large  state-owned  operations.  Consequently,  we  will  be
forced to increase imports, becoming more, not less, dependent on foreign
sources. Some, such as China, Cuba, and Mexico, may be willing to sell us
some of what they obtain from our own offshore Gulf Coast regions as US
companies experience continued depletion of their legacy fields.17

California Dreaming: A Wake-Up Call

 

In December 2009, Governor Arnold Schwarzenegger (R-CA) released
a report based upon a California Energy Commission study predicting that
global  warming  would  cause  San  Francisco  Bay  waters  to  cover
Fisherman’s Wharf and Treasure Island by 2100. This somber forecast adds
to a sea of rising debt the entire state is already experiencing in part due to
its recent GHG emissions legislation.

A law passed by the governor and legislative Democrats in 2006 (AB32)
mandates that GHGs be reduced to 1990 levels (about 25 percent) by 2020.
While  the  state  continues  to  lose  industries,  jobs,  and  people,  its  top

executive hasn’t relented, stating, “We must be prepared if climate change
continues to worsen.”18

A 2009 study undertaken by economists at California State University at
Sacramento estimated that AB32 implementation costs “could easily exceed
$100 billion” and that the program would raise the cost of living by $7,857
per household annually by 2020. The California Small Business Roundtable
commissioned the research.

While California focuses upon windmills, solar panels, and electric cars,
vast offshore oil resources go undeveloped and nuclear power is ignored.
Consequently, the energy-starved state’s economic future is bleak. A 2009
Milken Institute study shows a recent loss of nearly four hundred thousand
manufacturing  jobs.  Other,  well-intentioned  environmental  policies  have
caused more than 450,000 acres of previously fertile agriculture land in the

Opponents  of  AB32  have  submitted  eight  hundred  thousand  signatures
(nearly  double  the  number  needed),  seeking  to  suspend  the  law  in  a
November  2010  ballot.  If  approved,  AB32  would  not  be  reinstated  until
California has four consecutive quarters when the unemployment rate is 5.5
percent or less.

Among  noted  critics  of  the  AB32  suspension  initiative  is  a  small
company  called  Serious  Materials,  a  California  building  materials
manufacturer and the only window producer to receive tax credits through
the  Obama-Biden  administration’s  “cash  for  caulkers”  stimulus  package.
Perhaps coincidentally, the company’s director is married to Cathy Zoi, the
administration’s  assistant  secretary  of  energy  efficiency  and  renewable
energy, who controls $16.8 billion in stimulus funds. Zoi (formerly the CEO
of  Al  Gore’s  ACP)  and  her  husband  hold  120,000  shares  in  Serious
Materials,  along  with  stock  options.  The  two  are  also  reported  to  hold  a
substantial  interest  in  the  Swiss  firm  Landis+Gyr,  which  makes  “smart
meters,” a central component of the administration’s “smart grid” plans.19

State Assemblyman Chuck DeVore (R-Irvine) and others are questioning
the  alleged  science  behind  such  economically  damaging  decisions,
particularly  in  light  of  the  CRU  e-mail  revelations.  He  observed,
“Combined with the $21 billion deficit we’re facing in the coming year, this
shows we ought to be focusing our attention on more mundane things—like
living  within  our  means.”  Then  he  added,  “To  use  this  all-encompassing
rubric  of  climate  change  as  a  power  grab  to  usurp  property  rights  is
something we shouldn’t be doing.”20

San Joaquin Valley to be turned into desert through water diversion aimed
at saving an obscure species of tiny delta smelt fish. Farmers in that area
that  once  fed  the  world  now  line  up  at  food  pantries.  Valley-wide
unemployment  averaging  17  percent  has  soared  upward  to  40  percent  in
some small towns.

The regulatory environment in California has turned the dreams of good
lives into nightmares for many who are leaving in hordes, taking much of
the  state’s  tax  base  with  them.  About  2.14  million  fled  to  other  states
between  2005  and  2007,  while  only  about  1.44  million  moved  in.
Meanwhile, the state's debt rises at a rate of about $25 million per day.

Texas benefits from California’s population and business migrations; the
state has realized 70 percent of all new US job growth since 2008. The state
also leads all others in the number of Fortune 500 companies headquartered
there  (sixty-four,  compared  with  fifty-six  in  New  York  and  fifty-one  in
California).  Much  of  this  prosperity  can  be  attributed  to  an  emphasis  in
Texas on laissez-faire markets and individual responsibility, which contrasts
with  California’s  reliance  upon  central  planning,  tax-supported  energy
subsidies, and social entitlement programs. At the same time, Texas is also
becoming  a  leader  in  wind  power  development,  applying  an  “all-of-the-
above” energy policy and no personal state income tax.

Bad Science—Sponsored by Us

 

for 

Who  paid 

that  goofy  science  speculation 

that  Governor
Schwarzenegger’s  scary  global  warming,  oceans  rising,  engulfed  land,
emission control–urgent report was based on? We did, of course, along with
additional contributions from generous California taxpayers to cover about
$150,000  in  presentation  costs.  After  all,  through  our  federal  and  state
governments, we pay for most science, good and bad, which is entrusted to
agencies and their minions to distribute, administer, or conduct.

To suggest that science research trickles down from government would
be  a  gross  understatement.  Actually,  it  cascades  from  mountains  on  high,
presided over by people whom we generally assume to be knowledgeable
and  objective.  Often,  we  might  assume  wrong.  This  occurs  when  a
particularly orthodox view becomes inculcated into government leadership
and surrogate organization power structures . . . Yes, exactly like man-made

global warming, for example. Then follow the rivers, streams, and creeks as
those influences spread.

Agencies get funding appropriations based upon how important they are,
or more accurately, how important we are persuaded to think they are. In
the  case  of  environmental  issues,  they  are  a  lot  more  important  if  they
appear  to  address  (certainly  not  waste)  a  crisis.  Climate  change,  a  topic
offering an opportunity to regulate something really dangerous, like natural
air, is just too wonderful to pass up.

Who populates these agencies? People with correct orthodox credentials,
of course. It helps a lot if they have published books or articles that favor
and advance those views, or at least associate with influential organizations
that  do.  Let’s  call  that  the  “orthodox  mainstream.”  Then  again,  most  of
those books and articles wouldn’t have been published at all if the authors
didn’t  have  good  science  credentials,  right?  They  would  need  to  have
undertaken research that was published in respected journals.

Farther downriver, the universities that support learned research and hire
scientists  to  conduct  it  depend  upon  money  from  federal  and  state
government  agencies  (again,  from  us).  To  compete  for  that  money  they
must  address  topics  that  are  recognized  by  the  orthodox  mainstream  as
being  very  important.  Only  then  can  they  hire  and  produce  people  who
write successful proposals to support staff to do the research to prepare the
papers that get published in the respected journals.

But  what  if  those  learned  people’s  papers  can’t  get  published  in  the
respected  journals  because  they  contradict  views  of  influential  orthodox
mainstream gatekeepers who attack their merit—as with, for example, the
exact  circumstances  exposed  in  the  CRU  e-mail  communications.  In  this
case,  those  scientists  wouldn’t  win  grants  and  contracts  (from  tax  and
tuition  money  we  supply)  to  gain  tenure  and  promotions  at  leading
universities and research laboratories, or to gain credentials to get hired by
the agencies and surrogate organizations that distribute the funding. Others
who play the game by rules of politics are likely to fare much better.

Where is responsible journalism, the “fourth branch of government,” in
all this? All too often its mainstream is very far downstream from real facts.
Besides, sensationalism sells much better than scholarship does, and it earns
invitations to alarmist briefings high up in the mountains of power whence
all waters flow.

Green Envy

 

“Saving the planet” makes for a great cover story to conceal power and
wealth redistribution agendas that have no such laudable purposes. This was
witnessed at the 2009 Copenhagen Summit, and is also evident on the US
national  scene.  As  Czech  president  and  economist  Vaclav  Klaus  has
observed,  environmentalism  has  become  a  banner  name  under  which
governments are given license to seize commanding heights of economies
and societies. Another name for this—dare we say it aloud?— is socialism,
and manipulated science is its servant.

In  the  1970s  and  early  1980s,  Third  World  countries,  by  force  of
numbers,  and  European  socialist  green  parties,  through  powers  of
aggressiveness,  seized  control  of  the  UN  and  began  calling  for  a  New
International  Economic  Order  (NIEO).  The  NIEO’s  central  goals  were
unambiguous: namely, to transfer unfair wealth from the industrialized West
to their majority; to establish global socialism; and to obtain postcolonial
reparations for perceived past misdeeds. That dream lives on in the spirit
and actions of the IPCC, the Kyoto Protocol, the Copenhagen Summit, and
the hearts and minds of those who have perpetuated the crusade—and will
continue to do so.

One such voice is Maurice Strong, then-executive director of the UN’s
Environment  Programme,  who  expressed  an  even  stronger  NIEO  view  in
his opening speech at the UN-sponsored Rio Earth Summit in 1992: “We
may get to the point where the only way of saving the world will be for
industrial  civilization  to  collapse.  Isn’t  it  our  responsibility  to  bring  this
about?”  Mr.  Strong  coauthored  a  book  titled  Earth  Charter  with  Mikhail
Gorbachev in 1992. The former president of the Soviet Union recognized
the  importance  of  using  climate  alarmism  to  advance  socialistic  Marxist
objectives, stating in 1996, “The threat of environmental crisis will be the
international disaster key to unlock the New World Order.” This may have
seemed like the last hope for that agenda following the USSR’s economic
and political collapse in 1991. Yet, even today, Gorbachev continues to call
for a kind of perestroika or “restructuring” of societies around the world,
starting  with  the  US;  he  argues  that  the  economic  crisis  since  2007
demonstrates  that  our  economic  policy  model  is  failing  and  must  be
replaced.

Former US senator Timothy Wirth agreed with Maurice Strong about the
urgency  of  using  climate  crisis  as  a  means  to  force  social  and  economic
change,  even  if  there  was  no  science  to  support  it.  The  senator,  who  had
been  arranging  prayer  breakfasts,  came  to  head  the  National  Religious
Partnership for the Environment. Later, as undersecretary of state for global
issues, he addressed the Rio Earth Summit audience, saying: “We have got
to ride the global warming issue. Even if the theory of global warming is
wrong, we will be doing the right thing in terms of economic policy and
environmental policy.”21

The  call  to  “stop  man-made  global  warming”  has  served  as  a  central
rallying  theme  in  socialist  warfare  against  US  free  market  capitalism  in
Europe as well as in Third World countries. While the big guns may appear
to be aimed at a CO
 enemy, the real target is likely to be our economy and
world dominance.

2

The  2000  Lisbon  Declaration—supported  by  a  group  of  “civil  society
organizations,”  including  EU  institutions,  the  World  Bank,  and  the  UN
Council of Europe, among other government, private, and corporate entities
—asserted that the EU would “leapfrog” the US in productivity and output
by  2010,  yet  by  2005,  it  was  apparent  that  this  forecast  was  misguided.
Rather  than  surpassing  the  US  in  its  share  of  world  output,  the  EU  was
losing ground at an increasing rate.22

The EU’s contributions to world GDP have fallen from about 36 percent
in 1969 to roughly 27 percent today. While that is slightly higher than the
US percentage of the world’s $47.9 trillion output, it should be recognized
that the EU has about 80 million more people than this country does. In a
few  years,  economic  power  of  both  the  US  and  the  EU  is  likely  to  be
eclipsed  by  that  of  Asia  thanks  to  booming  economic  developments  in
China and India. Yet the US can be expected to remain a world GDP leader
for decades to come, provided we avoid self-inflicted carbon penalties and
bring runaway government spending and largesse under control.

European  green  parties  with  major  influences  over  UN  climate-based
agendas are really angry at the US for not adopting legally binding Kyoto-
type  GHG  emission  restrictions.  They  also  advocate  massive  wealth
transfers from developed countries (primarily the US) to the Third World to
combat  alleged  global  warming  impacts.  Not  surprisingly,  the  so-called
Group of 77 developing countries is pushing this idea, insisting that it won’t
accept  tokenism.  Secretary  of  State  Hillary  Clinton’s  2009  Copenhagen

promise  to  raise  an  annual  $100  billion  contribution  for  this  purpose
signifies  that  the  Obama-Biden  administration  acknowledges  US  climate
transgressions  and  intends  to  make  amends.  The  big  question  is  whether
China will continue to lend us the necessary money.

Meanwhile, as the US government proceeds on a colossal borrowing and
spending binge, added impacts of cap-and-trade legislation upon economic
recovery  and  growth  warrant  true  alarm.  And  if  this  isn’t  frightening
enough,  think  about  new  mechanisms  that  have  been  put  in  place  to
accomplish carbon regulations even without congressional consent.23

regulate  carbon  emissions  by  declaring 

On the very first day of the 2009 Copenhagen Summit, the EPA claimed
authority 
them  an
“endangerment” to public health. Given the broad and intrusive influence
this ruling establishes over every aspect of our economic life, it represents
an  epic  circumvention  of  constitutionally  established 
legislative
responsibilities. If the timing of this announcement was intended to impress
less democratic world audiences in Copenhagen, there is little evidence of
success.  Hugo  Chavez  didn’t  appear  to  notice—and  got  most  of  the
applause.

to 

Restoring the Republic

 

The US has recently been witnessing sweeping governance changes that
are rapidly shifting power from the private sector and state governments to
federal elected and appointed officials, regulatory agencies, and politically
favored special interest lobbies. While this trend is not altogether new, its
acceleration and outreach are unparalleled at any other period since the end
of World War II. Setting aside the nationalization of much of the banking
and  automotive  industries  within  the  period  of  a  single  year,  along  with
aggressive  efforts  to  do  the  same  with  medical  and  insurance  providers,
consider  looming  prospects  for  climate-based  energy  legislation.  To  help
put  this  into  perspective,  let’s  review  dossiers  of  some  nonelected  people
who even have power to end-run constitutional roles of the US Senate and
Congress. They are called “czars,” and probably for valid reasons.

Global warming energy czar Carol Browner was formally appointed as
the Obama administration’s director of the White House Office of Energy
and Climate Change Policy. The president has outlined these goals in her

job description: “to create jobs, achieve energy security, and combat climate
change,  which  requires  integration  among  different  agencies;  cooperation
between  federal,  state,  and  local  governments;  and  partnerships  with  the
private sector.”24 These duties involve key aspects of three different cabinet-
level  departments:  the  EPA,  the  Energy  Department,  and  the  Interior
Department.  Ms.  Browner  has  stated  that  “we  face  an  environmental,  a
public  health,  and  an  economic  challenge  in  global  warming  unlike
anything we have faced so far.”25

Insiders believe that Ms. Browner probably has more actual power than
does  either  the  head  of  the  EPA  or  the  secretary  of  energy,  both  Senate-
confirmed  positions.  She  also  served  as  a  lead  negotiator  with  the
automobile industries regarding emission levels, and reportedly told them
“to  put  nothing  in  writing.”  Some  may  argue  that  her  intent  was  to
encourage the spokespeople to speak freely.

Heritage Foundation director B. Kenneth Simon and Matthew Spalding
of  the  foundation's  Center  for  American  Studies  both  believe  that
Browner’s activities appear to “be beyond congressional legislative intent”
and seem to “circumvent the authority of the EPA administrator.”26

Carol Browner formerly ran the EPA, and she also served on the board of
John Podesta’s Center for American Progress, which he cofounded with Al
Gore  and  George  Soros.  Podesta  also  cochaired  President  Obama’s
transition team.

The  Socialist  International,  an  umbrella  organization  for  many  of  the
world’s  social-democratic  parties,  including  Britain’s  Labour  Party,  listed
Ms. Browner as a member with distinction. She was also noted as a leader
of  its  Commission  for  a  Sustainable  World  Society,  which  advocates
“global  governance,”  believing  that  wealthy  nations  must  shrink  their
economies  to  address  global  warming.  Browner’s  name  and  profile  were
removed from the organization’s website in January 2009.

International climate czar (or “Special Envoy for Climate Change”) Todd
Stern served as former assistant to the president and as staff secretary in the
Clinton-Gore  administration.  Acting  as  the  administration’s  chief  climate
negotiator from 1993–1998. He is now a principal adviser on international
climate  policy  issues  and  strategies.27  Stern,  like  Browner,  was  associated
with the Center for American Progress; as a senior fellow there, he focused
upon climate change. He has been a strong supporter of the Kyoto Protocol
and a US cap-and-trade policy. As a top aide to President Clinton, he helped

is 

revealed 

to negotiate the Kyoto and Buenos Aires climate pacts, both of which were
unanimously rejected in principle by the US Congress. Stern places a great
deal of blame for global warming on US businesses.

for  climate  change, 

Science  and  technology  czar  John  Holdren,  a  point  man  in  the  White
House 
in  hacked  CRU  e-mall
correspondence  as  a  staunch  defender  of  debunked  research  by  Michael
“Hockey  Stick”  Mann.  At  that  time,  Holdren  was  working  at  the  Woods
Hole  Research  Center  in  Massachusetts,  an  independent  environmental
policy organization—not to be confused with the prestigious Woods Hole
Oceanographic  Institute.  In  a  letter  sent  on  November  23,  2009,  Holdren
defended  his  earlier  e-mail  correspondence:  “I’m  happy  to  stand  by  my
contributions to this exchange. I think anyone who reads about what I wrote
in its entirety will find it a serious and balanced treatment of the questions
of ‘burden of proof ’ in situations where science germane to public policy is
in dispute.”28

law 

In the book Ecoscience: Population, Resources, Environment, which he
coauthored with Population Bomb author Paul Ehrlich, Holdren wrote that
families  “contribute  to  general  social  deterioration  by  overproducing
children”  and  “can  be  required  by 
to  exercise  reproductive
responsibility.” Page 943 of the book suggests the creation of a “Planetary
Regime  to  act  as  an  international  superagency  for  population,  resources,
and environment.” The coauthors envisioned that “such a Planetary Regime
could  control 
the  development,  administration,  conservation,  and
distribution of all natural resources, renewable and nonrenewable, at least
insofar  as  international  implications  exist”  (emphasis  in  original).  That
regime  “might  be  given  responsibility  for  determining  the  optimum
population  for  the  world  and  for  each  region  and  for  arbitrating  [the]
various  countries’  shares  within  regional  limits.”  Possible  methods  of
population control the authors discussed are “sterilizing women after their
second  or  third  child”  and  adding  “a  sterilant  to  drinking  water  or  staple
foods.”29

EPA administrator Lisa Jackson isn’t actually a czar, but she looks like a
good  candidate.  Her  agency  ignored  and  quashed  internal  research  that
contradicted  the  scientific  validity  of  a  CO
  endangerment  finding.  Ms.
Jackson  then  announced  in  December  2009  that  an  EPA  “tailoring  rule”
would follow that would set a GHG emissions threshold of 25,000 tons per
year  for  regulators  under  the  Clean  Air  Act.  As  a  result,  any  new

2

construction  or  modifications  that  would  affect  GHG  emissions  would
require an application for permits that include “best available technology.”
The  tailoring  rule  GHG  threshold  would  affect  between  1  million  and  4
million construction facilities across the country. “This is certainly not an
ending,” Ms. Jackson said. “We will continue to work under the Clean Air
Act.”30

Secretary  of  Energy  Steven  Chu  doesn’t  like  to  consider  all  energy
options . . . not even the one that provides half of our current US electricity.
He has repeatedly said, “Coal is my worst nightmare.” Why does he dislike
coal? Fly ash! He claims that fly ash from coal burning is a hundred times
more  radioactive  than  radiation  emitted  by  nuclear  plants,  which  really
amounts to one hundred times nothing. Plus, most fly ash from coal-fired
plants is readily recovered and recycled as a primary ingredient in concrete,
stucco, and other products to greatly increase their strength. Mr. Chu also
appears  to  have  nighttime  hallucinations  about  global  warming,  having
warned,  “Climate  change  .  .  .  will  cause  enormous  resource  wars,  over
water,  arable  land,  and  massive  population  displacements.  We’re  talking
about  .  .  .  hundreds  of  millions  to  billions  of  people  being  flooded  out,
permanently.”31

Is climate change now a big Department of Labor consideration? And if
so,  why?  The  first  answer  is  probably  yes.  The  second  appears  to  come
straight out of the IPCC/Marx/Kyoto lesson plan. Hilda Solis, the Obama-
Biden  administration’s  labor  secretary,  stated  her  conviction  that  “I  am
fighting for environmental justice.”32

Nancy Sutley, chairwoman of the President’s Council on Environmental
Quality, explained during her Senate confirmation hearing that no agency
would be left out of the choir: “[Climate change] is an issue that will affect
the  entire  federal  government;  almost  no  agency  is  untouched  by  climate
change.”33

We can continue to debate human impacts upon global temperatures, but
there is one anthropogenic influence that we cannot escape: The dangerous
political climate we are witnessing today is entirely one of our making, and
it urgently needs to change.

Healthy Skepticism

 

Whom  can  we  trust?  We  rely  upon  information  from  respected
government,  scientific,  and  media  sources 
to  be
authoritative and objective. It is disconcerting when we discover that they
have deceived us about important matters. Misrepresentation regarding the
existence  and  causes  of,  and  the  urgent  interventions  against,  a  climate
crisis rank among the greatest deceptions of all.

that  we  assume 

Here are some real facts that we can be confident about:

2

2

2

2

1.  There is no scientific evidence that any climate crisis exists other
than the hardships periodically imposed upon affected regions as a
result  of  naturally  occurring  changes.  The  next,  now  overdue  Ice
Age will present a big crisis, but no one can predict when that will
happen. Enjoy these warm times of ecological and human bounty.

2.  Over the past several glacial and interglacial climate fluctuations,
atmospheric CO
 concentrations have generally increased after, not
before,  temperatures  have  risen.  This  is  because  warming  oceans
, and colder oceans absorb it. There doesn’t appear to be
release CO
much,  if  any,  correlation  between  CO
  contributions  from  human
activities  and  temperature  fluctuations.  Yup,  temperatures  have
fallen many times when CO
 levels have continued to rise, just as
they are now.
  Climate  models  cannot  predict  climate  change  events  or
consequences, although the scary warnings we constantly hear are
based upon them. Many climate forcing mechanisms remain poorly
understood,  and  all  interact  in  interdependent  and  complex  ways.
Most scientists who create and apply climate models know this and
will freely admit it. Claims by IPCC officials that confidence in such
models  is  increasing  are  misleading.  Accurate  forecasting  over
years,  much  less  decadal  or  multidecadal  periods,  has  never  been
demonstrated.

3. 

4.    Most  of  the  “greenhouse  effect”  predicted  in  climate  models
suggests  that  warming  should  be  greatest  at  mid-range  to  high
atmospheric  elevations  in  the  tropics,  yet  balloon  and  satellite
observations  show  cooling  there.  Water  vapor  is  by  far  a  more
important  GHG  than  is  CO
,  yet  even  together,  their  climate
contributions  are  likely  to  be  much  smaller  than  current  models
suggest.

2

5.    More  scientific  attention  is  now  being  directed  to  the  effects  of
solar activity upon climate. A low level of activity appears to match
the prolonged period of cooling that occurred during the Little Ice
Age, while an active Sun in the 1930s and again near the end of the
last century corresponds with observed warming. The current solar
cycle  is  the  longest  low-activity  event  witnessed  in  more  than  a
century.  This  suggests  that  the  cooling  observed  since  about  1998
may continue for some time.

6.  Multidecadal cycles in the ocean correlate quite closely with solar
cycles and global temperatures. The Pacific Ocean began to cool in
the late 1990s, and the Atlantic began to cool from its peak in 2004.
Warmer  oceans  from  the  1930s  to  the  1950s,  and  again  from  the
1980s  to  the  early  2000s,  caused  diminished  summer  Arctic  ice
extent.  Antarctic  ice  has  actually  been  increasing  at  a  record  pace
within the span of satellite monitoring history. Although Arctic ice
has been thinning at a rate of about 3 percent per decade, Antarctic
ice, which is twenty times more expansive, has been growing at 1
percent per decade. So, while the 2007 summer Arctic ice melt was
the biggest on record, the 2008 Antarctic ice melt was the smallest.34
Melting ocean ice doesn’t cause sea levels to rise anymore than do
melting ice cubes in a drink cause that level to rise in a glass. (Of
course, when you take a sip, it changes everything, so if you try this
experiment, try not to cheat.)

You, the Jury

 

The corruption of science exposed in the CRU e-mails is finally putting
man-made  warming  alarmism  on  trial  in  the  “court  of  public  scrutiny.”
Imagine that one after another the crisis claimants are called to testify. You,
a member of the jury, must determine if their legitimacy warrants trust.

First  on  the  stand  is  Al  Gore.  He  presents  a  lengthy  PowerPoint
presentation, complete with melting ice caps, drowning polar bears, rising
sea  levels,  marauding  hurricanes,  and  all.  He  ends  with  an  impassioned
pitch for cap-and-trade legislation as humankind’s best chance of salvation
from sins we have wrought upon our planet.

that  most  of 

Perhaps  on  cross-examination  we  discover 

those
“inconvenient facts” don’t hold up. We can forgive him. After all, scientists
disagree  on  many  points.  Let’s  further  acknowledge  that  it  is  really  okay
that  he  has  accumulated  a  large  fortune  promoting  green  products  he  has
financial  interests  in  and  that  he  plans  to  make  a  bundle  more  marketing
carbon  offsets.  It  can  be  argued  that  these  actions  only  demonstrate  true
conviction  in  his  cause.  Let’s  forget  that  the  annual  utility  budget  for  his
twenty-room Nashville home and pool house would pay for a new Toyota
Prius. Let’s give him some credit for bravery in purchasing a new $8.875
million  ocean-view  home  in  Montecito,  California—from  which  he  can
keep watch to warn us of a dangerously rising sea level and such. And we
might cut him some slack for apparently confusing the Earth with the Sun
in  asserting  that  the  temperature  of  our  planet’s  core  is  “several  million
degrees.” Even Nobel laureates can’t be expected to be right all the time.
But maybe his science is not entirely settled after all.

Next to testify is Dr. James Hansen, the father of global warming hysteria
and  Mr.  Gore’s  science  adviser.  He  continues  to  be  celebrated  in
mainstream media as a top climate expert. Never mind that his doomsday
predictions  over  the  past  3  decades  have  never  materialized,  that  he  is
linked to conspiratorial CRU e-mail correspondence, and that his claim of
record-high  2008  temperatures  was  proven  to  be  hot  air.  In  fairness,  his
warning  that  world  temperatures  are  balancing  at  a  “tipping  point”  is
proving  correct,  but  not  because  of  dangerous  CO
.  Once  again,
temperatures  have  been  tipping  downward  since  1998,  while  atmospheric
CO

 concentrations have risen.
In  this  scenario,  we  might  assume  that  testimony  offered  by  key
participants  in  the  CRU  e-mail  climate  science  debauchery  follows.  As
potential  subjects  of  legal  indictments,  perhaps  some  may  have  been
advised by their attorneys to remain silent. Likewise, we might assume that
the  IPCC,  which  used  CRU  data,  responds  only  with  a  brief  written
statement.  Essentially,  it  reaffirms  its  conclusion  that  global  warming
presents  a  continuing  threat;  that  the  IPCC  is  even  more  confident  in  its
predictive  climate  models; 
industrialized  nations,  are
responsible  for  a  looming  crisis;  and  that  $100  billion  in  reparations  to
other, less developed countries for our excesses won’t be nearly enough.35

Then, perhaps, we might hear testimony from a few of the six hundred
fifty  scientists  who  expressed  “contrary”  views  in  a  2008  US  Senate

that  we, 

2

2

the 

minority report prepared by Senator Inhofe.36 For example:

• 

•    Ivar  Giaever,  Nobel  Prize  winner  for  physics:  “I  am  a  skeptic.

Global warming has become a new religion.”
  UN  IPCC  Japanese  scientist  Kiminori  Itoh,  award-winning
environmental  physical  chemist:  “[Warming  fears  are  the]  worst
scientific scandal in history … When people come to know what the
truth is, they will feel deceived by science and scientists.”

•    Indian  geologist  Arun  Ahluwalia,  board  member  of  the  UN-
supported  International  Year  of  Planet  Earth:  “The  IPCC  has
actually become a closed circuit; it doesn’t have open minds … I am
really  amazed  that  the  Nobel  Peace  Prize  has  been  given  on
scientifically 
incorrect  conclusions  by  people  who  are  not
geologists.”

•    Finnish  scientist,  chemical  engineer,  and  former  Greenpeace
member  Jarl  Ahlbeck:  “So  far,  real  measurements  give  no  ground
for concern about a catastrophic future warming.”

•  Norwegian Space Center senior adviser solar physicist Pål Brekke:
“Anyone who claims that the debate is over and the conclusions are
firm has a fundamentally unscientific approach.”

•  Institute of Geophysics, National Autonomous University of Mexico
researcher  Victor  Manuel  Velasco  Herrera:  “The  models  and
forecasts of the UN IPCC are incorrect because they . . . are based
[only]  on  mathematical  models  and  presented  results  at  scenarios
that do not include, for example, solar activity.”

•    New  Zealand  professor  Geoffrey  Duffy,  Department  of  Chemical
and Materials Engineering, University of Auckland: “Even doubling
or  tripling  the  amount  of  carbon  dioxide  will  virtually  have  little
impact, as water vapor and water condensed on particles as clouds
dominate the worldwide scene and always will.”

•    Russian  geographer  and  Antarctic  ice-core  researcher  Andrei
Kapitsa: “The Kyoto theorists have put the cart before the horse. It
is global warming that triggers higher levels of carbon dioxide in the
atmosphere, not the other way around … A large number of critical
documents  submitted  to  the  1995  UN  conference  in  Madrid
vanished without a trace. As a result, the discussion was one-sided

and  heavily  biased,  and  the  UN  declared  global  warming  to  be  a
scientific fact.”

•    Stanley  Goldberg,  US  government  atmospheric  scientist  at  the
Hurricane  Research  Division  of  NOAA:  “It  is  a  blatant  lie  to  put
forth  in  the  media  that  makes  it  seem  there  is  only  a  fringe  of
scientists who don’t buy into anthropogenic global warming.”

•    James  Peden,  atmospheric  physicist  formerly  with  the  Space
Research and Coordination Center in Pittsburgh: “Many [scientists]
are now searching for a way to break out (from promoting warming
fears), without having their professional careers ruined.”

•    Apollo  17  astronaut/geologist  Jack  Schmitt,  formerly  with  the
Norwegian  Geological  Survey  and  USGS:  “The  ‘global  warming
scare’  is  being  used  as  a  political  tool  to  increase  government
control over American lives, incomes, and decision making. It has
no place in society’s activities.”37

In  conclusion,  we  might  recall  remarks  offered  by  Czech  president
Vaclav Klaus during a speech at the Ambrosetti Forum, Villa d’Este, Italy,
in 2007:

The threat I have in mind is the irrationality with which the world
has accepted the climate change (or global warming) as a real danger
to the future of mankind, and the irrationality of suggested and already
implemented measures because they will fatally endanger our freedom
and  prosperity,  the  two  goals  we  consider—I  do  believe—our
priorities.

We  have  to  face  many  prejudices  and  misunderstandings  in  this
respect. The climate change debate is basically not about science; it is
about  ideology.  It  is  not  about  global  temperature;  it  is  about  the
concept of human society. It is not about nature or scientific ecology; it
is about environmentalism, about one— recently born—dirigistic and
collective  ideology,  which  goes  against  freedom  and  free  markets.  I
spent  most  of  my  life  in  a  communist  society,  which  makes  me
particularly sensitive to the dangers, traps, and pitfalls connected with
it.38

And  what  are  the  consequences  of  this  subterfuge?  They  are  costly.
Consider that billions of dollars are spent annually on climate science, only
to  have  much  of  the  most  important  information  distorted  for  ideological
and  political  purposes.  American  industries,  jobs,  and  revenues  may  be
forced to relocate overseas because of irrelevant CO
 emission regulations
that  other  countries  freely  ignore.  Environmentalism  predicated  on  false
premises blocks drilling in US waters, while unfriendly foreign coalitions
exploit these oil reserves. Coal plants that provide half of all US electricity
are  restricted  and  penalized,  while  uncompetitive  high-cost  and  low-
potential alternatives receive mandates and subsidies. Schemes are justified
to  price  and  trade  “carbon  sin  indulgences”  along  with  a  regulatory  EPA
power grab. Then there are also those therapeutic counseling expenses for
treatment of pandemic carbon footprint guilt sufferers.

Corrupt science that supports these travesties has many complicit agents.
It is perpetrated by sponsors who fail to provide competent oversight; by
ideologically,  politically,  and  financially  driven  authorities  who  twist  and
exploit conclusions; and by lockstep, headline-hungry media organizations
that  emphasize  sensationalism  over  substance.  And  we  can’t  forget  those
among  us  who,  through  complacency  and  denial  in  the  face  of  obvious
deception, willingly forfeit demands for accountability. When we abrogate
that responsibility, perhaps we become culpable too.

2

Ladies and gentlemen of the jury, the prosecution rests its case.

Chapter 12

EXERCISING US EXCEPTIONALISM

Bearing painful witness to the arrogance of unwarranted humility

on America’s behalf.

Now  that  we  understand  the  politics  and  people  behind  global
warming,  what  actions  can  we  take  to  mitigate  the  potentially  disastrous
consequences of such misguided pseudoscience?

First of all, we have to acknowledge the reality of the situation. Anyone
who has had an opportunity to visit other countries around the world should

recognize  America’s  exceptional  environmental,  social,  and  economic
achievements. Our environment—compared with the environments of other
developed  nations,  and  starkly  contrasted  with  those  of  communist
countries—is remarkable, clean, and improving. Real progress is occurring
in  the  development  of  highly  efficient  energy  production  and  utilization,
along  with  power  plant  emission  reductions  as  a  by-product.  Based  upon
the  amount  of  energy  used  to  produce  a  dollar  value  in  output,  the  US
voluntarily reduced energy intensity by 20 percent over the period between
1992  and  2004,  as  compared  to  only  11.5  percent  in  the  EU  under  a
mandatory  approach.  This  has  also  enabled  economic  growth,  which
averaged more than 3 percent annually between 1992 and 2005, compared
with about 1 percent in the EU prior to the recent global recession.

Despite  our  environmental  progress,  angry  views  toward  the  US  and
other developed countries expressed by detractors in less free and fortunate
parts of the world have gained a larger world stage with the advent of the
UN’s  climate-change  theater.  Eco-guilt  associated  with  demonization  of
CO
 emissions has been added to other sins of capitalism, including unfair
wealth, consumption, economic influence, and military power. Speaking to
like-minded  foreign  audiences,  our  own  president  has  apologized  for
America’s 
lack  of
sustained  engagement  with  neighbors,  and  dark  periods  of  history.  And
even  though  Nicaraguan  president  Daniel  Ortega,  Venezuelan  president
Hugo  Chavez,  and  other  unfriendly  leaders  have  expressed  strong
agreement,  they  still  don’t  like  him  or  us  any  better  for  offering  these
confessions.  Apparently,  some  of 
the  2009
Copenhagen Summit meetings, where riots broke out, don’t either.

imperfections,  arrogance, 

international  mistakes, 

2

those  present  during 

Despite expanding world economies embracing globalism through open
market capitalism, many would like to turn their international clocks back
to what they regard to be the better “good old days.” Thomas Friedman, in
his book The World Is Flat, attributes much of this sentiment to the strongly
anti-American  and  anti-globalization  movement  that  emerged  at  the  1999
World Trade Organization conference in Seattle. What began as a primarily
Western-driven phenomenon has subsequently gained influence throughout
the  world,  and  the  movement  now  represents  a  convergence  of  several
ideological groups.

One motivation is attributed to upper-middle-class American liberal guilt
in reaction to the incredible wealth and power the US amassed following

the  fall  of  the  Berlin  Wall  and  the  rise  of  the  dot-com  balloon.  Old  left
socialists,  anarchists,  and  Trotskyites,  in  their  alliances  with  protectionist
trade unions, advance another motivation. Particularly strong anti-American
sentiments arose in Europe and the Islamic world over disparities between
American economic, cultural, military, and political power versus those in
other countries after the implosion of the Soviet empire. Groups concerned
with governance influence, ranging from environmentalists to trade activists
to NGOs, became part of populist antiglobalization. Others, who constituted
a more amorphous group, were more generally concerned about the speed
at which the “old world” was disappearing and becoming globalized.

Many of the same people who are behind the global warming hysteria are
also  strongly  opposed 
to  economic  globalization.  They  perceive
globalization as a tool by which wealthy nations exploit poor nations in the
same way they exploit the environment. They view the American capitalist
free  market  engine  as  an  evil  machine  that  drives  globalism,  and  the
economic prosperity it yields as a sin against “social justice.”

Let’s  briefly  review  those  claims  on  the  basis  of  the  real  social  justice

that such detractors purport to care so much about.

According to the World Bank, about 375 million people in China lived in
extreme poverty (on less than an equivalent of $1/day) in 1990. The number
of  impoverished  Chinese  dropped  to  212  million  by  2001.  If  the  current
trend holds, only about 16 million Chinese will live on less that $1/day by
2015.

In  Southeastern  Asia—primarily  India,  Pakistan,  and  Bangladesh—the
number of people living on less than $1/day dropped from 462 million in
1990 to roughly 431 million by 2001, and that number is projected to be
216 million by 2015.

Now,  compare  those  numbers  with  those  in  sub-Saharan  Africa,  which
has  been  slow  to  become  part  of  globalized  markets.  There,  about  227
million people lived on less than $1/day in 1990. By 2001, there were 313
million people living in poverty, and the World Bank expects the number
will grow to 340 million by 2015.

Friedman  argues  that  the  world’s  poor  don’t  resent  those  who  are  rich
nearly as much as left-wing parties in the developed world imagine. Rather,
what they really resent is not having a pathway to get richer and cross the
line into the world’s middle-class opportunities.

As Deng Xiaoping declared when he released the Chinese economy from
its communist shackles and opened it to a free market system, “To get rich
is glorious.” He justified those actions with one sentence: “Black cat, white
cat, all that matters is that it catches mice.” India and Russia have followed
the  same  road  since  the  early  1990s,  and  together  with  China,  they  have
become competitive economic and technology forces.1

Ironically, Americans are now having to combat attacks from within the
very same economic system that offers hope and progress to other countries
now  following  our  successful  model.  If  a  greater  example  of  true  social
justice exists elsewhere, where is that place? Those who believe they know
might do well to expand their travel experiences abroad.

America’s Abundant Opportunities: Capitalizing on

Capitalism
 

Our nation remains a land of unique and enviable opportunity, the home
of the American Dream. Our history—and those participants who provided
the  foundational  visions,  courageous  sacrifices,  and  tools  of  science  and
industry  that  created  this  nation—warrants  no  apology.  We  have  learned
from past mistakes and are better for having recognized them and applying
those valuable lessons. We are also blessed with vast and diverse resources,
natural and human, and we share them generously as good and concerned
global citizens. Our position as leading world power is regarded as both a
privilege and an often-thankless obligation.

Yes, we care about our environment, and we acknowledge the need for
legislative and regulatory incentives and controls to protect it. Yet this will
not be accomplished by undermining the economic system that provides the
financial,  technical,  and  social  resources  needed  to  make  improvements.
Nor  are  these  goals  served  by  irresponsible  climate  crisis  fearmongering
and  exaggerated  alternative  energy  capacity  projections,  which,  when
exposed, only destroy confidence in the honest science and reporting that
are essential as a basis for sound decisions.

And  that  fearmongering  and  fraudulent  reporting  is  definitely  being
revealed:  through  exposure  of  damning  words  and  writings  of  false
prophets and profiteers; through direct observable testimony presented by
Mother Nature; and through conscientious efforts of some who have dared

to  challenge  dogma  served  up  by  high  priests  of  gloom-and-doom
orthodoxy.  These  dedicated  and  valiant  people  include  Senator  Inhofe,
Stephen  McIntyre,  Ross  McKitrick,  Richard  Lindzen,  Fred  Singer,  Roy
Spencer,  Anthony  Watts,  and  many  others  previously  discussed  in  this
book, plus the hundreds more who aren’t.

Rediscovering America

 

We have historically taken pride in “American know-how,” an ability to
recognize  marketable  opportunities  in  problems,  unlimited  capacities  for
invention, and willingness to proactively accept investment risks in pursuit
of  uncertain  rewards.  We  are  a  culture  of  creativity  that  is  recognized,
envied,  and  resented  around  the  world,  as  exemplified  by  our  music,
fashions, movies, sciences, and technologies. We have advanced medicine,
expanded agricultural and industrial productivity, and opened new frontiers
beyond our planet for peaceful purposes.

America  is  a  nation  without  ethnic,  racial,  gender,  or  class  boundaries.
Excellent  educational  opportunities  are  accessible  to  all  who  truly  seek
them. These resources include public and private institutions at all levels,
technical  specialization  programs,  community  colleges,  and  an  enormous
number and variety of distinguished universities for advanced studies that
attract international scholars.

So far, that’s the good news. But there are some more sobering aspects
regarding America’s education and innovation circumstances as well. Many
of the technical programs in top-ranked US universities are now dominated
by a majority of students from Asia and India, who are often supported by
their own nation's scholarships. These are the advanced math, science, and
engineering fields that tend to drive innovation in today’s technologically
competitive world. Foreign attendees, as a general rule, also tend to enter
our institutions with stronger math and science backgrounds, achieve higher
class  rankings,  and  return  to  their  home  countries  enriched  by  their  US
experiences.2

Asians,  in  particular,  appear  to  be  setting  the  international  pace  for
advanced science and math at all educational levels. Some indication of this
is  revealed  by  results  of  the  2003  international  Trends  in  International
Mathematics and Science Study involving half a million fourth- and eighth-

grade students from forty-one countries, including the US. The assessment
concluded that 44 percent of all eighth-graders in Singapore scored at the
most  advanced  level  in  math,  as  did  38  percent  from  Taiwan.  At  an
IntelScience Fair in 2004, Chinese students won thirty-four awards, more
than any other Asian country, including three top global awards.3

In  China,  strong  math  and  science  backgrounds  are  prerequisites  for
admittance  to  the  best  universities  or  to  be  hired  by  foreign  corporations
operating there. The Microsoft research center in Beijing is one of the most
sought-after  work-places  there,  and  the  competition  is  fierce.  There  is  a
popular saying, “If you are one in a million, there are 1,300 people just like
you.”4

Do  leaders  of  our  national  government  subscribe  to  this  philosophy?
Here is what John Holdren, director of the White House Office of Science
and Technology Policy, told students attending a meeting on April 10, 2010,
sponsored by the American Association for the Advancement of Science:
“We can’t expect to be number one in everything indefinitely.” Apparently,
that  goal  would  be  overreaching,  and  it  is  no  longer  encouraged  in  our
young  people.  Maybe  social  justice  promoted  by  Marx  and  the  UN
embodies  a  need  for  globally  distributed  scientific  and  technological
equality as well.

Whereas  most  Chinese  policy  makers  have  engineering  and  science
backgrounds,  the  majority  of  ours  are  lawyers.  Perhaps  this  raises  a
question regarding which nation will be best prepared for future leadership:
one  that  innovates  or  one  that  regulates?  If  China  is  now  embracing
principles  that  led  us  to  where  we  are  today,  perhaps  we  might  turn  our
attention to relearning from our past … and from the Chinese. We need to
get educated.

Free Market Entrepreneurship

 

And  what  about  “rugged  American  individualism,”  the  spirit  where
anything  you  can  dream  is  possible,  Horatio  Alger  rags-to-riches  stories,
Dale Carnegie confidence, and all that? Is that kind of thinking still legal?
Will it unfairly tilt the playing field to favor those who lust for success over
those who are less willing to compete? Will it create income disparities and
exploit  labor  markets?  Those  who  imagined  that  such  arguments  were

settled with the collapse of the Soviet Union and the embrace of capitalism
by China and Russia would be wrong. It seems that many people who enjoy
the products and prosperity created by entrepreneurship—the jobs it creates,
and the services it provides—aren’t comfortable with the ambitions of those
who practice it. Instead, they are more inclined to place faith in government
regulatory  mechanisms  rather  than  market  forces  to  provide  economic
equilibrium, progress, and social justice.

Government  interventions  into  free  market  mechanisms  can  have
unfortunate  consequences,  as  evidenced  by  US  energy  policies  where
Congress is now picking the winners and losers. Examples are the ethanol
debacle promoted under an “energy security” banner; price controls that led
to  the  gasoline  crisis  of  1974;  CO
emission  restrictions  that  are  blocking
development  of  fossil-fueled  power  plants;  mandated  alternative  fuel
purchases that drive up consumer costs; and restrictions on offshore drilling
that afford competitive advantages for foreign-owned companies operating
in our Gulf of Mexico neighborhood. Michael Economides speaks of some
penalties in his book The Color of Oil: “Regulations, unless imposed as part
of a well-thought-out, long-term national policy, stifle rugged individualists
and  capitalists.  Worse  even  in  developed  nations,  regulations  can  make
local  industry  comfortable  with,  and  then  dependent  on,  government-
mandated  market  reforms.  This,  of  course,  is  tremendously  destructive
because  it  thwarts  competition  and  entrepreneurialism,  two  of  the  most
important elements of economic success.”5

2

Real  or  imagined  crises  can  provoke  knee-jerk,  stopgap  government
regulatory  responses  that  are  painfully  costly  and  ineffective.  Global
climate change/CO
 alarmism is being used to block development and use
of fossil resources through a variety of legal and political actions, including
the Endangered Species Act listing of polar bears and the Supreme Court
characterization of CO
 as a pollutant. The latter decision now enables the
EPA to regulate CO

 emissions under the Clean Air Act.

The premises underlying virtually all this are the unproven IPCC climate
model apocalyptic scenarios and political agenda–tweaked summaries that
have never developed quantifiable links or consequences, either human or
natural, between CO
 and global warming. Even if this were not the case,
we can learn much from (1) failed EU attempts to achieve Kyoto Protocol
emission  reduction  targets,  (2)  EU  nations’  inabilities  to  meet  escalating
energy  needs  through  renewables,  and  (3)  resulting  shifts  among  EU

2

2

2

2

members  toward  fossils  and  nuclear  energy.  Why  repeat  their  earlier
mistakes?

Globalization versus Global Governance

 

It  is  time  to  differentiate  between  energy  security  and  energy
independence as well as between international partnering and subordination
to  international  authority.  National  pride  is  no  cause  for  shame,  and
strength, both economic and military, needs no apology. Many have worked
successfully and sacrificed greatly to endow America with these gifts. It is
our responsibility to preserve them, apply them, and pass them along to the
generations that follow. Quite understandably, some other nations have the
same idea, and we must respect that fact.

America  will  continue  to  depend  upon  oil  imports  for  the  foreseeable
future.  We  are  not  alone  in  this  regard,  nor  do  we  have  the  purchasing
leverage  we  once  enjoyed.  The  largest  US  oil  company,  ExxonMobil,
controls  only  about  1  percent  of  global  oil  reserves,  putting  it  far  behind
many  large  nationally  owned  companies  such  as  Saudi  Aramco,  NIOC
(Iran),  Pemex  (Mexico),  PDVSA  (Venezuela),  and  CNPC  (China).  As
world demands increase, so do the power and influence of Arab and Islamic
states  (i.e.  Saudi  Arabia,  Dubai,  and  Iran),  along  with  the  economic
purchasing power of China, India, and other developing countries.6

Global oil competition has become the dominant forcing factor driving
world  trade  and  international  military  alignments.  Accordingly,  we  must
realize that national security and energy security are codependent priorities.
Energy security is essential if the US is to maintain the economic strength
necessary  to  compete  in  all  world  trade  markets,  along  with  the  military
strength to protect our national and allied interests from aggressors. In this
regard,  our  energy  industries  are  the  first  and  arguably  most  vital  line  of
defense.

Just like other nations, we must diversify our access to energy sources of
all  types.  One  aspect  of  this  pertains  to  geographical  sourcing,  where  we
work to reduce dependence upon oil from the Middle East, Venezuela, and
other volatile parts of the world. Doing so, however, does not preclude all
possible  efforts,  including  diplomacy,  economic  support,  and  antiterrorist

actions,  to  enhance  stability  in  those  regions.  Our  security  is  linked  to
theirs.

We can participate in helping other nations develop energy resources as
trading partners, just as other countries are doing. Examples are investment
and assistance in oil and gas drilling and operations (e.g., China); refinery
development  and  services  (e.g.,  India);  and  advances  in  nuclear  electric
technologies (e.g., France, South Africa, and Japan). A key opportunity is to
provide  technologies  and  assistance  in  reprocessing  spent  nuclear  wastes
into fuels that cannot therefore be used against other nations for weapons.

The US can reduce dependence upon foreign oil and demands upon our
croplands by importing ethanol from Brazil. Produced from sugar cane, it is
a  much  more  energy-efficient  alcohol  source  than  is  corn.  This  will  once
again  encourage  American  farmers  to  diversify  crop  production,  relieve
consumer food costs, and accommodate lucrative and humanitarian export
advantages.

Yes,  America  can  and  should  continue  to  develop  and  expand  use  of
renewable  wind,  solar,  and  biotech  capacities,  along  with  hydropower,
geothermal, and hydrogen energy to the extent this makes sense. We should
not  be  misled,  however,  to  imagine  that  these  sources  will  promise  any
degree of energy security or independence, however greatly we might wish
that  to  be  the  case.  Much  greater  potential  lies  in  developing  improved
methods to optimize use of our large coal and shale reserves through clean
coal  technologies  and  transportation-fuel  derivatives.  These  will  become
even more attractive as foreign oil and gas demands continue to add market
pressures, which will also make support for expanded domestic drilling a
survival necessity for political aspirants.

Rational Environmentalism

 

All  responsible  people  care  about  human  impacts  of  energy  practices
and other activities upon the natural environment. Terrible examples include
the  inexcusable  Chernobyl  nuclear  reactor  failure,  the  massive  pipeline
leaks  in  Russia  that  spilled  10  percent  or  more  of  the  oil  en  route,  and
horrendous  air  pollution  conditions  in  China.  It  is  also  responsible  to
recognize  a  need  to  preserve  energy  resources  for  future  generations
through conservation efficiencies. Who can argue with that?

Just as everything humans have done in the name of industrial progress
hasn’t  been  kind  to  the  environment,  it  shouldn’t  be  assumed  that
everything  is  bad  either.  Yet  the  modern-day  environmental  activist
movement  tends  not  to  see  it  that  way.  If  there  is  a  perceived  climate
change, then it must be our fault, and the consequences must be dire. Since
industry  creates  pollution,  then  all  emissions,  including  CO
,  must  be
pollutants. If glaciers appear to be melting more rapidly than they were a
few decades before, then we must be responsible, and the condition must be
irreversible.

2

Maybe  it’s  time  to  apply  some  constructive  hindsight.  Not  so  many
decades ago, the glaciers advanced, and times were not really good at all.
And  while  industrialization  has  changed  our  planet,  the  technologies  and
progress  that  accompanied  it  have  not  been  entirely  negative.  As
civilizations  have  evolved,  so  have  sanitary  conditions.  Life  expectancies
have more than doubled since the times when raw sewage was heaped upon
the  streets  of  grand  Greek  and  Roman  empires  and  plagues  and  famines
devastated rural populations.

So much for the romantic visions of a peaceful, nirvana-like agrarian life
in  balance  with  nature.  There  are  big  differences  between  environmental
stewardship  ideals,  which  most  of  us  subscribe  to,  and  the  ideologically
moralistic, antidevelopment, obstructionist activism that exemplifies much
of today’s environmental zealotry. Premised on pseudoscientific rationale,
publicized  through  expensive  ad  campaigns  and  mainstream  media
messages, and lawyered up for battle, today’s green activist movement has
asserted dangerous influences over our energy-driven social, economic, and
security prospects.

What can we do to preserve and advance the energy priorities we care

about? It’s not going to be easy, but there is some hope.

To  begin  with,  we  can  seek  and  demand  objective  and  informed  news
sources  so  that  we  know  the  facts  and  keep  abreast  of  what  is  going  on
around us. The Internet— blog sites and all—is a tremendous resource for
data and opinions that cover all sides of events and issues. It’s remarkable
what the mainstream media doesn’t tell us, and equally amazing how often
they get the facts wrong in what they selectively present.

We can exert influences over who represents our interests in the arena of
politics, and we can tell them what we care about. We can support those we

trust through campaign participation, funding contributions, and votes. And
we can hold them accountable to earn that continued trust.

We  can  prepare  our  children  for  the  future  through  education  and
example. We can encourage them to develop survival knowledge in math
and sciences and to appreciate the importance of US and world history as
well. We can become involved with their schools and take interest in their
classroom and homework activities to find out what they are learning. Some
of that may prove to be alarmingly misguided.

Responsible Resourcefulness

 

Americans  will  experience  increasing  market  pricing  pressures  to
practice more effective energy conservation. In this regard, we must work to
do  more  with  less,  not  simply  try  to  do  less  using  less,  applying
retrogressive, utopian, ideological fantasies. If we can’t drill or dig our way
out of fossil-fuel energy dependence, imagining that we can starve our way
out doesn’t make any sense either. Fossil-fuel energy currently drives our
economy  and  progress.  It  finances  the  development  of  technologies  and
applications  that  are  more  efficient  and  environmentally  friendly.  It
provides  jobs—  yes,  for  coal  miners  and  oil  field  workers,  but  also  for
environmentalists  who  are  supported  by  energy 
industry–dependent
revenues.

Conservation is not just a matter of using less to do more; it also involves
using less of what is most limited so that it can be preserved for uses that
are more important. Why, for instance, should we use so much natural gas
for electricity if that power can be generated by nuclear plants that are even
cleaner?  That  natural  gas  can  be  used  for  transportation  fuel  to  reduce
drains on petroleum reserves and imports. How can we characterize wind
power as “free energy” if its use consumes lots of natural gas as a spinning
reserve?  And  why  would  we  want  to  use  natural  gas  as  a  source  for
hydrogen  when  the  direct  use  of  that  gas  is  so  much  more  efficient  and
safe? If we switch to electric plug-in cars, where will that energy needed to
recharge  the  batteries  come  from?  How  does  biofuel  reduce  fossil-fuel
demands  if  it  requires  nearly  an  equal  amount  to  grow  and  process  the
plants?

It’s  time  to  look  beyond  green  slogans  and  pursue  real  solutions.  The
marketplace, not politics, will ultimately determine which technologies will
succeed. Allowing this to happen may be one of the biggest energy security
challenges of all.

Transformational Trends

 

Can America do better? Yes, we can, and we are doing better. A major
report  released  in  May  2008  by  the  American  Council  for  an  Energy-
Efficient  Economy  (ACEEE)  observes  that  US  energy  consumption,  as
measured  per  dollar  of  GDP,  has  been  slashed  by  half  since  1970.7  The
report, prepared with support from the Civil Society Institute, the Kendall
Foundation, and the North American Insulation Manufacturers Association,
projected  that  use  of  cost-effective  technologies  can  reduce  US  energy
consumption by an additional 25 to 30 percent or more over the next 20 to
25 years.

The  ACEEE  report  cites  strong  evidence  of  progress  in  a  variety  of
energy  conservation  arenas.  Annual  investments  in  energy  efficiency
technologies  currently  support  1.6  million  US  jobs,  and  the  $300  billion
invested in 2004 was three times the amount spent on the traditional energy
infrastructure. The resulting energy saved that year was roughly equivalent
to  operations  of  forty  midsize  coal-fired  or  nuclear  plants.  Since  1970,
energy  efficiency  improvements  have  met  about  three-fourths  of  new
energy-related  service  demands.  The  annual  energy  efficiency  technology
market  is  projected  to  grow  by  more  than  $400  billion  by  2030,  with
investments over that period approaching $700 trillion.8

Can  we  interest  you  in  a  bargain  price  new  or  used  SUV?  Either  one
should be pretty easy to find. Consumers were gradually edging away from
big gas-guzzlers as gasoline prices were rising over the past couple of years,
but when pump pain reached $4 per gallon, many began to run. According
to the Kelley Blue Book, the resale value of a 3-year-old Cadillac Escalade
SUV  plummeted  24  percent  in  2008,  to  $24,500,  about  twice  the  rate  of
decline  two  years  earlier.  During  that  time  a  3-year-old  Honda  Civic  lost
only 8 percent of its value.9

America is becoming energy-conservation conscious because there is no
other choice. Many of the adjustments we can make to dramatically reduce

fuel and power use will be relatively inexpensive and painless relative to
the much-larger burdens of not doing so. We can purchase smaller and more
fuel-efficient cars without any real sacrifice of convenience or mobility. We
can  modify  our  driving  habits  and  opt  for  public  transportation  in  many
instances,  particularly  as  services  improve—and  they  most  certainly  will.
We can downsize our homes, plan new ones that apply natural conservation
principles, and upgrade windows and insulation to control heat transfer. We
can  reduce  energy  consumption  for  pool  heating  through  simple  devices,
and the simplest of all is by not heating them. We can use solar water and
space  heating,  and  geothermal  heating  and  cooling  in  locales  where  that
possibility  is  available.  We  can  install  power-efficient  illumination  and
other  devices,  adjust  thermostats,  and  turn  off  air  conditioners  and  lights
when  we  don’t  need  them.  We  can  recycle  aluminum  and  other  reusable
materials,  and  we  can  consume  a  lot  less  water,  too.  These  things  are
already beginning to happen, but some transitions—building infrastructure
redesigns  and  retrofits,  automobile  replacements,  and  upgraded  public
transportation services, for instance—will take some time, and will never
entirely alleviate sustained energy demands.

None of these adaptations need impose significant hardships or seriously
compromise satisfying lifestyles. And as they increasingly become common
practice, even small changes will make big differences on a national scale.
Imagine the significance of reducing transportation fuel use by 50 percent
and potentially doing the same through residential energy economies. No,
that won’t happen overnight. But the process is already beginning.

Demographic pressures, technology developments and energy challenges
will profoundly transform many aspects of future American life. Failure to
prepare for those changes will have unacceptable consequences. Fuel and
power  shortages  resulting  in  higher  costs  will  accelerate  relocations  of
households and businesses to warmer locales, particularly those with good
access  to  energy  sources.  The  result  will  be  to  leave  poorer  and  older
residents  behind.  These  shortfalls  will  amplify  competition  among
important  user  sectors  and  groups,  including  military,  manufacturing,
agriculture,  and  transportation.  International  relationships  and  commerce
will  be  impacted  in  major  ways,  such  as  shifts  in  production  and  trade,
business travel and tourism, and geopolitical tensions and alignments. The
circumstances we face are daunting.10

The  US  population  more  than  tripled  during  the  20th  century  to  reach
300 million in 2006, up from 200 million 39 years earlier; it is expected to
grow to more than 390 million by 2050. Although this growth rate is nearly
six times lower than that of many less developed countries, consumption of
energy, food, and natural resources continues to expand, and the amount of
land  available  for  agricultural  production  continues  to  decrease.  While
population growth due to birth rates is relatively slow, people are tending to
live longer, adding to costs for medical services that compete for energy and
food  budgets.  About  40  percent  of  our  country's  growth  is  from
immigration—both  legal  and  illegal—contributing  more  than  1  million
persons of all ages annually.

A large number of the baby boomers who are now entering retirement are
downsizing  from  detached  single-family  dwellings  to  much  smaller
apartments,  condominiums,  and  patio  homes.  In  2006,  this  generation  of
more  than  50  million  began  turning  sixty  at  the  rate  of  one  every  eight
seconds. Condominiums are becoming very popular in suburban as well as
metropolitan settings for a variety of reasons, both for empty nesters and for
younger  purchasers.  One  reason  is  the  recent  and  severe  downturn  in  the
economy and job market. Such moves also avoid costs and responsibilities
associated  with  building  and  lawn  maintenance,  and  they  provide  “lock-
and-go”  security  convenience  that  frees  residents  to  travel.  Newer  units,
whether  converted  from  existing  structures  or  built  from  the  ground  up,
tend  to  feature  quality  construction  with  improved  weatherproofing  and
insulation.11

Across  America,  passenger  and  freight  trains  are  gaining  appeal.  June
2008 witnessed 2.5 million Amtrak riders, a record for any month and up 12
percent  from  the  previous  year.12  Throughout  the  US,  the  number  of
commuters who abandoned cars for trains increased by about 15 percent in
some  major  cities  during  2007  as  gasoline  prices  soared  above  $4  per
gallon.

Many  local  transit  systems  are  rushing  to  add  train  cars  and  tracks  to
accommodate  escalating  demands.  A  new  rail  tunnel  is  planned  to  pass
under  the  Hudson  River  from  New  Jersey  to  lower  Manhattan,  and  an
entirely new line in New Jersey will connect Newark to coastal suburbs to
the  south  and  east.  The  Washington,  DC,  Metro  is  considering  extended
service  from  Georgetown  across  the  Potomac  River  to  Rosslyn,  Virginia,
and on to Dulles Airport. Salt Lake City is looking to add 70 miles of new

rail  to  its  commuter  suburbs.  Houston  proposes  to  quadruple  its  existing
new 8-mile rail network over the next few years, and Dallas plans to double
its  35-mile  local  system.  Seattle  proposes  to  add  40  miles  of  new  track
beyond the 16 miles now under construction, and new cars are being added
to Southern California lines, which occasionally operate at standing-room-
only capacities.

Sky-high jet fuel costs will increasingly discourage discretionary airline
travel due to higher ticket prices and service cutbacks. Airlines are already
reducing  numbers  of  flights  and  eliminating  less  profitable  destinations,
leaving  many  cities  without  regular  links.  These  conditions  are  likely  to
influence  more  and  more  Americans  to  vacation  within  the  contiguous
forty-eight states rather than abroad, and they will promote a greater market
for passenger train services in general.

Trains offer many inherent advantages. They are energy efficient when
they attract high ridership rates. And unlike cars, they are unencumbered by
congested freeways, delays, stress, and parking problems. They also allow
passengers  to  avoid  long  walks  and  waits  in  crowded  airline  terminals,
flight delays and cancellations due to weather, and uncertain arrival times.

America must develop a modern, expansive, and seamless passenger rail
infrastructure  with  nationwide  high-speed  services.  Other  countries  are
doing  the  same.  France  is  expanding  its  high-speed  TGV  network,  and
Spain and Italy are creating new ones. Germany plans to sell 24.9 percent of
Deutsche Bahn, a government-owned railway operator, in a public offering,
with  two-thirds  of  the  proceeds  earmarked  for  track  expansion  and  other
upgrades. Russia intends to add more than 5,000 miles of track by 2015,
including a new Trans-Siberian service.

China  has  completed  a  record-breaking  high-altitude  rail  link  between
Qinghai and Tibet, with a section that runs at 16,500 feet above sea level
and thus requires pressurized cars. Beijing plans to invest $160 billion over
the next 3 years to add 10,000 miles of new track to an existing 56,000-mile
network. About half of the expansion will be dedicated to rapid trains that
will operate at speeds of up to 200 miles per hour.

America’s rail freight industry has been doing very well since legislation
was passed nearly 30 years ago allowing companies the freedom to set rates
competitively  and  sell  off  money-losing  lines.  This  legislation  prompted
mergers  that  have  improved  operating  efficiencies  and  implementation  of

such new technologies as computer-controlled loading at freight yards that
have significantly lowered costs.

One of the most important fuel economy breakthroughs and trends may
prove  to  be  outside  the  energy  production,  building  construction,  and
transportation  sectors,  which  get  most  of  the  attention.  The  recent
development and exploding popularity of personal computers and Internet-
based telecommunications can be expected to impact future US commuting
and long-range transportation habits in important ways.

Telecommunications  and  teleconferencing  have  already  begun  to  make
much business travel unnecessary altogether. More and more Americans are
now working from home or other locations of their choice with little or no
need to commute anywhere, saving time, energy, and money in the process.
This advantage also applies to corporations that can utilize teleconferencing
to  dramatically  reduce  their  need  for  national  and  international  business
travel as well as for large, centralized management infrastructures that are
costly to maintain.

Advanced real-time video and data transfer technologies have opened up
remarkable new domestic and global enterprise opportunities for people and
organizations  everywhere.  Included  are  individuals  who  purchase  or  sell
personal  items  on  eBay;  small  manufacturers  and  consultants  that  market
products  and  services;  businesses  that  conduct  meetings  with  overseas
divisions,  affiliates,  and  clients;  and  scientific  and  professional
organizations engaged in workshops and conferences. These activities and
more  can  occur  without  the  need  to  purchase  extra  gasoline  or  airline
tickets, book expensive hotel rooms, pick up meal and entertainment tabs,
or  spend  days  or  longer  periods  away  from  families  and  obligations  that
compete with a work/travel schedule.

Just as necessity is the mother of invention, innovation is a parental agent
of 
intervention  and  adaptation.  Notwithstanding  some  adjustment
challenges,  societal  conservation  responses  born  out  of  necessities  and
technological  progress  can  ultimately  be  assimilated  into  painless  and
satisfying lifestyles. Advancing information and communication networks
may  not  fully  substitute  for  physical  travel  and  face-to-face  interpersonal
contact, but they may free many to live and work where they choose, avoid
fuel-  and  time-costly  transport,  and  wirelessly  connect  us  at  all  times  to
global  contacts  and  markets.  We  may  downsize  our  homes  and  cars  and
realize that we live just as comfortably and arrive just as soon. As we turn

to public transportation, it is likely that the services will improve, becoming
more seamless and convenient.

Embracing Changes and Challenges

 

Two realities are quite clear: (1) that the short era of inexpensive energy
resources  is  nearing  an  end;  and  (2)  that  there  are  no  single  or  simple
solutions.  No  known 
technology  advancement  or  combination  of
advancements  will  satisfy  the  needs  of  uncontrolled  consumption.  The
future we experience and introduce to those who follow will depend instead
upon our human resources of vision, intellect, creativity, and discipline. We
must  apply  all  available  means  to  expand  the  development  and  use  of
renewable  as  well  as  other  resources,  yet  recognize  their  realistic
limitations.  We  must  strive  to  implement  efficient  processes  and  systems
that  minimize,  recycle,  and  reuse  wastes.  We  must  apply  personal  and
corporate lifestyles that do more with less, recognizing that this makes good
economic and moral sense.

Our  human  ability  to  gain  knowledge  about  changes  we  are  imposing
upon our planet provides opportunities to adapt our living habits, industries,
and  technologies  to  prevent  avoidable  surprises  that  lead  to  unfortunate
events.  Earth-sensing  satellite  observations  and  advancements 
in
information  technology  are  yielding  a  better  understanding  of  nature’s
complexities  and  intricacies.  This  better  understanding  provides  us  with
lessons  we  can  apply  to  be  more  positive  contributors.  Humans  are  also
blessed  with  gifts  of  curiosity,  intelligence,  and  compassion,  all  of  which
enable us to recognize our responsibilities and interdependencies within a
larger world community.

There is inescapable evidence that human activities are impacting Earth’s
environment and ecosystems, often not for the better. Air, water, and land
pollution are an expanding global reality. Scientists who study these matters
do us great service in pointing such things out and helping us to do better.
We  are  not  beneficially  served,  however,  by  exaggerated  statements—
purporting to be based upon science—that are calibrated to get maximum
public attention. Alarmism, however well intentioned, is not conducive to
sound judgment and reasoned responses.

Failure  to  rapidly  develop  essential  energy  capacities  will  have
widespread,  destructive  social  and  economic  consequences  that  will  be
particularly  burdensome  upon  the  poorest  among  us.  A  3-decade-long
blockage of US nuclear power development has already caused depletion of
natural  gas  that  could  have  been  conserved  or  applied  for  other,  more
appropriate  fuel  and  feedstock  purposes.  This  has  contributed  to  high
natural gas prices that have forced many US energy- and chemical-intensive
industries overseas, along with the jobs and tax revenues they might have
otherwise provided.

Expansion  of  existing  energy  production  capacity  infrastructures,  and
creation  of  new  ones,  requires  lots  of  time  and  investment  in  a  friendly
legislative  environment.  This  applies  to  nuclear  plant  licensing;  fossil
drilling;  refinery  construction;  clean  coal  and  coal  shale  development;
organic and fossil synthetic liquid fuels; and yes, wind farms. While coal is
our most abundant long-term fossil source, onerous and unwarranted CO
sequestration mandates, in combination with prospective carbon caps, will
continue  to  kill  incentives  to  build  new  coal-fired  plants.  US  coal  use
decreased  in  relation  to  oil  and  gas  over  a  50-year  period  between  about
1910  and  1940,13  and  transitioning  back  to  cleaner  and  liquid  derivative
technologies  may  require  decades  under  the  best  circumstances.  Time  is
very much of the essence.

2

Some groups and individuals advertised as “environmentalists” seem to
want  society  to  return  to  what  they  regard  as  the  simpler,  ecologically
superior lifestyle of the past. This is neither possible nor desirable. Looking
back, earlier tribes may have had lighter ecological footprints only because
there weren’t nearly as many feet then. Their lives were much harder and
shorter  than  ours,  and  they  used  substantially  more  land  per  capita  to
survive and raise larger families.

We can, however, learn much from the past. From a truly “big picture”
time perspective, we can readily observe that Earth’s climate has changed
often  and  dramatically  over  long,  short,  and  irregular  cycles,  with  no
influence from our ancestors. From a human perspective, we can take heart
that our species has adapted to rapid and severe climate shifts on numerous
occasions, and the worst by far were periods of cold. We can relearn ways
that  indigenous  peoples  in  all  climate  zones  have  applied  logical
conservation principles in dwelling construction that make resourceful use
of sunlight and natural ventilation.

There should be no doubt that we humans are highly resilient creatures
with  remarkable  abilities  to  survive  in  difficult  times.  In  2002,  a  report
issued jointly by the Ocean Studies Board, the Polar Research Board, and
the Board on Atmospheric Sciences and Climate of the National Research
Council—titled “Abrupt Climate Change: Inevitable Surprises”—advocated
preparation without panic: “The climate record for the past 100,000 years
clearly  indicates  that  the  climate  system  has  undergone  periodic  and
extreme shifts, sometimes in as little as a decade or less … Societies have
faced  both  gradual  and  abrupt  changes  for  millennia  and  have  learned  to
adapt.”14

The report went on to advise: “It is important not to be fatalistic about the
threats of abrupt climate change … Nevertheless, because climate change
will  likely  continue  in  the  coming  decades,  denying  the  likelihood  or
downplaying the relevance of abrupt changes could be costly.”

How  do  we  prepare  for  rapid  climate  change?  Consider  that  the  most
important impact, whether average temperatures rise or drop, will be upon
energy  demands.  A  warmer  climate  will  increase  crop  yields,  just  as  it
always has, along with power consumption for air-conditioning. A cooler
climate may further accelerate population shifts from US northern states to
the Sun Belt, also increasing air-conditioning demands but increasing fuel
consumption for winter heating in much of the country as well. Those who
pay  attention  will  have  noticed  that  US  temperatures,  which  had  warmed
until the mid-1940s, then cooled through the late 1970s, warmed until the
late  1990s,  and  now  seem  to  be  cooling  again,  potentially  for  decades  to
come.  All  this  is  despite  steady  and  “alarming”  increases  in  human  CO
releases and other activities.

2

Apart  from  climate,  each  of  us  affects  the  course  of  human  events,
adaptation,  and  technological,  social,  and  economic  progress  through  our
choices and actions. Individually and collectively, we change the world for
better  or  worse  in  a  variety  of  important  ways.  We  determine  which
businesses and products will be successful in the marketplace through our
purchasing power. We decide how many resources we will consume, how
much waste will be created, and whether waste will be recycled, based upon
priorities  that  guide  how  we  live.  We  influence  our  children  and  others
around us through our conservation outlooks and the examples we put into
practice.  And  we  determine  whom  we  trust  to  lead  us  and  implement
policies  we  believe  in  through  active  participation  and  informed  votes  in

local, state, and national electoral processes that affect the political climate.
That’s the climate crisis that we urgently need to address.

Preface

At the end of June 2009, I will be leaving the University of Virginia,
as fine a public school as there is in the world. The university cannot
guarantee me both academic freedom and a full salary from the
Commonwealth of Virginia. My faculty position was ‘‘Research Pro-
fessor and State Climatologist, Department of Environmental Sci-
ences.’’ My salary was paid in its large majority by a separate line
in the university’s budget, labeled ‘‘State Climatology Office,’’ itself
a part of the overall budget for the Commonwealth of Virginia.

I was appointed Virginia State Climatologist on July 7, 1980. Like
most other State Climatologists, I was faculty at a major public
institution, and the appointment was without term, although the
faculty position itself was without academic tenure. It was nonethe-
less subject to the same review process (without teaching duties) for
promotion to associate and then to full professor.

I served Republican and Democratic administrations. I met all the
Virginia governors. I really liked Republican Governor George Allen.
I told Governor Jim Gilmore, also a Republican, how fortunate I
was to be able to speak the truth on climate change, even as it was
becoming politically unpopular. I was incredibly impressed by the
professional staff that served Democrat Mark Warner. His staff mem-
bers were as good as or better than many federal staffers I have
worked with.

Given the political nature of climate change, it was only a matter
of time until some governor went after his State Climatologist. I’ll
be happy to say I brought it on myself. I’m articulate, chatty, and,
thanks to the Cato Institute, have great access to TV, radio, and
major news outlets. I fully used my privileges as a University of
Virginia faculty member, which included the right to consult for
whomever I wanted without jeopardizing my position or the aca-
demic freedom that went with it.

Which meant, of course, consulting for entities ranging from the
Environmental Protection Agency to power producers with a dog

ix

CLIMATE OF EXTREMES

in the global warming hunt. One of those was Intermountain Rural
Electric Association, a small Colorado utility. When my work for
them became public knowledge, Virginia Governor Timothy Kaine
told me not to speak as State Climatologist when it came to global
warming. If the State Climatologist is a political appointment, that’s
his call. If it is a lifetime honorific, it’s not. But regardless of which
of those it is, almost all my university salary was contingent upon
my being State Climatologist.

The University of Virginia valiantly, if clumsily, attempted to
paper this over. All of a sudden, I was told I should no longer refer
to myself as Virginia State Climatologist. Instead, I should cite my
seal of certification as Director of the Virginia State Climatology
Office, given by the American Association of State Climatologists
(AASC). The position of State Climatologist had apparently become
a political appointment.

I wasn’t asked to do the impossible, merely the impossibly awk-

ward. The University of Virginia Provost wrote to me:

You should refer to yourself as the ‘‘AASC-designated state
climatologist’’ and your office as the ‘‘AASC-designated State
Climatology Office,’’ or if you prefer, ‘‘AASC-designated
State Climatology Office at the University of Virginia.’’ I
recognize that the titles may be awkward but the message
from the Governor’s Office was very clear about what
they expected.

Needless to say, this quickly became unworkable. Newspaper
editors wouldn’t suffer such encumbering verbiage, it didn’t fit on
a TV Chiron, and making a disclaimer every time I spoke about
climate that my views didn’t reflect those of the Commonwealth of
Virginia or the University of Virginia (despite their being correct!)
would never fit in a sound bite. So I had the choice of speaking on
global warming and having my salary line terminated, or leaving.
Other State Climatologists soon had similar difficulties. George
Taylor at Oregon State University, who is very popular with the
AASC (and the only person ever elected to consecutive terms as
president), was told that he was simply not to speak on global
warming. Having read the playbook established by Governor Kaine
in Virginia, Governor Ted Kulongoski (D) told Portland’s KGW-TV
that ‘‘Taylor’s contradictions interfere with the state’s stated goals
to reduce greenhouse gases.’’

x

Preface

Taylor had long questioned glib statements about a 50 percent
decline in Pacific Northwest snowpack, which were being made by
climate alarmists worldwide. The 50 percent figure is only part of
the story. That figure accrues if one starts the data in 1950 and
ends in the mid-1990s. If one uses the entire set of snowpack data
(1915–2004), a different picture emerges (Figure P. 1, bottom). Taylor
was told to shut up as State Climatologist even though he was
merely telling the truth.

Taylor resigned his Oregon State University position in Febru-

ary 2008.

David Legates, at the University of Delaware, was told by Gover-
nor Ruth Ann Minner (D) that he could no longer speak on global
warming as State Climatologist. His faculty position is a regular
tenured line in the geography department. He’s free, as State Clima-
tologist, to say anything about the weather, so long as there’s no
political implication. Unfortunately, as most State Climatologists
will attest, most reporters specifically ask whether this or that
unusual storm or unusually hot (or cold!) day is related to global
warming. Scientists who refuse to answer that question don’t get
return calls.

Minner was upset because Legates was an author of an amicus
brief to the U.S. Supreme Court (Baliunas et al.) in its first global
warming–related case, Massachusetts v. U.S. Environmental Protection
Agency. Baliunas et al. sided with the federal government (namely
the Environmental Protection Agency [EPA]), which maintained that
it was not required to issue regulations reducing carbon dioxide
emissions. Justice Antonin Scalia cited Baliunas et al. in his dissent,
as the court voted 5-4 that it was within the EPA’s purview to
propose and then enforce carbon dioxide limitations.

So Legates stopped speaking about global warming as Delaware’s

State Climatologist.

Out West, things got even uglier. The Assistant State Climatologist
for Washington, Mark Albright, was fired because, despite his boss’s
orders, he refused to stop e-mailing—to journalists, to inquiring
citizens, to anyone—the entire snowfall record for the Cascade Moun-
tains rather than the cherry-picked one. For e-mailing that record,
the assistant state climatologist in Washington lost his job.

What had started with Oregon’s George Taylor had migrated

across the Columbia River.

xi

CLIMATE OF EXTREMES

SNOWPACK IN MAXIMUM WINTER MONTH, EXPRESSED AS

DEPARTURE FROM THE 1971–2000 AVERAGE: FOR 1950–2004 (TOP)

Figure P. 1

AND FOR 1915–2004 (BOTTOM)

SOURCE: Jones 2007.
NOTE: Using all the data back to 1915 clearly shows that the current era is
hardly unusual, despite one very low reading in 2004.

State Climatologist Phil Mote terminated Albright. Both positions
were in the University of Washington’s atmospheric science depart-
ment, one of the world’s best. A senior member of that department,

xii

Preface

Professor Clifford Mass, commented, ‘‘In all my years of doing sci-
ence, I’ve never seen this sort of gag-order approach to doing
science.’’

What is so scary that some governors don’t want you to know it?
Apparently it is this: The world is not coming to an end because
of global warming. Further, we don’t really have the means to signifi-
cantly alter the temperature trajectory of the planet. All of this will
be spelled out in considerable detail within the rest of this book.
Governors Kaine, Kulongoski, and Minner, this book’s for you!
We would like to acknowledge the considerable effort put into
the research for this manuscript by Chip Knappenberger and Robert
E. Davis. Peter VanDoren, David Boaz, Sonja Boehmer-Christiansen
provided invaluable revi w comments. Rola Brentlin and Jonathan
Eidsness also provided insightful reviews. Amy Lemley cheerfully
did some extensive copyediting, making a boring global warming
story into something readable and, maybe, enjoyable. Thanks to all
of you for all your help.

e

—Patrick J. Michaels

Washington, DC, September 2008

xiii

Foreword: A Climate of Extremes

Something about the global warming debate has changed, and
changed for the worse. The debate itself has become a climate of
extremes. Truth and fact no longer matter, outrageous exaggerations
go unchallenged, unscientific speculation is unquestionably
accepted, and nonbelievers lose their jobs.

Consider this interview with former Vice President Al Gore, on

Larry King Live, May 22, 2007:

UNIDENTIFIED WOMAN: Vice President Al Gore, what
issues caused by climate change globally are likely to affect
the United States security in the next 10 years?

GORE: You know, even a one-meter increase, even a three-
foot increase in sea level would cause tens of millions of
climate refugees.

If Greenland were to break up and slip into the sea or West
Antarctica, or half of either and half of both, it would be a
20-feet increase, and that would lead to more than 450 million
climate refugees.

The direct impacts on the U.S. have already begun. Today,
49 percent of America is in conditions of drought or near-
drought. And we have had droughts in the past, but the
odds of serious droughts increase when the average tempera-
tures go up, as they have been going up.

We have fires in California, in Florida, in other states, unprec-
edented fire season last year, directly correlated with higher
temperatures, which dry out the soils, dry out the vegetation.

We have a very serious threat of losing enough soil moisture
in a hotter world that agriculture here in the United States
would be greatly affected. . . .

The fact is that there is not one shred of evidence in the scientific
literature, or in climate history, indicating that sea level could possi-
bly rise more than three feet (‘‘one meter’’) by 2017. The best estimate

1

CLIMATE OF EXTREMES

published by the United Nations Intergovernmental Panel on Cli-
mate Change (IPCC) in 2007, for the next 10 years, ranges between
0.8 and 1.7 inches.

The difference between Gore’s conjecture about Greenland ‘‘in
the next 10 years’’ and reality is stark. New satellites have found
that Greenland is losing ice at a rate of 25 cubic miles per year. This
information was published in Science in November 2006 by NASA
scientist Scott Luthcke and many coauthors. The world’s largest
island has a total of 685,000 cubic miles of ice on it, meaning that
the loss rate was measured at 0.4 percent per century. Gore had to
know that. Any reference to Greenland’s breaking up and slipping
into the sea in 10 years is wild fantasy.

Despite this tiny increment of ice loss, these data, from a gravity-
measuring satellite called GRACE, were greeted with some interest.
It had long been thought that Greenland’s ice was pretty much in
balance, with the amount of ice accumulating in the center of its
huge cap roughly equaling the amount being shed into the ocean.
GRACE had indeed picked up an acceleration in the oceanic
discharge.

But ice, like science, is pretty dynamic. A succeeding paper in
Science, published by Ian Howat in early 2007, showed that the
acceleration of ice loss detected by the satellite had reversed back
to the presatellite rate, at least in the two major ice streams that
Howat examined. Gore had to know that, too.

The IPCC’S 2007 ‘‘Fourth Assessment Report’’ on climate change
includes a computer model projection for the loss of Greenland ice.
It takes nearly 1,000 years to lose half its total. But the IPCC model
assumes that the concentration of carbon dioxide in the atmosphere
quadruples from its preindustrial background and then stays there
for the entire time.

Right now, we’re at 138 percent of the background, with an atmo-
spheric concentration of 385 parts per million (ppm). Before the
Industrial Revolution and for much of the period after the continents
lost their massive Ice Age glaciers, the concentration hung around
280 ppm. It’s highly debatable whether we could get to four times
the background, or 1,120 ppm, even if we deliberately tried to do
so. To maintain such a level for the next millennium assumes that
we will still be burning fossil fuel—and at more than three times
the current rate—in the year 3000. Even the Roman Curia wouldn’t,

2

Foreword: A Climate of Extremes

in 1000 AD, have had the audacity to project the future state of the
world for the next thousand years. Yet the United Nations (UN)
blithely looks 1,000 years forward, making completely unfounded
assumptions on energy use and human society. Many ‘‘thousand-
year’’ political statements have been known to flop within a century,
if not a decade.

Gore’s statement about drought is wrong. He has to know that
we have very good records about the area of the country under
drought, back to 1895, reproduced here as Figure F. 1 (top). Figure
F. 1 (bottom), on the same time scale, is the Northern Hemisphere’s
surface temperature history, from the IPCC. It’s a waste of computing
time to examine the correlation between the two in recent decades,
because there isn’t any.

Gore surely knew that, as the globe’s temperature has risen since
1975, yields of almost all U.S. crops have increased significantly,
and that they increased at a similar rate during the slight cooling
of the Northern Hemisphere that took place from 1945 through 1975
(when some people worried about global cooling and a coming
ice age). The American farmer is an adaptable creature, changing
agricultural practices, and even crops themselves, faster than climate
can change, and growing mainstay staples, such as corn, under a
tremendous variety of climatic conditions. Many of our fresh vegeta-
bles come from a natural desert called California.

Ten years ago, Gore would have been called out for his remarks
on Larry King Live as surely as he was for exaggerations about the
Internet or embellishments about his college love life. But no more.
For many, the truth no longer matters when it comes to climate
change. Science fiction movies such as The Day After Tomorrow or
Gore’s own An Inconvenient Truth cause the horde to clamor for
action on global warming.

How did we get to such an extreme world?
We have written several books on this subject, contrasting facts,
perceptions, and reality about global warming. Most recently, in late
2004, the Cato Institute published Michaels’ Meltdown: The Predictable
Distortion of Global Warming by Scientists, Politicians, and the Media.
Since that writing, the political and physical climates have changed,
as evinced by the preceding vignette. Michaels initially set out to
simply revise Meltdown, but soon realized that so much new informa-
tion has surfaced, and so many scientific changes have occurred, in

3

CLIMATE OF EXTREMES

PERCENTAGE OF THE LOWER 48 STATES EXPERIENCING SEVERE

(OR WORSE) DROUGHT (TOP) AND NORTHERN HEMISPHERE

Figure F. 1

TEMPERATURES FROM THE IPCC (BOTTOM),

JANUARY, 1900–AUGUST, 2008

SOURCE: National Climatic Data Center 2008 (top), http://www7.ncdc.
noaa.gov/CDO/CDODivisionalSelect.jsp; Intergovernmental Panel on Cli-
mate Change 2007 (bottom) and updates.

4

Foreword: A Climate of Extremes

a mere four years, that an entirely new book was required, one that
would quantitatively analyze the scientific literature.

The rhetoric has changed. Discourse has degenerated into dema-

goguery. Threatening demagoguery.

Why has it become so politically risky to not view global warming

as an unmitigated disaster?

In its larger incarnation, the political process is merely an instru-
ment that adapts to public perception. Elected officials who do not
echo popular perceptions risk losing the next election. Those who
hold unpopular views on political matters, elected or not, no matter
what the issue, will be ostracized because (1) they are an embarrass-
ment to the process, and (2) they could conceivably change percep-
tion, forcing political flip-flops.

As a result, almost all that the public hears or reads about global
warming is bad news. For the last two years, it seems that the
Drudge Report has featured a global warming item almost every
day. Hurricanes are stronger and more frequent. Greenland is shedding
ice at an alarming rate. So is Antarctica. Droughts and floods are increasing.
If we don’t do something drastic about our changing climate, it will soon
be too late. In 2007, actor Leonardo DiCaprio announced that global
warming could ultimately cause the extinction of Homo sapiens.

Indeed, the political process has already responded. A 2005 energy
bill, passed by a Republican congress and signed by a Republican
president, mandated the production of massive amounts of ethanol
from corn. It is easy to demonstrate, as did Tiffany Groode and John
Haywood from MIT, that ethanol is a loser. It’s used as a substitute
for gasoline. But it turns out that the overall production cycle results
in more carbon dioxide emissions than if one simply burned gasoline.
Yet the ethanol mandate was sold as, among other things, a cure
for global warming. That political process now has a clear mindset,
and, needless to say, a lot of people are upset if they hear that ethanol
won’t solve anything and will actually contribute to global warming.
Then there is the charge that skeptical global warming scientists
are ‘‘deniers’’ (named for Holocaust deniers), a peculiarly vicious
label originally given to those who claim there’s simply no such
thing as human-induced global warming. We don’t believe these
people are correct, but we also haven’t found one Nazi among them.
They have their scientific reasons, although their argument is quite a
stretch, given the nature of climate change in the last several decades.

5

CLIMATE OF EXTREMES

How Perception of Extremes Evolves

How did the perceived climate of extremes develop? Is it
because of a need, on the part of some scientists, to hype the
lurid aspects of climate change at the expense of the more
mundane? Do the publicity arms of universities or federal
agencies, usually not stocked with scientists, get carried away
with their rhetoric and emphasize extreme results?

Climate models, or compendia of models, usually give a
range of expected temperature changes for doubling atmo-
spheric carbon dioxide. But far too often, only the most extreme
result enters the public discourse. Here’s an example, courtesy
of BBC Radio’s Simon Cox and Richard Vadon.

It was January 2005, and Oxford University’s David Stain-
forth and a large number of colleagues had just published a
paper in Nature, which described a huge number of computer-
ized simulations of global warming. Stainforth and colleagues
created a virtual community of thousands of computer model
users called climateprediction.net. They put out a a press release
that only mentioned the most extreme value. Most of them
predicted about 3°C (5.4°F) of global warming for doubling
atmospheric carbon dioxide, but there were a very few outliers
extending up to 11°C (19.8°F).

Climateprediction.net produced a press release about its work
on January 26, 2005. There is only one sentence referring to
future temperature: ‘‘The first results from climateprediction.net,
a global experiment using computing time donated by the
general public, show that average temperatures could eventu-
ally rise by up to 11°C.’’

‘‘Up to 11°C.’’ When the press dutifully reported this figure
(and no other one, which was understandable, given that there
was no other number in the press release), Myles Allen, an author
of the paper and principal investigator for climateprediction.net,
then blamed the press! ‘‘If journalists decide to embroider
on a press release without referring to the paper which

(continued on next page)

6

Foreword: A Climate of Extremes

(continued)

the press release is about, then that’s really the journalists’
problem. We can’t as scientists guard against that.’’ In fact,
journalists did not embroider climateprediction.net’s press
release. They merely quoted it.

Cox and Vadon then presented several unnamed climate
scientists along with the press release and the original paper.
According to the BBC, ‘‘All were critical of the prominence
given to the prediction that the world could heat up by 11°C.’’
One (unnamed) scientist told the BBC: ‘‘I agree the 11°C
figure was unreasonably hyped. It’s a difficult line for all scien-
tists to tread, as we need something ‘exciting’ to have any
chance of publishing . . . to justify our funding.’’

That doesn’t happen every time, and plenty of scientists will
be entirely straight when communicating to the public about
the range of their climate results and their confidence in them.
But in this case, climateprediction.net’s press release did the
world a disservice by using only one high figure. Ask yourself
this: Which press release will get more attention, one saying
that ‘‘most computer simulations of global warming predict a
total warming of about 3°C (5.4°F),’’ or one that says, ‘‘The
earth could warm by as much as 11°C (19.8°F)’’?

Readers or viewers of news stories on climate change should
beware every time they hear the phrase ‘‘as much as.’’ There’s
obviously a range underneath the word ‘‘as,’’ and, for some
reason, the scientist, publicist, or reporter does not want you
to know what it is.

Recently the definition has been expanded. The charge of ‘‘denier’’
is also thrown at those who argue that human-induced climate
change is indeed real, but that this will not necessarily lead to an
environmental apocalypse. And that’s our stand. The data lead us
to conclude that anthropogenic global warming (AGW) is indeed
real, but relatively modest. We’re not arguing against AGW, but
rather against DAGW (dangerous anthropogenic global warming).
As Steven Hayward and Ken Green of the American Enterprise
Institute have written, ‘‘Anyone who does not sign up 100 percent

7

CLIMATE OF EXTREMES

behind the catastrophic scenario is deemed a ‘climate change
denier.’’’ Boston Globe columnist Ellen Goodman wrote, in November
2006, ‘‘Let’s just say that global warming deniers are now on a par
with Holocaust deniers.’’

The existence of a mere hurricane now cries for a lynching. In
December 2006, London Guardian columnist George Monbiot offered
the view that ‘‘every time someone dies as a result of floods in
Bangladesh, an airline executive should be dragged out of his office
and drowned.’’

Even those who claim that there is little, if any, human influence
on climate do not in fact deny the existence of climate change itself.
The evidence for warmer recent times is incontrovertible. In the mid-
19th century, glaciers threatened villagers in the Alps. Not 125 years
later, the ice had retreated so far up the mountain that Julie Andrews,
in The Sound of Music, crossed the Alps in dress shoes.

Perhaps all debate on climate change is irrelevant. After all, the
standard argument from the political class in Washington is that
‘‘The science is settled,’’ and that it’s time to move on to policy.
President Bush sees ethanol as a panacea for global warming, depen-
dence on foreign oil, and international tension. So does Barack
Obama. John McCain was the original author of S. 2191 (The name
of John Warner [R-VA] was substituted when McCain became a
viable Presidential candidate), a bill that mandated a reduction in
carbon dioxide emissions to 70 percent below current levels by 2050.
Sen. Barbara Boxer (D-CA), chair of the powerful Senate Environ-
ment and Public Works Committee, believes we can reduce emis-
sions of carbon dioxide by 90 percent by 2050—only 42 years from
now—if we simply pass a law saying we will do so. How we can
accomplish this goal does not appear in any legislation or documen-
tation, because no one in fact knows how to achieve such reductions.
How did we get to a world of apocalyptics and deniers, a world
that is also one of impossible or ineffective policies on climate
change? In other words, how did we get to such a climate of
extremes?

The answer, it turns out, is purely logical. We bought it with our
tax dollars, and we will pay the consequences for decades. There
was no conspiracy (or at least no effective one). Rather, our extreme
world of today is the result of ‘‘science as usual,’’ hyped up on the
steroids of massive public funding.

8

Foreword: A Climate of Extremes

This book’s first chapter describes the science of global warming,
something that many readers already know by heart. If you do (and
you’d rather not reminisce), skip to chapter 2, which describes our
temperature histories and how they have changed over time. We
detail six revisions to global records, each of which produces more
global warming from the same original data. Having the revisions
all in one direction, from three independent methods of temperature
monitoring, is like tossing six heads or tails in a row. The odds are
a little less than 1 in 50. So the continual upward-revising of warming
trends in the same data possibly reflects something real that in reality
is improbable.

Hurricanes are the subject of chapter 3, where we track the conten-
tious controversy about whether or not they are made worse or more
frequent because of global warming. Thanks to massive Hurricane
Katrina’s pillage of the Mississippi and Alabama gulf coasts (and
her destruction of a criminally weak levy system in New Orleans),
everyone seems to know that global warming was the cause, rather
than merely being a passive bystander to the ruin of a city built
several feet below sea level, literally sitting in wait of its destruction.
Chapter 4 deals with sea-level rise and melting ice, with particular
emphasis on the disaster du jour, which is that Greenland is going
to suddenly lose almost all its ice, perhaps before 2100, resulting in
more than 20 feet of sea-level rise. It turns out there is hardly any
data in support of this hypothesis, and an army of facts arrayed
against it. But it is the specter of a Greenland disaster that is behind
most of the current calls for dramatic cuts in carbon dioxide emis-
sions. Antarctica also displays some screwy behavior that seems
inconsistent and certainly confounds the myriad climate models that
predict it should be warming smartly and experiencing increasingly
heavy blizzards.

Forest fires, floods, and the various and sundry disasters associ-
ated with storms other than hurricanes are the subject of chapter 5.
Here we detail some real whoppers laid on the public by elected
officials you would think might know better. In the world of global
warming, fact-checking has become fantasy, and perceptions have
become the opposite of reality.

Chapter 6’s title, ‘‘Climate of Death and the Death of Our Climate,’’
refers to the phenomenon of warming-related deaths, such as the
massive human die-off in France in the summer of 2003. It turns

9

CLIMATE OF EXTREMES

out that the weather anomaly that caused it was a tiny bubble of
hot air embedded in a relatively cool summer around the planet.
We describe the phenomenon of adaptation—something obvious to
every economist but virtually ignored by every climatologist—in
which succeeding heat waves kill fewer and fewer people, providing
evidence that the response to changing climate is both political and
technological.

In chapter 7, we describe ‘‘publication bias,’’ which is an attempt
to answer the question, ‘‘Why is all the news we read about global
warming bad?’’ There is a voluminous literature on natural biases
in the scientific literature, and there are multiple causes. Curiously,
climatologists claim to be immune from bias in their literature. But
in making that claim, they are saying that they don’t do something
that virtually everyone else does, which is to publish more ‘‘positive’’
results (in this case, those fingering global warming for something)
than ‘‘negative’’ ones (in which no relationship with global warming
is discovered). There are several incentives for doing so, including
the simple desire for (and professional requirement of) publication.
Chapter 8 proposes a modest solution to provide some balance
between the mountain of bad news about climate change and the
molehill of good news. We’ll bet, for example, that if peer reviewers
could no longer hide behind a cloak of anonymity, then a lot of
biased shenanigans would stop. Finally, like any authors, we sum up
our book with some stirring prose and bid you a good night’s sleep.

10

1. A Global Warming Science Primer

Earth’s mean surface temperature is doubtlessly warmer than it

was 100 years ago. Get over it.

What matters is (1) how much it has warmed, (2) how much of that
warming is caused by human activity, and (3) how the relationship
between that activity and present temperatures can be translated
into a reliable estimate of future warming and its effects.

The temperature changes. But so does the way in which tempera-
ture data are processed. We will demonstrate that fact in chapter 2.
For now, however, we’ll rely on existing histories.

Let’s start out with a standard reference temperature history, the
ground-based record from the United Nations’ Intergovernmental
Panel on Climate Change (IPCC) (Figure 1.1).

GLOBAL TEMPERATURE DEPARTURE FROM THE 1961–90 AVERAGE,

Figure 1.1

1900–2007

SOURCE: IPCC 2007.

11

CLIMATE OF EXTREMES

The IPCC history shows two distinct periods of warming, one
roughly from 1910 through 1945, and then another that begins rather
abruptly in about 1975. Their warming rates are statistically indistin-
guishable. In the last three decades ending in 2005, the warming
rate was 0.178°C Ⳳ 0.021°C per decade (0.320°F Ⳳ 0.038°F). In the
period 1916–45, the rate was 0.151°C Ⳳ 0.014°C per decade (0.272°F
Ⳳ 0.025°F). Each of these is the observed trend plus or minus the
statistical margin of error associated with it.

If those figures were the results of a political poll, the pundits
would call it a tie—within the poll’s range of error. Similarly, with
temperature trends, adding in the ‘‘plus’’ to the first warming and
subtracting the ‘‘minus’’ in the second reveals that the rate of warm-
ing in recent decades cannot be discriminated from the warming that
occurred during a period of similar length in the early 20th century.
Their causes are very likely quite different, however. That said,
one thing is for sure: the first warming was associated with a far
smaller change in atmospheric carbon dioxide levels than the recent
one. After all, we had not added very much carbon dioxide to the
atmosphere before World War II.

Modeled vs. Observed Warming

Are recently observed climate changes consistent with computer
models of climate change? That depends on where you look. If you
examine surface temperatures observed at weather stations or as
estimated from satellites, you’ll conclude that the models can pro-
vide some quantitative guidance for the future. That doesn’t mean
that the models have all the answers, but it does suggest that they
are largely sufficient.

There’s another view—namely, that the models may have accu-
rately captured much of the surface temperature change, but that
they have missed the vertical dimension. If that’s the case, then the
match with surface temperatures is fortuitous—or worse.

Let’s start with the first notion: that the models have something

useful to tell us about future warming.

It’s quite easy to demonstrate that the natures of the two periods
of warming are quite different—and that the first one was probably
caused by changes in the sun, whereas the second one has more of
a relationship to human-caused emission of carbon dioxide and
other greenhouse gases. We say ‘‘more of’’ because there are still

12

A Global Warming Science Primer

other factors involved, such as a smaller solar effect and changes in
land use, such as turning a ‘‘naturally’’ vegetated surface into a
farmed one.

Greenhouse-effect warming occurs because certain constituents
of our atmosphere, mainly carbon dioxide and water, are molecules
whose shape allows them to absorb, and then release, radiation
emanating from the earth’s surface.

Bodies give off radiation that is proportional to their temperature.
The hotter a body is, the more energetic the energy emitted. The
sun, at 6,000°C (10,800°F), emits largely in the visible wavelengths
of the universal electromagnetic spectrum (which is why our eyes
evolved to ‘‘see’’ sunlight), as well as in the ultraviolet range (the
energetic wavelengths that cause sunburn). The much cooler earth
(with an average surface temperature of 15°C [27°F]) radiates largely
in the less energetic infrared wavelengths (no one gets ‘‘earthburn’’).
Carbon dioxide and water vapor resonate with this low-frequency
radiation and absorb some of it. The molecule reaches an unstable,
physically ‘‘excited’’ state and then releases the packet of energy
either up and out to space, or back down toward the surface. Conse-
quently, greenhouse gases ‘‘recycle’’ the warming radiation of the
earth in the lower atmosphere, resulting in a warmer surface and
lower atmosphere than there would be in their absence. Another
consequence is that the layer above most of the carbon dioxide—
the stratosphere—cools because more radiation has been ‘‘trapped’’
below.

The mathematical relationship between the concentration of a
greenhouse gas and surface temperature rise has been known for
more than a century. The function is logarithmic, which means that
the first increments of a greenhouse gas produce the greatest warm-
ing, and then increasingly large allotments are required to maintain
that rate of warming. You can plot this function on your old graphing
calculator or look it up on myriad websites.

Water vapor and carbon dioxide are known to behave quite simi-
larly with regard to potential warming, so they can be (partly) con-
sidered to behave as the same greenhouse gas. As a result, atmo-
spheres that are poor in both carbon dioxide and water vapor will
respond strongly to the first new increments of either, because of
the logarithmic nature of the temperature change. Again, increas-
ingly large amounts of greenhouse gas would be necessary to main-
tain the same rate of warming.

13

CLIMATE OF EXTREMES

Plenty of places on earth met this qualification before we put a
lot of carbon dioxide in the air. Siberia and northwestern North
America in winter are virtually devoid of water vapor: indeed, cold
air can hold hardly any before it dumps it onto the ground in the
form of frost or snow. It turns out that these are the places that
have seen the biggest warming in recent decades. (Note: Antarctica,
however, is not warming—a special case described in chapter 4).
Further the warming rate in (dry) winter is much greater than it is
in (moist) summer, consistent with greenhouse-effect theory.

Carbon dioxide concentrations in our atmosphere were approxi-
mately 280 parts per million (ppm) from the end of the last Ice Age
to the beginning of the Industrial Revolution. Since then, they have
risen to around 385 ppm, or a net increase of about 38 percent. In
the 20th century, roughly three-fourths of the increase in atmospheric
concentration took place after World War II.

There are several other emissions that alter the transmission of
radiation through the atmosphere. On a molecule-for-molecule basis,
methane, which is in much lower concentration, is 23 times more
efficient at warming the lower atmosphere than is carbon dioxide.
Its concentration has increased from about 875 parts per billion (ppb)
around 1900, rising linearly to around 1,750 ppb by the 1980s. The
increase was thought to have resulted from cow flatulence, coal
mining, and leaky gas pipes (mainly in the former Soviet Union).
Even so, none of these could possibly explain what happened
after the late 1980s (see ‘‘Methane and the Perils of Scientific
‘Consensus’’’).

Other industrial emissions are thought to counter the warming
effect of greenhouse gases. A major cooler is something called sulfate
aerosol—a particulate effluent emitted largely from coal-burning
power plants. The relative cooling effect of sulfate is only ‘‘known’’
to a very broad range, from no cooling to nearly 2°C (3.6°F), which
is very convenient, because it allows modelers to ‘‘choose’’ a value
that, when added to the warming effects from carbon dioxide, meth-
ane, and a few other minor actors, forces a climate model’s historical
output to match the observed record shown in Figure 1.1.

At any rate, carbon dioxide still remains the biggest contributor
to warming. A common counterargument is that most of the recent
warming is a result of changes in the sun. But ‘‘solar’’ warmings
should be a lot different from ‘‘greenhouse’’ ones. Rather than being

14

A Global Warming Science Primer

Methane and the Perils of Scientific ‘‘Consensus’’

The increase in methane was remarkably constant from the
early 20th century through the late 1980s. Every global climate
projection assumed a similar increase would continue at least
for another half-century.

No one disagreed. But, as if nature wanted to humble climate
scientists, the rate of increase began to decline about 20 years
ago, and the concentration of methane in the atmosphere has
actually dropped in recent years (Figure 1.2). Nonetheless,
IPCC’s projections continue to show an increase (Figure 1.3).

ATMOSPHERIC METHANE CONCENTRATION, 1983–2006

(TOP), AND CHANGE IN METHANE FROM

Figure 1.2

YEAR TO YEAR (BOTTOM)

SOURCE: Adapted from IPCC 2007.

(continued on next page)

15

CLIMATE OF EXTREMES

(continued)

THE IPCC’S MIDRANGE METHANE SCENARIO THROUGH

Figure 1.3

2100

SOURCE: IPCC 2007.

concentrated only in lower atmosphere, solar warming should be
distributed in a way that is more uniform, heating both the lower
atmosphere and the stratosphere, in which cooling has been
observed in recent decades (see below for other complications!). Nor
would a solar warming preferentially warm the winters so much as
a greenhouse warming would.

An innovative analysis of U.S. temperatures illustrates the differ-

ence between the solar and carbon dioxide–induced warming.

As in the global temperature record (Figure 1.1), there are three
distinct modes of behavior in the U.S. temperature history: a period
of early-century warming, a midcentury cooling, and a final warm-
ing beginning in the 1970s.

The 365 black bars in each plot in Figure 1.4 are the rates of
temperature change on the coldest night of the year (day 1) to the
warmest (day 365). Note that these plots are not showing January 1

16

A Global Warming Science Primer

TREND IN NIGHTTIME LOW TEMPERATURES, 1910–39 (TOP)

AND 1970–97 (BOTTOM)

Figure 1.4

SOURCE: Knappenberger, Michaels, and Davis 2001.
NOTE: Day 1 is the coldest night, day 2 is the second-coldest, etc.

on the left through December 31 on the right; rather, they are arrang-
ing the data from the coldest day in each year to the warmest one.
So the left side of each graph shows the trend in the coldest nights
of the year, and the right side shows the trend in the hottest nights.
The top of Figure 1.4 is during the warming of the early 20th
century (1910–39) and shows very little change in the trend of tem-
peratures from the coldest (left) to the warmest nights (right). The
bottom shows for the second warming, 1970 through 1997 (the last

17

CLIMATE OF EXTREMES

Temperature Variability and Global Warming:

Another Look

If the coldest nights are warming preferentially, then day-
to-day variation in temperature must be dropping. Martin
Beniston and Ste´phane Goyette, of Switzerland’s University of
Geneva, recently published an investigation of the phenome-
non of decreasing variability with greenhouse warming. They
begin their article by noting, ‘‘It has been assumed in numerous
investigations related to climatic change that a warmer climate
may also be a more variable climate; such statements are often
supported by climate model results.’’

They looked at low- and high-elevation temperature records
for Switzerland and found the same thing we did for the United
States—that the variability of temperature is decreasing, and
that the decrease is concurrent with the increase in anthropo-
genic greenhouse gases.

They concluded:

This investigation, carried out for a low (Basel) and a
high (Saentis) elevation site in Switzerland, has shown
that contrary to what is commonly hypothesized,
climate variability does not necessarily increase as cli-
mate warms. Indeed, it has been shown that the vari-
ance of temperature has actually decreased in Switzer-
land since the 1960s and 1970s at a time when mean
temperatures have risen considerably. Nevertheless,
these findings are consistent with the temperature
analysis carried out by Michaels et al. (1998), whose
results also do not support the hypothesis that temper-
atures have become more variable as global tempera-
tures have increased during the 20th century.

year in this particular study). Note how the coldest nights are warm-
ing up, much more than any others. This is the way greenhouse
warming is supposed to work—and indeed is what has happened.
In the recent era, cold nights are warming much more so than hot
ones. In other words, temperatures are becoming less variable. (A
global examination of this phenomenon was published in our 2000
book, The Satanic Gases.)

18

A Global Warming Science Primer

We are not saying that the sun has had no influence on recent
temperatures, but rather that the solar influence was clearly much
greater during the warming of the early 20th century.

Nicola Scafetta and Bruce West, from Duke University, published
an interesting paper along these lines in Geophysical Research Letters
in 2006. Like many skeptical scientists, they prefer observed relation-
ships to theoretical models. Scafetta and West examined the relation-
ship between cycles in solar variations and cycles in temperatures
using data back to the 17th century. Bottom line: ‘‘We estimate that
the sun contributed as much as 45 percent to 50 percent of the
1900–2000 global warming and 25 percent to 30 percent of the
1980–2000 global warming.’’

Do the math. If 25 percent of recent warming is caused by the
sun, and 50 percent of total warming since 1900 has the same cause,
then 75 percent of the warming of the early 20th century should
have had a solar origin. In 2007, using a different solar history and
long-term temperature history, Scafetta and West duplicated their
2006 findings.

In sum, you can’t throw the sun out completely when dealing
with the recent warming, but it is not a majority contributor. That
said, the bigger the solar impact, the smaller the human effect. The
more ‘‘something else’’ is causing warming, the less sensitive the
climate is to greenhouse emissions.

At any rate, the assumption that the majority of recent warming
is from greenhouse changes remains the grounding rock of the notion
that the models are providing some useful guidance with regard to
21st-century temperatures.

Because greenhouse gases tend to trap radiation close to the sur-
face, there’s less of a flux through the stratosphere, the layer of the
atmosphere that begins about seven miles in altitude in our latitude.
The stratosphere should cool slightly at the same time the surface
warms. But if the sun gets warmer, so should the stratosphere. In
fact, however, there is no record of stratospheric temperature that
shows significant recent warming.

Both satellite and weather balloon data show stratospheric cooling,
but carbon dioxide is only one cause. Changes in stratospheric com-
position owing to a slight loss of ozone have also contributed to
cooling.

The ozone loss is hypothesized to have been caused by the break-
down of chlorofluorocarbon (CFC) refrigerants. The ban on these,

19

CLIMATE OF EXTREMES

a UN treaty known as the Montreal Protocol, is often cited as an
example of successful global environmental regulation. If we man-
aged to regulate CFCs, the reasoning goes, we can do the same for
carbon dioxide. In reality the two are hardly analogous. CFCs are
one of any number of chemicals that can be used for cooling, so
substitutes exist; carbon dioxide, however, is the respiration of our
fossil fuel–powered civilization. There is no politically and economi-
cally acceptable substitute currently available.

Nature of Observed and Future Warming

It is quite obvious from Figure 1.1 (and Figure 1.8, later in this
chapter) that the rate of planetary warming since the mid-1970s has
been quite constant (despite a lack of warming since 1998—the
warmest year in the record). Computer models also tend to predict
a constant rate of warming.

Figure 1.5 (see insert) is taken from the most recent IPCC report,
published in 2007. It is the various warming projections from differ-
ent computer models for the ‘‘midrange’’ scenario for future carbon
dioxide emissions.

The IPCC’s midrange scenario assumes that a ‘‘balance’’ of fossil
and nonfossil sources of power evolves over the century, unlike its
other scenarios, which are almost exclusively fossil-powered, or else
presume the use of very little carbon-based energy by the end of
the century.

Note that the projected rate of temperature change tends to remain
the same once it is established (Figure 1.5; see insert); what the
various computer models do is simply project different rates of con-
stant change.

Figure 1.5 (see insert) also includes observed temperature changes
from the IPCC’s most recent iteration of the global history. (Note
the discussion in chapter 2 about how this record itself has been
altered and probably slightly overestimates recent warming). These
figures are from the beginning of the recent warming, from 1977
through 2007. Note that they are also a straight line, but a line that
tracks beneath the average of the climate models.

This would be the forecast of people who accept the fact that
models tend to predict constant rates of warming (just different rates
for different models), and combine that with the observed constant
rate of surface warming, which yields a temperature change for the

20

A Global Warming Science Primer

The Wisdom of a Crowd (of Models)?

James Surowiecki’s 2005 classic, The Wisdom of Crowds, dem-
onstrates repeatedly that ensembles of independent estimates
tend, over time, to perform better than a randomly selected
individual one when asked to estimate some unknown
quantity.

The common example pertains to the number of jelly beans
in a jar. Whereas each of 100 people will almost certainly guess
a different number, the real number will come close to the
average of the 100 guesses.

People who teach weather forecasting are intimately familiar
with this concept. Each student’s forecasts are quantitatively
evaluated, as are the average of all the students’ forecasts. One
student or another (or the group) may make the best forecast
for an individual day. But when averaged over the long haul,
the ‘‘group average’’ forecast is the likely winner.

This also applies to weather forecasting models. But it may

apply to climate models only in a very special fashion.

In the daily weather forecast, it is well known that an ‘‘ensem-
ble’’ of different computer models or different runs of the same
model tends to perform better, over the long haul, than any
individual model or run. That’s because the models are indeed
like individuals in a crowd in that they are ‘‘unbiased.’’

‘‘Unbiased’’ means that the models (or the people) have no
inherent problem that will make them systematically over- or
underestimate temperature (or the number of jelly beans). But
climate model ensembles can clearly have bias. For example,
when climate models have been ‘‘intercompared’’ (as in the
‘‘Coupled Model Intercomparison Project’’ studies published
initially by the U.S. National Center for Atmospheric
Research’s Gerald Meehl in 2000), they were all fed more carbon
dioxide than was known to be accumulating in the atmosphere.
Consequently, when compared with observed temperatures,
they tended to predict more warming than was actually
observed, a fact emphasized repeatedly in Meltdown.

(continued on next page)

21

CLIMATE OF EXTREMES

(continued)

But both in those studies and in the model collection shown
in Figure 1.5 (see insert), which uses a different carbon emission
scenario (though the same one is applied to all models), the
ensemble behavior resembles nature in that the warming is at
a constant (rather than an increasing) rate.

This increases confidence in a forecast of warming that is
indeed at the constant rate that has been observed for decades
(subject to the post-1998 behavior described later).

21st century of about 1.7°C (3.1°F). This is about 40 percent less
warming than the average projection given in Figure 1.5 (see insert).
There’s a certain logic on behalf of the use of the models for some
guidance for 21st-century temperatures, which can be summarized
as follows: Both models and observations show a linear (constant)
warming, but the observed warming is below the average model rate.
Perhaps the ‘‘sensitivity’’ of temperature to changes in atmospheric
carbon dioxide has simply been overestimated.

Has Global Warming Stopped?

Googling ‘‘global warming’’ will get you about 23 million hits.
Most are of the gloom-and-doom variety. But not all of them. One
thread that has emerged over the last year on many climate and
policy blogs is that global warming ‘‘stopped’’ in 1998.

That’s true (Figure 1.6) but caution is advised: 1998 saw one of
the largest El Nin˜ os in recent history, and the associated suppression
of the cold upwelling off of South America induced a huge tem-
perature spike—one that was never exceeded in the subsequent
decade. A plot of the year-to-year temperature change since then
(Figure 1.6) clearly shows no obvious upward or downward trend.
That leads us to a fairly fearless forecast: The next big El Nin˜ o is
likely to produce a temperature above that of 1998, resetting the
global record. But in general, the same pokey warming trend that
was established more than three decades ago will still be the rule.
In 2000, one of us (Michaels) published a paper in Geophysical
Research Letters, showing that almost all the fluctuations around
the warming trend that began in 1977 could be explained by the

22

A Global Warming Science Primer

‘‘El Nin˜ o’’ in the Temperature History

Every global warming book will refer repeatedly to ‘‘El
Nin˜ o,’’ which has been blamed for floods, droughts, fires, dis-
eases, and just about everything else—so many things, in fact,
that Laurence Kalkstein, a well-known climatologist recently
retired from the University of Delaware, used to deride it as
the ‘‘Vitamin E’’ of climate.

El Nin˜ o is a slowing or even a reversal of the trade winds
across the Pacific Ocean. No one knows exactly why it happens
(the proof being that forecasts of impending El Nin˜ os are pretty
lousy). Given that the trades are the largest single climate
phenomenon on earth, slowing them has an awful lot of down-
stream effects, including spiking global temperature. It earned
its name because there is a normal seasonal weakening of
the trades that takes place in December—around Christmas—
meriting the obvious title, The (male) Child. When an El Nin˜ o
occurs, this normal weakening is extended throughout the year.
The trade winds are associated with a strong east-to-west cur-
rent from South America, across much of the tropical Pacific.
This current drags up cold water from beneath the surface, which
is one reason why much of the Pacific shore of tropical South
America isn’t nearly as hot as one might think it should be.

When El Nin˜ o occurs, this cold ‘‘upwelling’’ is suppressed,
and instead, the waters off of South America, and westward
across much of the Pacific, become unusually hot. Needless to
say, global average temperature rises. One of the biggest El
Nin˜ os in the last 100 years occurred in 1998, and the tempera-
ture peak is quite evident in Figure 1.1. The year 1998 remains
the warmest year in the global record, so warm that the suc-
ceeding decade shows no net warming trend whatsoever.

Of course, when El Nin˜ o stops and the cold upwelling
returns, there’s a lot of cold water waiting under the surface,
and global temperatures drop. This phenomenon, not surpris-
ingly, is called La Nin˜ a, and can be seen in the 1999 and 2000
temperatures.

(continued on next page)

23

CLIMATE OF EXTREMES

(continued)

El Nin˜ o is actually correlated with a lot of weather anomalies
pretty far from the tropical Pacific. For example, it usually (but
not always) results in a much wetter-than-normal winter in
Southern California.

Nature is pretty attuned to this natural fluctuation. For exam-
ple, seeds of many plants in the Southwestern desert require the
physical disturbance caused by a flood in order to germinate, so
it’s fair to say that El Nin˜ o makes the desert bloom. But (as
described in chapter 5) the desert is, well, usually pretty dry,
so that when El Nin˜ o goes away there’s an unusually large
amount of vegetation left to dehydrate and ultimately combust.
So, though the chain of causation isn’t rock solid (in some El
Nin˜ o years, rainfall isn’t enhanced), it seems plausible to blame
an unusually vigorous fire-year in the Los Angeles basin on
recent El Nin˜ o activity. Given that El Nin˜ os have been around
forever (meaning many, many millions of years), Nature has
been able to take advantage of their disturbance of normal
weather regimes.

An El Nin˜ o year tends to be one in which global temperatures
are elevated above a rather smooth trend. Will the next big
El Nin˜ o year reset the global temperature record? And how
important is a ‘‘warming trend’’ that takes over a decade to
reset successive high temperature records?

magnitude of El Nin˜ o, changes in the sun’s output as evinced by
sunspots, and the amount of dust in the stratosphere contributed
by big volcanoes.

Note the phrase ‘‘fluctuations around the warming trend.’’ We’re
saying, whatever the cause (though it is probably carbon dioxide),
there is a warming trend in the data, and the temperature changes
around that trend are best explained by the other three variables.
So, to test if the warming trend has indeed ‘‘stopped,’’ we ran
our old model, which ended in 1997, and asked it to predict monthly
temperature variations from either a continuation of the warming
trend already established or a cessation of that trend at the end
of 1997.

24

A Global Warming Science Primer

GLOBAL SURFACE AND SATELLITE TEMPERATURES, 1998–2007

Figure 1.6

SOURCE: IPCC 2007 (surface temperature); University of Alabama-
Huntsville 2007 (satellite temperatures), http://vortex.nsstc.uah.edu/data/
msu/t21t/tltglhmam_5.2.

Which model predicted better? It was the one that assumed that
the warming trend continued through 2008. Models that assumed
temperatures were flat from 1998 to 2008 predicted surface tempera-
tures to be lower than they were actually observed to be. In other
words, El Nin˜ o and the sun conspired to halt the warming trend in
the first decade of the 21st century. But in the future, they could
behave in an equal and opposite fashion, as they did in 1998, creating
a huge (but temporary) spike in global temperatures.

Rather than starting in the big El Nin˜ o year of 1998, perhaps it’s
fairer to start in 2001, after global temperatures recovered from the
big El Nin˜ o–La Nin˜ a warming and cooling cycle. Figure 1.7 shows
monthly temperature departures from average for two different
records, the IPCC history and the University of Alabama-Huntsville
satellite history (known as the UAH record). The two are offset

25

CLIMATE OF EXTREMES

Figure 1.7

MONTHLY TEMPERATURE DEPARTURES FROM AVERAGE

TEMPERATURE FOR THE IPCC RECORD AND THE UNIVERSITY OF
ALABAMA–HUNTSVILLE SATELLITE, JANUARY 2001–JULY 2008

SOURCE: IPCC and updates 2007 (surface); University of Alabama-
Huntsville 2008 (satellite), http://vortex.nsstc.uah.edu/data/msu/t2lt/
tltglhmam_5.2.

because they are referenced to different averages. The IPCC is refer-
enced to its 1961–90 mean, and the satellite record, which begins in
1979, is referenced to its 1979–97 average.

The period 2001–07 is the longest interval in which the IPCC
record has shown no change since 1956–62. At the time, we had
only increased atmospheric carbon dioxide 14 percent above its
preindustrial value, compared with approximately 35 percent by
2006. Clearly the sun and El Nin˜ o are still capable of halting a
warming trend, but they don’t have nearly enough power to send
temperatures back to where they were in about 1900.

Because of an additional finding, published in Nature in 2008 by
Noah Keenlyside of Germany’s Leipzig Institute of Marine Science,
the implications of the recent lack of warming are remarkable. Keen-
lyside found that natural processes in the earth’s oceans are likely
to continue to offset much global warming through the middle of

26

A Global Warming Science Primer

the next decade. If that is true, then we will have gone nearly two
decades without any warming.

This is an arrow through the heart of the IPCC’s ‘‘scientific consen-
sus,’’ and a serious blow to reliance upon the models. Take a look
at Figure 1.5 (see insert). Is there a two-decade period in which any
model predicts no warming? Obviously not! Aside from observed
data, these models are our only guide to the future, and they clearly
can no longer provide scientific cover for any policies predicated
upon the notion of dangerous anthropogenic global warming
(DAGW).

There’s a further problem. The large warming that climate models
produce is mainly a result of an increase in atmospheric water vapor
that results from a much smaller warming produced by carbon
dioxide itself. The source of that water vapor, of course, is the ocean.
If the planet does not warm up for 20 years, there is a further, longer
delay in the so-called water-vapor feedback, because the ocean can-
not warm up instantaneously.

AGW (anthropogenic global warming), yes. But DAGW? We

think not!

Reasons to Disbelieve the Models

The earth’s atmosphere extends far above the planetary surface,
and it is the vertical distribution of temperature—from the surface
to the stratosphere (about 36,000 feet at our latitude)—that deter-
mines a lot of our weather. That zone is known as the troposphere,
and it is where almost all the weather action takes place.

For example, when the difference between surface- and upper-
tropospheric temperature is great, then the surface air is very buoy-
ant compared with what is above it. Put simply, hot air rises and
cold air sinks. The warmer the surface air, the more it is likely to
rise. As a result, large amounts of air can bubble up. As air moves
up, it cools, eventually to the point at which clouds form. The most
common signature of a relatively warm surface overlain by a cold
upper troposphere is the atmosphere’s most visible bubble—the
common thunderstorm.

Those who are skeptical of model projections point to a phenome-
nal mismatch between model predictions for temperatures above
the surface and actual observations of them.

27

CLIMATE OF EXTREMES

Is It Warming Faster than Predicted?

Much of the discussion in this chapter indicates that surface
warming is taking place at a relatively constant rate (the current
hiatus notwithstanding). But that’s not what we read in one
of the nation’s most prominent newspapers.

A few years ago the Washington Post’s advertising slogan
was, ‘‘If you don’t get it, you don’t get it.’’ When it comes to
global warming trends, it’s the Post that doesn’t ‘‘get it.’’

On January 29, 2006, Post global warming reporter Juliet
Eilperin wrote that ‘‘[the] Earth is warming much faster than
some researchers had predicted.’’

Where did this assertion come from? Certainly not from the
earth’s temperature history from the Intergovernmental Panel
on Climate Change.

For the past 30-plus years—the period during which the
earth’s rising temperature has been most strongly associated
with human activity—the average rate of warming (1977–2007)
as measured by the IPCC record has been 0.168°C Ⳳ 0.017°C
per decade (0.320°F Ⳳ 0.031°F) (Figure 1.8). Although there is
a certain degree of annual variation around this trend, the
overall rise has been incredibly steady; in other words, there
is no appreciable trend to the trend (Figure 1.9). That means
that the earth is warming at a constant, or linear, rate, not one
that is accelerating. This is by and large the same behavior
that the vast majority of climate models predict the earth’s
temperature will display when forced with ever-increasing
amounts of carbon dioxide.

(continued on next page)

There’s no doubt that getting the vertical temperature change
right is central to accurately projecting the changes in weather that
should accompany global warming. If the rate of temperature decline
with height is projected to become smaller, then there will be fewer
thunderstorms and a much more drought-prone world. If the oppo-
site is true, the future is replete with lush vegetation fed by increasing

28

A Global Warming Science Primer

(continued)

GLOBALLY-AVERAGED SURFACE TEMPERATURE DEPARTURE

FROM THE 1961–90 AVERAGE

Figure 1.8

SOURCE: IPCC 2001 and updates.
NOTE: These anomalies were available at the time of the Washington
Post’s January 2006 article.

(continued on next page)

rainfall during the growing season, when thunderstorms tend to
occur.

The most recent (and very persuasive) evidence against the models
was demonstrated late in 2007 in the International Journal of Climatol-
ogy by University of Rochester’s David Douglass and three col-
leagues—including John Christy, who developed the satellite-based
temperature history discussed in chapter 2.

‘‘The models are seen to disagree with the observations,’’ Douglass
et al. conclude. ‘‘We suggest, therefore, that projections of future
climate based on these models be viewed with much caution.’’

29

CLIMATE OF EXTREMES

(continued)

YEAR-TO-YEAR CHANGE IN ANNUAL GLOBALLY AVERAGED

TEMPERATURE ANOMALIES, 1978–2005

Figure 1.9

SOURCE: IPCC 2001 and updates.

Who are Eilperin’s researchers? If we turn to the ‘‘Third
Assessment Report’’ of the Intergovernmental Panel on Climate
Change (IPCC)—widely taken as the ‘‘consensus of scientists’’
at the time of Eilperin’s article—it states, ‘‘The globally aver-
aged surface temperature is projected to increase by 1.4°C to
5.8°C [2.5°F to 10.4°F] over the period 1990 to 2100.’’ That is
equivalent to a rise of 0.13°C to 0.53°C (0.235°F to 0.95°F) per
decade. Compare that with the observed rate of warming we’ve
established; clearly, the warming is running very close to the
lowest end of the IPCC warming range.

Rather, the predicted mean warming rate is clearly higher
than the observed one. Even NASA’s James Hansen, the world’s
most quoted global warming scientist (and a person whom

(continued on next page)

30

A Global Warming Science Primer

(continued)

Eilperin has lionized in other Post articles), has argued that the
warming rate over the next 50 years would be 0.15°C per
decade Ⳳ 0.05°C (0.27°F Ⳳ 0.09°F), assuming only very mod-
estly mandated changes in emissions.

Instead of hyping a nonissue, the Post would have done
a far greater service by reporting in January 2006 that the
earth’s annual average temperature for the year 2005 fell exactly
along the linear trend line established during the past 30 years
(Figure 1.8) and as such, acted to further support the notion
that the earth’s temperature is warming up less than most
people have predicted, assuming that the membership of the
IPCC includes most climate people.

Climate models predict that the greatest warming should occur
above the surface, not at or near the surface where we live. In 2000,
the National Research Council examined this issue of the differential
warming in various layers of the atmosphere and concluded that
the surface was warming far more than the lower atmosphere; that
pattern is not consistent with model predictions, and no obvious
explanation was apparent.

The Douglass et al. team gathered output from models, surface
observations, and balloon and satellite records, over the period
1979–2004, from which they calculated model-based and observed
temperature trends at the surface and various altitudes in the tropical
atmosphere. They focused on the tropics (20°N to 20°S) because
‘‘Much of the earth’s global mean temperature variability originates
in the tropics, which is also the place where the disparity between
model results and observations is most apparent.’’

Trends from the models and observations agree at the surface but
totally disagree from just above the surface to 14 kilometers (km)
(8.7 miles) above the surface (Figure 1.10)

The models all predict far more warming around 10 km (6.2 miles)
up in the atmosphere than is predicted at the surface. But all the
observational evidence shows no such pattern whatsoever. In fact,
there’s a lot of cooling being observed at high altitudes rather than
warming.

31

CLIMATE OF EXTREMES

MODELED (FILLED CIRCLES) VS. OBSERVED (OPEN SYMBOLS)

TEMPERATURE TRENDS FOR THE SATELLITE ERA (°C PER DECADE)

Figure 1.10

SOURCE: Adapted from Douglass et al. 2007.
NOTE: Observed temperatures begin in 1979. The model average comes from
an ensemble of 22 model simulations from the most widely used models
from throughout the world. The light gray area is the range of Ⳮ2 and ⳮ2
standard errors round the mean from the 22 models, which is the 95 percent
confidence band for the true model average. The acronyms refer to various
observational databases.

Douglass et al. conclude:

Model results and observed temperature trends are in dis-
agreement in most of the tropical troposphere, being sepa-
rated by more than twice the uncertainty of the model mean.
In layers near 5 km [3.1 miles], the modeled trend is
100 percent to 300 percent higher than observed, and, above
8 km [5.0 miles], modeled and observed trends have opposite
signs. On the whole, the evidence indicates that model trends
in the troposphere are very likely inconsistent with observa-
tions that indicate that, since 1979, there is no significant
long-term amplification factor relative to the surface.

The difference between surface and upper-tropospheric tempera-
tures is increasing, not decreasing. The implications are huge.

32

A Global Warming Science Primer

For example, an atmosphere with a greater difference between
the surface and upper layers is a more unstable one and will produce
more precipitation.

An inaccurate precipitation forecast has huge implications for
climate change predictions. Generally speaking, away from the high-
latitude land areas (which are too cold to dry out much), places that
get more rain have a wetter surface than those that do not. That
means that more of the sun’s energy is directed toward evaporation
of water than toward a direct heating of the surface. (You can observe
this phenomenon at the beach: Dry sand at noon will burn your
feet, but wet sand will not).

So the amount of rainfall is a determinant of surface temperature.
So is the amount of cloudiness. Everything else being equal, an
atmosphere with more vertical motion (i.e., one where the surface
is relatively warm compared with the upper layers) is one with more
clouds. In the tropics, that means cooler days. Again, to specify the
surface temperature correctly, it seems one has to get the vertical
distribution of temperature correct also.

So how can the models get the surface temperature correct if they

so dramatically miss the rest of the tropical atmosphere?

Intraday Temperature Issues

Clearly one signal consistent with greenhouse changes is an
increase in the coldest temperatures, and that appears to have been
observed (with the notable exception of Antarctica; see chapter 4).
But the models have also overestimated vertical changes in tempera-
ture. Are there any other important aspects of climate change that
they have gotten wrong?

One of the most prominent greenhouse-gas signals is the daily
temperature range (DTR), which is the difference between the high
and low temperature. Over most of the globe’s land regions, that
range has been declining over time—and the decline is thought
to be a global warming indicator. Both maximum and minimum
temperatures are rising, but the rise in daily low temperatures has
occurred at a much greater rate, so the temperature range has got-
ten narrower.

This trend is related to increasing greenhouse gas levels because,
everything else being equal, an atmosphere with higher greenhouse
gas concentrations will have elevated nighttime temperatures. The

33

CLIMATE OF EXTREMES

MODELED AND OBSERVED TRENDS IN MEAN, HIGH, LOW, AND

DAILY TEMPERATURE RANGE, 1951–2000

Figure 1.11

SOURCE: Braganza, Karoly, and Arblaster 2004.
NOTE: Acronyms refer to various models.

surface cools less at night because the earth’s ability to radiate away
heat from the lower layers is compromised by increasing green-
house gases.

But climate models do not accurately replicate this effect. Take,
for example, a 2004 study by Australian scientist Karl Braganza
and two coauthors from the United States published in Geophysical
Research Letters. The authors gathered data from all the global land
areas with sufficiently long periods of record (forcing them to
exclude Greenland, Antarctica, part of India, and most of Africa and
South America), and compared the observed global decadal trends
in maximum and minimum temperature and DTR with the output
of five climate models in which the observed changes in 20th-century
greenhouse gas and other atmospheric chemicals were simulated.
The results of the comparisons are summarized in Figure 1.11.
Although the climate models, in aggregate, do a good job of repro-
ducing the observed trend in minimum temperature, they overesti-
mate the trend in maximum temperature. Each model does increase

34

A Global Warming Science Primer

the daily high temperatures, but at a slower rate than the low temper-
atures. Actual observations show a much smaller increase in the
daily highs. The net effect of that discrepancy on DTR is that none
of the models can properly simulate the observed trend in DTR,
which is declining at a rate greater than the models indicate it
should be.

The critical issue here is that, given that DTR is really an indicator
of greenhouse warming, the models must be mischaracterizing some
very fundamental processes that are key to being able to accurately
model our climate at all. In this case, the models can hardly distin-
guish between the rates of day vs. night warming, while, in reality,
high temperatures are increasing more slowly than models predict
them to.

The flaw in the greenhouse models may be related to cloudiness.
Cloud cover over land areas increased during the last half of the
20th century. Cloudy afternoons are generally cooler than clear after-
noons, so clouds could account for this large discrepancy between
climate models and reality.

Of course, you could argue that you really can’t model earth’s
climate without getting cloud cover correct, given that clouds have
an awful lot to do with both planetary temperature and precipitation.
You could even argue that, because of this cloud problem, the models
might be getting the trends in minimum temperature correct by
dumb luck, given that the fundamental physics are not correct.

The bottom line? Over global land areas, nighttime low tempera-
tures are rising faster than daytime highs, and that trend is consistent
with increasing greenhouse gas levels. Climate models are incapable
of correctly reproducing the observed trends, and as a result are
showing that daytime high temperatures are increasing faster than
they are in reality. That error is present, in all likelihood, because
the models have not properly captured some fundamental physical
component of earth’s climate.

Model ‘‘Tuning’’

Can computer models be ‘‘tuned’’ to produce the right surface
temperature? And could doing so make the upper layers in the
computer model’s atmosphere go haywire? Further, can aspects of
a model be manipulated to give an expected output? How could
that be done?

35

CLIMATE OF EXTREMES

Go back to Figure 1.1, which is the IPCC’s surface temperature
history. Let’s stipulate that it’s correct (though the next chapter
will raise plenty of questions). Carbon dioxide has been increasing
throughout the 20th and 21st centuries, with relatively modest in-
creases in the earlier years compared with what is being observed now.
If carbon dioxide were the sole driver of climate change, then the
temperature would have changed in a similar fashion, with a constant
rate of warming as carbon dioxide increases as a small exponent.

Obviously, the temperature history does not mimic what would be
caused by the effect of carbon dioxide alone. That has been recognized
for at least 20 years. Sound and Fury, Michaels’ first book on global
warming, cited a 1987 paper by Thomas Wigley that indicated that
something other than carbon dioxide had to be influencing temperature.
Whatever that ‘‘something’’ was, it had to enhance warming in the early
20th century, and then limit it or cause cooling in the midcentury.

That ‘‘something’’ is hypothesized to be finely divided particulate
matter, usually in the form of sulfate aerosol. It is thought that such
particles reflect away the sun’s energy. The source: fossil fuels!

Fossil fuels, especially coal, contain some sulfur. When burned, the
sulfur combines with oxygen, which, through a series of chemical reac-
tions, ultimately appears as a finely divided dust, called sulfate aerosol,
which is thought to create a cooling effect. Because there wasn’t nearly
so much coal combusted in the early 20th century as there is now, either
carbon dioxide’s or the sun’s warming (the latter being more important
than the former at that time) wouldn’t be very attenuated by sulfates—
not until the world industrialized, which was contemporaneous with
World War II. And so, the story goes, sulfate cooling dominated carbon
dioxide warming until the late 1970s, when carbon dioxide won the day.
This explanation is commonly invoked to explain the warming of the
early 20th century, followed by a slight cooling to the mid-1970s, and
the subsequent second warming which continued through 1998. Sulfur
compounds emanating from coal-fired power plants are also thought
to be responsible for (remember this one?) acid rain. So, the story goes
on further, the sulfate effect was reduced (at least in North America
and Europe) as ‘‘scrubbers’’ were put on the power plants to wash out
the sulfur compounds before they could acidify precipitation. In other
words, cleaning up coal enhances warming.

So there are two ‘‘knobs’’ on global warming models that can interact
and produce something that mimics the surface temperature history.

36

A Global Warming Science Primer

One is the sensitivity of the temperature to changes in carbon dioxide,
or the amount of temperature change expected for each increment of
carbon dioxide. There’s plenty of debate about exactly what this value
is, so it can be specified as either high or low, depending upon the
model. The other knob is the countering effect of sulfate aerosol. If the
two knobs are adjusted just right, a model can show warming in the
early 20th century, a cooling in the middle of the century driven by
uncontrolled coal combustion, and another warming in the late 20th
century as coal is cleaned up and carbon dioxide continues to increase.
The problem is that no one really knows the magnitude of the sulfate
effects. Nor do we know precisely how the effects are distributed verti-
cally. For example, sulfate aerosol is hygroscopic, meaning that it tends
to gather water. Yes, that’s right: It accomplishes ‘‘cloud seeding’’—
because water droplets cannot form unless they have a ‘‘condensation
nucleus’’ to condense around. Simply put, sulfate aerosol should pro-
duce more water droplets in clouds.

The more cloud droplets there are, given a finite amount of moisture,
the smaller each individual droplet is. And smaller droplets are more
reflective, making whiter clouds, which should create even more cooling
than would result from the sulfate itself. The brighter the cloud, the
more the sun’s energy is kept from reaching (and warming) the surface.
It therefore might be easy to specify the surface temperature by turning
the carbon dioxide and sulfate knobs, though doing so might result in
major errors in the vertical temperature calculation. How much of that
has gone on is anyone’s guess.

Those who seriously doubt the models have quite a point. ‘‘Believers’’
may be placing too much faith in the models because of (1) the apparent
match with surface temperatures and (2) the fact that both observed
and modeled surface temperature changes are occurring at a constant
(rather than an increasing) rate. But the vertical temperature forecast
errors make the match between the models and the surface history a
possibly fortuitous result of model tuning.

The current state of global warming science is far from ‘‘settled.’’ It’s
true that both modeled and observed surface temperatures are rising at
a constant rate, but the models are clearly predicting too high a rate of
increase. Again, perhaps the ‘‘sensitivity’’ of climate to carbon dioxide
has simply been overestimated.

This is actually a minor problem, considering the problems with the
vertical distribution of temperature and the daily temperature range. The

37

CLIMATE OF EXTREMES

former calls into question the scientific basis for any model projections of
changes in cloudiness or rainfall. And if those are questionable, then any
match with surface warming may be fortuitous. What’s more, the fact
that none of the IPCC’s midrange models (Figure 1.5; see insert) generates
a warming-free 15-year period in the 21st century, which is happening
right now, is very disturbing.

Readers will note that we did not make a single argument for simply
taking the model results at face value. That’s because it is obvious that, in
general, the models have predicted too much warming in recent decades.
Another noteworthy aspect of this chapter’s discussion is that much
of the work showing the problems with the models is ‘‘new’’ to our
audience. Why has there been so little publicity about this good news?
Do scientists—and the journalists who write about their work—tend
to write about only ‘‘bad’’ news? Keep these questions in mind as you
read the rest of this book.

38

2. Our Changing Climate History

It is obvious that the planetary surface temperature is higher than
it was 100 years ago. But what about changes in the measurement
and analysis of temperature itself? Have new ways of collecting
information and analyzing it induced spurious warming or cooling
into our weather histories? As the science of temperature sensing and
the mathematical manipulation of those temperature data evolve, do
the histories themselves change? And if they do, is the tendency to
change in one direction?

Our model for the way science works would predict that over
time, we will see more global warming in the same data. That’s the
‘‘paradigm’’-based view of science, first published in 1962 by
Thomas Kuhn in The Structure of Scientific Revolutions. Kuhn pro-
posed that most scientific research is conducted in service of existing
‘‘paradigms,’’ or overarching philosophical structures that form a
consensus view of science. The obvious one with regard to recent
climate change is that carbon dioxide is the principal driver.

According to Kuhn, most scientific work tries either to explain
anomalies in the paradigm or to show that anomalous data are in fact
wrong. The use of sulfate aerosol to explain the obvious difference
between a temperature record showing a relatively smooth increase
in carbon dioxide and a temperature record showing warming . . .
then cooling . . . then warming is a typical example of the Kuhn-
ian view.

Is another one of Kuhn’s dynamics in play regarding the tempera-
ture histories? Namely, that successive revisions will tend to get rid
of more and more of that embarrassing midcentury cooling?

That’s the subject of this chapter. Unfortunately, the devil is in

the details!

There are three major ways in which the temperature of the surface
or of the lower layers of the atmosphere is determined: from long
thermometric histories at weather stations, from weather balloons
launched simultaneously twice daily around the globe, and from
orbiting satellites.

39

CLIMATE OF EXTREMES

Surface Readings from the United States

We’ll begin this discussion with surface thermometers. Specifi-

cally, we will start with the U.S. records, for several reasons.

The United States has maintained an extremely dense and high-
quality network of thermometers back into the late 19th century. It
is generally assumed that means that the U.S. temperature history
is an accurate one.

In 2000, the National Research Council published a report discuss-
ing discrepancies between the surface, satellite, and weather balloon
records. The panel found little disagreement between the U.S. surface
temperatures the IPCC used (Figure 1.1) and those sensed by satel-
lites over the United States. Over other parts of the globe, however,
there were regions of substantial disagreement between surface and
satellite data. Therefore, the U.S. surface temperature history is prob-
ably about as high-quality a record as there is.

The ‘‘Climatological Division’’ Record

The National Climatic Data Center (NCDC), a unit of the U.S.
Department of Commerce, collects and maintains the U.S. climate
history. There are many different networks and types of data. The
longest record with the most detailed history comes from an aggre-
gate of more than 16,000 stations (11,000 are currently active) that
have been operated largely by volunteer ‘‘cooperative observers.’’
(There are a few ‘‘professionally’’ monitored sites at airports and
National Weather Service offices.) The ‘‘co-op’’ network was estab-
lished in 1890; co-ops monitor temperature and/or precipitation,
depending upon the station.

The tremendous advantage of this network is that it was specifi-
cally designed to monitor weather in a uniform fashion. Conse-
quently, the type of instrumentation (thermometers or rain gauges)
tends to be the same over time. There is, of course, evolution in
technology, such as a switch that occurred from mercury-in-glass
thermometers to electronic temperature sensors, known as the Max-
imium/Minimum Temperature System (MMTS). That changeover
occurred mainly in the 1980s.

‘‘Climatological Divisions’’ (CDs) are 344 multicounty aggregates
in the lower 48 United States that are thought to have some geo-
graphic or climatic homogeneity. The CD data set is one of the least
‘‘massaged’’ of the U.S. records, and simply takes the large number

40

Our Changing Climate History

of co-op stations within a CD and averages the daily high and low
temperatures and 24-hour rainfalls.

There is one correction applied to the CD record, necessitated by
‘‘time-of-day’’ bias. Most—but not all—co-op observers record the
previous 24-hour low temperature early in the morning, which is
around the normal time that temperatures are at their low in the
daily cycle. Imagine recording the temperature on a record-breaking
cold winter morning. The result? Two record lows, one recorded at
7:00 am when a winter day’s temperature is reset and another at
7:01 am. Afternoon observers tend to record after work, or after the
time of the daily high temperature. Consequently, the likelihood of
two consecutive record high readings is lower than that for two
very low ones. And remember that there are more morning (cold)
than afternoon (warm) observers.

The CD records are corrected to account for the percentage of
morning observers, as well as for latitude and longitude of the CD
(which determines how close to the morning low temperature a
morning observation is likely to be).

Figure 2.1 gives the U.S. national average temperature based upon
the CD data. Note that each CD is weighted for its relative size so
that this represents a true national average (for the lower 48 states).
Each different analysis of national or global temperature data has
its problems. In the CD record, the number and location of stations
within each CD is not static. Nor is the environment surrounding
each co-op station.

Note that the CD averages include co-op stations that are in cities
or in which the environment may have changed from rural to subur-
ban. CD averages also include National Weather Service stations,
most of which are located at or near airports. Airports are usually
built in rural locations. But a problem arises once they attract the
inevitable related commerce (hotels, parking garages, etc.). Soon,
airport areas resemble small cities, complete with the attendant
‘‘urban warming’’ that skews the temperature record.

Odd demographic factors can also bias temperature histories. Lou-
isiana State University’s Barry Keim closely examined CD data from
New England and discovered that human migration patterns
induced statistically significant changes in average latitude, longi-
tude, and elevation of the stations within one Massachusetts CD.
This particular CD extended from the western half of the Boston

41

CLIMATE OF EXTREMES

U.S. AVERAGE ANNUAL TEMPERATURE CALCULATED FROM THE

CD RECORD, 1895–2007

Figure 2.1

SOURCE: National Climatic Data Center 2008. http://www7.ncdc.noaa.gov/
CDO/CDODivisionalSelect.jsp.

metropolitan area farther westward to the Connecticut River. Most
of the CD is higher in elevation than western Boston, so the normal
out-migration to further and further ’burbs raised the average eleva-
tion of the co-op stations. That would make that CD’s readings lower
than they should be. Other biases, including the general urbanization
of the nation, would make them higher. Including city stations them-
selves in the CD record should mean that the record has some
amount of artificial warming in it.

There are other factors that might give one pause before using
the co-op data. We are a nation that used to cut trees down to clear
land for agriculture. Consequently, the eastern United States was
hugely deforested in the 19th century. But as agriculture (and people)
moved West, the fertile soil and climate of the Midwest began to
make eastern farming less and less competitive, so the East reverted
back to forest. Many co-op stations that started off in the open might
rather suddenly find themselves in the shade, as a nearby tree grows
tall enough to cast an afternoon shadow. Obviously, that would
induce an artificial cooling bias over time.

42

Our Changing Climate History

The U.S. Historical Climate Network

NCDC scientists, aware that there were multiple problems with
the CD records, decided to create a history that they thought would
contain fewer systematic errors. The U.S. Historical Climate Network
(HCN) is a 1,221-station subset of co-op stations that are thought to
be free from most common contaminations. They are generally in
rural areas. An important feature is that each station was extensively
examined for its history, including changes associated with station
relocation. Records were examined to see if there was evidence of
‘‘discontinuities,’’ such as might be caused by a newly extended
shadow from a growing tree, general urbanization, or the building
of new structures near the recording site.

Figure 2.2 shows the HCN and the CD data averaged over the
United States. Both records begin in 1895. In the beginning of the
record, the CD data tend to be warmer than the HCN, but by 1905
the HCN becomes warmer. The difference between the two over
the entire length of record (Figure 2.3) is approximately 0.4°F (0.2°C).
In other words, either the HCN is biased toward detecting more
warming than there has been, or the CD record is somehow underes-
timating it.

Is there something inherently wrong with both records? Christo-
pher Davey (Colorado State University) and Roger Pielke Sr. (Uni-
versity of Colorado) examined the 57 co-op stations in eastern Colo-
rado. Ten of those are included in the HCN. They start by noting
that the HCN does not specifically examine whether the exposure
of the station corresponds to standards from the UN’s World Meteor-
ological Organization (WMO), which states that the site ‘‘should
offer free exposure to both sunshine and wind [and not be] close to
trees, buildings or other obstructions.’’

The majority of the HCN sites were in clear violation of the WMO
standards. The Lamar station was sandwiched between two mobile
homes, structures that tend to be lightly insulated and leak heat.
Another one, in Eads, Colorado, was only 10 feet from another
mobile home. One location was nothing short of ridiculous: ‘‘The
Las Animas site had, by far, the poorest exposure for the USHCN
that we visited,’’ the researchers reported. The sensor was located
six feet from a wall and an exhaust vent for a power plant!

Davey and Pielke couldn’t conclude that the HCN was any better
than the CD history. They found that the proportion of co-op stations
with major siting problems was the same as in the HCN subset.

43

CLIMATE OF EXTREMES

Urban Warming

It’s long been noted that cities are warmer than the surround-
ing countryside. The bricks, buildings, and pavement heat up
more than a vegetated surface, making daily high temperatures
higher than those in the surrounding exurb. The surface—
sidewalks and streets, highways and overpasses, skyscrapers,
townhouses, parks, parking lots, parking garages, and so
forth—is also much more uneven than that of flat farmland or
gently rolling forest terrain, so that ventilating winds are less
effective at dissipating the heat of the day.

There is no adjustment for urban warming in the CD history.
The HCN data have been adjusted based upon either a popula-
tion-based formula calculated by NCDC’s Thomas Karl (HCN
‘‘Version 1’’) or an analysis of neighboring stations (HCN ‘‘Ver-
sion 2’’). If one station shows a warming trend that is not
reflected in a neighboring one, the ‘‘warming’’ data are
adjusted downward. Research published by one of us (Balling
and Idso 1989) demonstrated that the ‘‘urban effect’’ (also
known as the ‘‘urban bias’’ and the ‘‘urban heat island effect’’)
was even evident at weather stations where the surrounding
population was so small (2,500) that no one would have
thought there could be an ‘‘urban’’ influence. Whether these
changes are ‘‘caught’’ in HCN Version 2 is unknown, because
NCDC has never explicitly published a list of precisely what
corrections it has applied to individual stations.

The IPCC has used several different techniques to remove
urban bias from its record. The original version looked for
trends in neighboring stations. The current version simply
adjusts temperatures downward by 0.0055°C (0.01°F) per
decade, beginning in 1900.

People have tinkered with the CD history and removed some
of the obvious urban stations, with only a minor (0.03°C to
0.06°C [0.05°F to 0.1°F]) reduction in net temperature change

(continued on next page)

44

Our Changing Climate History

(continued)

in the overall record. The IPCC claims that no more than 0.1°C
(0.2°F) of their observed warming of 0.8°C (1.4°F) since 1900
is a result of urbanization.

The fact that HCN Version 2 warms more than the CD record
(Figures 2.2 and 2.3), means that other corrections besides ones
for urbanization that are applied to the co-op stations that
make up the HCN are producing more ‘‘warming.’’

ANNUAL AVERAGE TEMPERATURE HISTORY FOR THE UNITED

STATES, BASED ON THE CD DATA (OPEN CIRCLES) AND THE HCN

Figure 2.2

VERSION 2 (FILLED CIRCLES), 1895–2007

SOURCE: National Climatic Data Center 2008: http://www.ncdc.noaa.gov/
oa/climate/research/ushcn/ (HNC); http://www7.ncdc.noaa.gov/CDO/
CDODivisionalSelect.jsp (CD).

Most of the problems were because the sites either experienced
poor ventilation (which would artificially raise both low and high
temperatures) or were located over or near surfaces, such as blacktop
or concrete, that would clearly affect daytime highs.

45

CLIMATE OF EXTREMES

CD TEMPERATURE SUBTRACTED FROM HCN VALUES

Figure 2.3

SOURCE: National Climatic Data Center 2008: http://www.ncdc.noaa.gov/
oa/climate/research/ushcn/ (HNC); http://www7.ncdc.noaa.gov/CDO/
CDODivisionalSelect.jsp (CD)

Remembering that most of the co-op stations are rural, what
accounts for the warm bias of the HCN compared to the CD record?
A perplexing notion arises: Does the urban correction applied to the
HCN data (which is not applied to the CD data) somehow induce
that bias? Our sidebar ‘‘Changing Central Park’s Climate Data’’
shows that circumstance could actually happen.

Global Histories

There are three global temperature histories from surface ther-
mometers. The most cited is the record from the IPCC, also known
as the CRU record because it originated from the Climate Research
Unit at the University of East Anglia. The other two records are the
Global Historical Climate Network from the U.S. National Climatic
Data Center and the global history from NASA’s Goddard Institute
for Space Studies.

46

Our Changing Climate History

Changing Central Park’s Climate Data

For our money, the best climate blog out there by far is
Steve McIntyre’s Climate Audit (http://www.climateaudit.org).
McIntyre, a mathematician and former mining executive, uses
the term ‘‘audit’’ because he believes there are a lot of Enron-
like shenanigans going on in the climate community—a lot of
fiddling with data and numbers and very little transparency.
As an example, he recently showed what all of the NCDC
corrections have done to one of America’s iconic weather sta-
tions: Central Park, New York.

CENTRAL PARK, NEW YORK, CO-OP STATION DATA

Figure 2.4

SOURCE: Climate Audit (http://www.climateaudit.org.)
NOTE: Closed circles are adjusted raw data for time-of-day bias and
for station moves. Open circles (from NASA’s Goddard Institute for
Space Studies) are further adjusted to make the record compatible
with surrounding rural stations. Open triangles are the NCDC-
adjusted data based upon population.

(continued on next page)

47

CLIMATE OF EXTREMES

(continued)

Go figure. Given that the raw data show no trend, there
would have to be a massive assumed decrease in population to
make the NCDC data suddenly warmer at the end of the record.
The Central Park history is an example of how much diver-
gence there can be between temperatures actually measured
on the ground and those that form our national and global
histories.

All three are pretty similar in that they have two periods of warm-
ing, from 1910 to 1945 and then from 1975 to 1998, with an interval
of slight cooling between the two. The most cited is the IPCC record.
This history was originally published by Phil Jones and several
coworkers in 1985. Since then, it has gone through several iterations,
which are displayed later in this chapter.

Unlike the HCN, the IPCC history does not correct for time-of-
day bias, but it has some other very quirky corrections. The latest
version is described in a 2006 paper in the Journal of Geophysical
Research by Philip Brohan and several others, including Jones.

One adjustment is a ‘‘Homogenisation Adjustment,’’ made when a
station is moved. These adjustments occurred mainly in the 1940–60
period, when it was common for the official temperature-tracking
site for a city to be moved from downtown to an airport. As a
result, they lower the temperature of the pre-1960s data which, shown
below, makes the mid-20th century temperatures colder. This has
the effect of reducing the magnitude of the cooling between 1945
and 1975.

Anyone who lives around an airport knows that commerce soon
migrates to that vicinity, with hotels, car-rental lots, and strip malls
sprouting like mushrooms after rain. Consequently, the temperature
should quickly rebound to the values measured in the previous
urban location. The result is that, not only does this adjustment
make the 1940–60 period cooler, it also probably makes the most
recent years warmer.

Their urban adjustment is, to put it lightly, strange. It used to be
that they considered rural–urban pairs, and when one (urban) station

48

Our Changing Climate History

showed a warming trend that its (rural) near neighbor did not, then
the urban station was either adjusted downward or removed. Some
big cities, like Buenos Aires, showed no change and remained in
the history.

They no longer do that. Instead, the temperature records are
adjusted downward by 0.0055°C (0.01°F) per decade for the globe’s
entire land surface. That means there is the same urban bias assumed
in both New York City and Antarctica.

As a consequence, our surface records are hardly ‘‘static.’’
Figure 2.5 shows the difference between the last two iterations. It
is very clear that the early years of the record have gotten colder.
The result is more warming from the same data!

The observed rate of recent warming (1977–2007) is 0.167°C

(0.301°F) per decade in the latest version.

Weather Balloon Records

Weather balloons are launched simultaneously around the planet,
twice a day, to provide a detailed ‘‘snapshot’’ of the vertical structure
of the atmosphere. They measure temperature, humidity, and alti-
tude (barometric pressure). Wind is measured by tracking the flight
from the ground. The data are then input into the giant computer
models that forecast the weather up to 16 days in advance (no
comment on how good that 16-day forecast is!).

The first temperature they record is at point of release—that is,
the ground-based temperature. They then measure the temperature
at various heights as they ascend. The instruments that record and
transmit the data are called radiosondes.

Their purpose is send back accurate data. Consequently, the
instrumentation is regularly calibrated. Even so, different nations
use different sensing technologies, and even those that use the same
instrumentation may process their information differently. Further,
data transmission and sensing technologies evolve. Weather bal-
loons are not designed to specifically determine our historical clima-
tology, but they provide useful data.

James Angell, of the National Oceanic and Atmospheric Adminis-
tration (NOAA), developed a temperature history from 63 balloon-
launch sites worldwide, beginning in 1958. His history was first
published in the journal Monthly Weather Review in 1975, and
he continued to update it through 2005. The history includes

49

CLIMATE OF EXTREMES

Figure 2.5

CURRENT (‘‘HADCRUT3V’’) AND PREVIOUS (‘‘HADCRUT2V’’)

VERSIONS OF THE IPCC TEMPERATURE HISTORY (TOP), AND

DIFFERENCE BETWEEN THE TWO (BOTTOM)

SOURCES: IPCC 2001, 2007.

50

Our Changing Climate History

the 5,000–25,000 foot layer, which should be free of any urban
contamination.

As is the case for the surface temperatures, the data themselves
have been revised. In 2003, Angell found that nine stations, all in
the tropics, were unreliable because their temperatures varied far
more than others from year to year. After noting this error and
finding other problems, Angell published a new and expanded (85-
station) history in 2005 that was carefully checked for changes in
data quality and instrumentation. (The senior author was Angell’s
coworker Melissa Free, also of NOAA.) The new record was called
RATPAC (Radiosonde Atmospheric Temperature Products for
Assessing Climate).

The difference between the Angell record, which was the standard
reference for decades, and RATPAC is considerable (Figure 2.6);
RATPAC starts out colder and ends up warmer than the Angell
record, adding a huge warming trend. The trend in the original data
was 0.09°C (0.17°F) per decade, starting in 1958. The revised trend
rose to 0.15°C (0.27°F), or 67 percent more warming than was in the
original record.

Perhaps it’s more important to look at the period from 1977 to
the present, generally considered to be the era of greenhouse warm-
ing. In that case, the RATPAC warming trend is 0.160°C (0.288°F)
per decade.

Satellite-Sensed Temperatures

In late 1978, NASA launched the first in a series of satellites
designed to monitor global temperature from space. Those instru-
ments are placed in orbits that measure the temperature at the same
time of day globally.

The temperature sensors are called microwave sounding units
(MSUs), and they actually measure the vibration of oxygen in the
atmosphere, which is proportional to temperature. Different ‘‘chan-
nels’’ in the satellites can discriminate between different levels in
the atmosphere.

The satellite record was first published by NASA scientist Roy
Spencer and University of Alabama climatologist John Christy in
Science in March 1990. They set off quite an uproar because the
record showed absolutely no evidence for global warming.

51

CLIMATE OF EXTREMES

RATPAC AND ANGELL GLOBAL TEMPERATURES FROM

Figure 2.6

WEATHER BALLOONS (TOP), AND

DIFFERENCE BETWEEN THE TWO RECORDS (BOTTOM)

SOURCE: Angell and Korshover 1975 and updates, Free et al. 2005. Data are
roughly the average from 5,000 to 25,000 feet.

52

Our Changing Climate History

The MSUs are functioning in the very harsh environment of space,
and they are also subject to some pretty severe buffeting during
launch and insertion into orbit. As a result, the individual sensors
are employed only for a few years before they are replaced with a
new satellite. As with weather balloons, calibration is critical. To
make the records homogeneous, a new sensor must be calibrated
against an existing one. That implicitly assumes that any ‘‘drift’’ in
sensor sensitivity or response is known and accounted for, and that
there are also no subtle changes in orbits over time that have not
been detected and compensated for.

In 1997, Kevin Trenberth and James Hurrell of the U.S. National
Center for Atmospheric Research, challenged the notion that the
succeeding sensors had been properly calibrated against each other.
Spencer and Christy took his objections into account and modified
their history. But there was very little change—the satellite still
showed no warming and, more important, was consistent with the
weather balloon data measuring temperature in the atmosphere
(roughly 5,000–25,000 feet).

In 1998, Frank Wentz and Mathias Schabel, from a small California
consulting company called Remote Sensing Systems, published a
paper in Nature in which they showed that the satellites’ orbits were
not so stable as had been assumed. Although the satellites were
placed to sense temperatures at the same time around the planet,
in fact, the orbits had been drifting.

Spencer and Christy began a log of the various corrections that
were made because of orbital drifts, changes in the MSU sensors
themselves, and other factors. Consequently, the MSU has become
a highly ‘‘dynamic’’ data set, with slight changes applied once or
twice a year. Spencer and Christy usually tweak the temperature
trend by a hundredth of a degree (C) per decade or so.

Some of the corrections have been pretty large. In 2005 Carl Meaps
and Wentz discovered errors in the way that Spencer and Christy
were correcting for how the satellites varied on a 24-hour cycle.
That correction, made in 2005, added a trend of 0.035°C (0.063°F)
per decade.

Figure 2.7 shows three versions of the satellite data that were
available at various times. Each correction can be applied to the
entire data set, but if one record has been discovered to be too
contaminated with errors, that record was be abandoned. As a result,

53

CLIMATE OF EXTREMES

A COMPARISON OF THREE SATELLITE DATA SETS (TOP), AND TWO

DIFFERENCES BETWEEN THE DATA SETS (BOTTOM)

Figure 2.7

SOURCE: University of Alabama-Huntsville; Christy et al. 1998 (MSU ‘c’);
Christy et al. 2000 (MSU ‘d’); http://vortex.nsstc.uah.edu/data/msu/t2lt/
tltglhmam_5.2 (MSU ‘5.2’), 2007 (satellite temperatures), http:// vortex
.nsstc.uah.edu/data/msu/t21t/tltglhmam_5.2.

version ‘‘c’’—the original record, which is very close to the one
published in Science in 1990—ends in 1998; version ‘‘d,’’ corrected

54

Our Changing Climate History

for orbital drift, ends in 2002; and version ‘‘5.2’’ is current and runs
through 2007.

The changes that resulted are quite remarkable: Each major itera-
tion of the satellite data produces more warming than the previous
version. Figure 2.7 (bottom) is the difference between the intermedi-
ate (‘‘d’’) record and the original (‘‘c’’) record. There was no signifi-
cant warming trend in the original data, but version ‘‘d’’ warmed
at 0.075°C (0.135°F) per decade. Remember that these records can
only be compared through 1998, when version ‘‘c’’ ends. Version
5.2 is very warm compared with version ‘‘c,’’ with a trend of 0.095°C
(0.171°F) per decade relative to ‘‘c.’’ The overall trend in version 5.2
is 0.142°C (0.256°F) per decade.

A Strange Convergence

Think about it. Every ‘‘new’’ record we have examined here shows
a greater warming trend than its previous iteration, over the same
period of time. That has certainly had an effect on the public discus-
sion and perception of global warming.

Let’s start in 1995, with the three main records—the IPCC’s,
Angell’s weather balloons, and Spencer and Christy’s MSU satellite.
(Figure 2.8)

You would notice several things in 1995. The IPCC surface temper-
atures appear to be quite constant through the mid-1970s, but since
then shows a slight warming trend that appears to be around 0.2°C
(0.36°F) per decade. The weather balloon data seem constant through
the mid-1970s. They jump suddenly in 1976, but then show little
change from the late 1970s through 1995. The satellite data, which begin
in 1979, show no warming trend whatsoever.

You’ll also note that there appears to be a great deal of agreement
between the satellite and the balloon temperatures from year to year,
except for a constant offset because they are referenced to different
averaging periods. In other words, when one record goes up from
year to year, so does the other, and by approximately the same
amount. It is reassuring that two sets of data are in such fine
agreement.

The IPCC surface history is the odd-record-out. There’s some
correlation with the balloon temperatures from year to year, but
there is an apparent warming trend in the IPCC temperatures in the
last 20 years that simply isn’t reflected in the other two histories.

55

CLIMATE OF EXTREMES

IPCC SURFACE TEMPERATURES (FILLED CIRCLES), BALLOON-

MEASURED TEMPERATURES AT 5,000–25,000 FEET (OPEN SQUARES),

AND SATELLITE-SENSED LOWER ATMOSPHERE TEMPERATURE

RECORDS (WHITE CIRCLES), ACCORDING TO HISTORIES AVAILABLE

Figure 2.8

IN 1996

SOURCE: IPCC 1995, http://cdiac.ornl.gov/trends/temp/angell/angell.html
(balloon); Christy et al. 2003 (satellite).
NOTE: Temperatures are expressed as departures from different mean values.

This discrepancy was often noted in public discussions of global
warming. These were virtually all the data we had at hand: One
record was in disagreement with two others, both of which were in
agreement with each other.

Move forward now to 2000 (Figure 2.9). The early years of the
CRU-IPCC surface records are actually a few hundreths of a degree
colder than they were in the 1996 comparison. Consequently, there’s
a bit more ‘‘global warming’’ in the same history. The change
resulted from the use of a different technique to transform the raw
weather station data into a global average.

The difference between the IPCC and the other two records, if
anything, has become greater. All three records clearly show the
spike in global temperatures that occurred with giant 1998 El Nin˜ o,

56

Our Changing Climate History

Figure 2.9

SURFACE (FILLED CIRCLES), BALLOON-MEASURED (OPEN SQUARES),

AND SATELLITE-SENSED TEMPERATURES (WHITE CIRCLES),

ACCORDING TO HISTORIES AVAILABLE IN 2001

SOURCE: http://www.cru.uea.ac.uk/cru/data/tem2/ (CRU2v); http://
cdiac.ornl.gov/trends/temp/angell/angell.html (balloon); Christy et al.
2003 (satellite).

but, after allowing for that one-year event, neither the satellite (since
1979) nor the balloon (since 1977) shows any warming trends that
bear any resemblance to what is in the IPCC record.

Fast-forward to 2007. In the intervening period, Angell’s RATPAC
was published in 2005, and the Climate Research Unit at University
of East Anglia (the source of the IPCC data) produced its 2005
revision. That one changed more than its last revision. We label it
‘‘CRUT3v,’’ our shorthand for ‘‘third version of the CRU tempera-
ture record.’’ (Figure 2.10).

The differences between the 1950–95 and 1950–2006 records are
striking. The surface temperature record is even colder in the early
years than it was in the previous iteration, and both the satellite and
the balloon records now show warming!

Note that the early weather balloon data have become much colder

in the early years. The result is more global warming.

57

CLIMATE OF EXTREMES

Figure 2.10

SURFACE (FILLED CIRCLES), BALLOON-MEASURED (OPEN SQUARES),

AND SATELLITE-SENSED TEMPERATURES (WHITE CIRCLES),

ACCORDING TO HISTORIES AVAILABLE IN 2007

SOURCE: http://www.cru.uea.ac.uk/cru/data/temperature/ (surface);
http://www.ncdc.noaa.gov/oa/climate/ratpac/index.php (balloon);
http://vortex.nsstc.uah.edu/data/msu/t2lt/tltglhmam_5.2 (satellite).

Each of the three records are now in agreement with the surface,
balloon, and satellite records showing warming rates per-decade of
0.167°C (0.301°F), 0.160°C (0.288°F), and 0.142°C (0.256°F) respec-
tively, from the beginning of concurrency in 1979, values that are
statistically indistinguishable from the other.

But Is It Real?

The IPCC considers its (changing) surface temperature history to
be definitive and largely free from any systematic biases. As noted
above, the temperature record is adjusted downward, beginning in
1900, for an arbitrary urban warming effect. The adjustment is linear
and totals 0.06°C (0.11°F) by 2000.

Is urbanization all that could be contaminating the data and inflat-
ing observed warming? It has been known for years that landscape
changes other than urbanization have an influence on temperature.

58

Our Changing Climate History

Figure 2.11

U.S. HISTORICAL CLIMATE NETWORK DATA THROUGH 2003 (SOLID
CIRCLES), AND DATA ADJUSTED FOR THE WARMING BIAS FOUND

BY KALNAY AND CAI (OPEN CIRCLES)

SOURCE: National Climatic Data Center, http://www.ncdc.noaa.gov/oa/
climate/research/ushcn.html.

For example, the amount of solar energy that is absorbed and ulti-
mately heats the lower atmosphere can change dramatically if a
forest is transformed into a cornfield.

Eugenia Kalnay (University of Maryland) and M. Cai (Williams
College) in 2003 performed a very interesting exercise in which they
took advantage of the fact that weather balloon–measured tempera-
tures show virtually no urban warming effect at a few thousand
feet. Because we know how much that temperature, on average,
changes with altitude, it is theoretically possible (and quite easy) to
take temperatures measured aloft and ‘‘reduce’’ them to the sur-
face value.

Figure 2.11 shows what happens to the U.S. Historical Climate
Network (HCN) record when one does this. As noted above, the
HCN has been revised; this figure uses ‘‘Version 1,’’ the one that
was valid at the time that Kalnay and Cai published.

The warming trend over the period of record (1895–2003) drops
from 0.6°C (1.0°F) to 0.4°C (0.7°F), or one-third. If that were true on

59

CLIMATE OF EXTREMES

a global scale, and we attributed recent warming to changes in
carbon dioxide, then that would have to mean that the ‘‘sensitivity’’
of temperature to carbon dioxide would be about two-thirds of what
it was thought to be.

Climate scientists have been writing about problems with long-
term temperature records for more than a century. Long ago,
researchers noticed that temperatures in London were substantially
higher than in the surrounding rural landscape; urban climatology
has been a subdiscipline in the atmospheric sciences ever since.
Recently, countless articles appeared in the literature on subjects as
diverse as the urban heat island, changes in instrumentation, and
changes in time of observation.

In 2004, one of us (Michaels) presented a paper at the Annual
Meeting of the American Meteorological Society demonstrating that,
although greenhouse warming is dominant in cold areas of the
Northern Hemisphere in the winter, ‘‘economic’’ signals dominated
elsewhere, especially in the summer. An expanded version of that
paper was published in the journal Climate Research, in collaboration
with Ross McKitrick from Canada’s University of Guelph.

Other papers began to appear with similar findings. In 2004, Jos
de Laat and Ahilleas Maurellis, both at the Earth Oriented Science
Division of the National Institute for Space Research in the Nether-
lands, determined that local surface changes caused by industrializa-
tion accounted for a significant portion of global temperature
increases in recent decades. They published their findings in Geophys-
ical Research Letters.

De Laat and Maurellis used an idea similar to ours, in which they
defined local carbon dioxide emissions as a proxy for the amount
of local industrialization. They then divided the world into ‘‘industri-
alized’’ and ‘‘nonindustrialized’’ regions and calculated the tempera-
ture trends within each region. De Laat and Maurellis then repeated
their analysis using a different cutoff value for what level of carbon
dioxide emissions defined industrial and nonindustrial.

The results of their analysis are presented in Figure 2.12. They
are quite striking, even if they aren’t very surprising to others ques-
tioning the temperature history. Industrial regions with high carbon
dioxide emissions have significantly larger warming trends than
nonindustrialized regions, and larger trends than the globe as a
whole. Similarly, as industrialization (as represented by carbon diox-
ide emissions) increases, so does the temperature trend. That is true

60

Our Changing Climate History

Figure 2.12

MEAN TEMPERATURE TRENDS (°C PER DECADE) FOR 1979–2001

FOR INDUSTRIALIZED REGIONS AND NONINDUSTRIALIZED REGIONS

FOR DIFFERENT CARBON DIOXIDE EMISSIONS (TOP LINES); IPCC

SURFACE TEMPERATURES (LEFT); SATELLITE-MEASURED

TEMPERATURES, 0–10,000 FEET (RIGHT); AND CARBON DIOXIDE

EMISSIONS LEVELS (BOTTOM LINES).

SOURCE: Adapted from de Laat and Maurellis 2004.
NOTE: The shaded regions indicate the uncertainties of the trend estimates.
The thick solid bar inside the x-axis in each panel represents the global
mean trend in each data set.

for both the surface and the balance of the lower atmosphere, or
troposphere.

Note that this is not a measure of the local greenhouse effect—
given that the concentration of atmospheric carbon dioxide is
roughly the same around the world (there are some systematic geo-
graphic variations, but they are not large). Rather, it is more a mea-
sure of how much the local land surface has been altered.

They examined surface temperatures as well as two levels mea-
sured by satellite. Here we show only the surface and the 0-to-10,000-
foot satellite record (labeled MSU, the sensor on the satellites), but
there was a significant difference between the warming of industrial-
ized regions compared with the more rural ones at all levels. Given
that nonindustrialized regions show significantly smaller or even
negligible temperature trends, the authors infer that a significant
portion of the global warming temperature signal is localized—that
is, confined to industrialized regions.

61

CLIMATE OF EXTREMES

The authors also describe a serious flaw in the IPCC’s surface
temperature record. According to the paper, the ‘‘global’’ warming
trend is about 0.2°C (0.36°F) per decade (it’s actually 0.17°C [0.31°F]
per decade for 1977 through 2007) but the data do not actually have
global coverage. For instance, there’s virtually no information from
Antarctica, which is known to have cooled slightly in recent decades.
When the authors calculate the satellite-based temperature trend for
the same regions actually covered by the IPCC, they find that the
IPCC’s geographic selection results in an overestimation of warming
by 33 percent. Applying this finding to the surface temperature data
will reduce the ‘‘real’’ global warming to something around 0.12°C
(0.22°F) per decade. It is interesting that this is the same reduction
one gets by applying Kalnay and Cai’s finding to the HCN.

If that finding is correct, a significant portion of surface tempera-
ture increase in recent decades has resulted from local surface-related
processes as well as anthropogenic greenhouse gases.

Ross McKitrick and one of us (Michaels) published late in 2007
what we think is a comprehensive investigation into ‘‘nonclimatic’’
biases in temperature records; the article appeared in Journal of Geo-
physical Research.

We noted that more than 50 years ago, pioneering climatologist
James Murray Mitchell warned that when using weather records to
determine trends in climate: ‘‘The problem remains one of determin-
ing what part of a given temperature trend is climatically real and
what part the result of observational difficulties and of artificial
modification of the local environment.’’ We maintained that

two types of bias continue to affect the measurement of
climate change. Observational difficulties, or data inhomo-
geneities (such as station moves and closure, record dis-
continuities, equipment change, and changes to the time of
observation) are known to have affected records of mean
temperature. Modification of the land surface, including
urbanization and other economic activity, has been shown
to affect local, regional and possibly global meteorology, and
thus locally measured temperature data.

The IPCC assumes that there are many minor contaminants
(besides urbanization, which it explicitly subtracts out) in the climate
record, but that they are relatively inconsequential in the long run.

62

Our Changing Climate History

What a testable hypothesis just waiting for examination by skepti-
cal scientists! It assumes that there should be no significant relation-
ship between socioeconomic variables and trends in temperature
over land areas. If significant relationships can be identified between
socioeconomic variables and temperature trends, then other contam-
inants to the temperature records would be confirmed, and the
IPCC’s hypothesis must be rejected.

We examined the latitude–longitude gridded temperature data
sets from the IPCC, and then we assigned to each grid cell informa-
tion on gross domestic product, literacy, months with missing data,
growth in human population, economic growth, and growth in coal
consumption. We also added the satellite-based lower-tropospheric
temperature trend to see if there was any local bias, sea-level pres-
sure, a dryness index, proximity to an ocean, and latitude. These
last four should filter out climate variability due to geographic fac-
tors. We looked only at land grid cells, because ocean temperatures
should not be subject to economic or social biases.

The socioeconomic signals in the temperature trend data were

loud and clear.

We concluded that our results were

consistent with previous findings showing that nonclimatic
factors, such as those related to land use change and varia-
tions in data quality, likely add up to a net warming bias in
climate data, suggesting an overstatement of the rate of
global warming over land.

We found that the data were pretty good over much of North
America with the exception of northern Canada and Mexico (two
economically poor regions). Where poverty was pervasive, over
Africa and much of south Asia, there was clearly much less warming
than temperature records indicated. Figure 2.13 (see insert) shows
our findings.

Figure 2.14 shows the frequency of observation of different rates
of warming in the IPCC surface and the satellite data, and in IPCC
surface data adjusted for the biases we found. A value of, say, 0.1
to 0.2 means that the observed trend was between 0.1°C and 0.2°C
(0.2°F and 0.4°F) per decade. The y-axis is the relative frequency
(number) of trends of various magnitudes. Interestingly, our
‘‘adjusted’’ data look a lot more like the satellite data than the IPCC’s
original record. Inspection of Figure 2.14 reveals that the biggest

63

CLIMATE OF EXTREMES

DISTRIBUTIONS OF TEMPERATURE TRENDS, 1979–2002:

SURFACE (IPCC) (TOP); TROPOSPHERE (SATELLITE) (MIDDLE); AND

Figure 2.14

ADJUSTED SURFACE (BOTTOM)

SOURCE: McKitrick and Michaels 2007.

change was that our adjustment lopped off the very warm right-
hand ‘‘tail’’ of the IPCC’s temperature distribution. In other words,
the places showing the greatest warming had the greatest noncli-
matic bias in their records.

Our conclusion:

Nonclimatic effects are present in the gridded temperature
data used by the IPCC and they likely add up to a net
warming bias at the global level that may explain as much
as half the observed land-based warming trend.

Remember that this does not mean that half the entire planet’s
warming may be spurious. One has to balance the fact that the land
surfaces have warmed more than the ocean, but that the land is only
about one-third of the total surface. As a result, the global warming
rate since the late 1970s drops from the familiar 0.17°C (0.31°F) to

64

Our Changing Climate History

0.13°C (0.23°F) per decade, pretty much the same number you get
if you apply Kalnay and Cai’s findings to the HCN, or the calculation
of de Laat and Maurellis.

After our findings were published, not one news article appeared
noting that the amount of ‘‘real’’ global warming since the late 1970s
is nearly 25 percent less than previously thought, and that this was
consistent with two other independent studies. Imagine what would
have happened if we had found an extra 25 percent of warming!

65

CLIMATE OF EXTREMES

‘‘Your Aim Is to Find Something Wrong with It’’

Perhaps the most fitting vignette demonstrating the ‘‘Climate
of Extremes’’ in global warming science comes from the main
author of what is now the IPCC’s temperature history.

Scientists often call the IPCC history the ‘‘Jones and Wigley’’
record, because of several landmark papers describing it pub-
lished by Phil Jones of the University of East Anglia and Tom
Wigley, now at the U.S. National Center for Atmospheric
Research in Boulder, Colorado.

In its ‘‘Third Assessment Report’’ in 2001, the IPCC gave the
100-year surface temperature trend as 0.6°C Ⳳ 0.2°C (1.1°F Ⳳ
0.4°F). Australian researcher Warwick Hughes became inter-
ested in how this error was calculated. Was it because of errors
inherent in the raw data? How were confounding effects, such
as the growth of cities, accounted for? How about changes
around the sensing equipment, such as the erection of a
building?

So, Hughes wrote to Phil Jones, asking for the original tem-

perature data. On February 21, 2005, Jones responded:
We have 25 years or so invested in the work. Why
should I make the data available to you, when your
aim is to try and find something wrong with it?

Normally science thrives on the free exchange of data. But

not in a climate of extremes.

66

3. Hurricane Warning!

‘‘Global warming isn’t to blame for the recent jump in the
number of hurricanes in the Atlantic, concludes a study by
a prominent federal scientist whose position has shifted on
the subject.’’

—Associated Press, May 19, 2008

‘‘After some prolonged deliberation, I have decided to with-
draw from participating in the Fourth Assessment Report of
the Intergovernmental Panel on Climate Change (IPCC). I
am withdrawing because I have come to view the part of
the IPCC to which my expertise applies as having become
politicized.’’

—Hurricane scientist Christopher Landsea,
in an ‘‘Open Letter’’ to his scientific colleagues,
January 17, 2005

Hurricanes and global warming are a hot item, but the relationship
became controversial even before Katrina demolished New Orleans
in August 2005.

In a 1995 telephone conversation, the authors of this book specu-
lated about what would happen if a Category 4 hurricane hit New
Orleans. We forecast an abject disaster because the city lies below
sea level, and such a storm would likely overwhelm the pumping
system that must run to keep it dry. One thing we were sure of: No
matter what the facts were, such a hurricane would be blamed on
global warming. Ten years later, it happened.

On August 21, 2005, satellite imagery showed a very diffuse, but
very large cloud mass beginning to organize east of the Bahamas.
By the wee hours of August 24, the clouds had coalesced into a
tropical depression, and by the evening of the 25th, just a few hours
away from Miami, tropical storm Katrina became Category 1 Hurri-
cane Katrina.

From the start, Katrina was an unusual tropical cyclone. It formed
as a result of an addition of two separate tropical systems, a decaying

67

CLIMATE OF EXTREMES

tropical depression (a tropical cyclone with winds of less than
39 [miles per hour] mph) and a mass of thunderstorms that migrated
across the Atlantic from Africa. As a result, Katrina was born with
an unusually large cloud mass of showers and thundershowers.

The energy for a hurricane is derived from the condensation of
water in the form of clouds. When matter goes from a less ordered
(gaseous) state to a more ordered (liquid) one, heat is released to
the surrounding environment. In a tropical cyclone, the center of
the system becomes warmer than the surrounding environment. The
more condensation, the stronger the system will eventually become.
Katrina was born huge, primed to explode.

Florida got in the way. Right before hitting land, Katrina devel-
oped an eye, and had it not been for the nearby land, most forecasters
think Katrina was on the edge of a catastrophic intensification, much
like occurred in 1992, when Category 5 Hurricane Andrew had
an extra 200 miles to run before hitting south Miami, compared
to Katrina.

Katrina whacked a nation already hypersensitized to hurricanes.
The previous year, 2004, was also a banner one, with 17 tropical
cyclones. Four major storms affected Florida.

Hurricanes: From Really Bad to Impossibly Worse?

In chapter 7, we will discuss at length two questions that often
arise in discussions about global warming among those who may
not think it spells the end of the world. First, why is so little publicity
given to scientific results consistent with that point of view, and,
second, why does almost every finding seem to indicate that warm-
ing or its effects will likely be ‘‘even worse than we thought’’?

One reason is that bad news sells. Even prestigious science peri-
odicals such as Nature aren’t beneath a bit of global warming
embellishment.

Consider the photoshopped cover (see http://www.nature.com/
nature/journal/v434/n7036/index.html) of its April 21, 2005, issue,
which simultaneously displayed 2004 hurricanes Charley, Frances,
Ivan, and Jeanne threatening Florida—a scientific impossibility. The
actual dates of the storms were August 13, and September 3, 16,
and 26.

On Nature’s cover, the hurricanes are in a physically unrealistic
proximity. When hurricanes get too close together, one or both

68

Hurricane Warning!

Hurricane Intensity

The Saffir-Simpson scale of hurricane intensity classifies
storms from Categories 1 (minimal) through 5 (extreme). Gen-
erally speaking, Category 1 and 2 storms are not particularly
destructive, although there have been some notable exceptions,
due to the potential for any tropical cyclone (including weak
tropical storms that don’t even make it to hurricane strength)
to produce major flooding. In fact, many of the northeastern
U.S. flood records are from Category 1 Hurricane Agnes in
1972.

Table 3.1

SAFFIR-SIMPSON SCALE OF HURRICANE INTENSITY
Category

Maximum One-Minute Average Wind
74–95 mph
96–110 mph
111–130 mph
131–155 mph
⬎155 mph

1
2
3
4
5

Category 3 or higher hurricanes are considered ‘‘major.’’
Their frequency has changed over time, with low numbers in
the 1930s and 1970s and high numbers in the 1950s and 1960s,
and again since 1995. Roughly 40 percent of all hurricanes
reach Category 3 at some point. Since 1900, 33 hurricanes have
hit Category 5. Eight made landfall at that intensity somewhere
in North or Central America, and three hit the United States.
They were in 1935 (Florida Keys), 1969 (Camille in Mississippi),
and 1992 (Andrew in South Florida).

storms will fall apart. That’s because a healthy hurricane requires a
large surrounding area aloft, called an outflow zone, in order to
‘‘vent’’ the rising air that forms the destructive vortex. When two
storms are in relative proximity, the venting from one storm often
destroys the venting from another. Four strong storms in such prox-
imity has simply never happened because it can’t happen.

69

CLIMATE OF EXTREMES

The cover is explained on page ix of the issue:

The 2004 hurricane season was one of the worst on record.
Four hurricanes struck Florida in August and September. . . .
On the cover (Courtesy University of Wisconsin–Madison,
Space Science and Engineering Center) is a composite satel-
lite image of hurricanes Charley, Francis, Ivan, and Jeanne
‘‘approaching’’ Florida in August and September 2004.

‘‘Approaching’’? I called University of Wisconsin in Madison to
find out what was going on, and they replied that they had initially
provided another image with the dates superimposed over each
storm—a much less incendiary presentation, which Nature had
declined as ‘‘too cluttered.’’

Ironically, immediately below the description of the cover is a
reference to a ‘‘News Feature’’ article on page 952 titled, ‘‘Picture
Imperfect’’:

The magic of digital photography and Photoshop means that
scientists can manipulate images so that key features are
visible. But there is a grey area between image enhancement
and misrepresentation. Helen Pearson reports (‘‘News Fea-
ture,’’ page 952).

Why bother separating the two paragraphs? They certainly would
flow smoothly together. Or perhaps Nature should have said, ‘‘But
there is a gray area between image enhancement and misrepresenta-
tion, as shown on our cover.’’

So what is the science on global warming, hurricanes, and hurri-
cane severity? Why are so many people convinced that they are
increasing because of global warming?

Such one-sidedness is as near at hand as Al Gore’s book and
movie, An Inconvenient Truth. From the accompanying book, on
pages 80–81:

As the oceans get warmer, storms get stronger. In 2004, Flor-
ida was hit by four unusually powerful hurricanes. A grow-
ing number of new scientific studies are confirming that
warmer water in the top layer of the ocean can drive more
convection energy to fuel more powerful hurricanes. . . . But
there is now a strong, new emerging consensus that global
warming is indeed linked to a significant increase in both
the duration and intensity of hurricanes.

70

Hurricane Warning!

In fact, buried within the scientific literature are a number of
articles saying precisely the opposite. They might be fewer and
further between than gloom-and-doom pieces, but they are there.
A good place to start this discussion is in the midst of the active
2004 season. On September 16 of that year, Thomas Knutson, of
the National Oceanic and Atmospheric Administration, and Robert
Tuleya published a paper in the Journal of Climate in which computer-
generated hurricanes showed a slight increase in strength as carbon
dioxide accumulated in the atmosphere.

New York Times science writer Andrew Revkin summarized their

paper this way:

Global warming is likely to produce a significant increase in
the intensity and rainfall of hurricanes in coming decades,
according to the most comprehensive computer analysis
done so far.

That’s not even close to what Knutson and Tuleya actually wrote!
‘‘CO2-induced tropical cyclone intensity changes are unlikely to be
detectable in historical observations,’’ they concluded, ‘‘and will
probably not be detectable for decades to come.’’

In the grand scheme of weather systems, hurricanes are actually
pretty small and ephemeral—so small and short-lived that large-
scale climate models that attempt to project global and regional
temperature do not include them.

For that reason, Knutson and Tuleya began with model projections
of future sea-surface temperatures (SSTs), vertical temperature pro-
files in the atmosphere, and vertical moisture profiles over regions
where tropical cyclones form, using them to define a climate in
which they used a finer-resolution hurricane model to spin up tropi-
cal cyclones. They then compared the characteristics of the computer-
generated storms in the computer-generated future climate with the
computer-generated storms in the current observed climate. They
found that in the future climate, model-derived hurricanes had a
14 percent decrease in their lowest barometric pressure (a measure
of intensity), a 6 percent increase in the maximum surface wind,
and an 18 percent increase in the average rate of precipitation with
60 miles of the storm center over the model-derived hurricanes in the
current climate. All these changes were indications that the model-
derived hurricanes of the model-derived future would be more
intense than the model-derived hurricanes of today.

71

CLIMATE OF EXTREMES

Let’s examine the modeled world that Knutson and Tuleya created

and compare it with its real-world counterpart.

Carbon dioxide levels in the modeled atmosphere were increased
at a rate of 1 percent per year. That rate leads to atmospheric carbon
dioxide concentrations 80 years from now (the year that Knutson
and Tuleya compared with current conditions) that are more than
double the levels of today. It was at the end of this 80-year period
when they calculated their changes in the storms.

Eighty years is a long time from now, but the actual time that
these forecast changes would appear could be even further away.
That’s because, in the real world, the concentration of atmospheric
carbon dioxide has been growing at slightly less than half the rate
used by Knutson and Tuleya. In the decade ending in 2004, the
average increase was 0.49 percent per year (despite the rapid indus-
trialization of China and India, the increase in the most recent year,
2006–07, remained at 0.49 percent), the decade before that 0.42 per-
cent, and the one before that (1974–84), 0.48 percent. Obviously
their model is based upon an overestimation of near-term carbon
dioxide growth.

That has important implications. There is a lag time of several
decades between changes in carbon dioxide and its final reflection
in oceanic temperature. The process is somewhat analogous to the
time it takes a pan of water to reach a constant temperature once a
burner is turned on underneath it, although the physical processes
for transferring heat are quite different between a shallow pan and
a deep ocean.

Because the increase in atmospheric carbon dioxide in the real
world isn’t likely to reach 1 percent per year in the near future, that
means that whatever changes in hurricanes are projected by Knutson
and Tuleya for the next several decades have to be overestimations.
So let’s be charitable and say that the 80-year changes they pro-
jected are reached in 100 years. By that time, it is highly likely that
the energy structure of the world will be significantly different than
it is today, possibly with fossil fuels being a curiosity of the past.
Those model-presumed concentrations might never be reached.

The modeled hurricanes grow in a climate that is ideal. Specifi-
cally, there is virtually no change in wind speed or direction with
height. That is called ‘‘wind shear,’’ which basically blows the tops
of the storms, preventing them from becoming well organized. One

72

Hurricane Warning!

El Nin˜ o ⴔ Los Ween˜ os Hurricanes

El Nin˜ o (‘‘the Child’’) is the oxymoronic name given to the
biggest climate phenomenon on earth. It is a periodic distur-
bance that is so strong that it can reverse the largest wind
system on the planet (the trade winds), and turn deserts into
blooming gardens. We provided a brief description of its effects
in chapter 1.

El Nin˜ o has a peculiar effect on hurricanes. The disturbance
of the Eastern Pacific winds carries over into the Atlantic, where
the midatmospheric winds acquire a much stronger westerly
(‘‘from the west’’) component than they would normally have.
This creates a condition called ‘‘wind shear,’’ in which the
wind’s velocity changes considerably with height.

Hurricanes can’t stand wind shear. Hurricanes are a massive
heat engine, with more and more air converged toward a cen-
tral warm core, spun skyward, and whirled away in the outflow
zone. If there’s wind shear, the central circulation becomes
distorted and—often—is completely blown apart, with the top
of a former hurricane over a hundred miles away from the
broken vortex at the surface, which remains visible until its
clouds dissipate. So, El Nin˜ o turns hurricanes into los ween˜ os.
When El Nin˜ o goes away, winds in the midatmosphere in
the tropical Atlantic are more uniform, and there’s usually a
pretty good hurricane season.

phenomenon that is responsible for increasing the vertical wind
shear in the tropical Atlantic is El Nin˜ o.

A number of studies have demonstrated that hurricane activity
in the Atlantic Ocean decreases in years with El Nin˜ os, as well as
the chance that those that do develop will hit the United States.
Some climate models suggest increased El Nin˜ o–like conditions in
the future; others don’t. Knutson and Tuleya assumed that not only
would there be no wind shear changes in the future, but that there
would be virtually no wind shear at all in any of their models. This is the
ideal climate for developing strong hurricanes—with the strength
of the storms largely governed by the temperature of the underlying
ocean surface.

73

CLIMATE OF EXTREMES

Figure 3.1

MODELED SEA-SURFACE TEMPERATURES AND

HURRICANE INTENSITY

SOURCE: Knutson and Tuleya 2004.
NOTE: Storms with lower central pressures are generally stronger.

The authors, in fact, note a strong correlation between sea-surface
temperatures and hurricane intensity—the warmer the sea surface,
the stronger the storm. Figure 3.1 shows an example of the relation-
ship between SSTs and hurricane intensity used by Knutson and
Tuleya. In all their models, sea-surface temperatures alone explain
an average of 55 percent of the changes in simulated hurricane
intensity. The strength is measured by minimum central pressure
(the lower the pressure, the stronger the storm). We show only one
model run, with 64 percent explained variance.

Given that all the global climate models warm up the oceans when
carbon dioxide levels are enhanced (even more so when the rate of
carbon dioxide increase is larger than it actually is!), higher carbon

74

Hurricane Warning!

Explained Variance

What does it mean to say that sea-surface temperatures
explained ‘‘an average of 55 percent of the changes in simulated
hurricane activity?’’ That’s the concept of ‘‘explained vari-
ance,’’ or EV.

EV is a mathematical measure of the correspondence
between two variables. If the EV between, say, sea-surface
temperature and hurricane intensity were 100 percent, a plot
of one vs. the other would correspond to a straight line or
some easily simulated curve. As the explained variance falls,
more and more points fall further and further off the line. When
there’s no explained variance, there’s no line or uncomplicated
curve that the points even appear to line up along.

In our hurricane example, the explained variance between
computer-generated hurricanes and computer-generated sea-
surface temperature was 64 percent, which is a pretty high
number considering how many different factors can influence
hurricane strength. In our study of real-world temperatures
and SST, we found that EV was only 11 percent, or almost six
times less. In other words, in reality, almost 90 percent of the
behavior of hurricanes is determined by factors other than sea-
surface temperature.

dioxide levels leads to higher SSTs, which lead to stronger tropi-
cal cyclones.

But the real world is not so kind to fledgling hurricanes. Though
certainly the temperature of the underlying ocean surface is critical
(the SST must be at least 26.7°C (80°F) for storms to even develop
at all), other factors, such as wind shear, are as important.

Getting Real. . .

Maybe it would be a good idea to look at the relationship between
sea-surface temperature and hurricane strength in the real world.
One of us (Michaels) looked at the number of major hurricanes
(Category 3 or higher) vs. the departure from normal in seasonal
SST back to 1950, and also the average peak wind speed in the five

75

CLIMATE OF EXTREMES

strongest storms in each year. Results are shown in Figure 3.2. These
are two pretty reasonable measures of variation in hurricane strength
from year to year.

The explained variance in this real-world analysis was far below
what Knutson and Tuleya found in the virtual world: 11 percent
(Michaels et al.) vs. 55 percent (the average of all the Knutson and
Tuleya experiments). In other words, when all the factors that influ-
ence hurricanes are allowed to act, as must be the case when looking
at real storms, the influence of SST drops by a factor of five.

More Powerful Storms?

Nine months later, two papers appeared in Nature and Science
within a month of each other, both arguing that hurricanes are
increasing in intensity.

The first, by MIT’s Kerry Emanuel, reported a doubling of the
‘‘power’’ of hurricanes since the mid-1970s. Emanuel’s mathematical
index was based upon the third power (cube) of the hurricane maxi-
mum winds, as well as the frequency and lifetime of storms.

Emanuel reported that there was a significant correlation between
the total power of storms in a year and the sea-surface temperature
in the tropics, as well as the pattern of temperature departures from
average in the North Atlantic and Pacific oceans.

Note the increase since in the number of major (Category 3 or
higher) hurricanes since 1975, as shown in Figure 3.3. Now consult
Figure 3.5 which is the Northern Hemisphere average surface tem-
perature history from the United Nations. It, too, reaches a low point
in 1975. So the earth warms, and the power of hurricanes increases.
Cause and effect or mere correlation? Anything—from hurricanes
to the Consumer Price Index—that has increased since 1975 will
obviously be correlated with global warming. But causation is much
more elusive.

Emanuel’s ‘‘power index’’ is largely determined by the cube of
the total maximum winds in a given year. So if there is a linear rise
(as has been observed) in the number of strong storms in recent
decades, raising that to the third power gives a spectacular increase
in his index.

Emanuel correlated his power index with three factors. He wrote:
‘‘I find that the record of net hurricane power dissipation is highly

76

Hurricane Warning!

Figure 3.2

OBSERVED RELATIONSHIP BETWEEN SEA-SURFACE TEMPERATURES
AND NUMBER OF MAJOR HURRICANES (TOP) AND AVERAGE PEAK
WIND SPEED IN THE FIVE STRONGEST YEARLY STORMS (BOTTOM)

SOURCE: Michaels, Knappenberger, and Landsea 2005.

77

CLIMATE OF EXTREMES

2005: The Biggest Year Ever?

The massive hurricane season of 2005—with 31 identified
tropical storms and hurricanes—certainly got the public’s
attention. Not only were there a lot of storms, but there were
also a lot of powerful ones, and some in pretty prominent
places. Katrina, at the end of August, was a massively large
cyclone. Though it made landfall in southeast Louisana as a
‘‘mere’’ Category 3 storm (down from a Category 5 with
170-mph sustained winds), its huge circulation piled a tremen-
dous amount of water against the Mississippi and Alabama
Gulf Coast, creating a storm surge that greatly exceeded the
previous record-holder, 1969 Category 5 Hurricane Camille,
which also hit the Mississippi coast.

Three weeks later, Hurricane Rita bombed out to Category 5,
took a brief aim at Houston (where many of Katrina’s evacuees
were bivouacked), before also weakening to Category 3 and hit-
ting the Texas–Louisana border. Then, in mid-October Hurricane
Wilma literally exploded in the Western Caribbean, smashing all
the previous records for speed of intensification and for lowest
barometric pressure ever recorded in the Western Hemisphere,
at 26.05 inches of mercury. Physically, that means that Wilma’s
intense cyclone blew away a full 12 percent of the atmosphere
near its center. Wilma’s winds also peaked at 170 mph.

There were also some pretty odd ones. In the far eastern Atlan-
tic, north of the Canary Islands, minimal Hurricane Vince sprung
up and made it to Spain as a tropical storm. Occasionally, weak
tropical cyclones have gotten caught in strong westerly winds
and hit Europe—the last time was in October 1992, when former
hurricane Frances hit Spain. In the blogosphere, Vince’s European
landfall was touted as even more evidence for global warming,
but in reality, it formed over unusually cold water and amelio-
rated a nasty drought in Portugal.

It’s doubtless there have been numerous strikes like Vince,
but before the days of satellites and hurricane-hunter aircraft,
who would know some blustery rainstorm in France was in
fact a former hurricane?

(continued on next page)

78

Hurricane Warning!

(continued)

Is 2005 truly the year with the largest number of tropical

cyclones on record? Hard to say.

There were 21 identified storms in 1933, but no satellites or
airplane sorties. Christopher Landsea, of the National Hurri-
cane Center, has noted that many of the 2005 storms would
have gone undetected back in the 1930s. In fact, examination
of the actual tracks in 2005 indicates that as many as 12 would
not have been reported in 1933, resulting in total of 33 storms
in 1933 with today’s detection technology (Figure 3.4; see
insert). It is plausible that the 1933 total was in fact similar to
the record number recorded in 2005.

Figure 3.3

NUMBER OF TROPICAL STORMS AND HURRICANES (LIGHT GRAY)

AND MAJOR HURRICANES (DARK GRAY)
(CATEGORIES 3, 4, AND 5), 1930–2007

SOURCE: Unisys Weather 2008: http://weather.unisys.com/hurricane/
atlantic/index.html.

correlated with tropical sea-surface temperature, reflecting well-
documented climate signals, including multidecadal oscillations in
the North Atlantic and North Pacific, and global warming.’’

79

CLIMATE OF EXTREMES

NORTHERN HEMISPHERE AVERAGE SURFACE TEMPERATURES,

Figure 3.5

1930–2007

SOURCE: IPCC 2007 and updates.

THE ATLANTIC MULTIDECADAL OSCILLATION, 1870–1999

Figure 3.6

SOURCE: Knight et al. 2005.

The ‘‘oscillation’’ he is talking about in the Atlantic is known
as the Atlantic Multidecadal Oscillation (AMO) and it has a history
that predates global warming (Figure 3.6). The AMO is an index

80

Hurricane Warning!

that reflects the temperature of the sea surface between Greenland
and the Equator. Our figure shows it is hardly constant.

The AMO has long been associated with hurricanes. From the
mid-1920s through the late 1960s, the AMO was in a warm state,
and hurricane activity tended to be high. Then it went negative, and
hurricanes decreased. Suddenly, in 1995, the AMO switched from a
cold to a warm phase, and hurricane activity immediately increased,
which is why some hurricane researchers, such as Christopher Land-
sea of the National Hurricane Center, believe the AMO’s warm
phase is what is primarily responsible for the recent uptick in hurri-
cane activity. In 1995—as soon as he saw the AMO switch—Landsea
actually predicted that hurricane activity was going to pick up, and
that this was likely to continue for decades. His warning—which
was not based on global warming—has proven correct.

What’s the relationship between the AMO and global warming?
Here’s what the U.S. National Oceanic and Atmospheric Administra-
tion has to say:

Instruments have observed AMO cycles only for the last 150
years, not long enough to conclusively answer this question.
However, studies of paleoclimate proxies, such as tree rings
and ice cores, have shown that oscillations similar to those
observed instrumentally have been occurring for at least the
last millennium. This is clearly longer than modern man has
been affecting climate, so the AMO is probably a natural
climate oscillation. In the 20th century, the climate swings
of the AMO have alternately camouflaged and exaggerated
the effects of global warming, and made attribution of global
warming more difficult to ascertain. (http://www.aoml
.noaa.gov/phod/amo faq.php#faq 10)

If Emanuel is correct, damages should be mounting rapidly, given
that a doubling of the strength of hurricanes would be exceedingly
costly. The insured value of property from Brownsville, Texas, to
Eastport, Maine—our hurricane-prone Atlantic Coast—is greater
than a year of our Gross Domestic Product. If hurricanes had actually
doubled in power, the financial effect would be catastrophic.

Roger Pielke Jr., from the University of Colorado in Boulder, has
studied this subject, and his work is well known. Hurricanes are
indeed causing greater dollar damages—because more and more
people are building increasingly expensive beachfront monstrosities

81

CLIMATE OF EXTREMES

that have financially appreciated during the recent real-estate bub-
ble. Account for those, and there is no significant change in hurricane
expenses along our coast. Pielke told us that ‘‘analysis of hurricane
damage over the past century shows no trend in hurricane destruc-
tiveness, once the data are adjusted to account for the dramatic
growth along the nation’s coasts.’’

Just a few weeks later, Peter Webster and colleagues, from Georgia
Institute of Technology, published a paper in Science showing that,
globally, since 1970, the number of tropical cyclones hasn’t changed,
but that the intensity has increased. They did find an increase in
the number of storms in the Atlantic, but no change globally. That
means that the frequency has to be in decline elsewhere. Hurricane
numbers are going down in the North Pacific and Southern Hemi-
sphere oceans. Unlike Emanuel, Webster specifically ruled out the
warming of sea-surface temperatures as a cause:

Only one region, the North Atlantic, shows a statistically
significant increase, which commenced in 1995. However, a
simple attribution of the increase in numbers of storms to a
warming [sea-surface temperature] environment is not sup-
ported, because of the lack of a comparable correlation in
other ocean basins where SST is also increasing.

That statement somehow didn’t make the news reports. But what
did get ink was the increase in intensity. Webster and his colleagues
reported that the number of weak (Category 1) hurricanes had
declined since 1970, that Categories 2 and 3 had shown no net
change, and that the number of severe (Category 4 and 5) hurricanes
had increased.

In the early 1970s, approximately 45 percent of all storms globally
were Category 1. Category 2 and 3 storms contributed another
40 percent, and the severe Category 4 and 5 storms made up the
remaining 15 percent. In the early 2000s, however, the annual contri-
butions from those three groups was approximately equal. That
made the news. Webster et al. had also reported that their result
was ‘‘not inconsistent’’ with ‘‘recent climate model simulations.’’

Other models say otherwise. Masato Sugi, who heads up hurricane
research at Japan’s Meteorological Research Institute (a government
entity), has run a global climate model to simulate hurricane behav-
ior in a warming world, and found that tropical cyclone frequency

82

Hurricane Warning!

decreases globally while there is no average change in intensity. The
sum-total of that would be a decrease in destructive potential.

Lennart Bengtsson, of Germany’s Max Planck Institut, has pub-
lished multiple papers using computer models that project decreases
in hurricane intensity or numbers. The first of those appeared in 1996.
More recently (and after Webster’s publication), Akira Hasegawa
of the Japan Agency for Marine-Earth Science and Technology simu-
lated a decrease in both the intensity and the frequency of tropical
cyclones in a warming world.

All in all, given that there are such conflicting studies on climate
change and hurricanes, it would be fairer to say that Webster’s
finding is either consistent or inconsistent with recent climate model
simulations.

Thanks to all the publicity, the UN’s World Meteorological Organi-
zation issued a ‘‘Statement on Tropical Cyclones and Climate
Change’’ in November 2006:

During 2005 two highly publicized scientific papers appeared
documenting evidence from the observational record for an
increase in tropical cyclone activity. [The report then
describes the Emanuel and Webster papers]. . . . Currently
published theory and numerical modeling results suggest [a
relatively small increase in tropical cyclone intensities several
decades in the future], which is inconsistent with the observa-
tional studies of Emanuel (2005) and Webster et al. (2005)
by a factor of 5 to 8 (for the Emanuel study) . . . this is
still hotly debated area [sic] for which we can provide no
definitive conclusion.

The problem with Webster et al. is the start date. Webster and his
colleagues started in 1970 because that’s the first year of satellite
coverage. That’s also very close to the start time for the warming
that has been observed since 1975. That makes for a lot of correlation,
but not a lot of causation.

Pielke and four very prominent coauthors1 published a much
different study in the Bulletin of the American Meteorological Society
in 2005. Here’s their major conclusion:

1 Pielke’s coauthors were Christopher Landsea, a leading researcher on hurricanes
and climate; Max Mayfield, former Director of the National Hurricane Center; James
Laver, head of the federal Climate Prediction Center; and Richard Pasch, hurricane
specialist at the National Hurricane Center.

83

CLIMATE OF EXTREMES

To summarize, claims of linkages between global warming
and hurricane impacts are premature for three reasons. First,
no connection has been established between greenhouse gas
emissions and the observed behavior of hurricanes
(Houghton et al. 2001; Walsh 2004). Emanuel (2005) is sugges-
tive of such a connection, but is by no means definitive. In
the future, such a connection may be established [e.g., in the
case of the observations of Emanuel (2005) or the projections
of Knutson and Tuleya (2004)] or made in the context of
other metrics of tropical cyclone intensity and duration that
remain to be closely examined. Second, the peer-reviewed
literature reflects that a scientific consensus exists that any
future changes in hurricane intensities will likely be small
in the context of observed variability (Knutson and Tuleya
2004; Henderson-Sellers et al. 1998), while the scientific prob-
lem of tropical cyclogenesis [formation] is so far from being
solved that little can be said about possible changes in fre-
quency. And third, under the assumptions of the IPCC,
expected future damages to society of its projected changes
in the behavior of hurricanes are dwarfed by the influence
of its own projections of growing wealth and population
(Pielke et al. 2000). While future research or experience may
yet overturn these conclusions, the state of the peer-reviewed
knowledge today is such that there are good reasons to expect
that any conclusive connection between global warming and
hurricanes or their impacts will not be made in the near term.

Pielke, a professor of environmental studies at University of Colo-
rado, is no political neophyte. He worked for the late Congressman
George Brown (D-CA), the powerful chair of the House Science
Committee. He’s a self-described ‘‘Blue-Dog Democrat,’’ and he has
written extensively on the interactions among science, policy, scien-
tists, and society. His website ‘‘Prometheus: the Science Policy
Weblog’’ (http://sciencpolicy.colorado.edu/prometheus) probably
has the best discussions in cyberspace on this nexus.

At any rate, Pielke et al. were not reluctant to share their feelings
about the political misuse of hurricanes and global warming. Here’s
an excerpt from the next paragraph in their paper:

Yet claims of such connections persist (cf. Epstein and McCar-
thy 2004; Eilperin 2005), particularly in support of a political
agenda focused on greenhouse gas emissions reduction (e.g.,
Harvard Medical School 2004). But a great irony here is
that invoking the modulation of future hurricanes to justify

84

Hurricane Warning!

energy policies to mitigate climate change may prove count-
erproductive. Not only does this provide a great opening for
criticism of the underlying scientific reasoning, it leads to
advocacy of policies that simply will not be effective with
respect to addressing future hurricane impacts. There are
much, much better ways to deal with the threat of hurricanes
than with energy policies (e.g., Pielke and Pielke 1997). There
are also much, much better ways to justify climate mitigation
policies than with hurricanes (e.g., Rayner 2004).

Pielke’s paper obviously ruffled some academic egos. Even before
it was published, Kevin Trenberth from the National Center for
Atmospheric Research (also in Boulder) told the local newspaper, ‘‘I
think [Pielke] should withdraw his article. This is a shameful article.’’
‘‘Shameful’’? In fact, Pielke’s logic is quite sound. If there were a
strong link between global warming and hurricanes, which costs
more? Adaptation to them, or a futile attempt to stop warming? The
latter takes away resources from the former, while accomplishing
nothing.

Dozens of news stories about the hyperactive 2005 hurricane sea-
son mentioned that ocean temperatures in the Atlantic Basin were
very warm that year, and, by implication, global warming juiced
up the monster storms. Is that really the case?

In spring of 2006, one of us (Michaels) published a paper in Geo-
physical Research Letters showing no relationship between the maxi-
mum sea-surface temperature over which a hurricane passes and
that maximum winds observed in the life of strong hurricanes. All
storms were studied that experienced waters of 28.25°C (82.9°F),
the threshold that is required for Category 3 (‘‘major’’) hurricanes.
Temperatures in the Gulf of Mexico, and much of the southwestern
Atlantic Ocean, exceed this value for many months every year.

Kerry Emanuel responded that we hadn’t looked at enough hurri-
canes. In fact, we had looked at all storms that experienced this
warm water since 1982, the year that an appropriate record of ocean
temperatures begins. There were 195, an average of about 9 a year.
Emanuel was able to generate a significant relationship by manufac-
turing 3,000 computer-generated hurricanes.

Philip Klotzbach of Colorado State University then published a
paper in Geophysical Research Letters that examined ‘‘worldwide trop-
ical cyclone frequency and intensity to determine trends in activity
over the past 20 years during which there has been an approximate

85

CLIMATE OF EXTREMES

Living with Hurricanes

On September 28, 1955, a Category 5 hurricane named Janet
slammed into Chetumal, on Mexico’s Yucatan Peninsula, kill-
ing more than 600 people.

On August 21, 2007, Hurricane Dean, another Category 5
and the third-strongest storm ever measured at landfall, hit
within a few miles of where Janet struck, and killed no one.
(Mountain flooding from the storm’s remnants resulted in eight
fatalities well inland). Maximum winds in Janet and Dean were
the same. Dean most likely marked the first instance in human
history in which a Category 5 hurricane hit a populated coast
and everyone lived.

In 2005, Hurricane Wilma, a Category 4 storm (the same
intensity of cyclone that killed 7,000 people in Galveston, Texas,
in 1900), hit the tourist-heavy northeast corner of the Yucatan
and killed only four people.

Because of its peculiar location, the Yucatan takes more big
hurricane hits than just about anywhere else in the Western
Hemisphere. When Mexico was dirt-poor, as it was in 1955,
hurricanes could kill hundreds. They were warned then, too.
Hurricane-hunter planes also monitored Janet. Only one of those
has ever been lost, and it was as Janet was making landfall.

Similar storms. Huge storms. Very different results. What

changed?

Prior to the development of tropical meteorology in the mid-
20th century, storms such as these used to kill hundreds, even
thousands, as they zeroed in on unsuspecting populations. But
we now have the technology to forecast their tracks, at least
for the critical last 48 hours, with reasonable confidence. That
gives people time to evacuate. Economic development gives
people the infrastructure necessary to accommodate evacua-
tion. When Janet killed hundreds, per capita income in Mexico
was less than a tenth of what it is now.

Will global warming change this? Note that Knutson and
Tuleya calculated that maximum winds should increase by
about 6 percent over the next 75 years. Even that may be
(continued on next page)

86

Hurricane Warning!

(continued)

an overestimate because they assume that carbon dioxide is
increasing in the atmosphere about twice as fast as it actually is.
Clearly, this small increase in hurricane strength is going to
be dramatically overshadowed by adaptation as the developing
world continues to develop. Mexico is a case in point.

Anyone concerned about climate change should take a lesson
from Hurricane Dean. Even if storms like that one become
more frequent in the future, people will adapt and survive—
provided they have sufficient financial resources. How silly it
seems to take resources away in futile attempts to ‘‘stop global
warming’’ when those same resources can be directed toward
adaptation, including infrastructure and hurricane-proof
housing.

The truth is that money in the hand is a lot more useful than
treaties on paper when it comes to adapting to severe weather.
So people truly worried about climate change should be cheer-
leading for economic development, which provides the resources
necessary to accommodate even the strongest hurricanes.

0.2°C to 0.4°C [0.4°F to 0.7°F] warming of SSTs.’’ Klotzbach found
‘‘a large increasing trend in tropical cyclone intensity and longevity
for the North Atlantic basin and a considerable decreasing trend for
the North Pacific.’’ The increase in the Atlantic was exactly the same
as the decrease observed in the Northeast Pacific ocean (Figure
3.7). Other tropical cyclone-producing ocean basins showed only
small variations.

Overall, Klotzbach noted ‘‘no significant change in global net
tropical cyclone activity’’ but a ‘‘small increase in global Category
4–5 hurricanes from the period 1986–95 to the period 1996–2005.’’
His metric was ‘‘Accumulated Cyclone Energy’’ (ACE), an integrated
measure of total storm strength in a year. From this analysis, he
concluded that factors other than sea-surface temperatures are
important in governing tropical cyclone frequency and intensity and
noted the likelihood that ‘‘improved observational technology’’ has
also had an influence on the small increases that he did observe.

87

CLIMATE OF EXTREMES

Figure 3.7

ACCUMULATED CYCLONE ENERGY (ACE) INDEX FOR THE

WORLD’S HURRICANE BASINS, 1986–2005

SOURCE: Klotzbach and Gray 2006.

Klotzbach ultimately summed up his findings as

. . . contradictory to the conclusions drawn by Emanuel (2005)
and Webster et al. (2005). They do not support the argument

88

Hurricane Warning!

that global [tropical cyclone] frequency, intensity, and lon-
gevity have undergone increases in recent years. Utilizing
global ‘‘best track’’ data, there has been no significant increas-
ing trend in ACE and only a small increase (⬃10 percent) in
Category 4–5 hurricanes over the past 20 years, despite an
increase in the trend of warming sea-surface temperatures
during this time period.

Two more major hurricane papers followed in fall 2006. First,
Klotzbach (along with Colorado State University’s William Gray)
examined the very destructive 2004 season.

They noted, as Klotzbach had in his earlier paper, that there was
an increase in activity in the Atlantic beginning in 1995, but that
there were equivalent or greater declines in the rest of the world.
They attributed those findings to changes in the distribution of tem-
perature in the Atlantic, rather than to global warming. They also
echoed the conclusions of Pielke et al.: ‘‘Due to increased coastal
population and wealth, the U.S. coastline can expect hurricane-
spawned damage and destruction in the coming few decades to be
on a scale much greater than has occurred in the past.’’

Chris Landsea et al. then weighed in, in Science, asking whether
the data for tropical cyclones are in fact reliable enough to be used
to detect long-term trends.

Before landfall, hurricane intensities are either measured by hurri-
cane-hunter aircraft or by satellite. Only two regions, the western
Atlantic and western Pacific, have had regular aircraft reconnais-
sance, which provides a fairly homogenous set of data back to at
least 1960. Satellite monitoring began in 1970, but the onboard instru-
mentation has improved over time, with more recent orbiters able
to provide higher-resolution images and direct views of storms that
allow for more accurate estimates of highest winds. So, any histories
that are primarily satellite-based are likely to show an artificial
upward trend in intensity. The aircraft-based histories show no sig-
nificant trends at all. Landsea and his colleagues concluded ‘‘that
extreme tropical cyclones and overall tropical cyclone activity have
globally been flat from 1986 until 2005, despite a sea-surface tempera-
ture warming of 0.25°C [0.45°F].’’

The more you look, the less obvious it becomes that anthropogenic
global warming has significantly (i.e., measurably) contributed to
the current increase in hurricane activity in the North Atlantic basin,
or anywhere else in the world, for that matter.

89

CLIMATE OF EXTREMES

270 Years of Hurricane History!

Sounds like the same-old same-old. An article appeared in a 2007
issue of Nature with the first sentence in the abstract stating, ‘‘Hurri-
cane activity in the North Atlantic Ocean has increased significantly
since 1995.’’ One can only guess the dire global warming news
to follow.

Johan Nyberg, from the Geological Survey of Sweden, and his

coauthors began with this:

The years from 1995 to 2005 experienced an average of 4.1
major Atlantic hurricanes (Category 3 to 5) per year, while
the years 1971 to 1994 experienced an average of 1.5 major
hurricanes per year. This increase in major hurricane fre-
quency is thought to be caused by weaker vertical wind
shear [the strength of winds with height] and warmer sea-
surface temperatures (SSTs) in the tropical and subtropical
Atlantic.

The title of the Nyberg et al. article, ‘‘Low Atlantic Hurricane
Activity in the 1970s and 1980s Compared to the Past 270 Years,’’
indicates that hurricane activity was low in the 1970s and 1980s
compared with the past 270 years. Could it be that what we are
seeing now is actually a return to more normal conditions?

How does one get a 270-year history of hurricanes? Corals growing
in the Caribbean Sea preserve a year-to-year luminescence intensity
(something like color differences), and as noted by Nyberg et al.,
‘‘luminescence intensity in corals reflects the degree of terrestrial
water runoff, a result of low precipitation, which is highly correlated
with low hurricane activity.’’ Nyberg and colleagues also examined
plankton from a sediment core from the Caribbean. Certain plankton
are associated with weaker hurricane regimes. The deeper the plank-
ton are buried, the older they are. They presented convincing
evidence that the corals and plankton accurately reflect hurricane
activity during the period of reliable records (Figure 3.8). Their
conclusion:

The record indicates that the average frequency of major
hurricanes decreased gradually from the 1760s until the early
1990s, reaching anomalously low values during the 1970s
and 1980s. Furthermore, the phase of enhanced hurricane
activity since 1995 is not unusual compared to other periods
of high hurricane activity in the record and thus appears to

90

Hurricane Warning!

OBSERVED MAJOR HURRICANES AND ‘‘RECONSTRUCTED’’ MAJOR

HURRICANES BASED ON CORAL RECORDS, 1740–2005

Figure 3.8

SOURCE: Adapted from Nyberg et al. 2007.
NOTE: The dashed lines are the 95 percent confidence intervals about each
reconstructed value.

represent a recovery to normal hurricane activity, rather than
a direct response to increasing sea-surface temperature.

More specifically, they note:

Only the periods ⬃1730–1736, 1793–1799, 1827–1830,
1852–1866 and 1915–1926 appear to have been marked by
similarly low major hurricane activity. . . . Furthermore, the
current active phase (1995–2005) is unexceptional compared
to the other high-activity periods of ⬃1756–1774, 1780–1785,
1801–1812, 1840–1850, 1873–1890, and 1928–1933, and
appears to represent a recovery to normal hurricane activity,
despite the increase in SST.

Instead of being unusually active, it looks like the current hurri-
cane regime is simply a return to more normal conditions, following
an unusually tranquil couple of decades.

800 Years of Hurricane History!

Another long-term record of hurricanes comes from Australia
(hurricanes are called ‘‘tropical cyclones’’ in Australia), where their
passages are recorded in caves.

91

CLIMATE OF EXTREMES

Stalagmites growing upward in caves can be ‘‘dated’’ with each
year’s rainy season. Jonathan Nott of Australia’s James Cook Univer-
sity used them to create a record of tropical cyclone activity in
northeastern Australia for the last 800 years. He published his record
in a 2007 edition of Earth and Planetary Science Letters.

Let’s pause for a science lesson: Water can contain two different
isotopes of oxygen. Almost all the oxygen in water has a molecular
weight of 16 (8 protons, 8 neutrons, remember?). But a very small
fraction incorporates Oxygen-18, which contains two extra neutrons.
What does this have to do with hurricanes? Hurricanes are a major
source of very dense, high cloudiness in that part of Australia, and
rain that forms at high altitude contains very little Oxygen-18. Years
in which there is very little of that isotope incorporated in a
stalagmite will likely be years in which there was major hurricane
activity.

A cave in nearby Chillagoe (see map, Figure 3.9) is full of upward-
growing stalagmites, and the water from tropical cyclones contri-
butes to the growth of each year’s layer.

The Australian Meteorological Office has kept an excellent obser-
vational record of tropical cyclone activity in the region from 1907
to 2003. Consequently, they could compare yearly Oxygen-18 values
in the stalagmites with the actual hurricane frequency. The research-
ers found that each peak in the depletion of this isotope ‘‘corresponds
to the passage of a cyclone within 400 km (250 miles) of Chillagoe.’’
There were 27 such storms in the record, and many passed much
more closely. Those storms accounted for 63 percent of all the hurri-
canes that passed within 200 km (125 miles) of the cave.

Nott et al. noted:

Despite the absence of many cyclones, it is important to note
that every intense cyclone (i.e., AD 1911, 1918, 1925, 1934,
1986 as determined by barometer or damage to urban infra-
structure and loss of life) to make landfall in the [400 km
region] since AD 1907 is registered by a peak in the isotope
depletion curve.

This is an amazing history. Figure 3.10 shows the tropical cyclone
record from the Chillagoe cave. Note that the current era (1800 to
present) is pretty wimpy in the broad historical sweep. The solid
black line in Figure 3.10 is the threshold for an extreme storm,
matching the great 1911 cyclone. Note that it is the only event of

92

Hurricane Warning!

Figure 3.9

NORTHEASTERN AUSTRALIA, THE LOCATION OF

NOTT’S CAVE STUDY SITE

SOURCE: Nott et al. 2007.
NOTE: 1 kilometer ⳱ 0.62 miles.

such magnitude in the past 200 years, but that there were seven
such storms in the previous two centuries.

5,000 Years of Hurricane History!

The marine forest at Vieques, a few miles east of Puerto Rico, has
a nice tropical beach backed by a vegetated barrier ridge about
10 feet tall. Behind the ridge is a back-barrier lagoon that over
time became a ‘‘playa,’’ which is a flat-bottomed feature that is
occasionally covered with water—such as when hurricanes flood it.
During large hurricanes, which are fairly common in that area, the
ridge is breached and a large amount of material is deposited on
the playa. Brown University’s Jeffrey Donnelly and Woods Hole
Oceanographic Institution’s J. D. Woodruff extracted cores from the
playa, noting:

93

CLIMATE OF EXTREMES

STRENGTH INDEX OF TROPICAL CYCLONE EVENTS, 1226–2003

Figure 3.10

SOURCE: Adapted from Nott et al. 2007.

Cores collected from the site contain several metres of
organic-rich silt interbedded with coarse-grained event lay-
ers comprised of a mixture of siliciclastic sand and calcium
carbonate shells and shell fragments. These layers are the
result of marine flooding events overtopping or breaching
the barrier and transporting these barrier and nearshore sedi-
ments into the lagoon.

Organic material can be dated with commonly used techniques
(such as carbon dating, which has been employed for decades), and
just like magic, a long-term record of intense hurricane activity
is produced.

Donnelly and Woodruff reported:

On the basis of our age model an interval of relatively fre-
quent intense hurricane strikes at Vieques is evident between
5,400 and 3,600 calendar years before present (‘‘yr BP,’’ where
present is defined as AD 1950 by convention), with the excep-
tion of a short-lived quiescent interval between approxi-
mately 4,900 and 5,050 yr BP. Following this relatively active
period is an interval of relatively few extreme coastal flood-
ing events persisting from 3,600 until roughly 2,500 yr BP.

94

Hurricane Warning!

Evidence of another relatively active interval of intense hurri-
cane strikes is evident between 2,500 and approximately
1,000 yr BP. The interval from 1,000 to 250 yr BP was relatively
quiescent with evidence of only one prominent event occur-
ring around 500 yr BP. A relatively active regime has resumed
since about 250 yr BP (1700 AD).

With respect to the linkage between higher sea-surface tempera-

tures and hurricane activity, the pair notes:

Given the increase of intense hurricane landfalls during the
later half of the Little Ice Age (around AD 1700), tropical
SSTs as warm as at present are apparently not a requisite
condition for increased intense hurricane activity. In addi-
tion, the Caribbean experienced a relatively active interval
of intense hurricanes for more than a millennium when local
SSTs were on average cooler than modern.

The authors obviously can’t finger global warming as the cause,
but instead cite variations in El Nin˜ o and African disturbances that
for years have been associated with hurricane frequency.

Should another unusually intense hurricane season (such as 2005)
spin up in the Atlantic, global warming advocates will be in front
of every camera in sight claiming we are witnessing yet another
manifestation of global warming. Why, then, was there more hurri-
cane activity when the planet was cooler?

Endnote: Hurricanes in the Big Apple!

We started this chapter with Katrina in New Orleans. But if a
large hurricane strikes New York City, the amount of devastation
could be incredible, as well as the hue and cry blaming global warm-
ing. Perhaps this will be the $500 billion hurricane. (The costliest
storm, assuming today’s property values and population, was the
1926 hurricane that struck southeast Florida and Alabama, at
$164 billion) But could a hurricane really devastate the Big Apple?
Indeed it could. The New York area has been struck many times
in the past by tropical cyclones, so it’s just a matter of time before
another one passes directly over the city.

A recent article in Geochemistry, Geophysics, Geosystems by geologi-
cal scientists at Brown University and Woods Hole Oceanographic

95

CLIMATE OF EXTREMES

Figure 3.11

STORM SURGE HEIGHTS, 1788 TO PRESENT

SOURCE: Adapted from Scileppi and Donnelly 2007.
NOTE: Storm surge heights relative to the modern mean sea level that accom-
panied the 1788, 1821, and 1893 hurricanes are inferred from historic archives
and records of the most extreme flooding events of the 20th century recorded
by the Battery Park, New York City, tide gauge from 1920 to present.

Institution focuses on hurricanes in the New York City area—specifi-
cally western Long Island. Elyse Scileppi and the aforementioned
Jeffrey Donnelly begin their article by noting:

Historical records show that New York City is at risk of being
struck by a hurricane. Four documented strong hurricanes
(Category 2 or higher on the Saffir-Simpson Scale) with high
storm surges (⬃3 m) [10 feet] have made landfall in the New
York City area since 1693 with the last occurring in 1893.
Population growth during the 20th century has significantly
increased the risk to lives and property should a strong hurri-
cane recur today. The frequency of hurricane landfalls is
difficult to estimate from the instrumental and documentary
records due to the relative rarity of these events and the
short historical observation period.

Hurricanes scour Long Island’s beaches, and their storm surges
then deposit sand inland, in muddy marshlands. The sand layers
are pretty obvious, and they can be dated using a variety of methods.
The largest inundations were in 1788, 1821, and 1893 (Figure 3.11).
The three large surges occurred during a time when the Northern
Hemisphere was considerably cooler than it is now.

96

Hurricane Warning!

TRACKS OF FIVE HURRICANES AFFECTING THE WESTERN LONG

ISLAND, NEW YORK, REGION, 1788–1985

Figure 3.12

SOURCE: Adapted from Scileppi and Donnelly 2007.
NOTE: The dark tracks (1788, 1821, and 1893) indicate full-impact hurricanes.
The gray track (1985) indicates a near-miss—in this case, Hurricane Gloria.
The dashed track (1693) indicates uncertainty about the hurricane’s path.

In terms of actual hurricane activity (Figure 3.12), they state: ‘‘Four
historically documented hurricanes that caused approximately 3 m
[10 feet] of storm surge made landfall in the New York City area in
1893, 1821, 1788, and likely 1693.’’ They correctly note that things
were quite a bit cooler than today:

Interestingly, several major hurricanes occur in the western
Long Island record during the latter part of the Little Ice
Age (⬃1550–1850 A.D.) when SSTs were generally colder
than present. According to paleoclimate estimates, SSTs were

97

CLIMATE OF EXTREMES

likely 2°C cooler than present in the Caribbean, 1°C cooler
than present in the Florida Keys during the latter part of the
Little Ice Age, and 1°C cooler than present during the 17th
and 18th centuries at the Bermuda Rise.

The authors note, ‘‘Despite significantly cooler than modern [sea-
surface temperatures] in the Atlantic during the latter half of the
Little Ice Age, the frequency of intense hurricane landfalls increased
during this time.’’

If they had found increasing New York hurricanes, they would
have been paraded right down Broadway. But finding increased
activity in colder periods certainly rained on that parade!

Summing Up Hurricanes

The hurricane–global warming link is clearly much more compli-
cated than the simple ‘‘warming in—bigger hurricanes out’’ stories
that abound today. It’s not at all clear that a warmer world will have
more storms, and many studies indicate that increases in hurricane
strength will be hard to detect because of the tremendous year-
to-year natural variability. We don’t even have decent hurricane
histories, because satellite coverage—the only way to truly measure
global activity—has only been maintained for 35 years. Aircraft have
investigated storms in the Atlantic and Western Pacific since World
War II, but coverage was certainly not complete, and may not be
reliable before 1960. Long-term geological records for individual
sites can be used to find evidence for storms as far back as 5,000
years ago, and those histories indicate that there is nothing really
unusual about the current hurricane regime.

That’s all a far cry from the current noise about tropical cyclones
and global warming. Although these scientific findings are there for
all to see in the refereed literature, they certainly have received a
lot less coverage then their more gloomy counterparts.

98

4. Sea-Level Rise and the Great

Unfreezing World

Warning: You’re going to read about data sets ‘‘guaranteed’’ to
show large losses of Arctic ice, a newly discovered ‘‘island’’ uncov-
ered by global warming (that was actually an island a mere half-
century ago), and a scientific ‘‘urban legend’’ that almost all of
Greenland’s ice is going to crash into the sea, pronto.

Horrifying images of Greenland’s ice sliding into the sea and
raising the sea-level 10 or more feet by the year 2100 are common
now. They owe their viability to one NASA scientist: James Hansen.

Hansen’s Scenario

James E. Hansen, director of the NASA Goddard Institute for
Space Studies, is the clear progenitor of the modern apocalyptic
theory of climate change. His disaster hypothesis first appeared in
2004 in Scientific American (which does not vet its articles via the
peer review process that academic journals use), and then he swore
by it in two legal proceedings.

Hansen was involved in two important court cases in 2006–07.
The first was in California, in which ‘‘Central Valley Chrysler-Jeep’’
sued the state of California, claiming that regulations for carbon
dioxide emissions promulgated by the California Air Resources
Board (at the direction of the California Legislature) would be impos-
sible to meet and would result in their bankruptcy. The Vermont
legislature then agreed to do everything that California would do,
so there was another suit in Vermont. The California judge continued
the case until Massachusetts v. EPA (see chapter 7) was settled by
the U.S. Supreme Court; but the Vermont case went forward.

Hansen testified that if there were even less than 1°C [1.8°F] of
post-2000 warming, then there was the ‘‘possibility of initiating ice
sheet response that begins to run out of control’’ with ultimately
‘‘several’’ meters of sea-level rise. If warming proceeded according
to the midrange estimate for carbon dioxide changes (Figure 1.5; see

99

CLIMATE OF EXTREMES

insert), Hansen stated that a sea-level rise of 6 meters (roughly 20
feet) by 2100 would be within the confidence limits of his estimate.
Note how far Hansen’s predictions are from those of the Intergov-
ernmental Panel on Climate Change, which says that the Greenland
contribution to sea-level rise by 2100 is likely to be around two
inches. (The IPCC puts in a small caveat that their estimate does not
take into account changes in the ice that have not been modeled.)
Hansen even has an explanation for why climate scientists don’t
support his position. In a 2007 essay in Environmental Research Letters,
Hansen claimed the reason for this was something he called ‘‘scien-
tific reticence,’’ or the desire of scientists to not publish or speak of
bad news. As will be seen in chapter 7, quite the opposite is true.
At any rate, Hansen is by far the most quoted climate researcher
in the world on Greenland (Google it, and you’ll get about 80,000
hits for ‘‘James Hansen Ⳮ Greenland’’), despite his repeated protes-
tations that he is being prevented from speaking out.

The Greenland myth became video in Al Gore’s An Inconvenient
Truth, in which he shows a montage of Florida being slowly sub-
merged as Greenland loses its ice. Is that truth or fiction?

The IPCC’s 2007 ‘‘Fourth Assessment Report’’ projects sea-level
rise of between 8.5 and 18.5 inches for the 21st century for its ‘‘mid-
range’’ estimate of carbon dioxide and other greenhouse gas
emissions.

At the top end, that represents a 32 percent reduction in estimated
sea-level rise for the century (down from 27 inches) from its ‘‘Third
Assessment Report,’’ published in 2001. The mean, or central, value
is 13.5 inches.

Of that amount, 66 percent of the rise, or 8.8 inches, is from
expansion of warm water. That is directly proportional to the
expected temperature rise in global temperature. The midrange
emission scenario (Figure 1.5; see insert) results in an average mod-
eled warming of approximately 4.9°F (2.7°C) between 2000 and 2100.
If, as we argued in chapter 1, the warming is likely to be less, around
3.2°F (1.75°C), then the sea-level rise from thermal expansion will
drop proportionally, to about 5.7 inches.
Another Perspective

In the ‘‘Policymaker’s Summary’’ of its 2007 science compendium,

the IPCC states:

Global average sea-level rose at an average rate of 1.8 [1.3
to 2.3] mm per year over 1961 to 2003. The rate was faster

100

Sea-Level Rise and the Great Unfreezing World

over 1993 to 2003: about 3.1 [2.4 to 3.8] mm per year. Whether
the faster rate for 1993 to 2003 reflects decadal variability or
an increase in the longer-term trend is unclear.

One problem with science compendia such as the IPCC reports
is that they must have discrete ‘‘cut-off’’ dates beyond which they
include no published science. Otherwise, reports would be in a
continuous state of revision.

It’s too bad. A 2007 article by G. B. Wo¨ppelmann and others
published in Global and Planetary Change was beyond the cut-off,
and could have changed the IPCC’s speculation that the rate of sea-
level rise may be increasing.

Measuring sea-level rise is far from simple. One main reason is
the (geologically) recent ice age. The enormous ice sheets that cov-
ered much of our hemisphere pushed down on the crust, and the
recovery process is slow. In fact, the crust is still rising. Scientists
attempt to account for this effect using numerical ‘‘Glacial-Isostatic
Adjustment’’ routines in their estimates of true sea-level rise. But
movements of the continental plates, wind and ocean currents, and
differing magnitudes of gravity also confound measurement of true
sea level.

Wo¨ppelmann et al. note that

two important problems arise when using tide gauges to
estimate the rate of global sea-level rise. The first is the fact
that tide gauges measure sea level relative to a point attached
to the land which can move vertically at rates comparable
to the long-term sea-level signal. The second problem is the
spatial distribution of the tide gauges, in particular those
with long records, which are restricted to the coastlines.

Chances are that you now have a global positioning systems (GPS)
unit in your car or boat, a GPS upgrade for your cell phone, or a hand-
held GPS unit for hiking. GPS satellites are taking measurements of
anything and everything, and data from advanced GPS networks
now resolve questions about sea-level rise. Noting this new source
of objective data, Wo¨ppelmann et al. analyzed 224 GPS stations;
160 were located within 15 km [9.3 miles] of a tide gauge station
(Figure 4.1). The data allowed them to very accurately measure the
vertical motion of the crust from January 1999 to August 2005, and
although the 7.7-year time span would seem rather short, they effec-
tively argue that vertical motion of the crust is not like the weather—
the vertical motion remains the same over long periods of time.

101

CLIMATE OF EXTREMES

Figure 4.1

DISTRIBUTION OF 224 GPS STATIONS PROCESSED BY

WO¨ PPELMANN ET AL., 2007

SOURCE: Wo¨ppelmann et al. 2007.
NOTE: Stars are GPS stations less than 15 km (9.3 miles) from a tide gauge.
Dots are continental stations.

When Wo¨ppelmann et al. factored their measurements of land
motion into the estimate of sea-level rise, they determined a global
value of 1.31 Ⳳ 0.30 mm per year (0.05 Ⳳ 0.01 inches) compared
with the 3.1 mm value given for recent years by the UN.

Where’s the headline? ‘‘Objective Measurements Reduce Recent
Sea-Level Rise by Nearly 70 Percent!’’ Of course, this book is about
the asymmetry between global warming science and what the public
ultimately hears. It’s a good bet that, had Wo¨ppelmann et al. found
that sea level was rising at a rate 70 percent higher than the IPCC
estimated, their findings would be on the front page of every news-
paper in the world.

Greenland’s ice sheets and glaciers make up the largest ice mass
in the Northern Hemisphere, some 2.85 ⳯ 106 cubic kilometers (6.8
⳯ 105 cubic miles), or 9.9 percent of total global ice volume. Together,
Greenland and Antarctica hold 99.4 percent of the world’s ice. The
remaining nonpolar ice volume, including the vast Himalayan Ice
Cap, is a mere 0.6 percent.

102

Sea-Level Rise and the Great Unfreezing World

A 2006 Science paper by Eric Rignot and Pannir Kanagaratnam
received a tremendous amount of publicity when it claimed that
there has been a widespread and accelerating loss of Greenland’s
peripheral glaciers during the past 10 years, and increasing runoff
from the main ice sheet, as measured by satellites. The rate given
was 224 Ⳳ 41 cubic kilometers (53Ⳳ10 cubic miles) per year for 2005.
For comparative purposes, the Greenland ice mass given above,
in standard numerical notation, is 2,850,000 cubic kilometers
(685,000 cubic miles), yielding a loss of eight-thousandths of a per-
cent per year. That translates into a sea-level rise of two-hundredths
of an inch per year.

Amazingly, there was no reference in this paper to Ola Johannes-
sen’s 2005 paper, in the same journal, that showed that the Greenland
ice cap is accumulating at a rate of 5.4 Ⳳ 0.2 centimeters per year
(2.1 Ⳳ 0.1 inches) That increase in the elevation of the ice cap was
measured by the very same satellites that Rignot and Kanagarat-
nam used!

What’s the difference? Rignot and Kanagaratnam combined obser-
vations of ice loss from the coastal glaciers with models of changes
over the inland ice cap, whereas Johannessen et al. observed changes
in the ice cap directly. Johannessen et al. found that the rise in ice-
cap elevation converts to about 75 cubic kilometers (18 cubic miles)
per year. Had Rignot and Kanagaratnam used real data as opposed
to a computer simulation, they would have found that any loss of
Greenland ice had occurred only in the last five years (it was gaining
ice before then, even after accounting for the loss from the glaciers),
and the total loss would be around 93 cubic kilometers (22.3 cubic
miles), which is slightly more than 40 percent of the already tiny
loss they originally found.

Figure 4.2 displays the temperature history for southern Green-
land from the Danish Meteorological Institute, from 1782 through
2007 (understandably, some years in the late 18th and early 19th
century don’t have enough data). That is the area with the greatest
glacial retreat. Note that temperatures from 1925 through roughly
1960 were generally higher than they are today.

Writing about the mass balance of Greenland ice in Science in

2000, Krabill et al. said:

Greenland temperature records from 1900–1995 [note: Figure
4.2 is through 2007] show the highest summer temperatures

103

CLIMATE OF EXTREMES

SOUTHERN GREENLAND TEMPERATURES, 1782–2007

Figure 4.2

SOURCE: Danish Meteorological Institute 2008: http://www.dmi.dk/dmi/
tr08-40.pdf.

in the 1930s, followed by a steady decline until the early
1970s and a slow increase since. The 1980s and 1990s were
about half a degree colder than the 96-year mean. Conse-
quently, if present-day thinning is attributable to warmer
temperatures, thinning must have been even higher earlier
this century.

In 2006, Petr Chylek, from the Los Alamos National Laboratory,

and colleagues wrote:

Since 1940, however, the Greenland coastal stations have
undergone predominantly a cooling trend. At the summit
of the Greenland ice sheet, the summer average temperature
has decreased at the rate of 2.2°C per decade since the begin-
ning of measurements in 1987. This suggests that the Green-
land ice sheet and coastal regions are not following the cur-
rent global warming trend.

In 2006, Chylek et al. also put recent Greenland temperatures in

perspective, particularly in the summer, when ice melts:

104

Sea-Level Rise and the Great Unfreezing World

1. The years 1995 to 2005 have been characterized by gener-
ally increasing temperatures at the Greenland coastal sta-
tions. The year 2003 was extremely warm on the southeastern
coast of Greenland. The average annual temperature and
the average summer temperature for 2003 at Ammassalik
(southeast coast) was a record high since 1895. The years
2004 and 2005 were closer to normal being well below tem-
peratures reached in 1930s and 1940s. Although the annual
average temperatures and the average summer temperatures
at Godthab Nuuk, representing the southwestern coast, were
also increasing during the 1995–2005 period, they generally
stayed below values typical for the 1920–1940 period.

2. The 1955 to 2005 averages of the summer temperatures
and the temperatures of the warmest month at both Godthab
Nuuk and Ammassalik are significantly lower than the cor-
responding averages for the previous 50 years (1905–1955).
The summers at both the southwestern and southeastern
coast of Greenland were significantly colder within the
1955–2005 period compared to the 1905–1955 years.

3. Although the last decade of 1995–2005 was relatively
warm, almost all decades within 1915–1965 were even warm-
er at both the southwestern (Godthab Nuuk) and southeast-
ern (Ammassalik) coasts of Greenland.

4. The Greenland warming of the 1995–2005 period is similar
to the warming of 1920–1930 although the rate of temperature
increase was about 50 percent higher during the 1920–1930
warming period.

In 2007, Chylek et al. published another paper in Journal of Geophys-
ical Research in which they developed a computer model relating
the loss (or gain) in Greenland ice as a function of temperature.

Chylek et al.’s previous paper and the southern Greenland temper-
ature history gave us a hint of what’s to come. They use a concept
of what they call ‘‘melt-days’’ that reflect the integrated warming
of a particular Greenland summer. This particular study was for
Western Greenland, where there are two very good long-running
weather stations.

Chylek et al. concluded:

We infer that the melt-day area of the western part of the
ice sheet doubled between the mid-1990s and mid-2000s, and
that the largest ice sheet surface melting probably occurred

105

CLIMATE OF EXTREMES

between the 1920s and the 1930s, concurrent with the warm-
ing in that period.

They then go on to quote Hans Ahlmann (see below) who, in
1948, noted a large loss of ice from Greenland, concurrent with the
end of the warm period shown in Figure 4.2. Speaking to Hansen’s
hypothesis of rapid ice loss, Chylek et al. said:

An important historical fact is that this decades-long Green-
land warming apparently did not exceed a threshold for
rapid ice sheet disintegration as evidenced by ice sheet stabi-
lization and regrowth that followed.

Helheim Glacier: A Cautionary Tale

In December 2005, the BBC reported ‘‘Greenland Glacier Races to
Ocean,’’ describing the behavior of two major glaciers in eastern
Greenland. The two they describe, Kangerdlugssuaq and Helheim,
were in rapid retreat, with the termini receding more than two miles
per year. Together, these two massive glaciers comprise about
8 percent of total drainage area of the very large island. They are
the two glaciers with the largest annual ‘‘discharge’’ into the ocean.
Gordon Hamilton, of the Climate Change Institute at the University
of Maine, was quoted as saying that those movements ‘‘suggest that
the predictions for both the rate and timing for sea-level rise in the
next few decades will be largely underestimated.’’

Wouldn’t you know, that during the 18 months after the BBC ran
its story, both glaciers slowed down, stopped receding (despite
warm temperatures), and began advancing?

Ian Howat, from the University of Washington, and colleagues
noted in Science in spring 2007 things had changed dramatically:
‘‘Average thinning over the [Kangerdlugssuaq] glacier during the
summer of 2006 declined to near zero, with some apparent thicken-
ing.’’ Helheim also ‘‘decelerated.’’

‘‘Decelerated’’? Not really. How about ‘‘advanced’’? Figure 4.3
(see insert) is a Landsat image of Helheim from August 30, 2006
(red line). You can see that the glacier advanced substantially in the
last year (the black line is August 29, 2005). In fact, it has returned
to beyond its position in 1933, which we found in the U.S. Geological
Survey’s 1995 publication ‘‘Satellite Image Atlas of Glaciers of the
World: Greenland’’ by Anker Weidick.

106

Sea-Level Rise and the Great Unfreezing World

SUMMER TEMPERATURES AT ANGMAGSSALIK, GREENLAND

Figure 4.4

SOURCE: Danish Meteorological Institute 2008. http://www.dmi.dk/dmi/
tr08-04.pdf.

It is very apparent that the one-year advance between 2005 and
2006 made up half the entire loss from May 2001 through August
2005.

We searched the BBC’s website to see if it covered any of this and
came up empty, even as Howat and his colleagues cautioned that

The highly variable dynamics of outlet glaciers suggests that
special care must be taken . . . particularly when extrapolat-
ing into the future, because short-term spikes could yield
erroneous long-term trends.

We’ve also included summer temperatures at Angmagssalik (Fig-
ure 4.4), which Google Earth tells us is conveniently located only
52 miles away. It is pretty apparent that there were at least two
warm decades through 1950, so Helheim was likely to have retreated
far beyond its 1933 position before advancing during the cooling
that extended from roughly 1950 through 1995.

A bit farther to the north of Helheim and Kangerdlugssuaq gla-
ciers, Britannia glacier—carefully mapped out in the early 1950s by

107

CLIMATE OF EXTREMES

a Great Britain expedition—is shown in recent satellite photographs
to currently be larger and farther-reaching that when it was first
visited (Figure 4.5; see insert).

More Scary Greenland Stories

In its ‘‘Fourth Assessment Report’’ on climate change, the IPCC
summarized a large number of climate models for Greenland in the
21st century. On average, the IPCC projected a rise in sea level of
2 inches or so as a result of a net loss of ice from Greenland.

Yet we are bombarded by stories that Greenland is shedding ice
at a tremendous rate, and that even a small amount of additional
warming will result in a massive instability that will crash much of
its ice into the sea by 2100, raising sea level nearly 20 feet.

In very large type, the New York Times of January 16, 2007, pro-
claimed ‘‘The Warming of Greenland.’’ Rather than consulting the
latest in the refereed scientific literature, the Times relied on an off-
the-cuff estimate of ice loss given to them by Professor Carl Boggild
from the University Center at Svalbard, an archipelago about half-
way between Norway and the North Pole. According to the Times,
Boggild ‘‘said Greenland could be losing more than 80 cubic miles
of ice per year.’’ The real amount determined by meticulous analysis
of recent satellite data is around 25 cubic miles per year, published
by NASA’s Scott Luthcke in Science two months before the Times
piece. (Note that that number is about half the rate claimed in the
earlier Rignot study.) Twenty-five cubic miles per year is the same
mean value estimated by Andrew Shepard and Duncan Wingham
in a summary of recent literature that they published in Science
in 2007.

Rather than citing a mainstream estimate, such as the IPCC’s, the
Times quoted Richard Alley, from Penn State, who stated that ‘‘a sea-
level rise of a foot or two in the coming decades is entirely possible.’’
What does ‘‘coming decades’’ mean? We should expect a bit more
precision from scientists. Does Alley mean the next decade, which
Al Gore alluded to at the beginning of in the Larry King Live interview
we quote from at the beginning of this book? Or does it mean ‘‘some
time in this century’’? How many decades?

The current sea-level rise contributed by the Greenland ice loss
is too small to even be able to measure in the next decade or two.
The satellite data show a reduction of four hundred-thousandths of

108

Sea-Level Rise and the Great Unfreezing World

Greenland’s total ice per year (whereas Boggild’s figure ‘‘could’’
be around twelve hundred-thousandths). Multiplying the satellite-
based fraction by the 23 feet of sea-level rise that would result if all
of the ice were lost results in a current Greenland-induced rise in
sea level of 0.01 inch per year. Averaged over three decades, that’s
a third of an inch, which is indeed too small to be detectable. Over
a century, the rise becomes a bit more than an inch. Boggild’s unpub-
lished ‘‘guesstimate’’ yields 3.5 inches per century.

In fact, there’s nothing very new going on in Greenland. Although
the Times paid great attention to ice-loss in eastern Greenland caused
by recent temperatures, it conveniently forgot to look at nearby
temperature histories (as well as the overall one shown as Figure 4.2).
The longest record in that region is from Angmagssalik (Figure 4.4).
In the summer (when Greenland’s ice melts) the temperature has
averaged 6.1°C (43.1°F), over the last 10 summers, which is very
close to the average for the entire record. There’s one very warm
summer, in 2003. The other nine years aren’t unusual at all.

From 1930 through 1960, the average was 6.5°C (43.7°F). In other
words, it was warmer for three decades, and there was clearly no
acceleration in sea-level rise. What happened between 1945 and the
mid-1990s was a cooling trend, with the period 1985–95 being the
coldest in the entire Angmagssalik record, which goes back to the
late 19th century. Only in recent years have temperatures begun to
look like those that were characteristic of the early 20th century.

The Times could have written pretty much the exact same story
in 1948, before humans had much of a hand in global warming.
That’s when Hans Ahlmann wrote, in the Geographical Journal, a
publication of the British Royal Geographic Society: ‘‘The last
decades have reduced the ice in some parts of Greenland to such
an extent that the whole landscape has changed in character.’’ So
it’s hardly something new when the Times reports, almost 60 years
later, that temperatures in Greenland ‘‘are changing the very geogra-
phy of coastlines.’’

Ahlmann prepared a booklet accompanying a lecture to the Amer-
ican Geographical Society in the fall of 1953 on ‘‘Glacier Variations
and Climatic Fluctuations,’’ describing receding glaciers and rising
temperatures across many disparate regions of the Arctic, as well
as changes in plant life and animal behavior, and range shifts that
have accompanied the climate warming. The northern migration of

109

CLIMATE OF EXTREMES

codfish in the Atlantic brought the species into southern Greenland
for the first time in recent memory, and ushered newfound prosper-
ity into the region. Ahlmann quotes the Prime Minister of Denmark:

In the last generation changes that have had a decisive influ-
ence on all social life have occurred in Greenland. A new
era has begun. These changes are primarily due to two cir-
cumstances. Firstly, the Greenland climate has changed, and
with it Greenland’s natural and economic prospects.

The ‘‘Discovery’’ of ‘‘Warming Island’’

A peninsula long thought to be a part of Greenland’s mainland
turned out to be an island when a glacier retreated. . . .
[The] ominous implications are not lost on [Dennis] Schmitt,
who says he hopes that the island he discovered [italics added]
in Greenland in September will become an international symbol
of the effects of climate change. Mr. Schmitt, who speaks Inuit,
has provisionally named it Uunartoq Qeqertoq: The Warm-
ing Island.

—John Collins Rudolf, New York Times, January 16, 2007

‘‘Warming Island’’ was such a hit with the environmental commu-
nity that it generated its own website, http://www.warmingis
land.org. Rudolf bragged about his news exploits on his blog:

I wrote a story about the new island for The New York
Times. . . . A short video posted on the Internet [appeared]
on ABC, BBC, CBC . . . in May 2007 Dennis Schmitt returned
to Warming Island with Anderson Cooper of CNN for a live
broadcast about climate change.

‘‘Warming Island’’ is a pretty distinctive place, with a very odd
shape, comprising three long ‘‘fingers’’ (Figure 4.6; see insert). The
figure shows the loss of the ice bridge between it and the mainland,
based upon satellite imagery from 1985, 2002, and 2005. When the
loss of ice revealed open water, it became apparent that this land
was in fact an island.

Another image (Figure 4.7; see insert), taken from land, reveals
the obvious separation between Warming Island and the mainland.
Note the general surroundings of eastern Greenland in the area
map (Figure 4.8; see insert). To the left of the area of concern is
‘‘Carlsbad Fjord.’’

110

Sea-Level Rise and the Great Unfreezing World

The history of Southern Greenland’s temperature clearly reveals
a much larger integrated warming in the early and mid-20th century
than the current decade. This prompts an obvious inquiry. If 10
years of not-so-unusual temperatures revealed that ‘‘Warming
Island’’ was indeed an island and not a peninsula, was it also an
island at the end of the last warm period, roughly 50 years ago?

In the early 1950s, Switzerland’s Ernst Hofer spent four summers
as an aerial photographer in support of ground-based geological
research and mapping efforts. In 1957, he published a remarkable
book about northeast Greenland, entitled Arctic Riviera (Figure 4.9;
see insert; our copy of this rare book was quite damaged, so the
cover isn’t reproduced in its entirety).

Remember that 1957 was near the end of several decades that
averaged warmer than the most recent 10 years. In the introduction,
Danish explorer Lauge Koch praised the regional climate:

[Hofer] has indeed given a characteristic description of the
fjord-region of North-East Greenland, which, owing to favor-
able circumstances, enjoys a distinctly mild climate. . . . Dur-
ing this [summer] period the glaciers supply enough water
to produce a small Arctic oasis . . . the midnight sun warms
the steep walls of the fjords and produces temperatures that
can otherwise rarely be registered in such northern degrees
of latitude.

Arctic Riviera includes a map of Northeastern Greenland, shown
in Figure 4.10 (see insert). Warming Island is shown as an island!
What’s remarkable about the Warming Island story is that every
scientist who has researched Greenland temperatures knows of the
warmth of the early 20th century, and yet no one rose to question
the claims that ‘‘for unknown centuries’’ it had been assumed to be
part of the mainland.

NASA: Greenland Bigger than U.S.!

From a September 25, 2007, NASA press release:

A new NASA-supported study reports that 2007 marked an
overall rise in the melting trend over the entire Greenland
ice sheet and, remarkably, melting in high-altitude areas was
greater than ever at 150 percent more than average. In fact,
the amount of snow that has melted this year over Greenland
could cover the surface size of the U.S. more than twice.

111

CLIMATE OF EXTREMES

A record high in high places? High-altitude melting ‘‘greater than
ever’’? Amount of snow melt this year ‘‘could cover the surface size
of the U.S. more than twice’’? At what depth?

The NASA press release also included the following graphic,
which, in these writers’ humble opinions, takes the ice-cream cake
for rhetorical chutzpah in the field of scientific data presentation
(Figure 4.11).

Figure 4.11 shows the annual ‘‘melt area index’’ of Greenland in
relation to the size of the United States for each year from 1988 to
2007. The value for this year is a bit more than two times the size
of the continental United States. Now, considering that the total area
of Greenland is just more than one-quarter the area of the lower 48,
you may wonder how an area of more than twice the size of the
continental United States melted this year in Greenland.

The answer lies in what exactly the ‘‘melt area index’’ represents.
Readers probably think it is the area of Greenland where there is
some snowmelt over the year.

In this case, the ‘‘United States’’ units represent in fact the sum of
the area of Greenland that experienced surface snowmelt across all
days of the year that melting occurred. That is to say, if an area of
Greenland equal to 1/365 the area of the United States experienced
melting every day of the year, that would produce a ‘‘melt area
index’’ for that location equal to the size of the entire contiguous
United States.

Rather than using a picture of the continental United States as a
metric in its graph, even though it would have been less sensational,
NASA should simply have plotted out the time history of the ‘‘melt
area index’’ for Greenland and left it at that (we’ve done this service
for you in Figure 4.12).

As is obvious, there is a general rise since NASA satellite records
began in 1988, but it is all confined to the period 1988–97. Since then,
the ‘‘melt area index’’ varies from year to year, but there is no overall
net change.

What’s the news here? Six of the last ten years have a larger ‘‘melt
area index’’ than 2007. If, instead of calculating the ‘‘melt area index’’
for all of Greenland, you limited the calculation to only those regions
that lie at elevations above 2,000 meters (1.25 miles, or, in NASA
parlance, ‘‘high places’’), then 2007 is indeed the highest on record
since 1988 (or, as NASA described it, ‘‘greater than ever’’) (see Fig-
ure 4.13).

112

Sea-Level Rise and the Great Unfreezing World

MELTING INDEX TREND IN GREENLAND, 1988–2007, IN RELATION

Figure 4.11

TO U.S. SURFACE AREA

0

1988

1989

1990

1991

1992

1993

1994

1995

1996

1997

1998

1999

2000

2001

2002

2003

2004

2005

2006

2007

Melting index (Km2 x day)
5
25

20

10

15

30

Millions

trend

US continental size
M. Tedesco

SOURCE: NASA 2007. http://www.nasa.gov/vision/earth/environment/
greenland_recordhigh.html.
NOTE: km2 ⳱ square kilometer.

113

CLIMATE OF EXTREMES

GREENLAND’S MELT AREA INDEX, 1988–2007

Figure 4.12

SOURCE: Adapted from NASA September 25, 2007, Press Release.
NOTE: km2 ⳱ square kilometer.

Figure 4.13

MELT AREA INDEX OF THE GREENLAND ICE SHEET ABOVE

2,000 METERS, 1988–2007

SOURCE: Adapted from Tedesco 2007.
NOTE: 2,000 meters ⳱ 1.25 miles; km2 ⳱ square kilometer.

Inquiring minds might want to know what the high-elevation
melt area was during the warm period in the early and mid-20th
century. Obviously, we didn’t have satellites taking measurements
from space back then, but there was a good deal of climate research
taking place on the ground across Greenland. In 1961, much of this
work was summarized in an article by R. W. Gerdel titled, ‘‘A
Climatological Study of the Greenland Ice Sheet.’’ This was a part

114

PROJECTED WARMING TRENDS BASED ON COMPUTER MODELS FOR

THE MIDRANGE SCENARIO FOR CARBON DIOXIDE EMISSIONS,

Figure 1.5

2000–2100

4

3

2

1

0

)

C
º
(
 
e
r
u
t
r
a
p
e
d
 
e
r
u
t
a
r
e
p
m
e
T

–1
1980

2000

2020

2040

2060

2080

2100

Year

SOURCE: IPCC, 2007.
NOTE: The colored lines represent projected warming trends based on vari-
ous climate models. The black dots represent the projected trends’ average
for the IPPC’s midrange scenario for carbon dioxide emissions. The red
line supperimposes observed temperatures from 1975 through 2007, and a
projection of that rate through the end of the century.

Figure 2.13

DIFFERENCES BETWEEN OBSERVED AND ADJUSTED TRENDS

AROUND THE WORLD (UNITS ⳱ °C/DECADE)

ºC per decade

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
–0.1
–0.2
–0.3
–0.4
–0.5
–0.6
–0.7

50

0

–50

)
s
e
e
r
g
e
d
(
 
e
d
u

t
i
t

a
L

–150

–100

–50

0

50

100

150

SOURCE: McKitrick and Michaels, 2007.

Longitude (degrees)

COMPARISON OF HURRICANE SEASON MAPS OF 1933 (BOTTOM

RIGHT) AND 2005 (TOP LEFT)

Figure 3.3

SOURCE: Landsea, 2007.

3

.

4

6
0
0
2
–
3
3
9
1

,

T
S
A
O
C
D
N
A
L
N
E
E
R
G
N
R
E
T
S
A
E
L
A
R
T
N
E
C

,

R
E
I
C
A
L
G
M
I
E
H
L
E
H

e
r
u
g
i

F

s
t
i

g
n
i
h
s
u
p

,

y
l
t
h
g
i
l
s

d
e
c
n
a
v
d
a

r
e
i
c
a
l
G
m
i
e
h

l
e
H
e
h
t

)
e
n

i
l

e
g
n
a
r
o
(

1
0
0
2

y
a
M
o
t

)
e
n

i
l

h
c
a
e
p
(

9
9
9
1

r
e
b
m
e
t
p
e
S
m
o
r
F

:

E
T
O
N

t
a
e
r
t
e
r

i

d
p
a
r

a

y
b

d
e
w
o
l
l
o
f

s
a
w
1
0
0
2

n

i

n
a
g
e
b

t
a
h
t

t
a
e
r
t
e
r

w
o
l
s
A

.
)
e
n

i
l

n
e
e
r
g
(

2
7
9
1

n

i

n
o
i
t
a
c
o
l

s
t
i

d
n
o
y
e
b

t
n
o
r
f

g
n
i
v
l
a
c

t
s
u
g
u
A
y
B

.

n
i
a
g
a

g
n

i
c
n
a
v
d
a

n
a
g
e
b

d
n
a

i

g
n
d
e
c
e
r

d
e
p
p
o
t
s

r
e
i
c
a
l
g

e
h
t

,
r
e
t
f
a
e
r
e
h
T

.
)
e
n

i
l

k
c
a
l
b
(

5
0
0
2

t
s
u
g
u
A
o
t

4
0
0
2
m
o
r
f

.

5
9
9
1

,

k
c
i
d
i
e

W

;

7
0
0
2

,
.
l
a

t
e

t
a
w
o
H

;

y
r
e
g
a
m

I

t
a
s
d
n
a
L

:

S
E
C
R
U
O
S

r
e
m
m
u
s

s
t
i

g
n
i
h
c
a
o
r
p
p
a

n
i
a
g
a

s
i

d
n
a

)
e
n

i
l

e
u
l
b
(

3
3
9
1

n

i

n
o
i
t
a
c
o
l

s
t
i

d
n
o
y
e
b

d
e
c
n
a
v
d
a

d
a
h

t
n
o
r
f

g
n

i
v
l
a
c

e
h
t

.

n
o
i
t
a
c
o
l

,

6
0
0
2

4
0
0
2

Figure 4.5

BRITANNIA GLACIER, GREENLAND, 2008 POSITION (A)

AND 1954 POSITION (B)

a. 2008

b. 1954

SOURCES: Yahoo! Maps, 2008; Hamilton et al., 1956.
NOTE: Figure 4.5a shows the current position of the Britannia Glacier as
captured from a satellite photo available from Yahoo! Maps. Figure 4.5b is
a detailed map of the position of the same glacier produced from photo-
graphs and a ground survey done in 1954 (Hamilton et al., 2006). Currently
the Britannia Glacier and a smaller side glacier are advanced beyond their
1954 termini (red circles).

WARMING ISLAND, GREENLAND, 1985, 2002, AND 2005

Figure 4.6

a. August 11, 1985

b. September 5, 2002

c. September 4, 2005

SOURCE: U.S. Geological Survey, 2005.

Figure 4.7

WARMING ISLAND, GREENLAND, 2006

a. Overhead View

b. Oblique View

SOURCE: Adapted from U.S. Geological Survey, 2005; New York Times, January
16, 2007.

Figure 4.8

MAP OF WARMING ISLAND, GREENLAND

SOURCE: New York Times, January 16, 2007.

Figure 4.9

COVER OF ARCTIC RIVIERA

SOURCE: Hofer, 1957.

Figure 4.10

WARMING ISLAND, GREENLAND, 1957

SOURCE: Hofer, 1957.

Figure 4.14

DAYS WITH MELTING IN 2006; OBSERVED (COLORS) AND

CALCULATED IN 1961 (NUMBERS)

0

20

40

60+

Number of days with melting

SOURCES: NASA, 2007; Gerdel, 1961.

TEMPERATURES RELATIVE TO PREINDUSTRIAL LEVELS WORLDWIDE,

Figure 4.19

LAST 12,000 YEARS, FIRST DRAFT OF THE

IPCC’S 2007 REPORT

80

60

40

40

0

–20

–40

–60

–80

)
s
e
e
r
g
e
d
(
 
e
d
u

t
i
t

a
L

Greenland

Barents Sea

Siberia and east Russia

northeast Europe

northwest North America

northeast North America

southeast North America

southeast Europe

west China

east China

tropical Atlantic Ocean

tropical west Pacific Ocean

and

tropical east Pacific Ocean

South Africa

Tasmania

east Antarctica

Temperatures above pre-industrial
levels by 2ºC or more

Temperatures above pre-industrial
levels by 0.5-2ºC 

Temperatures below pre-industrial
levels by 0.5-2ºC 

12000

10000

8000

6000

4000

2000

0

SOURCE: First Order Draft, Fourth Assessment Report, IPCC, 2007.

Time (Years before present)

TEMPERATURES RELATIVE TO PREINDUSTRIAL LEVELS WORLDWIDE,

LAST 12,000 YEARS, SECOND DRAFT OF THE

Figure 4.20

IPCC’S 2007 REPORT

)
s
e
e
r
g
e
d
(
 
e
d
u
t
i
t
a
L

80

60

40

40

0

–20

–40

–60

–80

northwest

North America

Barents Sea

Greenland

northeast Europe

northeast 
North America

southeast
Europe

west
China

southeast
North America

east China

tropical North Indian Ocean

equatorial west
Pacific Ocean

tropical Pacific Ocean

South Africa

New Zealand

Tasmania

east Antarctica

Temperatures above pre-industrial
levels by 2ºC or more

Temperatures above pre-industrial
levels by 0.5-2ºC 

Temperatures below pre-industrial
levels by 0.5-2ºC 

12000

10000

8000

6000

4000

2000

0

SOURCE: Second Order Draft, Fourth Assessment Report, IPCC, 2007.

Time (Years before present)

TEMPERATURES RELATIVE TO PREINDUSTRIAL LEVELS WORLDWIDE,

LAST 12,000 YEARS, PUBLISHED VERSION OF THE IPCC’S 2007

Figure 4.21

REPORT

90

60

30

0

–30

–60

)
s
e
e
r
g
e
d
(
 
e
d
u
t
i
t
a
L

Nordic Seas

Greenland

NE Europe

NW North America

SE Europe

W

China

North Eurasia

 NE North America

SE N America

E China

Tropical North Indian Ocean

Equatorial West
Pacific Ocean

Tropical Pacific Ocean

South Africa

New Zealand

Tasmania

Antarctica

Above pre-industrial by 2ºC or more

Above pre-industrial by 0.5 to 2ºC 

Below pre-industrial by 0.5 to 2ºC 

–90

1200

10000

8000

6000

4000

2000

0

Time (Years before present)

SOURCE: IPCC, 2007.

2
2

.

4

e
r
u
g
i

F

7
0
0
2

,

A
K
S
A
L
A

,

K
O
T
W
E
N

h
s
i
u
q
s

s
k
l
a
w
d
r
a
o
B

.
r
e
w
o
t

r
e
t
a
w
s
t
i

m
o
r
f

d
e
w
e
i
v

s
a

,

g
n

i
r
p
s

n

i

,

a
k
s
a
l
A

,

k
o
t
w
e
N

‘
‘

,

d
a
e
r

n
o
i
t
p
a
c

s
’
h
p
a
r
g
o
t
o
h
p

e
h
T

:

E
T
O
N

’
’
.

d
n
a
l
s
i

n
a

o
t
n

i

d
e
n
r
u
t

s
a
h

n
o
i
s
o
r
e

h
c
i

h
w

,

k
o
t
w
e
N
n

i

k
c
u
m
e
h
t

o
t
n
i

.

7
0
0
2

,

7
2

y
a
M

,
s
e

m
T

i

k
r
o
Y
w
e

N

:

E
C
R
U
O
S

LINEAR TRENDS OF ANNUAL MEAN SURFACE AIR TEMPERATURE,

Figure 4.23

1958–2002

–0.3

–0.2

–0.1

0.0

0.1

0.2

0.3

SOURCE: Chapman and Walsh, 2007.

ºC per decade

GLOBAL THICKNESS ANOMALIES, JUNE, JULY, AND AUGUST 2003

Figure 6.1

90N

60N

30N

EQ

30S

60S

90S

180

120W

60W

0

60E

120E

180

–3.2 –2.4 –2.0 –1.6 –0.8 –0.4

0.4

0.8

1.6

2.0

2.4

3.2

SOURCE: Chase et al., 2006.
NOTE: These anomalies are proportional to the average temperature of the
bottom half of the atmosphere. Areas exceeding 2.0, 2.5, and 3.0 standard
deviations from the 1979–2003 mean are contoured in thick lines for anoma-
lies of both signs.

RESULTS OF FIRST ‘‘COUPLED MODEL INTERCOMPARISON PROJECT’’

Figure 7.1

3

2

1

0

 
)

C
º
(
 
e
g
n
a
h
c
 
e
r
u

t

a
r
e
p
m
e

t
 
l

a
b
o
G

l

BMRC
CGCM1
CCSR
ARPEGE/OPA
CSIRO
ECHAM3/LSG
GFDL_R15
GISS1
HadCM2
HadCM3
IAP/LSG
LMD
MRI
CSM
PCM
Average

1

0

20

40

60

80

Years from start of experiment

SOURCE: Meehl et al., 2001.
NOTE: Acronyms refer to various climate models.

Figure 7.3

THE ‘‘HOCKEY STICK’’ AS IT APPEARED IN THE IPCC’S

THIRD ASSESSMENT REPORT

Instrumental data (AD 1902 to 1999)
Reconstruction (AD 1000 to 1980)
Reconstruction (40 year smoothed)
Linear trend (AD 1000 to 1900)

1998 instrumental value

1.0

0.5

0.0

0
9
9
1

 

o

t
 

1
6
9
1

 

o

t
 

e
v
i
t

l

a
e
r

)

l

C
º
(
 
y
a
m
o
n
a

 

i

e
r
e
h
p
s
m
e
H
n
r
e
h

 

t
r
o
N

–0.5

–1.0

1000

1200

1400

1600

1800

2000

Year

SOURCE: IPCC, 2001.
NOTE: Michael Mann’s original Nature paper featured a 600-year history.

Sea-Level Rise and the Great Unfreezing World

Table 4.1

AVERAGE NUMBER OF SUMMER DAYS WITH MAXIMUM

TEMPERATURE CALCULATED TO BE AT OR ABOVE FREEZING FOR

VARIOUS ELEVATIONS AND LATITUDES ACROSS GREENLAND

Elevation (meters)

Latitude (°N) 500 1,000 1,300 1,500 1,700 2,000 2,500 2,700 3,000
0
76
16
67
61
15
SOURCE: Gerdel 1961.

5
66
69

73
91
92

51
91
92

14
81
86

32
89
91

22
86
90

0
41
41

0
34
33

of the 1961 Symposium on the Physical Geography of Greenland of the
XIX International Geographical Congress. Included among Gerdel’s
discussions of temperature, precipitation, winds, fog, radiation, and
so on is a section called ‘‘The Occurrence of Melting on the High
Ice Sheet.’’

Gerdel reports that there is evidence of summer melting occurring
at least as high as 1,700 meters (1 mile) above sea level on the interior
of the ice sheet east of Thule, at latitude 76°N. That is extremely far
north, less than a thousand miles from the North Pole. Air tempera-
ture measurements from the Thule air base (along the northwestern
Greenland coast) coupled with those taken from elevations on the
ice sheet indicate that the temperature ‘‘lapse rate’’ (decline in tem-
perature with height) were found to be 0.6°C (1.0°F) per 100 meters.
Gerdel used that lapse rate to calculate the elevation on the ice
sheet where the air temperature would reach the freezing point,
extrapolated from the temperatures taken from coastal stations
around Greenland (there were and are very few temperature mea-
surements from locations on the interior ice sheet itself). From those
data, Gerdel produced Table 4.1, which indicates the calculated aver-
age number of days during the summer (June, July, and August)
that the maximum air temperature on the ice sheet was at or above
freezing for various latitudes and elevations. This is based upon
temperatures observed during the period 1946–56.

As Table 4.1 shows, there’s likely to have been plenty of melting

at altitudes as high as 2,700 meters (1.7 miles).

As our Figure 4.12 illustrates, the ‘‘melt area index’’ of 2006 was
very close to the 2007 value. A comparison of Gerdel’s 1961 numbers

115

CLIMATE OF EXTREMES

with the 2006 data provides a fair assessment of how current condi-
tions at high elevation compare with mid-20th century ones (Figure
4.14; see insert).

Figure 4.14 (color insert) shows the number of days with melting
observed across Greenland in 2006 as reported by NASA, along with
the number of days of melting for the locations as calculated by
Gerdel in 1961. Notice that in every case, Gerdel calculated a greater
number of days with melting than occurred in 2006 including in the
‘‘high places’’ on the ice sheet in the north, south central, and south-
ern portions of Greenland.

Arctic Sea Ice in Perspective

A September 2005 press release highlighted a decrease in satellite-
sensed Arctic ice extent from September 1979 (the beginning of the
record) to September 2005, with that year showing the lowest values
in the entire record. Note that this is a seasonal phenomenon. The
ice extent reaches its annual minimum sometime around mid-
September. On the first day of autumn, night falls at the North Pole
and the sun does not return for six months. The ice soon begins to
re-form.

Nowhere does NASA’s press release mention that 1979 was very
close to the end of the second-coldest period in the Arctic in the
20th century (Figure 4.15). Because temperatures in 1979 had just
recovered from their lowest values since the early 1920s, Arctic ice
should have been near a maximum for the last eight decades when
the first satellite imagery was returned.

The September 2005 record minimum for Arctic sea ice was broken
again in September 2007, making headlines worldwide. (As noted
later in this chapter, within a few months, satellites would detect
record high ice extent in the Southern Hemisphere.)

Is this unprecedented? A very interesting 2003 study by National
Oceanic and Atmospheric Administration scientists James Overland
and Kevin Wood examined the logs of 44 Arctic exploration vessels
from 1818 to 1910 and found that ‘‘climate indicators such as naviga-
bility, the distribution and thickness of annual sea ice, monthly
surface air temperatures, and the onset of melt and freeze were
within the present range of variability.’’ Commenting on the early
exploration logs, they noted that ‘‘overwinter locations of Arctic

116

Sea-Level Rise and the Great Unfreezing World

Figure 4.15

ANNUAL ARCTIC TEMPERATURES FROM LAND STATIONS,

60° NORTH TO 90° NORTH, 1900–2001

SOURCE: Adapted from Arctic Climate Impact Assessment 2004. http://
www.acia.uaf.edu/pages/overview.html.

discovery expeditions from 1818 to 1859 are surprisingly consistent
with present sea ice climatology.’’

It’s easy to blame recent Arctic warming on greenhouse gases.
After all, computer models all show that warming is enhanced in
the high northern latitudes, more so than on the rest of the planet.
So the Arctic should give some of the earliest signals of change.

The warming that peaked around 1940 remains troublesome,
though. Given the forecast strength of the greenhouse signal in the
Arctic, are present temperatures unprecedented? And what caused
the large and rapid warming of the early 20th century (1900–40),
whose magnitude of 2°C (3.6°F) isn’t statistically distinguishable
from the amount of warming in the most recent four decades?

Writing in the journal Geophysical Research Letters in 2003, Vladimir
Semenov and Lennart Bengstsson, from Germany’s Max Planck
Institut, found that the recent arctic temperature rise is largely related

117

CLIMATE OF EXTREMES

to atmospheric circulation factors in the North Atlantic region, while
the early 20th-century warming was probably because of sea ice
variations.

This creates further problems concerning the current warming.
One of the reasons that the Arctic is forecast to warm up more than
other places is because the ice disappears. Here’s why: The bright
ice reflects away much of the sun’s energy, whereas the darker water
absorbs it. So when the ice goes down, the temperature should go
up, which melts more ice, which raises the temperature more, and
so on—an example of a ‘‘positive feedback loop’’ in the climate
system. Indeed, in the 2005 NASA press release, National Snow and
Ice Data Center senior scientist Ted Scambos confirmed as much,
saying, ‘‘Feedbacks in the system are starting to take hold.’’

It’s fair to say that Semenov and Bengtsson’s study casts doubt
on an irreversible positive feedback. If the substantial warming of
the early 20th century indeed resulted from sea ice changes, then
why did the warming not continue? The fact is that the cause of the
substantial arctic cooling trend from 1940 through the mid-1970s
remains mysterious.

Gore-ing of History

Obviously, there was a low point in Arctic ice during the last
temperature maximum, observed in the 1930s. But you wouldn’t
get that from the book version of Al Gore’s science fiction classic
An Inconvenient Truth.

Shown below, as Figure 4.16, is a figure Gore labeled ‘‘Sea-Ice

Extent: Northern Hemisphere.’’

Gore’s depiction of the Northern Hemisphere sea ice extent (Figure
4.16) shows basically small annual variations, but no trend from
about 1900 through about 1970, and then a large decrease. The loss
of ice extent certainly looks pretty dramatic and gives the distinct
impression that human activities are producing changes that are
quite unusual, at least in the context of the last 100 years.

As additional historical data and analyses come to light, however,
it is looking less and less likely that the early-to-mid-20th-century
variations in Arctic sea ice were as small as Gore claims.

In his 1953 American Geographical Society pamphlet, Ahlmann

noted the following:

118

Sea-Level Rise and the Great Unfreezing World

Figure 4.16

ARCTIC SEA-ICE EXTENT, 1900–2005,

FROM AN INCONVENIENT TRUTH

SOURCE: Adapted from Gore 2006, p. 143.
NOTE: km2 ⳱ square kilometer.

The thickness of the ice forming annually in the North Polar 
Sea has diminished from an average of 365 centimeters [12 
feet] at the time of Nansen’s Fram expedition of 1893–96 to 
218  centimeters  [7.2  feet]  during  the  drift  of  the  Russian 
icebreaker Sedov in 1937–40. The extent of drift ice in Arctic 
waters has also diminished considerably in the last decades. 
According  to  information  received  in  the  U.S.S.R.  in  1945, 
the area of drift ice in the Russian sector of the Arctic was 
reduced by no less than 1,000,000 square kilometers [386,100 
square miles] between 1924 and 1944. The shipping season 
in  West  Spitsbergen  has  lengthened  from  three  months  at 
the beginning of this century to about seven months at the 
beginning of the 1940s. The Northern Sea Route, the North-
East Passage, could never have been put into regular usage 
if the ice conditions in recent years had been as difficult as 
they were during the first decades of this century.

In a September 21, 2007, seminar  at University of Colorado, U.S.

Department of Commerce scientist Andy Mahoney explained:

[Historic]  time series  of air  temperature and  the extents  of 
pack ice, multiyear ice and landfast ice extents reveal three

119

CLIMATE OF EXTREMES

distinct periods of variability over the last eight decades: a
period of warm winters and decreasing summer and fall sea
ice extent (period A), followed by a cool period of stable or
slightly increasing extent (period B) before a period of year-
round warm temperatures and ice loss (period C).

What’s wrong with Gore’s picture? Gore wants to relate recent
Arctic sea-ice declines to the recent warm-up there. But since the
record of Arctic temperatures shows not only a warm-up in recent
decades, but also one similar in relative magnitude from the early
years of the 20th century to about the mid-1940s (Figure 4.15),
shouldn’t there have been a loss in sea ice in both periods? Wasn’t
Gore suspicious that his figure didn’t show one? And what about
the period from the mid-1940s to the mid-1970s when Arctic average
temperatures declined a healthy amount? Shouldn’t there have been
an increase in sea ice extent during that period?

It is not acceptable for Gore to hide behind the source of his figure,
which he lists as ‘‘Hadley Carter.’’ (Who is that? We’ll bet it means
‘‘Hadley Center,’’ a British government entity.) The closest thing we
can find is Figure 4.17, the annual data from the Cryosphere Today
website of the University of Illinois Polar Research Group (available
at http://arctic.atmos.uiuc.edu/cryosphere/IMAGES/seasonal.extent
.1900-2007.jpg). But Gore made one critical omission: Cryosphere
Today’s accompanying text explains that the data prior to 1953 are
pretty unreliable.

Accompanying the Cryosphere Today figure is a data set docu-
mentation file (http://arctic.atmos.uiuc.edu/SEAICE/arctic.histori
cal.seaice.doc.txt) that is full of caveats about how the data set was
put together. It cautions: ‘‘Please note that much of the pre-1953 data
is either climatology or interpolated data and the user is cautioned to
use this data with care.’’ Well, the incorporation of climatology
(long-term averages) goes a long way toward explaining the lack of
variation in early 20th-century data. And this is obviously what was
done: note the autumn line is completely flat prior to 1953. Nowhere
in the book An Inconvenient Truth is any of this made clear. Instead,
we just see a graph with little ice variability for 70 years, and then
a steep drop-off during the past 30.

And finally, a paper was published in 2004 (before An Inconvenient
Truth was released or the book of the same title published) that
discussed some Arctic ice data that wasn’t included in the data set

120

Sea-Level Rise and the Great Unfreezing World

Figure 4.17

ANNUAL SEA ICE EXTENT, 1900–2007

SOURCE: Cryosphere Today 2007. http://arctic.atmos.uiuc.edu/cryosphere/.
NOTE: km2 ⳱ square kilometer.

that underlies Gore’s image. This ‘‘new’’ set of old data sounds like
the same Russian data set discussed by Ahlmann and more recently
by Mahoney. When O. M. Johannessen and his colleagues, of Nor-
way’s Nansen Research Center, used the ‘‘hitherto under-
recognized’’ Russian sea ice extent observations to create a long-
term 20th-century record of sea ice, they produced an Arctic sea
ice–extent history that looked quite different from the Gore version,
and, in fact, exhibits a much higher correspondence to the Arctic
temperature history (Figure 4.18).

The authors note that the Russian sea ice observations do not
encompass the entire Arctic (they are missing about 23 percent of
the total area, primarily along the coast of North America, including
the eastern Chukchi Sea, the Beaufort Sea, and the Canadian Arctic
straits and bays), and that the data are inadequate during World
War II and the early postwar years. That probably explains the lack
of correspondence between the Arctic sea ice extent and falling

121

CLIMATE OF EXTREMES

ARCTIC SEA-ICE EXTENT RECONSTRUCTED FROM RUSSIAN DATA

SET AND GORE’S LIKELY DATA SET, 1900–2000

Figure 4.18

SOURCE: Adapted from Johannesssen et al. 2004.
NOTE: km2 ⳱ square kilometer.

temperatures during the 1940s, as well as why the Russian recon-
struction doesn’t fall off as much in recent years (where a lot of sea
ice loss took place off the northern coast of North America). But
despite these difficulties, the Russian reconstruction shows far more
interdecadal variation, including a large decline from 1900 to the
1940s, a recovery from the 1940s into the late 1960s (quite possibly
underestimated due to insufficient data during the early part of that
period), and then a subsequent decline to the present. The present
decline has resulted in the absolute lowest sea ice extent area, but
it has not progressed at the absolute fastest rate—which occurred
early in the 20th century.

Mahoney explained in his seminar: ‘‘[T]he Russian Arctic ice pack
did not fully recover during [the midcentury], suggesting that the
early 20th century warming . . . mayhave preconditioned the Arctic
for greater change in recent decades.’’ In other words, human activity
may be responsible for pushing Arctic sea ice to its lowest extent in

122

Sea-Level Rise and the Great Unfreezing World

the past 100 years or so, but we had quite a bit of help from Mother
Nature. There has been a large degree of variation in Arctic sea ice
extent over the course of the 20th century, much of which was fueled
by non-human–induced climate variations.

We’ll now progress a bit further to the South, into the permafrost
and northern limits of tree growth. What’s happening isn’t
exactly new.

Permafrost’s Future

Anyone who Googles global warming will quickly discover that
Arctic region permafrost is melting at an unprecedented rate, and
somehow this will lead us to a runaway greenhouse effect that might
warm the earth far more than any of us ever feared. The melting of
permafrost is one of the main pillars of the global warming hor-
ror story.

There has been a flurry of recent research about the melting perma-
frost issue. If you do nothing more that search the Internet for ‘‘global
warming Ⳮ permafrost,’’ literally hundreds of thousands of pages
will pop up. You will read repeatedly that permafrost is a sink
(storage mechanism) for carbon dioxide and another greenhouse
gas, methane. When the soils of the high latitudes froze at the begin-
ning of the last ice age, they trapped very large amounts of organic
material (carbon-rich grasses, animal remains, and soil material) in
frozen permafrost. As it thaws, then, the carbon trapped within the
once-frozen soils will be released, mainly as methane, causing even
more warming worldwide. We know that the warming is predicted
to occur the most in the high-latitude land areas of the Northern
Hemisphere—exactly where the permafrost is found today. With
more warming in permafrost regions, more permafrost will melt,
more methane will be released, and so forth.

The entire process is described by many as a time bomb that is
going off before our very eyes. The bomb is not just causing the
world to warm at a more rapid pace, but also the melting permafrost
is also routinely connected to the destruction of forests, collapse of
homes and other structures (e.g., pipelines), erosion of coastal areas
and hillsides, disruption of animal habitat—and, well, you name it.
A very interesting article appeared in a 2007 issue of Geophysical
Research Letters titled ‘‘Near-Surface Permafrost Degradation: How

123

CLIMATE OF EXTREMES

Severe during the 21st Century?’’ by G. Delisle from Hanover, Ger-
many. Bet you haven’t heard a word of it.

The final sentence of the abstract states: ‘‘Based on paleoclimatic
data and in consequence of this study, it is suggested that scenarios
calling for massive release of methane in the near future from
degrading permafrost are questionable.’’ Delisle notes that warming
is occurring in the Arctic regions, and that the warming will
undoubtedly impact the permafrost of the high latitudes. Delisle
notes, however, that many numerical models used to simulate the
impact of warming on permafrost deal only with the upper 10 feet
of the earth’s surface. The previously used models do not take into
account the cooling effect of deeper and colder zones that interact
thermodynamically with the active layer near the surface. Delisle
also exposes other assumptions of previous models that are ‘‘in clear
conflict with field evidence.’’

Delisle presents ‘‘a unidimensional long term permafrost tempera-
ture model of general application . . . which is capable to fully incor-
porate all relevant thermal processes within the active layer and the
permafrost, and between the permafrost and the non frozen ground
below.’’ The model space is made up of 600 layers, with a minimum
spacing of 10 cm (four inches) within the active layer and the upper-
most ‘‘permafrost zone.’’ Rather than looking at only 10 feet beneath
the surface as was done by previous models, the new model goes
100 yards into the surface, which provides a more realistic picture.
Using this more complete model, Delisle reports that continuous
permafrost in Alaska and Siberia will persist over the next 100 years,
even if a significant warming takes place. Further, we learn that

Based on this result and on the presented analysis, it appears
that all areas north of 60°N will maintain permafrost at least
at depth. North of 70°N, surface temperature values today
are in general below ⳮ11°C. These areas should maintain
their active layer. It appears unlikely that almost all areas
with near-surface permafrost today will lose their active layer
within the next 100 years.

Delisle claims that the new model is far more consistent with
field measurements and far more realistic in terms of including the
interaction with the deeper and colder permafrost core.

Another common fear is that melting of permafrost will release

trapped methane. Delisle notes this at the end of his article:

124

Sea-Level Rise and the Great Unfreezing World

A second, rarely touched upon question is associated with
the apparently limited amount of organic carbon that had
been released from permafrost terrain in previous periods
of climatic warming such as e.g. the Medieval Warm Period
or during the Holocene Climatic Optimum [the warmer mil-
lennia after the end of the recent ice age—see our next sec-
tion]. There appear to be no significant [methane] excursions
in ice core records of Antarctica or Greenland during these
time periods which otherwise might serve as evidence for a
massive release of methane.

A Long-Term History

What about the longer perspective? French climate researcher
Jean-Claude Duplessy found that the Barents Sea—the portion of
the Arctic Ocean bordering on Scandinavia and northwestern Rus-
sia—was 2°C (3.6°F) warmer 7,000 to 8,000 years ago than it was at
the beginning of the Industrial Revolution. In the same era, Sigfus
Johnsen of the University of Copenhagen found Greenland tempera-
tures to be 0.5°C to 2°C (0.9°F to 3.6°F) warmer.

The most comprehensive analysis of Eurasian temperature histor-
ies back to the end of the last ice age was published in 2000 by Glen
MacDonald, who chairs the Geography Department at UCLA.

MacDonald et al. collated records of trees preserved in the acidic
environment that is now the Arctic tundra. The remains can be dated
by radiocarbon analysis.

The boundary between the northern forest and the bare tundra
is currently south of the Arctic Ocean, and is determined by summer
maximum temperatures. MacDonald found: ‘‘Over most of Russia,
forest advanced to or near the current arctic coastline between 9000
and 7000 yr B.P. [before present] and retreated to its present position
by between 4000 and 3000 yr B.P.’’

In other words, the Eurasian arctic was considerably warmer than

today for seven millennia!

How warm? ‘‘During the period of maximum forest extension, the
mean July temperature along the northern coastline may have been
2.5 to 7°C [4.5°F to 12.6°F] warmer than modern [temperatures].’’

One reason he gives for this warmth is ‘‘extreme Arctic penetration

of warm North Atlantic Waters.’’

Imagine yourself on a satellite over the North Pole. What you will
note is that there is only one ‘‘gate’’ through which such water can

125

CLIMATE OF EXTREMES

pass, and it is via the passage between Greenland and Europe. In
other words, the east coast of Greenland was likely to have been
warmer for several millennia and it did not shed its ice. James Hansen’s
speculation that a 1°C (1.8°F) rise in global temperature will therefore
cause this is simply bad science fiction (as opposed to good sci fi,
which is at least plausible). Further, the polar bear survived and the
Inuit culture radiated.

Goodbye to three modern climate myths: Arctic temperatures are
higher than any observed since before the last ice age; Greenland is
about to shed all its ice; and global warming is driving the Inuit
and the polar bear to extinction.

Postglacial warming was Arcticwide. Darrell Kaufman, of North-
ern Arizona University, noted that for 2,000 years—from 9,000 to
11,000 years ago, Alaskan temperatures averaged 3°F (1.7°C) warmer
than now. Feng Sheng Hu, of the University of Illiniois, found that
there have been three similarly warm periods in Alaska: 0 to 300,
850–1200, and 1800 to the present. Thompson Webb III, found tim-
ings similar to MacDonald: northwestern and northeastern North
America were more than 4°F (2.2°C) warmer than the baseline from
7,000 to 9,000 and 3,000 to 5,000 years ago, respectively. And in a
2006 comprehensive review of regional temperature histories, the
University of Buffalo’s Jason Briner and colleagues wrote:

. . . summer temperatures from Qipisarqo Lake on southern
Greenland were 2°C to 4°C [3.6°F to 7.2°F] warmer in the
early Holocene [post–ice age era beginning around 11,500
years ago] versus the late Holocene [more recent era]. . . .
Greenland ice sheet borehole paleothermometry indicates a
temperature change of ⬃3.5°C [6.3°F] between the middle
and late Holocene [roughly 4,000 to 7,000 years ago].

. . . and ice did not fall off of Greenland, despite millennia of warmer
temperatures.

Baked Alaska?

‘‘Global Warming is Killing Us, Too, Say Inuit.’’

The Inuit people of Canada and Alaska are launching a
human rights case against the Bush administration claiming
they face extinction because of global warming.

—The Guardian, December 11, 2003

126

Sea-Level Rise and the Great Unfreezing World

Low-Balling Warming?

Obviously, MacDonald’s paper is somewhat disturbing to
global warming alarmists. Among other things, it surely argues
that polar bears and Arctic people are pretty resilient, and that
the Arctic can be warm for millennia and still recover its sea ice.
What’s disturbing to us, though, is the way it was played
in the IPCC process. As a reviewer of the 2007 ‘‘Fourth Assess-
ment Report,’’ one of us (Michaels) was privy to the various
drafts that eventually became the final report.

Figure 4.19 (see insert) is from the ‘‘First Order Draft.’’ The
figure clearly shows ‘‘Siberia and East Russia’’ to be more
than 2°C (3.6°F) warmer than the preindustrial era. That is a
profound understatement, given that MacDonald noted sum-
mer temperatures 2.5°C to 7°C (4.5°F to 12.6°F) warmer than
what he calls ‘‘modern.’’ And the Alaskan (‘‘Northwestern
North America’’) temperatures should have been red, not
orange, because Kaufman’s work used the present rather than
the colder ‘‘preindustrial’’ data as a baseline.

Between the first and second drafts, an amazing thing

happened.

As shown in Figure 4.20 (see insert), the 7,000 years in which
‘‘Siberia and East Russia’’ were colored red (more than 2°C
[3.6°F] warmer than preindustrial) simply disappeared, despite
the reference to MacDonald et al. in the accompanying fig-
ure caption.

Figure 4.21 (see insert) is from the final, published version,
which appeared in May 2007. ‘‘Siberia and East Russia’’ from
Figure 4.19 (absent in Figure 4.20) have now been replaced by
‘‘North Eurasia,’’ with the warm period ending 8,000 years
ago, instead of 10,000. Contrast that with Macdonald’s text:

Radiocarbon-dated macrofossils are used to document
Holocene [post-ice age] treeline history across northern
Russia (including Siberia). Boreal forest development
in this region commenced by 10,000 yr B.P. Over most
of Russia, forest advanced to or near the current arctic
coastline between 9000 and 7000 yr B.P. and retreated

(continued on next page)

127

CLIMATE OF EXTREMES

(continued)

to its present position by between 4000 and 3000 yr
B.P. Forest establishment and retreat was roughly syn-
chronous across most of northern Russia.

All of these shenanigans certainly provoke a number of ques-
tions. Why did the entire warming disappear from the second
draft, why did it appear in a form that was obviously not what
the author intended in the final draft, and why didn’t the IPCC
mention how much warmer it actually was, rather than dialing
it down to simply ‘‘above 2°C [3.6°F]’’?

Really?
The Alaska Climate Research Center, at the University of Alaska–
Fairbanks, maintains the statewide temperature database along with
historical analyses. According to the center’s website (http://cli-
mate.gi.alaska.edu), ‘‘[T]he period 1949–1975 was substantially
colder than the period 1977–2003; however, since 1977 no additional
warming has occurred in Alaska [emphasis added] with the exception
of Barrow and a few other locations.’’

In 1976, a stepwise shift appears in the temperature data, which
corresponds to a change in the distribution of Pacific Ocean tempera-
tures. Commenting on this shift, in 2005 Brian Hartmann and Gerd
Wendler wrote in the Journal of Climate:

The regime shift [was] also examined for its effect on long-
term temperature trends throughout the state. The trends
that have shown climatic warming are strongly biased by
the sudden shift from the cooler regime to a warmer regime
in 1976. When analyzing the total time period from 1951 to
2001, warming is observed, however the 25-year period trend
analyses before 1976 (1951–75) and thereafter (1977–2001) both
display cooling [emphasis added].

This behavior is certainly contrary to what is produced by climate
models forced with increasing carbon dioxide. The warmings those
models calculate are generally smooth in character. None of them
predicts a sudden one-year shift in Pacific temperatures, followed
by a quarter-century of stable temperatures in nearby land regions.

128

Sea-Level Rise and the Great Unfreezing World

Of course, it could be the fact that greenhouse warmings do in fact
manifest themselves largely as a series of fits and starts, but if that
is true, then the models are currently incapable of accurately simulat-
ing the response of regional temperature to changes in global car-
bon dioxide.

Vanishing Alaska?

The Times and other big papers have recently published a tremen-
dous number of articles about changes in the Arctic, noting that
there have been massive changes, including erosion along the
sparsely settled north coast of Alaska, presumably caused by global
warming; Figure 4.22 (see insert) shows a photograph that accompa-
nied one such article. In fact, there’s little ‘‘news’’ here that is fit to
print. It’s a very old story. Alaskans were warned of this almost a
half-century ago.

An intense storm struck the northwestern tip of Alaska during
the fall of 1963, causing over $3 million in damage in the much more
valuable dollar of the time. A 10-foot storm surge (which would
be respectable for a tropical hurricane) gravely damaged a U.S.
government research camp that was located at Barrow as winds
gusted to hurricane force. The storm hit during an unusual ice-free
period in early October—the primary reason why the seas grew to
such damaging heights. During most months of the year, near-shore
sea ice coverage is sufficient to dampen (or prevent entirely) the
buildup of significant wave heights.

James Hume (at Smith College) and Marshall Schalk (then of Tufts
University) described the damage from the 1963 storm in an article
written for the journal Arctic in 1967. On the basis of historical
weather records and the recollection of Inuit elders, they estimated
that it was about a ‘‘200-year’’ storm at this location.

This storm, and others like it (it’s easy to have many ‘‘200-year’’
storms in a few years, because they can strike in different places),
should have served as ample warning against settling on the unstable
coastline of much of Alaska.

The wind and waves from the great 1963 storm took a huge toll
on the Barrow shoreline. Hume and Schalk estimated the erosion
damage from the single 1963 storm to be equivalent to about 20
years of ‘‘normal’’ erosive processes. And the ‘‘normal’’ erosive
processes themselves were known to be substantial along much of

129

CLIMATE OF EXTREMES

Alaska’s coast, which is made up of loose sediments held together
by ice. Erosion rates have been measured to range from a few feet
to a few tens of feet per year along much of Alaska’s western and
northern shorelines.

A. D. Hartwell, then at the U.S. Army’s Cold Regions Laboratory
in Hanover, New Hampshire, described the processes acting on the
northern Alaska coast in a 1973 paper in Arctic:

Most of this coastline is marked by an abrupt break in slope
between the relatively horizontal terrain of the mainland and
the gently-sloping sea floor. In bedrock areas this break is
generally a steep sea cliff with loose talus material at its base.
In areas of perennially frozen sediment which are exposed
to direct wave attack along the coast, the relief is often sheer
and is formed by slumping of large blocks of frozen sedimen-
tary material. This is a result of both thermal and mechanical
erosion along the base of the sea cliff and inland along the
banks of estuaries and rivers where undercutting of the fro-
zen sediments forms a ‘‘thermoerosional niche.’’ Such niches
which are unique to this environment can form rapidly and
may extend several metres under the bank, making the over-
hanging bank unstable and susceptible to collapse especially
where ice wedges are intersected.

Acting on top of those erosive processes are strong late summer
and early fall storms, such as the one in October 1963, After compar-
ing the high rates of event-based erosion (such as the 1963 storm)
with the ongoing long-term erosion rates, the Hume and Schalk
paper ended with an eerie warning, in 1967, about the future:

A practical consideration also arises from this study. If, as
has been suggested, the climate is becoming warmer as a
result of the addition of carbon dioxide to the atmosphere
(Plass 1956; Callender 1958; Kaplan 1960; Mitchell 1965), the
likelihood of an open ocean and strong winds coinciding to
produce such a storm in the future is constantly increasing.
Another such storm can be expected, and care should be
exercised in the selection of building sites and construction
methods. The best sites would be at least 30 feet above sea
level and either inland or along a coast which is not eroding.

Much of this advice went unheeded. Nowadays, we hear story
after story describing the plight of the native Alaskans as their

130

Sea-Level Rise and the Great Unfreezing World

villages, which were constructed on the unstable bluffs along the
Alaskan coast, are undermined by the retreating shoreline.

As native Alaskans began a transition from their traditional
nomadic lifestyles to more permanent villages, replacing snow
houses with tin and plywood buildings, dogsleds with snowmobiles,
and seal-oil lamps with electric lights, they located many of those
settlements very near the (already receding) shoreline to provide
ready access to the oceans, a primary source of the coastal Inuit’s
sustenance.

In earlier times, when the Inuit were more nomadic, they simply
would have broken camp and moved to a more suitable location.
In fact, the historical scientific literature contains references to aban-
doned Inuit camps located on the precipices of an eroding coast.
Gerald MacCarthy, then at University of North Carolina, in an article
published in Arctic in 1953 titled, ‘‘Recent Changes in the Shoreline
near Point Barrow, Alaska,’’ wrote:

At ‘‘Nuwuk’’ [also called ‘‘Newtok,’’ the same location pho-
tographed by the New York Times in 2007; see Figure 4.22 in
insert] the evidence of rapid retreat is especially striking. The
abandoned native village of the same name, which formerly
occupied most of the area immediately surrounding the sta-
tion site, is being rapidly eaten away by the retreat of the
bluff and in October 1949 the remains of four old pit dwell-
ings, then partially collapsed and filled with solid ice, were
exposed in cross section in the face of the bluff. In 1951 these
four dwellings had been completely eroded away and several
more exposed.

What’s new here? Not much. Hume et al., in a 1972 paper, include
a 1969 photograph with the caption: ‘‘Aerial view of the bluffs near
the village recently settled. One building collapsed and one has been
moved from the bluffs as a result of the 1968 storm. The beach
formerly was 30 m. in width at this point. Photo taken in August
1969.’’ The authors go on to add, ‘‘The village will probably have
to be moved sometime in the future; when depends chiefly on the
weather. . . .’’ (We do not reproduce the picture here because it is
of very poor quality.)

Clearly, erosion has been gnawing away at the Alaska coast for
many, many decades and this fact has been known for a long time.
Wind and waves acting on soil held together by ice acts through a

131

CLIMATE OF EXTREMES

destructive feedback to expose more frozen soil to the above-freezing
temperatures of summer and the warm rays of sunshine, softening
it for the next round of waves and wind. And so the process contin-
ues. A decline in near-shore ice cover helps to exacerbate the process.
Ignoring these well-known environmental conditions has led to the
unfortunate situation today where Inuit villages are facing an immi-
nent pressure to relocate. This situation has less to do with anthropo-
genic climate change than it does with poor planning in the light of
well-established environmental threats, with or without global
warming.

Antarctica

Antarctica’s ice sheets and glaciers are the largest mass of ice on
the planet, comprising some 25.71 ⳯ 106 cubic kilometers (6.18 ⳯
106 cubic miles), or 89.5 percent of total global land ice. Global
warming theory predicts, in general, that warming is enhanced in
cold, dry regions, but Antarctica is an exception. There’s plenty of
evidence that, as a whole, it hasn’t warmed a bit in the last four
decades, or even may have cooled, as shown by Peter Doran in
Nature in 2002.

Doran’s work was extensively noted in Meltdown. He found a net
cooling since 1966, but a strong warming around the small Antarctic
Peninsula, the strip of land comprising less than 2 percent of the
continent that juts out toward South America.

There are more recent analyses. In 2007, William Chapman and
John Walsh of the University of Illinois extensively reviewed and
updated the climate literature on Antarctic warming, concluding
‘‘These studies are essentially unanimous in their finding that the
Antarctic Peninsula has warmed since the 1950s, when many of the
surface stations were established.’’ But that’s hardly the true picture
of what is happening over the continent as a whole. They wrote,
‘‘Recent summaries of station data show that, aside from the Antarc-
tic Peninsula and the McMurdo area, one is hard-pressed to argue
that warming has occurred, even at the Antarctic coastal stations
away from the peninsula and McMurdo.’’ Furthermore, they state,
‘‘Recent attempts to broaden the spatial coverage of temperature
estimates have shown a similar lack of evidence of spatially wide-
spread warming.’’

132

Sea-Level Rise and the Great Unfreezing World

Chapman and Walsh collected temperature in and around Antarc-
tica from 460 locations including 19 manned surface observation
stations located on the continent, 73 automated weather stations,
and a 2°-latitude-by-2°-longitude gridded sea-surface temperature
time series. They made every attempt to have complete records from
1958 to 2002.

When averaged over the entire region from 60°S to 90°S (an area
much larger than Antarctica proper), Chapman and Walsh found:

The 45-yr linear temperature change is largest in winter
(Ⳮ0.776°C; 1.40°F) and spring (Ⳮ0.405°C; 0.73°F), and small-
est in summer (Ⳮ0.193°C; 0.35°F) and autumn (Ⳮ0.179°C;
0.33°F). These temperature changes correspond to linear
trends of Ⳮ0.172°C/decade; 0.31°F (winter), Ⳮ0.090°C/
decade; 0.16°F (spring), Ⳮ0.045°C/decade; 0.08°F (summer),
and Ⳮ0.040°C/decade; 0.07°F (autumn).

But these are for the whole region, rather than the continent.
Referring specifically to Antarctica, they found that ‘‘the 45 yr
(1958–2002) linear temperature change of annual mean temperatures
is Ⳮ0.371°C (0.66°F) with a corresponding trend of Ⳮ0.082°C per
decade (0.14°F).’’ Furthermore, the authors found: ‘‘Statistically sig-
nificant warming is confined to the Antarctic Peninsula and a small
region along the eastern coast of the continent. Temperature trends
over the remainder of the Antarctic continent do not exceed significance
thresholds [emphasis added].’’ So, at this point, we have learned that
Antarctica is warming in its bitterly cold winter season and most of
the warming is confined to a small area surrounding the Antarctic
Peninsula (Figure 4.23; see insert).

Here is the interesting twist to the story. The results are actually
quite consistent with Doran’s finding of a cooling (noted in Melt-
down), given that the Doran study began in 1966. Graphs of seasonal
and annual temperature trends show that the coldest years tend to
occur at or near the beginning of the record, in the 1950s. Chapman
and Walsh find: ‘‘Trends computed using these analyses show con-
siderable sensitivity to start and end dates with starting dates before
1965 producing overall warming and starting dates from 1966 to
1982 producing net cooling rates over the region.’’ In this case,
‘‘region’’ refers to the entire area south of 60° latitude.

Because a cooling or a constant-temperature Antarctica seems so
counter to greenhouse theory, it’s necessary to come up with some
logical construct to explain its behavior.

133

CLIMATE OF EXTREMES

There are two competing hypotheses. The first involves another
set of industrial emissions, the chlorofluorocarbons (CFCs). CFCs
were originally used as inert propellants in aerosol spray cans, and
as refrigerants. They haven’t been seen in a spray can since the late
1970s (contrary to what many people think), and they were phased
out as refrigerants via the Montreal Protocol, a UN agreement that
went into force in 1989. (The protocol actually allows developing
countries to produce CFCs through 2010, but there has already been
a slight reduction in their overall atmospheric concentrations.)

CFCs break down into their constituent elements, including chlo-
rine and fluorine, which have long been known to assist in the
destruction of ozone under conditions observed in the stratosphere.
Ozone absorbs ultraviolet radiation from the sun (the same type of
radiation that gives us sunburn), and therefore it keeps the strato-
sphere warmer than it would be if it were absent.

Over most of the planet, the stratosphere begins some 8 to 10
miles above the surface. But, in frigid Antarctica, it lies much closer,
as the coldness of the atmosphere compresses its layers. The strato-
sphere gets down to about 25,000 feet above sea level at the South
Pole, which itself is at 9,300 feet. The distance between the strato-
sphere and the Pole is therefore roughly 16,000 feet (on the U.S.
East Coast, the distance is about 35,000 feet), so the unusually cold
stratospheric air, chilled further by ozone depletion, occasionally
mixes down and is found over the entire continent. Hence, a cool-
ing trend.

A second (and not necessarily competing) explanation is that the

lack of warming is caused by warming of the surrounding ocean.

Huh? Most people might lump anyone who says this with our
neighbors who insist that if you put hot water in the ice-cube tray,
then it freezes faster than cold water. That’s just not so and violates
physical thermodynamics.

But special factors come into play owing to Antarctic geography.
The surrounding ocean has warmed a few tenths of a degree (Cel-
sius). That might not seem like much, but even a teensy warming
of water increases the amount of water vapor that is given up to
the atmosphere, and the vastness of the Southern Ocean will give
up a lot of molecules even if it warms only a bit. Northerly winds,
which are common, will push this moisture up and over the continent.
Antarctica (or any continent) is a lot rougher than the surrounding
smooth ocean, so any air stream that impinges upon it has to slow

134

Sea-Level Rise and the Great Unfreezing World

down. That phenomenon, known as ‘‘convergence,’’ leaves the air
nowhere to go but up (it can’t go down into the continent!) and this
results in the formation of clouds, as the air ascends, cools, and
reaches the level at which clouds condense out.

Clouds of this derivation are pretty shallow—usually less than
10,000 feet in height—and bright white when viewed from above.
As a result, they reflect away much of the sun’s energy—much more
than they absorb from the earth because of their extensive amount
of water vapor. Consequently, they could produce a net cooling of
the continent.

Further, they should produce more snow. In fact, computer mod-
els for 21st-century climate have Antarctica gaining ice—that is,
contributing to a relative lowering of sea-level rise, because of an
increase in snowpack.

The two possible mechanisms for Antarctic cooling—stratospheric
ozone loss and increased reflective cloudiness caused by a warming
ocean—aren’t going to go away soon.

Long-term studies of Antarctic ice differ. A paper published in
Science in 2005 by the University of Missouri’s Curt Davis and col-
leagues indicated a net increase in ice mass over the continent (Figure
4.24). Though there were declines in ice over the Antarctic Peninsula
and the adjacent West Antarctic ice sheet, there were gains in the
much larger East Antarctic ice sheet. A highly cited much shorter-
term study, written by Isabella Velicogna and John Wahr and pub-
lished in Science in 2006, found a decline in the West and no net
change in the East (Figure 4.25).

Note that in the Davis study that the East reaches its maximum
elevation in mid-2002. This is the same time as the starting point
for the Velicogna and Wahr study, where the first data point, in
mid-2002, is also the highest. The Velicogna and Wahr study is
necessarily short because the satellite measuring system, called
GRACE (‘‘Gravity Recovery and Climate Experiment’’) satellite only
became operational in early 2002. If one puts the two together, it
still looks like a net increase for the East.

Andrew Shepard and Duncan Wingham, two British scientists,
summarized six recent studies of Antarctic ice and concluded that
it is most likely that the East Antarctic ice sheet is gaining about
6 cubic miles of ice per year, and the West Antarctic losing about 12,
for a net loss of 6. They argue that the data are so poor that it is

135

CLIMATE OF EXTREMES

Antarctica: More Fact-Checking, Please

Sure enough, one can use a computer to combine the effects
of ozone depletion and the patterns of warming in the far
Southern Hemisphere to forecast the future, which is what
NASA’s Drew Shindell and Gavin Schmidt did in Geophysical
Research Letters in 2004.

Without being technical to the point of boredom, it has been
the ‘‘normal’’ condition for temperatures around Antarctica
and those on the continent to change in different directions.
Scientists even give this a name, the ‘‘Southern Annular Mode’’
(SAM), ‘‘Annular’’ referring to its ringlike structure, owing to
the geometry of the ocean surrounding Antarctica.

Their computer model predicts that SAM is going to pretty
much disappear because of global warming. NASA’s press
office then produced a lurid press release (!) about Shindell
and Schmidt’s modeling results, promising certain disaster for
the region because of ‘‘ice sheets melting and sliding into the
ocean’’ leading to ‘‘greatly increasing sea levels.’’ It might be
worth noting that James Hansen is Gavin Schmidt’s boss.

Only one problem, which is more than vaguely analogous
to the difficulties with Hansen’s hypothesis that Greenland is
about to fall apart: This study, too, does not comport with
history.

Shindell and Schmidt claim that SAM is in its current position
because of stratospheric ozone depletion over Antarctica,
which, they say will become much less significant in coming
decades as the putative cause—CFCs—are phased out.

But J. M. Jones and M. Widmann showed in Nature in 2004
that the SAM looked a lot like it does today some 40 years
ago—long before ozone loss. Further, they found it resembled
what NASA forecasts for the future, even though the planet
was cooler during the first half of the 20th century!

difficult to assume that there have been any major changes in the
last decade. Nor are the mechanisms clear. It’s clearly too cold to
be caused by melting. For example, the Amundsen Sea, in which the
West Antarctic ice sheet terminates, shows no evidence for warming.

136

Sea-Level Rise and the Great Unfreezing World

ELEVATION CHANGE OF THE EAST ANTARCTICA ICE SHEET,

Figure 4.24

1992–2003

SOURCE: Davis et al. 2005.
NOTE: cm ⳱ centimeter.

As noted earlier, Shepard and Wingham also summarized recent
findings for Greenland’s ice and estimated a net annual loss of
25 cubic miles. When combined with the data from Antarctica, the
loss figures contribute to an annual sea-level rise of 0.01 inches per
year, an amount far too small to measure. Although they express
concern that current models for ice dynamics are very crude, oddly
enough, they make no reference to the long period in the early 20th
century when Greenland was warmer and that condition obviously
triggered no catastrophic change in the behavior of the 685,000 cubic
miles of ice that sit atop the island continent.

Antarctic Paradox: Does Less Equal More?

‘‘Escalating Ice Loss found in Antarctica: Sheets Melting an
Area Once Thought to Be Unaffected by Global Warming’’
—Washington Post, January 14, 2008

In a multipage article beginning above the fold on the front page,
Post writer Michael Kaufman describes the just-published research

137

CLIMATE OF EXTREMES

ICE-MASS ESTIMATE BY THE GRACE SATELLITE FOR WEST

ANTARCTIC AND EAST ANTARCTIC ICE SHEETS, 2002 TO MID-2005

Figure 4.25

SOURCE: Velicogna and Wahr 2006.
NOTE: km3 ⳱ cubic kilometer.

results by Eric Rignot and colleagues: ‘‘Climatic changes appear
to be destabilizing vast ice sheets of western Antarctica that had
previously seemed relatively protected from global warming . . .
raising the prospect of faster sea-level rise than current estimates.’’
Rignot et al.’s study came out after the summary article of Shep-
herd and Wingham. But in reality, it’s merely consistent with many
of the studies noted there.

Rignot et al.’s study is at variance with all recent simulations of
21st-century climate in the Antarctic, which predict a gain in ice
because of increasing snowfall. Because of this conflict, and because
of clear indications of a net gain in the entire Southern Hemisphere
ice extent (see below) it’s currently impossible to say what is really
happening.

Rignot et al. use satellite observations to determine ice stream
velocity and ice thickness, which are combined to calculate out how

138

Sea-Level Rise and the Great Unfreezing World

much ice is flowing off Antarctica and into the oceans. This repre-
sents the total ice loss from Antarctica, as it is generally much too
cold for melting to be a significant source of ice loss. To determine
ice gains—through snowfall accumulation—the researchers used a
weather model to simulate snowfall. The net change in ice is the
difference in the input minus the outflow. Their study covered the
period 1992–2006.

Why use a model of ice input (snowfall) and actual observations
of outflow? Well, actual snowfall measurements are few and far
between over the vast continent of Antarctica, and it’s very hard to
measure, because much ‘‘new’’ snow is admixed with old stuff
blown off of the surface. Rignot et al. do not use the modeled annual
values of snowfall, but instead use the average modeled snowfall
across the years 1980 through 2004. Rignot calculates actual outflow
for various years from the satellite observations, but uses a fixed
amount of input (i.e., snowfall) that represents average conditions
rather than the year-to-year variations.

In reality, there is a tremendous amount of interannual and inter-
decadal variation in snowfall across Antarctica. In 2006, Andrew
Monaghan, of Ohio State University’s Byrd Polar Research Center,
and colleagues examined the snowfall history over Antarctica from
1955 through 2004 (again using snowfall amounts produced by
weather models and verified by the available observations—mostly
ice cores rather than direct snowfall observations). They concluded
that there has not been any appreciable change in snowfall over
Antarctica over the full period of record of their study, but that there
is a fairly large year-to-year and decade-to-decade variation.

Again, it is worth noting that all modern climate models predict
that Antarctica will gain mass as the climate warms because the
continent will see an increase in snowfall—enough to offset glacial
ice losses along the periphery. But, as Monaghan et al. recently
reported, this snowfall increase has not been detected. Does this
represent a failure of all climate models? If this is indeed true, what
does this leave us for future projection?

Kaufman closed his Post article by noting that the head of the
Intergovernmental Panel on Climate Change, Rajenda Pachauri, was
heading down to Antarctica that week ‘‘to get a firsthand view of
the situation.’’

If the weather was clear, before he even got there he was in for
a shock. Despite an overall slight warming of the Southern Ocean,

139

CLIMATE OF EXTREMES

SATELLITE-SENSED RECORD OF SOUTHERN HEMISPHERE ICE AREA,

Figure 4.26

1979 TO PRESENT

SOURCE: Cryosphere Today 2007. http://arctic.atmos.uiuc.edu/cryosphere/.
NOTE: km2 ⳱ square kilometer.

the amount of ice surrounding Antarctica reached all-time record
levels in 2007. Satellites first began to monitor this almost 30 years
ago. Figure 4.26 shows that the ice reached its greatest extent in
southern winter 2007 (northern summer), and that departure from
average for a given month, in January 2008 (when the Post article
came out), was the greatest ever measured (Figure 4.27).

Midway through the very long January 14 article is a statement
that ‘‘these new findings come as the Arctic is losing ice at a record
rate.’’ Wouldn’t that have been the appropriate place to note that
the Southern Hemisphere was, at the same time, setting records for
overall ice extent? (As of this writing, in September 2008, the South-
ern Hemisphere ice anomaly is back to its normal range.)

Kilimanjaro Redux

The ‘‘snows’’ of Africa’s Mount Kilimanjaro inspired the title
of an iconic American short story, but now its dwindling

140

Sea-Level Rise and the Great Unfreezing World

SOUTHERN HEMISPHERE SEA ICE ANOMALY, 1979–2008

Figure 4.27

SOURCE: Cryosphere Today 2007. http://arctic.atmos.uiuc.edu/cryosphere/.
NOTE: km2 ⳱ square kilometer.

icecap is being cited as proof for human-induced global
warming.

However, two researchers writing in the July–August edition
of American Scientist magazine say global warming has noth-
ing to do with the decline of Kilimanjaro’s ice, and using the
mountain in northern Tanzania as a ‘‘poster child’’ for climate
change is simply inaccurate.

—University of Washington Press Release, June 11, 2007

Meltdown led off its parade of climate horrors with ‘‘The Snowjob
of Kilimanjaro,’’ demonstrating that, if it were 1976 (after three
decades of global cooling), one could have written ‘‘Kilimanjaro’s
glaciers will completely disappear by 2015 if this cooling trend
continues.’’

What’s new since Meltdown?
Al Gore featured Kilimanjaro in his book and movie, giving it
a four-page spread in the print version, and featuring Ohio State

141

CLIMATE OF EXTREMES

University’s Lonnie Thompson, a scientist and chief publicist for the
Kilimanjaro-Will-Disappear-Soon story, standing next to the ‘‘pitiful
last remnants of one of [Kilimanjaro’s] glaciers.’’

Really?
One nice thing about glaciers is they leave records of where they
were, in piles of debris, called moraines, and these can be dated. It
is well known that Kilimanjaro’s glaciers were far advanced beyond
where they are today when the earth was warmer, for several millen-
nia after the end of the last ice age. So the glacier can obviously take
(and prosper under) warmer conditions.

The old adage ‘‘it’s not the heat, it’s the humidity!’’ clearly applies

to Kilimanjaro.

In 2004, Georg Kaser and four colleagues wrote in the International
Journal of Climatology, that ‘‘a drastic drop in atmospheric moisture
at the end of the 19th century and ensuing drier climatic conditions
are likely forcing glacial retreat on Kilimanjaro.’’ Overall, Kaser
et al. wrote that ‘‘. . . the climatic evolution of East Africa . . . is
characterized by a drastic dislocation around 1880, when lake levels
dropped notably and glaciers started to recede from their maxi-
mum extent.’’

That is concurrent with the end of the ‘‘Little Ice Age,’’ a cold
period noted in many locations around the planet that lasted about
400 years. It also is near the point in time when surface temperatures
around the planet began to climb, but long before there could have
been much influence from increased carbon dioxide.

Kaser et al. say that the glaciers will survive, despite Gore’s protes-
tations. ‘‘If the present precipitation regime persists,’’ they conclude,
‘‘then these glaciers will most probably survive in positions and
extents not much different than today. This is supported by the [fact]
that slope glaciers retreated more from 1912 to 1953 than since then.’’
In a 2006 edition of Geophysical Research Letters, N. J. Cullen et al.,
of the Tropical Glaciology Group at University of Innsbruck, should
have finally stilled the Kilimanjaro hue-and-cry (but alas, did not).
Cullen et al. point out that the glaciers on the mountain are above
(higher than) the mean freezing level, meaning that it is ‘‘difficult
to suggest that air temperature changes alone are responsible for
glacier recession on Kilimanjaro.’’

To re-evaluate possible causes of glacier retreat on Kilimanjaro,
Cullen et al. employed recent high spatial-resolution satellite images

142

Sea-Level Rise and the Great Unfreezing World

of the mountain to construct a new detailed map of the ice bodies.
They compared the new data to long-term variations in ice extent
to assess its retreat in the context of 20th-century changes in air
temperature, atmospheric moisture, and precipitation in East Africa.
The group of researchers found that glacial retreat during the 20th
century was profound, as their work shows that only 21 percent of
the 1912 ice cover on Kilimanjaro existed in 2003 when the satellite
images were taken. However, the highest recession rates occurred
in the first part of the 20th century, while the recession rate over
the last 15-year interval (1989–2003) was smaller than in any of the
other defined intervals in the study period of 1912–2003. Obviously,
this is counterfactual to the notion that the recession is largely caused
by recent (anthropogenic) warming. Given this curious finding, Cul-
len et al. set out to interpret the findings in the context of 20th-
century climate change.

The ice bodies of Kilimanjaro are stratified into two types of
glaciers—plateau (elevation ⱖ5700 m [18,700 feet]) and slope
(⬍5700 meters)—to help differentiate by physical features such as
shape, slope, thickness, and bed shape. Characterized by name, pla-
teau glaciers are tabular-shaped ice bodies that rest stably on flat
surfaces near the top of the mountain. In contrast, slope glaciers are
found on steeper surfaces and move downward. The retreat of all
plateau glaciers was found to be continuous and linear since 1912,
whereas slope ice bodies experienced a rather rapid recession
between 1912 and 1953, followed by a decreasing rate of retreat. The
constant retreat of the plateau glaciers does not indicate that climate
fluctuations during the 20th century affected their demise. Instead,
solar radiation incident on vertical walls of the glaciers produces
irreversible melting despite air temperatures that remain below
freezing. Cullen et al. conclude that the demise of the plateau glaciers
of Kilimanjaro is unavoidable given their geometry, and that any
recent change in climate has had no significant impact. Thus, the
decline of the plateau glaciers of Kilimanjaro are excused from the
global warming debate.

Unlike with plateau glaciers, Cullen et al. believe that slope gla-
ciers have a much shorter adjustment time to changes in climate—
on the order of a few years. The research group believes that the rapid
recession in the early part of the 20th century (1912–53) indicates that
the glaciers were wildly out of equilibrium in responding to the

143

CLIMATE OF EXTREMES

prior shift in climate during the late 19th century. Cullen et al.
contend that the magnitude of the shift is so large that the slope
glaciers are still out of equilibrium. They note there is no evidence
for any atmospheric warming in the 20th century in the vicinity of
the glaciers. Joining plateau glaciers, the slope glaciers of Kilimanjaro
are excused from the global warming debate.

Cullen et al. conclude that ‘‘Rather than changes in 20th century
climate being responsible for their demise, glaciers on Kilimanjaro
appear to be remnants of a past [19th-century] climate that was once
able to sustain them.’’

Snowpack in the Andes

Halfway around the world from Kilimanjaro run the spectacular
Andes Mountains, the largest and most impressive range in the
Americas.

Meltdown featured a 2001 Washington Post story about the decline
and fall of Peru’s glaciers, which we found odd, because we couldn’t
find any net temperature change for the last three decades. (Hmm
. . . that sounds a lot like Kilimanjaro).

Outside Antarctica, snow cover in the Southern Hemisphere has
not received much attention in the climate change debate. In fact,
within the snow, ice, and frozen ground chapter of the 2007 IPCC
report, approximately 800 words along with three figures and one
table are dedicated to snow cover variability in the Northern Hemi-
sphere, compared with less than 400 words and no accompanying
graphics for variability in the Southern Hemisphere.

Very late in 2006, the Journal of Climate published a paper by
Mariano Masiokas (Instituto Argentino de Nivologı´a, Glaciologı´a y
Ciencias Ambientales and University of Western Ontario geography
department) and colleagues titled, ‘‘Snowpack Variations in the Cen-
tral Andes of Argentina and Chile, 1951–2005: Large-Scale Atmo-
spheric Influences and Implications for Water Resources in the
Region.’’ The research team used snow data from each side of the
central Andes in Chile and Argentina to develop the ‘‘first regional
snowpack series.’’ They examined the six longest and most complete
snow records for the 55-year period in the region, covering an area
stretching from 30°S to 37°S latitude. Their variable for study is
annual maximum snow water equivalent (MSWE).

144

Sea-Level Rise and the Great Unfreezing World

REGIONAL SNOWPACK (MSWE) FROM THE CENTRAL ANDES,

Figure 4.28

1950–2005

SOURCE: Adapted from Masiokas et al. 2006.

The snowpack of the central Andes serves as much more than a
monitor of climate change. The authors explain that ‘‘over 10 million
people in Central Chile and central-western Argentina depend on
the freshwater originating from the winter snowpack of the central
Andes.’’ Alarming is their charge that ‘‘coupled atmosphere–ocean
general circulation models especially targeted to investigate high-
elevation sites’’ have indicated that ‘‘for the next 80 years the central
Andes will probably experience significant temperature increases.’’
To make matters worse, Masiokas et al. note, ‘‘independent general
circulation model simulations also predict a significant decrease
in precipitation over the region for the next five decades.’’ The
combination of higher air temperature and less precipitation in the
central Andes over the rest of this century is not the recipe for a
problem-free regional water supply. Climate models seem to be
sending a strong message to the more than 10 million people in
Chile and Argentina.

Masiokas et al. found no such trend in MSWE, stating that
the regional record ‘‘shows a nonsignificant positive linear trend
(Ⳮ3.95 percent per decade) over the 1951–2005 interval,’’ or an
absolute increase of greater than 21 percent over the period (Figure
4.28). The group matched the MSWE record with mean monthly

145

CLIMATE OF EXTREMES

streamflow data for the primary rivers in the region. They found
that river discharges on both sides of the central Andes ‘‘are strongly
correlated with the snowpack record and show remarkably similar
interannual variability and trends.’’ In other words, the water supply
is hardly decreasing.

In conclusion, there are a number of very interesting papers in
the refereed literature revealing that the ice, snow, and sea-level rise
story is a very complicated one. The latest compilations indicate an
extremely small contribution of Greenland and Antarctica to sea-
level rise, with little evidence for any marked change in the past
decade. The causes of any putative loss in Antarctica are simply
unknown, but one thing is for certain—it hasn’t warmed up down
there. Horror stories about an imminent collapse of Greenland’s ice
simply aren’t borne out by the fact that it was warmer there for
decades in the early 20th century, and for millennia after the end of
the last ice age. And, finally, breathless stories about the end of the
glaciers of Kilimanjaro and a decline in Andean water supply turn
out to be snow jobs.

146

5. Extreme Climate: Floods, Fires,

and Droughts

The Rest of the Storms

Although hurricanes are of special merit owing to their mediage-
nicity and their destructive potential, there are several other types
of cyclones (low pressure areas) on this planet. The generic name
for hurricane is ‘‘tropical cyclone,’’ but their nontropical cousins
(called extratropical cyclones) are orders of magnitude more com-
mon. In fact, hurricanes are quite rare.

Prove this for yourself by watching some cable weather channel.
You’ll see the maps dominated by nameless low pressure systems
in the mid- and high latitudes. The occasional named tropical cyclone
obviously commands a lot of attention (‘‘Category 1 Hurricane Blow-
hard is about to demolish Cocoa Beach! We’ll update you on this
every two minutes!’’—i.e., between commercials.) There are about
a thousand more extratropical storms than hurricanes every year.
Some can be pretty destructive. In the last chapter, you read how
a 1963 cyclone in Alaska caused dramatic erosion along the North
Coast, and prompted a warning from scientists to build permanent
structures far inland.

In October 1962, the Columbus Day storm brought wind gusts to
100 miles an hour in the Pacific Northwest, resulting in massive
blowdowns of the extensive regional forest. In April 1974, a huge
cyclone in the Midwest spun the atmosphere so hard that a still-
record 148 twisters touched down from that single storm. Six were
Category 5 tornadoes, an unheard-of number for one day.

Those storms occurred before the warming that started in the

mid-1970s.

On March 13, 1993, a huge low-pressure system spun up in the
Gulf of Mexico and exploded up the East Coast, earning the title
‘‘Storm of the Century,’’ because it set many records for lowest
barometric pressure ever measured at inland locations. (Note that

147

CLIMATE OF EXTREMES

hurricanes routinely have lower pressures, but that their barometric
pressures rise as soon as they come ashore).

Kevin Trenberth, of the U.S. National Center for Atmospheric
Research, went on Meet the Press the next day and blamed global
warming.

Every strong European cyclone in the last decade has prompted
a similar outcry. If there’s a big storm, a reporter will find an ‘‘expert’’
who will conflate the wind with global warming. Just Google ‘‘news’’
after the next one to prove this to yourself.

All of this seems a bit illogical. Although hurricanes are, in part,
driven by the heat of the ocean, there’s a pretty strong debate,
noted in chapter 3, about their relation to global warming. But the
mechanism that creates and feeds extratropical cyclones is a lot
different. They’re driven by the jet stream, a circumpolar vortex of
high-energy westerly winds that undulates over all our hemisphere
with the exception of the low latitudes. In fact, when the jet does
manage to reach into the tropics and encounters a hurricane, the
hurricane’s days, if not hours, are numbered because of massive
wind shear. The top of the storm can be blown a hundred miles
away from the bottom. Consequently, the same mechanism that
causes extratropical cyclones is one that destroys hurricanes. You
would think, then, if global warming were making extratropical
storms stronger, there should be some concomitant weakening of
hurricanes.

The jet stream is nature’s way of dissipating the temperature
difference between polar and tropical regions in the form of motion.
The greater the temperature difference between the poles and the
tropics, the stronger the jet, and, everything else being equal (danger-
ous words), the stronger extratropical storms can become. But the
reverse is what should happen.

As noted in chapter 1, changes in atmospheric carbon dioxide
result in a preferential warming of the coldest days, and of cold,
dry air more than warm, moist air. Changing the greenhouse effect
then must reduce the temperature contrast between the (warm, moist)
tropical and (dry, cold) polar regions, which reduces the temperature
difference that drives the jet stream. In turn, this should tame the
power of extratropical cyclones.

Computer models nonetheless indicate that some rather small
regions might see an increase in extratropical cyclones. The Novem-
ber 17, 2007, Synthesis Report of the IPCC is a 23-page document that

148

Extreme Climate: Floods, Fires, and Droughts

attempts to summarize thousands of pages of the 2007 ‘‘Fourth
Assessment Report’’ on climate change. It contains only two oblique
references to increasing extratropical storms, talking about ‘‘increased
erosion due to storminess’’ in Europe and ‘‘increases in the severity
and frequency of storms’’ affecting coastal development in Australia
and New Zealand. New Zealand is so far south that hurricanes are
exceedingly weak and rare, so this reference must mean extratropi-
cal cyclones.

In the original 1,009-page science section of the ‘‘Fourth Assess-
ment Report,’’ there’s one page, in the chapter on global climate
projections (chapter 10), devoted to extratropical cyclones. It features
a large number of citations and very dense prose. Some computer
models increase their frequency, some decrease it. Others increase
intensity, and still others decrease intensity. About the only thing
that is agreed upon is something that doesn’t require a computer
model: if you preferentially reduce the amount of cold air with a
changing greenhouse effect, the average track of these storms will
shift slightly poleward. The notion of storm ‘‘tracks’’ itself is a little
misleading, as low pressure systems can appear and travel to any-
where. ‘‘Track’’ just means that more storms tend to appear in some
places rather than others.

In the IPCC report’s next chapter, there are some projected
changes, given with ‘‘low’’ confidence. They include a ‘‘decrease in
the total number,’’ and a ‘‘slight poleward shift of storm track’’ of
extratropical cyclones, an ‘‘increased number of intense cyclones’’
and an ‘‘increased occurrence of high waves.’’

One reason for ‘‘low’’ confidence may have to do with the behavior

of extratropical cyclones as the greenhouse effect has enhanced.

Here’s the expanded (and very testable) statement from chapter

11 of the IPCC’s 2007 science report:

[Low confidence in] [i]ncreased number of intense cyclones
and associated strong winds, particularly in winter over the
North Atlantic, central Europe, and Southern Island of
New Zealand.

What has happened as the planet warmed?

North Atlantic and European Cyclones

Christoph Matulla of Environment Canada and colleagues took
a look at European storminess and published the results in the 2007
volume of the refereed journal Climate Dynamics.

149

CLIMATE OF EXTREMES

Matulla et al. began by noting, ‘‘In the North–East Atlantic and
the North Sea, a roughening storminess was perceived and public
concern was raised in the early 1990s.’’ Of course, in the early 1990s
global warming hype shifted into high gear, especially in Europe.
Matulla et al. note that others have investigated trends in stormi-
ness in Europe over the time scale of 100 years, and on the basis of
daily wind data, found no trend. However, long-term wind data ‘‘are
characterized by spatial sparseness and inhomogeneities, caused by
instrumentation changes, site moves and environmental changes.’’
They state that this fact ‘‘highlights the importance of employing
data that reach far back in time before any judgment about stormi-
ness can be made.’’

The scientists argue that ‘‘High wind speeds across Europe are
generally associated with extratropical cyclones, which occur in
North or North-Western Europe all year but in Central Europe
almost entirely from November to February.’’ Therefore, if we had
evidence of the strength of the cyclones, we would have a way to
detect if they have become more or less fierce in recent decades.
Some of our longest weather records come from European locations.
Many include barometric pressure—a direct measure of an extra-
tropical cyclone’s intensity—which is given by the difference in
pressure between the outer and inner regions of a storm. The greater
the difference, the stronger the wind.

Using long barometric records, they calculated daily winds back
to 1875. Figure 5.1 shows the results. They found that northwestern
European storminess starts at rather high levels in the 1880s,
decreases to below-average conditions around 1930, and generally
continues declining through the 1960s. From then until the mid-
1990s, a pronounced rise occurs, and levels similar to those early in
the century are reached. Since the mid-1990s, storminess is around
average or below. This picture—a decline that lasts several decades
followed by an increase from the 1960s to the 1990s and a return to
calmer conditions recently, is found in Northern Europe, too. The
increase, however, is far less pronounced. In Central Europe, stormi-
ness peaks around the turn of the 20th century, followed by a rapid
decrease to 1960. That is followed by the familiar increase to the
1990s, with the most recent years showing a return to average
conditions.

150

Extreme Climate: Floods, Fires, and Droughts

INDEX OF 99TH PERCENTILE OF DAILY WIND STRENGTH FOR

Figure 5.1

EUROPE, 1875–2002

SOURCE: Adapted from Matulla et al. 2007.

They conclude that their work is in agreement with other studies
in Europe showing ‘‘that storminess has not significantly changed
over the past 200 years.’’

This study was predated by a much longer history over a much
smaller geographic region. Writing in the 2004 edition of Geophysical
Research Letters, Lars Barring and Hans Von Storch were very con-
cerned about the public perception of increased storminess and
global warming. They wrote:

The public and ecosystems in storm-prone areas . . . are well
adjusted to the continuous stream of passing windstorms.
However, every now and then extreme windstorms cause
severe damage. Together with the perspective of anthropo-
genic climate change, such extreme events create the percep-
tion that the storm climate would change; that the storms
lately have become more violent, a trend that may continue

151

CLIMATE OF EXTREMES

into the future. The question is, of course, whether this per-
ception is essentially caused by certain deeply rooted cultural
notions about the relationship between man and nature, or
whether such changes are real.

Again, they used barometric records, this time extending back
more than 200 years for two locations in Sweden—Lund and
Stockholm.

The barometer was invented by Torricelli in the mid-17th century,
and some routine pressure observations can be found from the 18th
century, although they typically are sporadic.

But the Lund and Stockholm data are of remarkable quality, with
consistent readings as far back as the late 1700s. Station pressure
readings were taken at least three times daily at Lund since 1780
and at Stockholm from 1820. Barring had examined these records
for potential biases and inhomogeneities in prior publications and
has developed a very long history of air pressure at these two sites.
In the current paper, the authors looked at three variables: the
annual number of ‘‘storms’’ (station pressure less than 980 millibars
[mb]; 28.94 inches of mercury on your home weather station); the
annual number of observed pressure drops of more than 16 millibars
(0.47 inches) in 12 hours; and extremes in the within-year distribution
of 12-hour pressure changes. Each of those variables was then exam-
ined over the entire period of record to look for evidence of cli-
mate change.

A low pressure system of 980 mb is deep enough to generate
winds capable of some damage. A change of 16 mb in a day requires
a very active jet stream, which is one of the main sources of power
for the common cyclone.

Figure 5.2 shows the long-term record of the annual number of
observations of pressure below 980 mb. The records show very little
change. The smoothed lines fitted through the data to better present
long-term variations do show some minor undulations.

Of course, greenhouse gases have been increasing over this entire
period (albeit only very slowly back in 1780), but the concentration
of carbon dioxide shot up most rapidly since the mid-20th century.
If you looked at records since only World War II, you could spot a
teensy increase to the 1980s. But in the context of this more complete
long-term record, it was equally stormy in the 1860s–1870s, when
greenhouse-gas changes were virtually nil. Further, since 1990, the

152

Extreme Climate: Floods, Fires, and Droughts

ANNUAL NUMBER OF OBSERVATIONS OF BAROMETRIC PRESSURE
BELOW 980 MB IN LUND AND STOCKHOLM, SWEDEN, 1779–2002

Figure 5.2

SOURCE: Adapted from Barring and Von Storch 2004.

pressure readings appear to have settled back to values near their
long-term average.

The researchers’ main conclusions?

1. ‘‘No significant robust long-term trends’’
2. ‘‘The conspicuous increase in [the frequency of large 12-hour
pressure drops] in Stockholm in the 1980s is evident but much
less pronounced in the other storminess indices for Stockholm.’’
3. ‘‘The 1860s–70s was a period when the storminess indices
showed general higher values of comparable magnitude as
during the 1980s–90s. However . . . it is also clear that the
indices have returned to close to their long-term average.’’

4. ‘‘The time series are remarkably stationary in their mean, with
little variations on time scales of more than one or two decades.’’

They write: ‘‘[Our results] support the notion of an amplified
storminess in the 1980s, but show no indication of a long-term robust
change towards a more vigorous storm climate.’’

All climate records, particularly records prior to the 20th century,
should be viewed with a healthy dose of skepticism. But those
Scandinavian pressure histories are of remarkably high quality and
simply show no evidence of unusual storminess as the planet
warmed.

153

CLIMATE OF EXTREMES

Nice Timing: New York Times Splashes Unrefereed Science
during UN Global Warming Confab

On December 3, 2007, 15,000 climate bureaucrats, under the aus-
pices of the UN, descended on Bali to discuss what is going to
happen after the failed Kyoto Protocol on global warming expires
in 2012. If everyone adhered to it, Kyoto would reduce global warm-
ing by 0.13°F (0.07°C) by 2050. The treaty was supposed to reduce
net emissions of carbon dioxide by a bit over 5 percent. Instead,
emissions rose approximately 5 percent (‘‘approximately,’’ because
different sources, such the U.S. Department of Energy and the Euro-
pean Environmental Agency, among others, give slightly different
figures).

Actually, the 2007 Bali confab was to discuss what to discuss next!
Hey, isn’t this what the Internet is for—electronically communicat-
ing unless travel is really necessary? Who cares about all the carbon
dioxide emitted by the jets of the climate nomenklatura? It was winter
in the Northern Hemisphere, and Bali is hot.

It hardly seemed an accident, when, on December 5, an article
appeared in the New York Times extensively citing a nonrefereed
study from a climate lobby called ‘‘Environment America’’ claiming
that ‘‘extreme’’ precipitation has been increasing across the United
States in the last half-century. Too bad that the sponsor, the Pew
Charitable Trusts, didn’t even get a refereed scientific article for
their money (though they did hit the Times without one).

The Times quoted Environment America: ‘‘Across the United
States, the number of severe rainfalls and heavy snows has grown
significantly in the last half-century, with the greatest increases in
New England and the Middle Atlantic region.’’ Of course, the Times
mentioned that that was just what was predicted to occur from
global warming.

While most American farmers think more precipitation is a good
thing, as evaporation exceeds normal rainfall most every summer,
Environment America was quick to warn that ‘‘An increase in the
frequency of storms delivering large amounts of rain or snow does
not necessarily mean more water will be available’’ and that ‘‘[w]hile
it may seem like a paradox, scientists expect that extreme downpours
will be punctuated by longer periods of relative dryness, increasing
the risk of drought.’’

Traditionally, the Times only writes about refereed science. Here
they reported on none and ignored a paper that had appeared in

154

Extreme Climate: Floods, Fires, and Droughts

Geophysical Research Letters only the day before (by David Brommer,
of the University of Alabama, and two coauthors), which concluded,
with less rhetoric, that there has been a very slight increase in the
frequency of the heaviest rainfalls in the United States. Indeed, none
of this is news. A similar finding appeared by Michaels et al., in
2004 in the International Journal of Climatology

Environment America makes alarmist claims, tests the one claim
it knows it can prove, does not discuss those findings in the proper
context, chooses not to investigate claims that likely won’t be sup-
ported by the data, and then throws in some factual errors for
good measure.

Environment America asserts the following:

In summary, scientists expect global warming to alter general
precipitation patterns over the contiguous United States in
four key ways:
● Storms with extreme rates and amounts of rain or snowfall
will become more frequent.
● Summers will tend to be drier while winters will be wetter.
Total precipitation will increase over most of the country but
not in the Southwest.
● The frequency of extreme events will increase much more
than total precipitation.
● Precipitation will become increasingly likely to fall as rain
rather than snow— a simple consequence of increased tem-
peratures. Paradoxically, the number of dry days will also
increase, because intense downpours will punctuate longer
intervals of relatively dry weather.

Those assertions are all testable with the precipitation data set
compiled by Environment America, but the only one the group
examined was the first one. And, given that annual total average
precipitation has shown a general increase of about 10 percent in
the United States over the past century or so (Figure 5.3, top), it was
a pretty safe bet that the number of ‘‘extreme’’ precipitation events
must be increasing as well.

Why a safe bet? Because, based on the nature of the distribution
of precipitation events, there is a strong association between total
precipitation and precipitation from heavy or extreme events. If you
think about it, it’s obvious. You can’t very well get a lot more
precipitation from a lot of light precipitation events—it takes 20

155

CLIMATE OF EXTREMES

Figure 5.3

ANNUAL PRECIPITATION (TOP) AND ANNUAL AVERAGE

FREQUENCY OF EXTREME PRECIPITATION EVENTS ACROSS THE

UNITED STATES (BOTTOM), 1948–2006

SOURCE:: National Climatic Data Center (top), http://www.ncdc.noaa.gov/
oa/climate/research/cag3/na.html; Environment America (bottom) Envi-
ronment America, http://www.environmentamerica.org/home/reports/
report-archives/global-warming-solutions/global-warming-solutions/
when-it-rains-it-pours-global-warming-and-the-rising-frequency-of-
extreme-precipitation-in-the-united-states.

156

Extreme Climate: Floods, Fires, and Droughts

additional 0.10-inch events per year to contribute as much additional
precipitation as one 2.00-inch event, or four 0.5-inch events. Twenty
additional rain-days in a year is a large change while a few extra
days with thunderstorms are hardly noticeable. By and large, the
total annual precipitation that a location in the United States receives
is highly dependent on the number of heavy or ‘‘extreme’’ rain days.
This is true now, it was true 100 years ago, and it will be true in
the future.

This becomes obvious when comparing the findings from Envi-
ronment America for changes in extreme precipitation frequency
(Figure 5.3, bottom) to a plot of the annual total precipitation aver-
aged across the United States (figure 5.3, top).

Notice how closely Environment America’s ‘‘average annual fre-
quency of extreme rainstorms’’ tracks the observed average total
annual precipitation across the United States. Both data sets show
low values in the 1950s (when the United States was in a major
drought), an increase from the 1950s to the early 1980s, low values
in the late 1980s, high values in the mid-1990s, low values in the
late 1990s to early 2000s and near-average in 2006, the last year
in the study. Overall there is an upward trend from the drought
conditions of the 1950s to the general wetness of the past few
decades. That close correspondence between the frequency of
extreme precipitation events and total average precipitation is what
we wrote about in 2004 in International Journal of Climatology, and
what is by and large the way things have to be. The higher the
precipitation, the more of it comes via heavier events and vice versa.
So if the climate is changing so that we get more precipitation, it
virtually has to be the case that more of it will come in heavy or
extreme events. C’est la vie.

So while Environment America’s claim had to be true, it was a

no-brainer.

The second claim, that summers will tend to be drier while winters

will be wetter, could have easily been tested.

Alas, Environment America presents no seasonal data. Wonder
why? Well, it is because it likely wouldn’t have found any evidence
for this assumed interseasonal behavior.

Figure 5.4 shows seasonally averaged U.S. precipitation data from
1948 to 2006 by season. Reality is virtually the opposite of Environ-
ment America’s assertion: There is no trend in winter precipitation;
spring, summer, and fall seasons all show slight increases.

157

CLIMATE OF EXTREMES

U.S. PRECIPITATION BY SEASON, 1948–2006

Figure 5.4

What about the third claim, that the ‘‘frequency of extreme events
will increase much more than total precipitation?’’ Another easily

158

Extreme Climate: Floods, Fires, and Droughts

Figure 5.4 (continued)

SOURCE: National Climatic Data Center 2007. http://www.ncdc.noaa.gov/
oa/climate/research/cag3/na.html.

159

CLIMATE OF EXTREMES

tested hypothesis given the group’s precipitation data set, but no
results are presented. Why not?

Figure 5.5 depicts what we published in our 2004 paper, showing
the amount of precipitation that falls on the wettest day each year
(i.e., ‘‘extreme’’ precipitation) averaged across the United States from
1910 to 2001. Notice that we indeed found that the wettest days of
the year were getting wetter, or to put it another way, precipitation
during extreme events was increasing. But contrary to the expecta-
tion of Environment America, the amount of precipitation falling
on the wettest day as a percentage of the annual precipitation did
not change at all over the same period (Figure 5.5). In other words,
precipitation amounts in extreme events were not increasing more
than total precipitation, or, to put it another way, the increase in
extreme precipitation was not ‘‘disproportionate’’ when compared
with the overall precipitation.

Half of Environment America’s fourth claim, that ‘‘precipitation
will become increasingly likely to fall as rain rather than snow—a
simple consequence of increased temperatures’’—is true (for the
climate of the United States—not so in Antarctica) as demonstrated
in a 1999 paper in Journal of Geophysical Research by Robert Davis et
al. As to the second part of the claim, that ‘‘paradoxically, the number
of dry days will also increase, because intense downpours will punc-
tuate longer intervals of relatively dry weather,’’ there is no indica-
tion that that is happening at all.

The people at Environment America could have tested this claim
by simply tallying the number of dry days in the precipitation data
set each year and seeing if the number was increasing. But, appar-
ently they chose not to (or did so and didn’t like the results), because
they present no analysis of their own to support the claim. Instead,
they attempt to use the scientific literature to do so. They fail misera-
bly, mischaracterizing the results they cite and failing to cite other
results that show that both the number of rainy days and the amount
of soil moisture has been increasing across the United States.

On page 23 of its ‘‘When It Rains, It Pours’’ report, Environment
America writes: ‘‘Since the 1970s in the contiguous United States,
an apparently unusual increase in precipitation intensity has occurred.
At the same time, the annual number of days with rain or snowfall
has decreased.’’

160

Extreme Climate: Floods, Fires, and Droughts

Figure 5.5

AVERAGE AMOUNT OF PRECIPITATION (TOP) AND PERCENTAGE OF
ANNUAL PRECIPITATION (BOTTOM) THAT FELL ON THE WETTEST
DAY OF EACH YEAR ACROSS THE UNITED STATES, 1910–2001

SOURCE: Adapted from Michaels et al. 2004.

161

CLIMATE OF EXTREMES

We know total precipitation, in general, across the United States
has been increasing, but have never heard that the number of precipi-
tation days was decreasing. No reference was given for this state-
ment. One reference that we know of that did examine the trend in
precipitation days across the United States was Karl and Knight, so
we checked that source. They concluded: ‘‘Clearly, the total annual
increase in precipitation frequency [across the United States] of 6.3
days per century significantly contributes to the increase in precipita-
tion.’’ In other words, precipitation across the U.S. has been increas-
ing, in part due to increases in the number of days with precipitation, the
opposite of what Environment America reported. Another scientific
journal article published in 2006 in Geophysical Research Letters, by
University of Washington’s Konstantos Andreadis and Dennis Let-
tenmaier, investigated trends in 20th century drought characteristics
across the United States, and found: ‘‘Droughts have, for the most
part, become shorter, less frequent, and cover a smaller portion
of the country over the last century,’’ completely the opposite of
Environment America’s claims.

So just where did Environment America get the idea that the
number of days with rain was declining across the United States?
From page 23 of their report:

In 2002, Vladimir Semenov and Lennart Bengtsson at the
Max Planck Institute for Meteorology in Germany compared
actual observations of precipitation intensity with the results
of two climate models over the contiguous United States
during the 20th century. They found general agreement
between the model and reality in terms of the trend toward
more frequent extreme precipitation. They also observed that
for the northeastern quadrant of the United States, the annual
number of days with precipitation has been declining since
the 1970s (simultaneously with an increase in the frequency
of extreme downpours)—and the model generally repro-
duces the trend, albeit overestimating the absolute number
of days with precipitation. [ref. 61]

Their reference 61 is this:

V. A. Semenov and L. Bengtsson, ‘‘Secular Trends in Daily
Precipitation Characteristics: Greenhouse Gas Simulation
with a Coupled AOGCM [Atmosphere-Ocean General Circu-
lation Model],’’ Climate Dynamics 19: 123–40, 2002.

162

Extreme Climate: Floods, Fires, and Droughts

Did Semenov and Bengstsson report that ‘‘for the northeastern
quadrant of the United States, the annual number of days with
precipitation has been declining since the 1970s’’? They wrote no
such thing. In fact, what Environment America interpreted and
reported as observed changes across the northeastern United States
were actually climate model simulations of the precipitation charac-
teristics there. So Environment America’s lone piece of support for
its contention that there has been a decline in rainy days across the
United States is not based upon observations (which show an increase
in rainy days), but instead upon a climate model simulation that
was wrong.

All the claims that Environment America made about precipitation
across the United States were testable. Environment America chose
(or only reported on) the few that it knew had to be correct simply
on the basis of the general characteristic of the weather and the
weather trends over the United States during the past 50 years (that
is, the more rain you get, the more rain comes from ‘‘extreme’’
events, and the warmer it gets, the less precipitation falls as snow).
Its other claims were dead wrong.

So, what we are left with is nothing but a basic climatology lesson
from Environment America, with a soupc¸on of untested (but easily
tested) alarmist assertions that turn out to be false.

What’s strange here is how this unrefereed, loosey-goosey study
got the attention of the New York Times during an important UN
conference on climate change, when the real, hard science was out
there for all to see.

Another View of Extreme Rainfall

The notion that human-induced climate change is making more
extreme weather is everywhere. A search for ‘‘extreme weather Ⳮ
climate change’’ will get you 1.5 million hits on Google. If it makes
for that much Internet traffic, there must be a connection, right?

The Illinois State Water Survey’s Ken Kunkel presented some new
results and a summary of some recent papers on extreme weather
in the continental United States at the Climate Specialty Group’s
plenary session at the Association of American Geographers annual
conference in Chicago in 2006. His analysis shed some new light on
long-term extremes in heavy precipitation based on new data.

163

CLIMATE OF EXTREMES

Figure 5.6

RELATIVE FREQUENCY OF CO-OP STATIONS EXPERIENCING A
SINGLE-DAY RAIN TOTAL NORMALLY OCCURRING ONCE IN

20 YEARS, 1895–2000, 7-YEAR RUNNING AVERAGE

SOURCE: Adapted from Kunkel 2006.

Since the late 1970s, records from the co-op network (see chapter
2) were available in computer-readable form only back to 1948.
Recent interest in climate change spurred an effort to digitize the
earlier paper records, adding many new stations and much more
data to the existing network. Now, the computer-readable precipita-
tion database extends back to 1890, and the number of stations has
more than doubled.

Precipitation tends to be pretty spotty, which means that a lot of
new data dramatically increase the number of places that could
experience, say, a single-day rainfall that would normally occur only
once in 20 years. Kunkel calls this number the Extreme Precipitation
Index. We show his values for the number of stations receiving a
once-in-20-years single-day rain total in each year back to 1900 (Fig-
ure 5.6). Positive values of the Extreme Precipitation Index indicate
above average occurrence of extreme events, whereas negative val-
ues mean less frequent extremes. The data since 1950 show a clear
positive trend in the index, which fits nicely with all the scare stories.
But inclusion of the pre-1950 data paints a much different picture.
The frequency of extreme rainfall in the late 1890s was at least

164

Extreme Climate: Floods, Fires, and Droughts

comparable to that in our current climate. Kunkel did some statistical
tests demonstrating that the most recent period (1983–2004) is not
statistically different from the earliest period (1895–1916). The bot-
tom line? The assertion that U.S. rainfall is clearly getting more
extreme because of global warming can’t be supported if the fre-
quency of extreme rain was as great 100 (colder) years ago as it
is now.

The Fire This Time

‘‘Fire’’ and ‘‘drought,’’ at first blush, should go hand-in-glove.
But, like a lot of other things about global warming, what people
think should be happening isn’t necessarily what is happening.

Take the huge Southern California conflagration of October 2007.
Senate Majority Leader Harry Reid (D-NV) blamed it on global
warming, saying in Washington’s authoritative political newspaper,
The Hill, on October 24, ‘‘One reason we have the fires in California
is global warming.’’

Global warming is a great issue, because it affords such easy
opportunities for climate scientists to test glib hypotheses made by
politicians who often have no training whatsoever in climate science.
Reid’s statement is particularly easy to test.

Don’t blame California wildfires on summer’s heat. By the end
of each and every summer (to paraphrase the song, ‘‘it never rains
in California’’ in that season), Southern California is drier than the
world’s best martini.

California’s big wildfires are, ironically, caused by excessive win-
ter rains. Normally, the Southern California region that blew up in
2007 averages about a foot from December through March, the local
rainy season. That moisture turns Southern California green in the
winter and early spring. Owing to the fact that just about every day
after the rainy season is warm and sunny, it’s only a matter of a
month or two before the surface dries out to the point that there’s
not enough water to support additional plant growth, and Southern
California dries up.

The distribution of Southern California rainfall from year to year
is a bit unusual. The vast majority of the years have below-normal
precipitation—about four or so inches below average. But in the
few years that are above average, when it rains, it pours, with rainfall
often 100 percent (one foot or more) above the mean.

165

CLIMATE OF EXTREMES

Some of the very wet years are caused by El Nin˜ o—which you’ll
recall is a reversal of winds every few years over the tropical Pacific
Ocean that has been going on for millenia (see chapter 1). People
such as Senator Reid (and Vice President Gore) will cite computer
models predicting that El Nin˜ os should become stronger or more
frequent with global warming, but there are other models that show
they won’t change or that they might lessen in frequency. The last
big El Nin˜ o was in 1998, and we are way overdue for a strong one
(which will probably reset the surface temperature record, which
also dates back to 1998).

So, some computer models say global warming means more El

Nin˜ os, which means more vegetation, which means more fire.

When things get very wet, there’s plenty more time for the soil
to remain moist, producing a much longer growing season in the
hills where suburbs and very expensive homes are proliferating.
The problem is that these rooms-with-a-view also are houses-with-
a-risk; that is, they’re in the path of wildfires.

If Senator Reid is right, and the catena from global warming
to wildfires is mediated by more vegetation, then rainfall, or the
frequency of rainy years in Southern California must be increasing
as the planet warms. Like so many others, Reid’s is a very testable
hypothesis.

Figure 5.7 depicts the total December–March precipitation for the
California South Coast Drainage Climatological Division from 1895
through 2007.

Remember, most of the years are below the long-term average of
about 12 inches, but the relatively few that are above the mean are
often way above it. If global warming is causing the increase in
Southern California wildfires, then the frequency of very wet years
has to be increasing in a significant fashion.

Obviously, it is not. In fact, the biggest agglomeration of far-above-

normal years was a 12-year period beginning in 1905.

Ironically, those rains were concurrent with some of the massive
westward migration of U.S. population, in an era when both Califor-
nia and Arizona were touted as green paradises, which they were,
thanks to all that unusually lush vegetation. Sure, there had to have
been enhanced wildfires back then, but very few people lived within
their reach.

166

Extreme Climate: Floods, Fires, and Droughts

DECEMBER–MARCH RAINFALL FOR THE CALIFORNIA SOUTH COAST

DRAINAGE CLIMATOLOGICAL DIVISION, 1895–2007

Figure 5.7

SOURCE: National Climatic Data Center 2007. http://www7.ncdc.noaa.gov/
CDO/CDODivisionalSelect.jsp.

After a very wet year (2005 being the last big one), it’s only a
matter of time before a thousand or more homes get torched in the
hills around Los Angeles.

But don’t blame the 2007 Southern California conflagration on
global warming: There is no trend whatsoever in the frequency of
heavy-rainfall years that would promote wildfires. And our public
officials could do us all a service by not making statements like
Reid’s to The Hill, which almost all U.S. Senators and Congressmen
read and therefore believe.

Southwestern Drought: The Long-Term Perspective

Some 38,000,000 Californians largely (but not completely) depend
upon two water sources: winter snowpack in the Sierra Nevada and
the Colorado River, which forms the eastern boundary of the state
with Arizona.

So much water is drawn out of the Colorado River by Californians
and everyone else that, by the time it reaches its mouth at San Luis

167

CLIMATE OF EXTREMES

Rio Colorado on the Gulf of California, it’s barely a trickle. Most
streams increase in volume as they approach the sea, but not the
Colorado, thanks to the huge amount of water withdrawn by the
citizens of the burgeoning Pacific Southwest. Consequently, it’s a
guaranteed front-page headline when anyone says that global warm-
ing will increase drought frequency, reducing the Colorado’s flow.
Such assertions, alas, like Senator Reid’s linking of global warming
to fires, are eminently testable.

California has apparently warmed up about 2°F (1.1°C) since 1950,
and Arizona has warmed 4°F (2.2°C). We use the word ‘‘apparently’’
because former California State Climatologist James Goodridge
detected a strong urban warming bias in California temperatures,
which he published in the Atmospheric Environment in 1992. Given
that Arizona has recently experienced major urbanization, the same
thing is likely there.

David Meko, at the University of Arizona, and colleagues studied
the relationship between tree rings and streamflow. Desert trees are
very responsive to rainfall, so the correlation between the width of
an annual ring (the yearly growth increment) and precipitation (and
therefore streamflow) is very high.

There are also excellent records of Colorado River streamflow
back to 1906, which gave Meko almost 100 years of data to compare
with the tree rings. Some of the trees out there are very old, and
Meko was able take the known relationship between streamflow
and tree rings all the way back to the year 762.

Apparently there is nothing new under the California sun. In

Meko’s words:

The most extreme low-frequency feature of the new recon-
struction, covering A.D. 762–2005, is a hydrologic drought
in the mid-1100s. The drought is characterized by a decrease
of more than 15 percent in mean annual flow averaged over
25 years, and by the absence of high annual flows over a
longer period of about six decades.

Figure 5.8 shows Meko’s record of observed and tree-ring-
constructed streamflow. It is obvious that there is simply nothing
unusual in this record that is concurrent with the planetary warming
that began around 1975. For what it’s worth, the average flow for
the period of historical record (1904–2005) is generally larger than
the estimated flow from 762 to 1905.

168

Extreme Climate: Floods, Fires, and Droughts

OBSERVED COLORADO RIVER STREAMFLOW, 1905–2005, AND

STREAMFLOW RECONSTRUCTED BY USE OF TREE RINGS, 762–2005

Figure 5.8

SOURCE: Adapted from Meko et al. 2007.

Anyone suggesting that recent droughts are somehow the result
of emissions of greenhouse gases is overlooking a tremendous piece
of evidence suggesting otherwise. Droughts are a natural part of
the climate of the Pacific Southwest—they have been around a long
time, and they are not going away anytime soon. Droughts impacted
the region in warm periods of the past and cold periods of the past,
and will return whether the future is warmer or colder.

Global and Local Wildfires

What could be simpler? Global warming without a compensating
increase in precipitation will make much of the world drier. Drier
vegetation is more subject to immolation. There should even be a
positive feedback: After all, vegetation burns largely to carbon diox-
ide and water, which should contribute additional warming. Couple
that with the fact that most burnt areas are black (black surfaces
absorb more solar radiation than white ones, which is why so much
traditional Middle Eastern clothing is white), and . . . even more
warming!

Back in the big El Nin˜ o of 1998, Al Gore traveled to Florida, where
wildfires were rampant, and proclaimed that this was the world of
the future. In other words, as the planet warms, there’s going to be
more fire.

169

CLIMATE OF EXTREMES

Yet another eminently testable hypothesis made by a politician

about global warming.

A 2007 article in Global Change Biology did precisely that. In the
paper titled ‘‘Global Spatial Patterns and Temporal Trends of Burned
Area between 1981 and 2000 Using NOAA-NASA Pathfinder,’’
David Rian˜ o, from University of California-Davis, and several col-
leagues examined 20 years of global satellite data. Conclusion? ‘‘The
total annual burned area has not increased in the last 20 years.’’

Well, certainly the temperature has, so the chain of causation from
warming to drying to burning to more warming to more drying to
more burning mustn’t be so simple!

Rian˜ o et al. made the case that determining the burn area and/
or biomass consumed annually would be critical in understanding
various dimensions of the global ecosystem, but that to date, the
data have been collected annually using highly irregular criteria
from country to country. The UN Food and Agriculture Organization
has tried to collect the statistics, but its data are notoriously suspi-
cious in terms of accuracy. Rian˜ o et al. argued:

A single remote sensing data source can provide globally
coherent multitemporal spatial information, not only from
the visible part of the spectrum but also reflected solar infra-
red, which can be used to obtain consistent environmental
monitoring at the global level.

Rian˜ o et al. collected the 8-km-resolution global satellite-based
Advanced Very High-Resolution Radiometer data from July 1981
to December 2000. They developed a computer algorithm that could
spot pixels (the spatial units returned by satellites) in which there
were recent fires, total them, and determine percentage of burned
area for any defined region of the world. The global and regional
data were ultimately assembled on a monthly basis.

In analyzing trends in the burn area data, his team found: ‘‘The
global trend statistics in the total number of pixels burned in any
month or annually were not significantly different from zero. . . .
Therefore, no significant upward or downward global trend was
found in the burned area data.’’

The research team then did a mathematical analysis, called cluster
analysis, to identify areas with similar fire histories, and they
reported that cluster group 1 did show a significant increase in
burned area. This area included the western United States, where

170

Extreme Climate: Floods, Fires, and Droughts

wildfires have received a tremendous amount of press recently. But
they also found a significant decrease in the burn area that included
Central America and much of Southeast Asia. When viewed globally
(a good idea when looking at global warming), Rian˜ o et al. once
again reported, ‘‘There was also no significant upward or downward
global trend in the burned area for any individual month.’’

What an inconvenient truth! The study period, 1981 to 2000, was
a two-decade time period when greenhouse gases increased substan-
tially in the global atmosphere, when the earth warmed, and when
the media breathlessly covered every fire from Indonesia to Australia
to the western United States. Many scientists are quoted in the nearly
two million websites on wildfires claiming that the increase in fire
activity is well under way.

Rian˜ o et al. developed the first truly global data set on burn area,
conducted an analysis that is uniform across the globe, and found
no trend in the global burn area data. So much for getting your
science off the Internet!

Rian˜ o et al.’s study was preceded as is a more local analysis but
one that was cleverly extended to long before the 1980 dawn of the
fire-sensing satellite.

A 2006 article in Journal of Geophysical Research by M. P. Girardin
and colleagues showed that the burned area from forest fires in
Ontario, Canada, has increased since 1970. However, the increase
in fires pales in comparison with what was observed in the more
distant past.

At the outset, the authors note: ‘‘It is also possible that the provin-
cial fire statistics (number and size of fires) were underreported
prior to the 1960s.’’ Girardin et al. were interested in extending data
on the burned area of Ontario going back more than a hundred
years, and they used tree rings to estimate burned areas of the past.
Not surprisingly, forest fires produce a recognizable signal in the
annual tree rings, and the Canadian team used sophisticated statisti-
cal wizardry to go from tree ring patterns to total area burned in a
given year. They tested their statistical model from 1917 to 1981,
and once satisfied that the model was working with reasonable
accuracy, they used tree rings to extend burned area estimates back
to 1781.

Figure 5.9 shows the results. According to the authors, ‘‘Episodes
of succeeding years of large area burned were estimated approxi-
mately at 1790–94, 1803–07, 1818–22, 1838–41, 1906–12, 1920–21,

171

CLIMATE OF EXTREMES

Figure 5.9

BURNED AREA IN ONTARIO, CANADA: RECONSTRUCTED,

1782–1981, AND OBSERVED, 1917–2003 (TOP); AND 10-YEAR

SMOOTHED DATA, 1782–1980 (BOTTOM)

SOURCE: Girardin et al. 2006.
NOTE: Thin line in top figure represents observed rather than modeled data
(1917–2003).
1 hectare ⳱ 2.47 acres.

and 1933–36.’’ Furthermore,‘‘The tree ring model revealed the year
1804 as the year of most extreme area burned.’’ In addition, ‘‘The
sliding window analysis showed higher mean area burned values
prior to 1840 and through the 1860s–80s and the 1910s–20s. Mean
values during the mid-20th century were the lowest in the record.’’
They concluded: ‘‘The reconstruction indicated that the most
recent increase in area burned in the province of Ontario (circa
1970–81) was preceded by the period of lowest fire activity ever
estimated for the past 200 years (1940s–60s).’’ Finally, they wrote,
‘‘[W]hile in recent decades (circa 1970–81) area burned has increased,
it remained below the level recorded prior to 1850 and particularly
below levels recorded in the 1910s and 1920s.’’ Figure 5.9 shows
that the biggest burn periods all occurred prior to World War II.

172

Extreme Climate: Floods, Fires, and Droughts

What about an increase in fires in western North America?
Again, looking at the multicentury record, and the propensity for
forest fires to leave their traces in tree rings, it appears that fire
frequency for the last 500 years has been fundamentally the result
of natural ocean climate cycles, and not global warming.

That is the conclusion reached by researchers from Colorado,
Arizona, Montana, and Argentina in a 2007 study published in the
Proceedings of the National Academy of Sciences. Lead author Thomas
Kitzberger, from Argentina’s Universidad Nacional del Comahue,
and his coauthors used extensive tree-ring analyses to determine
the underlying causes of these widespread and episodic wildfires.
The researchers examined more than 33,000 fire events from nearly
5,000 fire-scarred trees, primarily ponderosa pine and Douglas fir,
throughout the western portion of North America. The exact calen-
dar year of the fire event was obtained by noting the specific year
of the tree ring record in which a fire scar occurred. The researchers
investigated 238 sites throughout the western half of North America.
This is an extensive study!

The scientists then linked the fire frequencies with measures of
drought and corresponding sea-surface temperature. They concen-
trated on three specific oceanic climate phenomena: First, they
obtained records of the sea-surface temperatures in the central equa-
torial Pacific in order to study the effects of El Nin˜ o on forest fires.
Another strong, but longer-term, ocean climate phenomenon in
the Northern Pacific is called the Pacific Decadal Oscillation (PDO).
The PDO is a pattern of temperature variation that changes very
slowly—in a multi-decadal fashion—compared to the one- or two-
year El Nin˜ os. The PDO changes whether or not there is global
warming.

The third climate phenomenon the researchers related to fire fre-
quency is a pattern of temperature variation in the north Atlantic
known as the Atlantic Multidecadal Oscillation (AMO), which oper-
ates on even longer time frames, with periods of up to 60 to 80 years.
The researchers noted that there had, until then, been no compre-
hensive study linking the overall patterns of these oceanic oscilla-
tions with the overall patterns and frequency of fires in the western
United States. Consequently, they took this massive database of
33,000 fire events derived from tree-ring records across the entire
western North America and compared it with those ocean climate
phenomena.

173

CLIMATE OF EXTREMES

Not surprisingly, Kitzberger and colleagues found that, since the
year 1550, fire events and droughts have been linked—the occur-
rence of drought and wildfire in the West have coincided. More
important, they found those occurrences have been strongly tied to
variations in the three ocean-climate phenomena—El Nin˜ o, the PDO,
and the AMO. They found that the magnitude of the two Pacific
climate phenomena, El Nin˜ o and the PDO, were crucial to determin-
ing the frequency of fires in subregions across the West. But given
the recent spate of massive wildfires across that region, they were
most interested in whether widespread fires—extending across large
areas of the West—were linked to those oceanic climate variations.
They found that the ocean temperature variations associated with
the AMO were the dominant factor. In particular, they found when
the AMO is acting like it has since 1995 (also spawning some terrific
hurricane seasons), there’s a propensity for huge western conflagra-
tions. They wrote: ‘‘The key issue is that the Atlantic Multi-Decadal
Oscillation persists on time scales of 60 to 80 years, compared to
just one year or a few years for El Nin˜ o.’’

So does their study have any implications for the future? Can we
tell something about upcoming widespread fire occurrence in the
western portion of North America based on this extensive tree-ring
analysis? The answer is yes—and the implications aren’t too good.
Unfortunately, given the very long-term nature of this Atlantic
ocean-climate phenomenon, we are likely to be in the AMO warm
phase for quite some time to come. That suggests we will likely
continue to see more and more massive western fires, global warm-
ing or not.

Further, it leads to the conclusion that big hurricane seasons and
wildfires are correlated with the AMO. How hard will it be for
anyone to not say the world is going to heck in a hurricane because
of global warming, even though that’s not the cause?

174

6. Climate of Death and the Death of

Our Climate

Who can forget the massive European heat wave of 2003? Certainly
not cyberspace, where Googling ‘‘global warming Ⳮ mortality’’ will
get you 723,000 hits. Although accurate numbers are virtually impos-
sible to come by in such a situation, it appears that 15,000 people
in France died from heat-related causes, and there were 35,000 total
deaths in Europe.

Needless to say, this event was probably the single incident most
responsible for the remarkable perseveration on global warming that
afflicts the Continent. It certainly prompts two questions: (1) Was it
climatologically unprecedented? and (2) What can be expected as
the planet warms?

The first seems like a hands-down yes. After all, no heat wave in
recorded history killed so many French. But a closer inspection
reveals a much more complicated picture.

A 2006 article in Geophysical Research Letters dares to ask the ques-
tion: ‘‘Was the 2003 European summer heat wave unusual in a
global context?’’

T. N. Chase and several colleagues from Colorado and France

begin their study by noting:

The European heat wave of summer 2003 has received con-
siderable attention, both because of a potential link to larger-
scale warming patterns (e.g., ‘‘global warming’’), and the
large loss of life. Several studies find that this regional heat
wave was quite unique and it has been suggested that such
an extreme event could be accounted for only by a shift of
statistical regime to one with higher variance [i.e., a
‘‘changed’’ climate].

The argument is that climate change caused by the buildup of
greenhouse gases increases temperature variability, and this
increased variability makes heat waves like the one in 2003 more
likely. Chase et al. decided to test this hypothesis.

175

CLIMATE OF EXTREMES

Recall the discussion in chapter 1 about how changing the atmo-
sphere’s greenhouse effect preferentially warms the high-latitude
land areas (despite the unexplained lack of warming in Antarctica),
which reduces the temperature contrast between the North Pole and
the equator, which reduces the strength of the jet stream and there-
fore reduces temperature variability. Perhaps a prolonged heat
wave—where temperatures remain high from day to day—could
be interpreted as such a reduction in variability.

The Chase team collected data on the temperature of the atmo-
sphere from throughout the world for the surface to the midat-
mosphere for the period 1979–2003. For each month, they computed
the mean and standard deviation of the temperature, thereby allow-
ing them to map temperature anomalies in terms of standard devia-
tions above or below the mean.

The standard deviation is a measure of the ‘‘spread’’ of data
around the mean value. Think of it as variability. There’s an average
temperature for, say, the 4th of July at a given location in the United
States. That average is calculated by taking all of the July 4 readings,
adding them up, and dividing by the number of observations. Some
days are warmer than the average, some are colder. Same for, say,
New Year’s Day.

But the variability of 4th of July temperatures from year to year
will be less than that for New Year’s. Why? In winter, the nation is
subject to invasions of frigid arctic air, as well as milder conditions
in its absence. In summer, the air masses over the country are of much
more uniform (and hot) temperature. So the standard deviation of
4th of July temperatures will be smaller.

Roughly two-thirds of all observations of temperature at a given
location and date are within one standard deviation of the mean.
The probability of being two standard deviations above the mean
is 0.05 (1 in 20), and three is 0.003, or 3 in 1,000.

Technically, Chase et al. used something a little different from
temperature. Instead, they measured the distance from sea level
at which a weather balloon would find one-half the atmosphere
underneath it. The greater the height to which the balloon must
ascend, the warmer the air is beneath it. Under cold conditions, the
atmosphere is more dense, and the balloon doesn’t have to ascend
as far in order to be above half the atmosphere.

This distance is known as ‘‘thickness’’ to meteorologists, and it is
directly proportional to surface temperatures absent any local factors

176

Climate of Death and the Death of Our Climate

such as lakes or forests or cities. Consequently, it gives a truer
picture of the atmospheric temperature than one gets from most
weather stations.

As seen in Figure 6.1 (see insert), for June, July, and August of
2003, Europe was definitely ground zero for what is an extreme,
extreme event. Note that far more than half the planet is portrayed
in green and blue tones, indicating normal to below-normal tempera-
tures. Europe was simply located in the wrong place at the wrong
time, and the heat wave was anything but global in nature.

The three-standard-deviation anomaly over Europe has a statisti-
cal probability of only 1 in 333. Given that there are a lot of places
on the planet, and four seasonal slices to examine, it’s just not that
odd that such an anomaly shows up somewhere. But this time it
happened to be at the epicenter of global warming fever (in the
midst of a climatically moderate summer around the planet).

Chase et al. analyzed the thickness anomalies for all parts of the
globe for the 25-year study period and concluded: ‘‘Extreme warm
anomalies equally, or more, unusual than the 2003 heat wave occur
regularly.’’ They can occur at any time of the year. They will rarely
appear in summer, directly over Europe, where many residents
eschew(ed—see later text) air-conditioning. They also note:
‘‘Extreme cold anomalies also occur regularly and can exceed the
magnitude of the 2003 warm anomaly in terms of the value of SD
[standard deviation].’’ Of course, it’s pretty tough to sell the idea
that global warming is causing cold anomalies, so cold anomalies
are not nearly so newsworthy.

It should come as no surprise that Chase and company noted that
warm years tend to have heat waves and cold years have cold waves.
But their next two conclusions are more interesting. They found:

Natural variability in the form of El Nin˜ o and volcanism
appears of much greater importance than any general warm-
ing trend in causing extreme regional temperature anomalies
as regional extremes during 1998 [a year with a huge El
Nin˜ o] in particular were larger than the anomalies seen in
summer 2003 both in area affected and [standard deviation]
extremes exceeded.

In summer 2003, 2 percent of the planet experienced thickness-
temperature anomalies above two standard deviations. In the big El
Nin˜ o year of 1998, that figure was nearly 30 percent. Three standard

177

CLIMATE OF EXTREMES

deviations above normal covered 5 percent of the planet for the
entire year of 1998, while the nowhere on the planet exceeded this
criterion in the year 2003.

Chase et al. also examined the trends in the data over the 25 years
and reported: ‘‘Analyses do not provide strong support for the idea
that regional heat or cold waves are significantly increasing or decreas-
ing with time during the period considered here (1979–2003).’’ In other
words, heat waves like the one in Europe in 2003 can and will occur
by chance even if temperature does not rise or the variability of temper-
ature does not change.

There is no question that the heat wave of 2003 was a natural
disaster in Europe with a substantial loss of human life. Europe was
not prepared for an event that, from a purely statistical view point,
was inevitable, with or without global warming.

In 2006, another article appeared in the International Journal of
Biometeorology that put the 2003 disaster in perspective. Mohamed
Laaidi and two coauthors, from the Medical University at Dijon,
France, examined daily temperature and mortality data from
1991–95 for six ‘‘departments’’ (a.k.a., states or counties) located in
urban, oceanic, interior, mountain, and two different Mediterranean
settings (Figure 6.2). They broke the data into three age groups
including less than 1 year old, 1 to 64 years old, and greater than
64 years old. They also divided the data by sex and by major causes
of death including respiratory disease, cerebrovascular disease or
stroke, heart disease, and other diseases of the circulatory system.
Murders and accidents were excluded.

The Laaidi et al. team found that for the whole population

As expected, temperature and daily deaths exhibited a
marked temporal pattern. For all the departments investi-
gated, mean daily counts of deaths showed an asymmetrical
V-like or U-like pattern with higher mortality rates at the
time of the lowest temperatures experienced in the area than
at the time of the highest temperatures.

The data also clearly showed that people adjust to their environ-
ments. Individuals living in cold regions experience more mortality
in warm temperatures, and those from warm areas are more suscep-
tible to cold ones. There is also a range in temperature, called the
thermal optimum, in which mortality is low; the authors noted:

178

Climate of Death and the Death of Our Climate

SIX STUDY AREAS IN FRANCE WITH DIFFERENT CLIMATES

Figure 6.2

SOURCE: Laaidi et al. 2006.
NOTE: 1 ⳱ Seine-Saint-Denis; 2 ⳱ Finiste`re; 3 ⳱ Coˆte d’Or; 4 ⳱ Hautes-
Alps; 5 ⳱ Alpes-Maritimes; and 6 ⳱ He´rault.

The level of the thermal optimum rises in line with the war-
mer climatic conditions of each department. The thermal
optimum is greater in Paris, probably due to the urban heat
island, than in the He´rault, which is situated in the extreme
south of France in a Mediterranean climate.

In other words, here’s the shocking news: People adjust to the
climate in which they reside. In Meltdown, one of us (Michaels) cited
work he had done with Robert Davis at the University of Virginia
in which they found that heat-related mortality declined as cities get

179

CLIMATE OF EXTREMES

warmer, which cities do with or without global warming. The same
phenomenon was seen by Laaidi et al., except they added in the
adjustment for cold climates, showing less mortality there from cold
waves than occurs when temperatures fall dramatically in warm
climates.

Concerning any temperature rise for any reason, Laaidi et al.
found: ‘‘For both men and women mortality was higher at low
temperatures, suggesting a lesser ability to adapt to the cold.’’ On
the basis of another related study, they state, ‘‘In England and Wales,
the higher temperatures predicted for 2050 might result in nearly
9,000 fewer winter deaths each year.’’ Laaidi et al. conclude: ‘‘Our
findings give grounds for confidence in the near future: the relatively
moderate (2°C) [3.6°F] warming predicted to occur in the next half
century would not increase annual mortality rates.’’

Computer models for carbon dioxide–induced global warming
consistently predict more warming in winter in midlatitude locations
such as France and less warming in the summer. The Laaidi et al.
study shows that the greater threat of human mortality lies in the
cold end of the thermal spectrum rather than the warm end. Higher
temperatures in the winter would certainly decrease mortality, and
we could conclude from this and other studies that in terms of
temperature-related mortality, global warming would save lives—
a message not well conveyed in the hundreds of thousands of web-
sites on the subject.

Death vs. Life with Global Warming

The best way to make headlines in the global warming game is
to generate scary scenarios about how many people are going to die.
It’s little surprise then that a ‘‘Review’’ article by Jonathan Patz
of the University of Wisconsin–Madison and colleagues that
appeared in Nature in November 2005 caught tremendous attention.
It focused on global warming and death.

Patz et al. began with the 2003 heat wave in Europe. As demon-
strated earlier in this chapter, it was hardly unusual in the statistical
sense and was more an accident of geography than anything else.
(Note that the Chase et al. study appeared after Patz et al.’s review).
A substantial (and not discussed) cause of a large number of
fatalities was cultural. The month-long August vacation is a cher-
ished European tradition. It’s not unusual for many countries to

180

Climate of Death and the Death of Our Climate

effectively shut down while the epicenter of the population shifts
southward to Mediterranean beaches. That exodus reduces both
medical staffing and oversight of those who may be affected by the
heat. Undoubtedly, the same weather conditions in July would have
produced substantially fewer deaths.

The ‘‘theory’’ that allows climate change to be blamed for an
increase in heat waves is that with global warming, climate will be
more variable. Though the jury is still out worldwide on that, there
is plenty of evidence for the United States that the opposite is true.
A series of studies by Indiana University’s Scott Robeson found that,
for a large set of U.S. cities, places that have warmed most exhibited
less temperature variability, not more. Regrettably, these and other
key papers were not part of Patz et al.’s review.

Patz et al. also blame mortality on urban heat islands—the heat-
trapping effects of buildings and paved surfaces combined with
less vegetation—that result in most large cities’ being significantly
warmer than the surrounding countryside. They are correct. In fact,
the urbanization effect can exceed the background rate of regional
warming significantly, by one order of magnitude or more. So, if
this is such a problem, we should expect people living in cities to
be dying from heat exposure in increasing numbers.

Figure 6.3 shows the aggregate heat-related death rate toll for 28
of the largest U.S. cities from 1964 to 1998. There is a statistically
significant decline in heat-related mortality over the period. During
the same time, effective temperature (a combination of temperature
and humidity) increased by an average of almost 1°C (1.8°F), mostly
because of heat island effects. Why aren’t more people, instead of
fewer people, dying from heat exposure, as Patz et al. postulate?

It’s simple. People, by and large, are not stupid. If it’s too hot,
they find an air-conditioned spot. If it’s too cold, they turn up the
heat, go out in the sun, or put on a jacket. The fact that Phoenix has
a thriving population in a valley that is essentially inhospitable to
pretechnological human habitation speaks volumes about human
adaptability. Most elderly people move to Phoenix or Miami think-
ing in part they might prolong their lives (or at least live more
comfortably) by living away from harsh winter weather, not so they
could die sooner.

The review later waxes poetically about the potential health
impacts of El Nin˜ o across the globe. Other horrors follow: epidemics

181

CLIMATE OF EXTREMES

AVERAGE ANNUAL POPULATION-ADJUSTED HEAT-RELATED

MORTALITY RATE FOR 28 MAJOR U.S. CITIES, 1964–98

Figure 6.3

SOURCE: Adapted from Davis et al. 2003.

of malaria and Rift Valley fever, dengue hemorrhagic fever in Thai-
land, hantavirus pulmonary syndrome in the Desert Southwest in
the United States, waterborne diseases in Peru, and cholera in
Bangladesh.

One teensy problem . . . there’s no clear causal link between El
Nin˜ o (often acronymed ENSO for El Nin˜ o–Southern Oscillation, its
true scientific name) and warming. If there were, it would be obvious
by now. Patz et al. admit this (sort of): ‘‘Although it is not clear
whether and how ENSO dynamics will change in a warmer world,
regions that are currently strongly affected by ENSO . . . could expe-
rience heightened risks if ENSO variability, or the strength of events
intensifies.’’ Sure. An equally likely scenario is that the impact of
all of these diseases will be reduced if global warming generates
fewer and weaker El Nin˜ os.

Finally, the review paper pulls out a 3-year-old World Health
Organization study and suggests that climate changes that have
occurred in the last 30 years could have caused 150,000 deaths per
year worldwide. On the basis of back-of-the-envelope calculations

182

Climate of Death and the Death of Our Climate

using current global population and mortality rate estimates, we
determine that ‘‘global warming’’ is responsible for 0.2 percent of
all deaths. This is a remarkably small number based upon WHO
figures that are controversial in the first place.

Another way to look at global warming and mortality is on the
benefits side. Since 1900, primarily as a result of technologies devel-
oped in a world powered by fossil fuels, average human life expec-
tancy has increased significantly, doubling in the industrialized
world. Doubling life expectancy is equivalent to saving one life. If
two billion people lived during this period, then that would be
equivalent to saving a billion lives. The World Health Organization’s
numbers don’t bother to take that into consideration.

Noting that human-induced warming appears to ‘‘begin’’ around
1975, and that surely there were less than 150,000 excess deaths at
the beginning, let’s average the number of deaths per year from
1975 to 2000 at 75,000, giving a body count of roughly 1.9 million.
Assuming that this 25-year period also includes 25 percent of the
‘‘saved’’ lives means 250 million ‘‘excess’’ lives. The net result after
allowing for both deaths and ‘‘saves’’ is 248.2 million more living
people.

The most important and interesting aspect of the Nature review
article is that Patz, whose primary expertise is in vector-borne dis-
eases such as malaria, and colleagues have the least confidence about
the global warming–malaria link. Their discussions and review of
the vector-borne disease literature are fairly balanced and contain
many of the key caveats. Unfortunately, that balanced tone does not
permeate most other aspects of the review.

There is no doubt that climate change will have some impacts,
both positive and negative, on human health. One could just as
easily write a review about how a warming planet is producing
myriad health benefits.

Fewer French Fried in 2006

In the history of global-warming scare stories, the 2003 European
heat wave was a lulu. But did you hear about the huge French heat
wave of 2006?

You guessed it: many fewer people died. The 2006 heat wave is
the subject of a recent paper in the International Journal of Epidemiology
by a group of French researchers, led by A. Fouillet of the University

183

CLIMATE OF EXTREMES

OBSERVED DAILY MORTALITY RATE AND MAXIMUM AND

Figure 6.4

MINIMUM TEMPERATURES IN FRANCE,

JUNE 1–SEPTEMBER 30, 2006

SOURCE: Adapted from Fouillet et al. 2008.

of Paris, titled, ‘‘Has the Impact of Heat Waves on Mortality Changed
in France since the European Heat Wave of Summer 2003? A Study
of the 2006 Heat Wave.’’

Fouillet and colleagues began by developing a model in which
they used temperature data to predict daily summer mortality rates
over the historical record (1975–99). There is a surprisingly strong
relationship between temperatures and summer death rates (their
overall model-explained variance was 79 percent). Figure 6.4 demon-
strates this by comparing minimum and maximum temperatures
and mortality rates for June through September 2006. It’s easy to
visually track the linkage between the temperature and mortality
lines. Note during mid- to late July in particular, when the heat
wave was going full blast, that mortality rates persisted at well over
7 daily deaths per 100,000 population.

Because of these very consistent linkages between temperature
and mortality, the authors were able to statistically estimate the

184

Climate of Death and the Death of Our Climate

Figure 6.5

OBSERVED AND PREDICTED DAILY MORTALITY RATES IN FRANCE,

JUNE 1 THROUGH SEPTEMBER 30, 2003 (TOP), AND JUNE 1

THROUGH SEPTEMBER 30, 2006 (BOTTOM)

SOURCE: Adapted from Fouillet et al. 2008.

number of deaths expected based upon observed temperatures. Fig-
ure 6.5 (top) shows the predicted and observed values for the notori-
ous summer of 2003. It’s not hard to find the heat wave in this graph.
At the mortality peak, the model predicted about 17 deaths per
100,000, but nearly 21 deaths per 100,000 were observed. In other

185

CLIMATE OF EXTREMES

words, far more people died than would have been expected on the
basis of the observed temperatures.

In July 2006, the situation was quite different (Figure 6.5, bottom).
Now, the temperature model predicted far more deaths than actually
occurred. Although there was a mortality spike, the model estimated
almost 6,500 excess deaths during the heat wave, but only around
2,000 occurred. Specifically, the death count was 4,388 less than
expected.

Can we attribute these saved lives to global warming? Well, maybe
indirectly. In response to the tragedy of 2003, the French government
implemented a National Heat Wave Plan that included surveillance
of health data, recommendations on the prevention and treatment
of heat-related morbidity, air-conditioning for hospitals and rest
homes, and emergency plans for retirement homes, among other
things. In other words, France adapted after the 2003 heat wave
by providing information to the population at large and air-
conditioning to the most vulnerable. No doubt people were also
personally more aware of the dangers of summer heat in 2006 than
they were three years earlier. In reality, there is no excuse for such
mass heat-related mortality that occurred in 2003 in any technologi-
cally advanced country.

Let’s face it. The planet is warming, and, short of China and India
going green, there’s nothing anybody can really do about it with
current technology. (Go ahead and install energy-saving light bulbs,
but it’s not going to do much to change global temperatures.) It
would be foolish to argue that we’re not going to see more and
longer summer heat waves as greenhouse gas levels continue to
increase. But that certainly doesn’t mean that death rates are going
to skyrocket. In fact, with some relatively simple adaptive measures,
death rates could very well go down regardless of future tempera-
tures. That’s been the trend in the United States and should be the
standard throughout the developed world.

Science Fiction: The Imminent Ice Age

Atlantic’s Salt Balance Poses Threat, Study Says

The delicate salt balance of the Atlantic Ocean has altered so
dramatically in the last four decades through global warming
that it is changing the very heat-conduction mechanism of the
ocean and stands to turn Northern Europe into a frigid zone.

186

Climate of Death and the Death of Our Climate

The conclusions are from a study in the journal Nature that
is to be published today. The study describes planet-scale
changes in the regulatory function of the ocean that affect
precipitation, evaporation, fresh-water cycles, and climate.
This has the potential to change the circulation of the ocean
significantly in our lifetime,’’ said Ruth Curry of the Woods
Hole Oceanographic Institution in Massachusetts, the study’s
lead author.

—Toronto Globe and Mail, December 18, 2003

Who hasn’t read something similar to this quotation from the
Toronto paper? Or seen headlines such as ‘‘Global Warming to Cause
Next Ice Age!’’ or ‘‘Global Warming to Send Europe into a Deep
Freeze!’’ In fact, next time New England or Europe has a cold winter,
it’s a guarantee that you’ll see them again. Why? Because history is
somewhat repetitious.

Nor are these stories confined to a normally left-leaning press.
For example, New England saw a pretty decent and prolonged cold
snap in January 2003, prompting Wall Street Journal science writer
Sharon Begley to proclaim:

The juxtaposition of a big chill in the Northeast and near-
record warmth globally seems eerily like the most dire pre-
dictions of climate change: As most of the world gets toastier,
average winter temperature in Northeastern America and
Western Europe could plunge 9°F.

The idea behind these scary stories (and the premise of the monu-
mentally bad science fiction film The Day After Tomorrow) is that
the ocean’s ‘‘thermohaline’’ circulation slows down or, even worse,
stops, sending the climate into disarray—all because of anthropo-
genic global warming. In the case of The Day After Tomorrow, this
circulation shutdown led to a flash freeze of the planet. Peter
Schwartz and Douglas Randall, two nonclimatologist contractors to
the U.S. Department of Defense said, in an October 2003 report, that
this could happen in the next decade! Their highly amusing (to
climatologists) report was titled, ‘‘An Abrupt Climate Change Sce-
nario and Its Implications for United States National Security.’’

The thermohaline (thermo ⳱ heat; haline ⳱ salt) circulation works
like this: Strong solar heating and warm waters in the tropical Atlan-
tic result in enhanced evaporation there, leaving the surface waters
there saltier than the average ocean. These warm, salty waters are

187

CLIMATE OF EXTREMES

carried northward via the Gulf Stream, and in the high latitudes
they release their heat into the atmosphere, and subsequently cool.
This cool, salty current of water becomes more dense than the less
salty waters surrounding it and consequently sinks and flows back
southward, acting as a sort of pump that drives this major circulation
system that circuitously winds its way though most of the world’s
major oceans.

There are indications from paleoclimatological studies that the
thermohaline circulation has shut down in the past, causing
‘‘abrupt’’ change. This happened 8,200 years ago, when the world
was still emerging from the last ice age. As the ice sheet that covered
North America melted, it formed a huge lake in central Canada
(Lake Agassiz) that contained more water than the combined Great
Lakes do currently. Lake Agassiz was held back by an ice dam that
eventually disintegrated as the climate warmed, and immediately
the lake’s contents roared through Hudson Bay and into the North
Atlantic Ocean. This fresh ‘‘meltwater pulse’’ apparently shut down
the pump, which took a couple hundred years to get up and running
again, during which time Greenland and Europe were considerably
cooler, around 5.5°F (3.1°C), than they were prior to this event.

Today, there is no bigger-than-all-the-Great-Lakes-combined glacial
meltwater lake in central Canada held back by an ice dam on the verge
of collapse. But that hasn’t led folks to abandon the idea that human-
induced climate warming may cause a shutdown of the ocean’s ther-
mohaline circulation. The idea is that global warming will lead to a
meltdown of the Northern Hemisphere’s last significant ice sheet rem-
nant from the Ice Age—the one lying atop Greenland. The meltwater
from the Greenland ice sheet, together with a projected enhancement
of high-latitude precipitation, will eventually provide a large enough
input of fresh water to the subpolar North Atlantic ocean to slow and
eventually halt the thermohaline circulation.

As a result, researchers have been poring over data in search of
any indications that that is happening. Any hint that they may
have identified a thermohaline slowdown produces a rasher of lurid
stories. For example, in December 2003, Ruth Curry, from Woods
Hole Oceanographic Institution, and colleagues reported in Nature
magazine that they had detected evidence that suggested the thermo-
haline circulation was slowing down. From data collected from the
1950s to the 1990s, they reported that the tropical Atlantic was grow-
ing saltier while the northern latitudes of the Atlantic were growing

188

Climate of Death and the Death of Our Climate

fresher and suggested that anthropogenic global warming was the
probable cause. That prompted the Toronto Globe and Mail article.

Then, two years later, in December 2005, Harry Bryden and col-
leagues published an article, again in Nature that seemed to show
additional evidence of the thermohaline circulation’s long, slow
death march. They examined a series of ship transects of the Atlantic
Ocean, the first in 1957 and the last in 2004, and declared that they
had detected a circulation slowdown of about 30 percent, which
primarily had taken place during the past 10 to 15 years. That
prompted this headline and story:

Alarm Over Dramatic Weakening of Gulf Stream

The powerful ocean current that bathes Britain and northern
Europe in warm waters from the tropics has weakened dra-
matically in recent years, a consequence of global warming
that could trigger more severe winters and cooler summers
across the region, scientists warn today.

Researchers on a scientific expedition in the Atlantic Ocean
measured the strength of the current between Africa and the
east coast of America and found that the circulation has
slowed by 30 percent since a previous expedition 12 years
ago.

—The Guardian, December 1, 2005

In the intervening time, more and better data have come in,
prompting articles demonstrating that it is impossible to verify a
slowdown in the thermohaline circulation related to anthropogenic
influences. Instead, they conclude that natural variations in the
strength and speed of the thermohaline circulation can explain the
observed behavior.

The latest of these appeared in 2007 in Geophysical Research Letters,
by Tim Boyer and colleagues, who hail primarily from the U.S.
National Oceanic and Atmospheric Administration (NOAA). Boyer
et al. examined salinity trends in the waters at various depths and
latitudes covering the North Atlantic Ocean. They found, overall,
a general salinity increase over the entire basin. As shown in
Figure 6.6, although the waters of the tropical latitudes showed a
fairly steady trend toward enhanced salinity between the beginning
of their study period (1955) and the end (2006), northern waters
showed a clear freshening trend lasting from about the late 1960s

189

CLIMATE OF EXTREMES

EQUIVALENT FRESHWATER CONTENT FOR DIFFERENT AREAS OF
THE NORTHERN ATLANTIC OCEAN, 0–2,000 METERS, 1955–2006

Figure 6.6

SOURCE: Adapted from Boyer et al. 2007.
NOTE: 2,000 meters ⳱ 1.24 miles; km3 ⳱ cubic kilometer.

through the early to mid-1990s. Since then, the freshwater content
of the northern latitudes of the Atlantic Ocean has been declining.
The freshening trend of the high-latitude Atlantic coupled with
increasing salinity in the lower latitudes undoubtedly made it seem,
at least when it was occurring, that the thermohaline circulation was
slowing down as a result of global warming—as reported by Curry
et al. in 2003. But their data ended in 1999, which was too early to
pick up the end of the freshening trend and the increasing salinity.
All of that indicates that the thermohaline circulation is still quite
healthy, and in fact, that it has likely strengthened over the past 10
years or so—counter to the suggestions of Curry et al. and Bryden
et al. in 2005.

Interestingly, there may be a tie-in to Atlantic hurricane activity.
For years, hurricane guru William Gray has been saying that the
strength of the thermohaline circulation is an important determinant
of Atlantic hurricane activity.

Boyer et al. seem to give added credence to Gray’s idea. If you
take the freshwater content of the northern portions of the North
Atlantic as an indicator of thermohaline strength (fresher is weaker,

190

Climate of Death and the Death of Our Climate

saltier is stronger), there’s a strong association between circulation
strength (Figure 6.6) and the number and strength of Atlantic hurri-
canes (Figure 3.3).

From the late 1950s through the early 1970s, when the high-
latitude, subpolar Atlantic waters were relatively salty, there were
plenty of hurricanes and a lot of strong ones. As the subpolar Atlantic
freshened from the early 1970s to the mid-1990s, Atlantic hurricane
activity diminished. Hurricane activity once again picked up in
intensity and frequency in 1995 at precisely the time when the subpo-
lar waters began to increase in salinity. Because salinity changes
evolve over several years, the current hurricane regime is likely
to continue.

So, we either get a strong hurricane season (which will be blamed
on global warming) with a healthy thermohaline circulation, or weak
and infrequent hurricanes, with a sick thermohaline circulation
(which will be blamed on global warming). What a win-win situation
for those fanning global warming fears.

Extreme Heat and Cold in the United States

The previous chapter detailed Ken Kunkel’s work on the fre-
quency of extreme rainfall in the U.S., where he found that the recent
era sure looks a lot like it did 100 (colder) years ago. Kunkel exploited
newly digitized Cooperative Observer data from 1895 through 1948.
Using this expanded set of climate data, Kunkel also examined
the frequency and magnitude of heat and cold waves, defined by a
4-day event with an average appearance of once every five years.
The heat wave record (Figure 6.7, bottom) is dominated by the
huge spike during the 1930s ‘‘Dust Bowl’’ era. Recent heat is hardly
noticeable in the longer-term context, even though the number of
heat waves has increased recently after the cool summers of the
1960s and 1970s.

For cold waves (Figure 6.7, top), you’d think that would be a no-
brainer in greenhouse world. After all, we showed in chapter 1 that
the coldest nights of the year are warming more than any others.
Most climatologists (including us) believe they are arguing sensibly
when they say we should expect fewer cold waves as our winter
air masses warm.

Data induce humility. Figure 6.7 shows no obvious signal in the
frequency of cold waves.What about arguments that global warming

191

CLIMATE OF EXTREMES

U.S. COLD WAVE (TOP) AND HEAT WAVE (BOTTOM) FREQUENCY

Figure 6.7

INDICES, 1895–1997

SOURCE: Kunkel 2006.

will produce more extreme heat and cold? Kunkel’s data support
none of this. If more cold waves are the result of global warming,
why have peaks that dominated the 1980s completely disappeared?
And if we should expect fewer cold outbreaks, then how does one
account for all the cold air outbreaks in the 1980s when the atmo-
sphere had plenty of greenhouse gases? The cold wave record clearly
shows some interesting long-term variability but no obvious trend.
Those data, from the lower 48 states, are for less than 2 percent
of the earth’s surface. But the United States has the most dense long-
term weather records of any similar-sized place on the planet. Note

192

Climate of Death and the Death of Our Climate

how the addition of the pre-1948 Cooperative Observer figures can
completely change one’s interpretation of trends (which we also
showed for extreme precipitation in the previous chapter).

As with all issues of global climate change, the devil’s in the
details, but details about weather records hardly make for block-
buster headlines.

193

7. Pervasive Bias and Climate Extremism

The story portrayed in this book is that there is a body of science—
an internally consistent one—that paints a picture that is much
different than the gloom-and-doom vision of climate change that
we read about almost every day. The material should prompt several
questions, including the obvious one: ‘‘Why haven’t I heard about
this?’’

This chapter is an attempt to explain. It does not provide a defini-
tive answer, because we believe that the answer is complex and
currently unknown in any universal sense. But there are known
aspects of the behavior of science itself, and of the public presentation
of scientific information, that certainly have some explanatory
power.

The larger question is this: How did we get to global warming’s
climate of extremes? Is it a product of the reporting of science, or
is it because of the behavior of scientists themselves and global
warming science itself?

Certainly, few scientists would blame themselves. But, it turns
out, what climate scientists believe to be true about their own global-
warming research is not in fact true. Climate scientists believe that
there are no inherent biases in their work as it is published in the
scientific literature.

If bias is inherent in the science itself, then that primary informa-
tion stream that is fed to the media is itself biased. Even if the
journalistic community were philosophically, rhetorically, and edito-
rially neutral, the bias would come shining through.

That is best demonstrated with an example from something with

very little political baggage: the daily weather forecast.

Science as an Unbiased Penny

Step away from forecasts of the temperature 100 years from now.
Go to more familiar territory: the weather forecast several days
ahead.

195

CLIMATE OF EXTREMES

As of this writing, on a pleasant Sunday in May, the forecast high
temperature for next Thursday here in Washington, DC, is 84°F
(29°C). That forecast was generated by a computer model, based
upon a worldwide, simultaneous, horizontal and vertical snapshot
of the physical atmosphere taken at 8:00 a.m. Eastern Daylight Time
(EDT) today.

Every day, two of these worldwide snaps are taken simultane-
ously: at noon Greenwich Mean Time (GMT) (named for Britain’s
Greenwich Observatory) and at midnight GMT (8:00 a.m. and 8:00
p.m. EDT, respectively).

Four hours from now, today’s second observation will be taken,
and it will take an additional hour and a half to be input to and to
run through the world’s various weather forecasting models.

Although some of the models will predict the original 84°F, others
will predict some other temperature. After discounting those models
that didn’t change, what are the probabilities that the new forecast
will be either warmer or colder than the original 84°F?

Exactly equal. If we assume there wasn’t something systematically
wrong with the original forecast—meaning that bad data or calcula-
tion errors made it too warm or too cold—the probability of a new
forecast raising or lowering the result from the previous one is
the same.

Weather forecasters believe that, and so do climate forecasters.
They literally took this claim all the way to the U.S. Supreme Court
(more on that in a moment). And they were wrong. The world of
climate science turns out to be naturally biased.

Yet the belief in nonbias permeates the community. That’s what
David Battisti and 18 other people, calling themselves ‘‘the Climate
Scientists,’’ showed in an amicus brief concerning the first case relat-
ing to the regulation of greenhouse gases (Commonwealth of Massachu-
setts v. U.S. Environmental Protection Agency). Indeed, ‘‘the Climate
Scientists’’ included some very big names, such as NASA’s James
E. Hansen, head of the Goddard Institute for Space Studies, and
Nobel Prize winners Mario Molina and F. Sherwood Rowland.1

1 In an amicus curiae (Latin for ‘‘friend of the court’’), authors are always listed
alphabetically, rather than by rank or by the amount of contribution to the brief;
amicus briefs are submitted to offer information intended to inform the court’s
decisionmaking.

196

Pervasive Bias and Climate Extremism

Equal Justice Under Law for Climate Scientists?

‘‘Equal Justice Under Law.’’ Those words are carved into
the frieze of the U.S. Supreme Court building in Washington—
but do they apply to climate scientists’ freedom of speech?

NASA’s James Hansen was a coauthor of Alan Battisti et al.’s
brief in Massachusetts v. U.S. Environmental Protection Agency,
which maintained that the preponderance of scientific evidence
demonstrated that global warming caused by carbon dioxide
had such potential for environmental damage that it fell under
the 1990 Amendments to the Clean Air Act, requiring the EPA
to issue rules and regulations. In the section of the brief labeled,
‘‘Interests of the Amici Curiae,’’ Hansen describes himself as
director of NASA’s Goddard Institute for Space Studies. There-
fore, in expressing that claim, Hansen was siding against his
employer, the federal government, which administers both
NASA and the EPA.

Harvard’s Sallie Baliunas et al. wrote another amicus brief
countering ‘‘the Climate Scientists,’’ in which they argued that
there was no extant comprehensive study of the full net effect
of carbon dioxide–induced global warming, and that therefore
there was no technical basis for regulation. She and her fellow
amici, including Delaware State Climatologist David Legates
and the authors of this book, were therefore siding with the
federal government.

Delaware was well represented, albeit on both sides. Dela-
ware Attorney General Carl Danberg had entered a separate
brief in support of Massachusetts (i.e., against the federal gov-
ernment). As a result, two entities that would both appear to
represent the state of Delaware had come down on opposite
sides of the case. As this all came to a head, Delaware Governor
Ruth Ann Minner (D) told Legates that he could not speak
about global warming using the title of State Climatologist.
Hansen, however, received no notification from NASA that he
could not refer to himself as director of the Goddard Institute
(an obviously federal entity) when giving his views on global
warming, even though he argued against the federal case.

197

CLIMATE OF EXTREMES

In discussing the effects of carbon dioxide–induced warming on

human health, the Battisti et al. brief said:

EPA also ignored the two-sidedness of scientific uncertainty.
Outcomes may turn out better than our best current predic-
tions, but it is just as possible that environmental and health
damages will be more severe than the best prediction.

The key words are ‘‘just as possible.’’ What Battisti et al. are
claiming is that each new research finding stands an equal probabil-
ity of making the predicted effects of global warming on ‘‘environ-
mental and health damages’’ ‘‘better’’ or ‘‘more severe.’’

Battisti et al. are contending that updated climate science is analo-
gous to updated weather forecasts—that is, that there is an equal
probability that new results would be either more severe or more
moderate.

Battisti et al. would certainly claim to represent the mainstream
of scientific opinion on this issue. The primary citation in their
amicus brief is a 2001 report on climate change from the U.S. National
Research Council titled Climate Change Science: An Analysis of Some
Key Questions. What could be more mainstream then the National
Research Council?

Battisti et al. actually framed a very testable hypothesis, by assum-
ing that new findings have an equal chance of projecting either the
warming or its effects ‘‘better’’ or ‘‘worse.’’

If their hypothesis is true, this climate research community must
believe that there is no ‘‘publication bias’’ in climate change research
in either a positive or a negative direction. Such an assumption is
natural for atmospheric scientists, given that many of them are
trained in weather forecasting. In general, forecast models are cor-
rected for internal biases to the fullest extent possible before they
become operational. In fact, the quantitative output of weather mod-
els, called Model Output Statistics, are mathematically constrained
to be unbiased in any direction. So each new iteration of a forecast
model does indeed have an equal probability of moving the output
statistics up or down.

If climate models are similarly unbiased, then new information
on forecast climate change or its impact should also have an equal
probability of making existing forecasts worse or better. On the
other hand, the assumption will be untrue if the consensus of extant

198

Pervasive Bias and Climate Extremism

forecasts is simply wrong with regard to the severity or lack of
warming because of some basic flaw in model concept or design.
Still, there are a lot of climate models out there, all supposedly
of independent design. There have now been four studies ‘‘intercom-
paring’’ them under common conditions (i.e., similar changes in
carbon dioxide), and (the first is shown as Figure 7.1; see insert)
they tend to behave similarly. Consequently, there is no compelling
reason to believe in some pre-existing, pervasive model bias that
needs correction in one direction.

So, at first blush, the ‘‘consensus’’ that there is no bias would
seem to be reasonable. But economists and biomedical scientists
would disagree. They have written an extensive literature on ‘‘publi-
cation bias.’’

How does the scientific literature become biased in one direction?
One cause has nothing to do with any personal bias or self-interest
on the part of the scientists. Rather, it has to do with the nature of
scientific ‘‘news.’’

Negative results are generally considered not noteworthy. For
example, if a researcher ‘‘discovered’’ that there was no relationship
between, say, regional birth rates and global warming, no scientific
journal is likely to accept such a paper because (presumably) no
relationship was expected.

In other words, scientific journals are skewed by a prejudice for
the publication of statistically significant, ‘‘positive’’ results and prej-
udiced against findings of no relationship between hypothesized
variables.

Harvard’s Robert Rosenthal is generally thought to have pub-
lished the seminal paper in 1979 on this type of publication bias,
which he called the ‘‘file drawer problem.’’ According to Rosenthal:

For any given research area, one cannot tell how many stud-
ies have been conducted but never reported. The extreme
view of the ‘‘file drawer problem’’ is that journals are filled
with the 5 percent of the studies that show Type I errors [a
‘‘positive’’ result], while the file drawers are filled with the
95 percent of the studies that show nonsignificant results.

Rosenthal even went on to develop a quantitative methodology
to correct for ‘‘missing’’ negative results when analyzing large num-
bers of papers dealing with a specific topic. The notion is quite
important in the biomedical literature, where there can be quite clear

199

CLIMATE OF EXTREMES

Peer Review, Bias, and the Refereed Literature

The canon of science consists of the peer-reviewed or ‘‘refe-
reed’’ literature. In the classic sense, the process is rather
straightforward.

A scientist submits an article to a peer-reviewed journal,
such as Science, Nature, or Geophysical Research Letters. The editor
first decides if the material is appropriate in general, and then
sends it to two or three outside reviewers for comment. Those
reviewers—who are peers from the author’s professional com-
munity—advise the editor whether to publish the manuscript
in its original form, to accept it with modification, to reject it
but to entertain another submission with modification, or to
reject it outright.

Ideally, the name(s) of the authors should not be made avail-
able to the reviewers, but that practice has long since vanished,
at least in climate science. Further, the reviewers should have
no particular interest whether or not the submitted manuscript
sees publication, and they should have a considerable degree
of professional independence from the writers of the manu-
script in question.

In a 2007 book, Controversy in Global Warming: A Case Study
in Statistics, statistician Edward Wegman expressed concern
about the possibility of bias within the peer review process:
‘‘In particular, if there is a tight relationship among the authors,
and there are not a large number of individuals engaged in a
particular topic area, then one may suspect that the peer review
process does not fully vet papers before they are published.’’
Wegman was interested in the scientific review of papers by
Pennsylvania State University’s Michael Mann, who published
several articles purporting to show that the warmth of the
second half of the 20th century was unprecedented in at least
the last millennium. The principal result is a graph widely
known as the ‘‘hockey stick’’ plot because it shows very little
change for 900 years (the handle) and then a sudden jump in
the last century (the blade).

(continued on next page)

200

Pervasive Bias and Climate Extremism

(continued)

Wegman found that it was highly likely that the reviewers
of Mann’s work were also people with whom he had coau-
thored other papers, and so the probability of an independent
review was greatly diminished.

And what’s really left of peer review, anyway? The American
Geophysical Union asks people who submit scientific papers to
provide the names of five people they think would be desirable
reviewers and also (optional) a list of people that the author
belives could not provide an objective review. When the writer
can influence the selection of reviewers, peer review is pretty
much dead.

In other words, authors can now tell editors who their friends
are and who can’t be ‘‘objective.’’ This sounds like a formula
for publication bias.

consequences for nonpublication of negative results. People may
continue drugs or treatments that in fact have no effect on disease
progression. But there seems to be no discussion or any documenta-
tion of publication bias in the climate change literature.

Rosenthal’s model should apply to climate change. Perhaps there
is a bias toward ‘‘positive’’ manuscripts’ relating global warming
to some effect, and a bias against results where there is no relation.
A related logic applies to the magnitude or importance of some
phenomenon. In global warming, are there an improbable number
of publications indicating that warming, or its effects, will be greater
(i.e., ‘‘worse than expected’’) rather than more modest (i.e., ‘‘not as
bad as expected’’)?

The famous polymath Stephen Jay Gould described another cause
of publication bias in 2002. He stated that publication bias results
from ‘‘prejudices arising from hope, cultural expectation, or the
definitions of a particular theory [that] dictate that only certain kinds
of data will be viewed as worthy of publication, or even documenta-
tion at all.’’ Gould’s definition is quite analogous to the late Thomas
Kuhn’s notion of a scientific paradigm, first elucidated in his 1962

201

CLIMATE OF EXTREMES

classic, The Structure of Scientific Revolutions. In Kuhn’s view, para-
digms are overarching logical structures that provide the best expla-
nation of a family of phenomena. When interfaced with publication
bias, a paradigm should become increasingly defensive and
exclusionary.

The body of scientific work that makes up the peer-reviewed
literature is what defines a reigning paradigm. Are there other pro-
cesses that intrude in this literature that go beyond the ‘‘file drawer’’
and paradigm effects?

Consider the dynamics of ‘‘Public Choice Theory,’’ first described
in 1962 by Nobel Prize winner James Buchanan and Gordon Tullock.
They argued that ‘‘individuals will, on the average, choose ‘more’
rather than ‘less’ when confronted with the opportunity for choice
in a political process, with ‘more’ and ‘less’ being defined in terms
of measurable economic position.’’

In the case of global warming, public choice influence may occur
at several levels that could promote publication bias. As virtually
all global warming science is a publicly funded enterprise, political
dynamics must in part be involved. At the simplest level, global
warming is just one of many scientific issues competing for funding.
AIDS and cancer, for example, are competitors.

Because the scientific budget is finite, the perceived importance
of each competing issue determines in part how much support each
one receives. It is difficult to imagine, at the level of Congressional
hearings, that high-level managers or funding recipients for any of
these issues would dare portray them as relatively unimportant.
That creates a culture in which any scientific finding undermining
importance (or in this case, indicating less, or less costly, climate
change) becomes economically threatening. As a result, the peer
review process would have to become biased, unless reviewers never
act in their own interest.

Further, the reward structure in academia—promotion, tenure,
and salary—is based on the quality and quantity of peer-reviewed
research. The requisite level and number of publications for tenure
is virtually impossible to achieve without substantial public funding.
Interestingly, Julia Koricheva, of the Royal Holloway University in
London, found in 2003 no evidence that dissertation-related publica-
tions suffered from publication bias. Perhaps, then, it is a correlate
of professional development.

202

Pervasive Bias and Climate Extremism

This funding stream, and therefore career advancement, is threat-
ened by findings downplaying the significance or magnitude of
climate change. Under this model, articles submitted for publication
making the case for less-than-alarming findings would likely receive
more vigorous and negative reviews than articles arguing otherwise.
Stanley Trimble, a geographer at UCLA, recently summarized
what he calls ‘‘the double standard in environmental science.’’ He
wrote (in the Cato Institute journal Regulation) that, in the major
journals, specifically and repeatedly citing Science and Nature, ‘‘The
implication is that flimsy or even no evidence for environmental
degradation is acceptable, but any evidence for improvement is
suspect.’’
Further:

For those of us in academe, all of this can have profound
implications for careers that depend upon having many (but
not necessarily good) publications for advancement. And
based upon prima facie evidence, environmental extremism
is good for the career.

In summary, there are at least three social processes that argue
against the assertion of ‘‘the Climate Scientists’’ that new research
results have an equal probability of describing a more or less severe
effect from global warming: The file drawer problem, the paradigma-
tic nature of science, and public choice dynamics.

While there are no obvious citations concerning the existence of
publication bias in the atmospheric science, meteorological, or clima-
tological literature, it has been discussed in evolutionary ecology
(i.e., Gould’s 2002 statement), and by University of Birmingham’s
(U.K) Phillip Cassey et al., in 2004 in Proceedings of the Royal Society.
Cassey et al. noted that analyses of collections of papers (‘‘meta
analyses’’) assume that the original work is nonbiased. In their study,
Cassey et al. found pervasive bias against negative results related
to sex ratios.

Cassey et al. concluded that ‘‘publication bias is not just a ‘file
drawer’ problem but can also be manifest within the primary litera-
ture. This is particularly likely to be a problem in research fields
where results are presented for a large number of independent test
variables, such as in ecology.’’ It certainly seems reasonable that
climate change research (and its interface with biology) would

203

CLIMATE OF EXTREMES

behave analogously. Clearly, the nexus of interaction between cli-
mate and biology is ecology.

Determining whether the public choice dynamics, the cultural
expectation–derived bias, or the file drawer problem, or some combi-
nation of the three is creating publication bias in climate science is
a daunting task.

But, before undertaking that, we need to establish whether or not

publication bias exists in the world of climate change.

That’s an easy determination. Assume that each new piece of
information that contributes to some forecast of a future phenome-
non (or the effects of one) should have an equal probability of making
that forecast ‘‘worse’’ or ‘‘better.’’ Documentation of a highly
improbable distribution of ‘‘worse’’ and ‘‘better’’ would support the
hypothesis that publication bias in climate change research is a real
phenomenon.

Let’s do a little experiment, examining climate change–related
articles in the journals Science and Nature from July 1, 2005, through
July 31, 2006. In Science, let’s look at the sections titled ‘‘Perspectives,’’
‘‘Reviews,’’ ‘‘Brevia,’’ ‘‘Research Articles,’’ and ‘‘Reports.’’ In Nature,
we’ll do ‘‘News,’’ ‘‘News Features,’’ ‘‘Correspondence,’’ ‘‘News &
Views,’’ ‘‘Articles,’’ and ‘‘Letters.’’ A total of 116 relevant articles
were counted in this period, or slightly more than two per week (or
one per magazine per week)—52 in Science and 64 in Nature.

Let’s place each article into one of three classes, labeled for conven-

ience as ‘‘better,’’ ‘‘worse,’’ or ‘‘neutral or could not classify.’’

Articles were placed in the ‘‘better’’ bin if the findings reduced the
amount of prospective global or regional warming or unfavorable
weather or climate, or reduced some impact or effect that had been
previously established in the scientific literature. Similarly, articles in
the ‘‘worse’’ bin increased the amount of global or regional warming,
increased the frequency and/or magnitude of unfavorable weather
or climate events, or increased the impact or effect of climate change
on some responding phenomenon.

Articles were placed in the ‘‘neutral or could not classify’’ bin if
they had approximately offsetting ‘‘better’’ and ‘‘worse’’ implica-
tions, or simply were not classifiable because of content.

Of the 116 articles, 84 were ‘‘worse,’’ 10 were ‘‘better,’’ and 22

were ‘‘neutral.’’ The results are summarized in Table 7.1.

Some of the 116 articles were fairly easy and straightforward to
classify. For example, King et al. (2006) wrote, ‘‘Acclimation of plants

204

Pervasive Bias and Climate Extremism

Table 7.1

CLASSIFICATION OF ARTICLES ON CLIMATE CHANGE IN

SCIENCE AND NATURE, JULY 2005–JULY 2006

‘‘Better’’

‘‘Worse’’

Total
Journal
52
Science
64
Nature
Total
116
SOURCE: 116 articles in Science and Nature, July 1, 2005, through July 31,
2006; a full bibliography with classifications appears at the end of this book.

5
5
10

34
50
84

Neutral

13
9
22

to higher temperatures may reduce the extra warming caused by
increased plant respiration in a future warmer world.’’ That clearly
belongs in the ‘‘better’’ bin. Similarly, when Schimel (2006) titled an
article ‘‘Climate Change and Crop Yields: Beyond Cassandra,’’ and
wrote that ‘‘previous studies overestimated the positive effects of
higher carbon dioxide concentrations on crop yields,’’ there’s little
debating that it goes in the ‘‘worse’’ bin.

But some weren’t very straightforward at all, indicating both
‘‘worse’’ and ‘‘better’’ aspects within the same publication. Classifi-
cation was made on the basis of whether one or the other dominated.
If neither did, the report was classified as ‘‘neutral.’’ In some cases,
though the aspects were mixed, one side outweighed the other, as
with Feddema et al. (2005) on land-use influences on climate change;
their article included both positive and negative effects, but there
were more negative than positive, so the paper was classified as
‘‘worse.’’ An example of a ‘‘neutral’’ result from competing effects
can be seen in Gedney et al. (2006), who, writing about water budgets
and carbon dioxide, concluded, ‘‘As the direct CO2 effect reduces
surface energy loss due to evaporation, it is likely to add to surface
warming as well as increasing freshwater availability.’’ Sounds like
some good, some bad.

‘‘Neutral’’ reports included those that were consistent with pre-
viously published either negative or positive effects. Consequently,
a paper by Field et al. (2006) finding biological evidence for deep-
ocean warming is ‘‘neutral,’’ given the previous large body of work
(Barnett, Pierce, and Schnur 2001; Levitus et al. 2000; etc.) showing
the same results. Note that such labeling does not mean that the
original background work was necessarily neutral, only that it was
published before the beginning of our examination period.

205

CLIMATE OF EXTREMES

Timing and Publication Bias

Two concurrent events in December 2005 exemplified how
publication bias and news go hand-in-glove. They were the
Montreal ‘‘Conference of the Parties’’ that had signed the UN
Kyoto Protocol on global warming and the fall meeting of the
American Geophysical Union (AGU) in San Francisco.

The sheer volume of hype was impressive. Following are the
headlines, along with the sources, generated on the afternoon of
December 7, first from the Montreal UN conference. (Univer-
sity news sources are those that were eventually picked up
in other stories). These were obtained from Google’s news
search page.
● ‘‘Global Warming to Halt Ocean Circulation’’ (University

of Illinois)

● ‘‘Warming Trend Adds to Hazard of Arctic Trek’’ (Salem

[Oregon] News)

● ‘‘Pacific Islanders Move to Escape Global Warming’’

(Reuters)

● ‘‘Tuvalu: That Sinking Feeling’’ (PBS)
● ‘‘World Weather Disasters Spell Record Losses in 2005’’

(Malaysia Star)

● ‘‘Arctic Peoples Urge UN Aid to Protect Cultures’’ (Reuters)
● ‘‘Threatened by Warming, Arctic People File Suit Against

US’’ (Agency France Press)
Next, from San Francisco:

● ‘‘Ozone Layer May Take a Decade Longer to Recover’’ (New

York Times)

● ‘‘Earth is All Out of New Farmland’’ ([London] Guardian)
● ‘‘Forests Could Worsen Global Warming’’ (UPI)
● ‘‘Warming Could Free Far More Carbon from High Arctic

Soil than Earlier Thought’’ (University of Washington)

● ‘‘Rain Will Take Greater Toll on Reindeer, Climate Change

Model Shows’’ (University of Washington)

(continued on next page)

206

Pervasive Bias and Climate Extremism

(continued)

● ‘‘Methane’s Impacts on Climate Change May Be Twice Previ-

ous Estimates’’ (NASA)

● ‘‘Average Temperatures Climbing Faster than Thought in

North America’’ (Oregon State University)
How can things be so bad?
Each one of those stories carries an ‘‘it’s worse than we
thought’’ subtext. There was a single additional story to the
contrary, published by AP, that indicated that plants may store
more carbon dioxide than was previously thought, which
would help to limit warming.

Similarly, many observational studies, such as ice core research
published in 2005 by Siegenthaler et al., are largely extensions of
previous work. Unless such studies revealed behavior that was
inconsistent with or largely different from related research, they
were classified as ‘‘neutral.’’

The assumption of nonbias of ‘‘worse’’ vs. ‘‘better’’ is analogous
to flipping a coin. The observed better/worse ratio has the same
probability as flipping a coin 94 times and getting 10 or fewer heads
(or tails). That would arise by chance in an unbiased sample with
a probability of less than 5.2 ⳯ 10ⳮ16 (or a chance of less than 1 in
50,000,000,000,000,000).

Trimble writes that ‘‘Science and Nature are truly major gatekeep-
ers of science; indeed they are the gold standard.’’ Rather than
performing a time-sequence analysis, as was done here, he examined
their handling of soil erosion studies. He found a similar bias toward
‘‘worse’’ results, and concluded that

[Science and Nature] have a special obligation to objectivity
and even-handedness. But it seems clear that they sometimes
have not maintained their charge as it pertains to environ-
mental science.

An alternative explanation to publication bias is simply that the
magnitude and impacts of warming have been systematically under-
estimated, and that the recent literature we examined is merely
reflective of science that ‘‘self-corrects.’’

207

CLIMATE OF EXTREMES

But the self-proclaimed climate community in Massachusetts v.
EPA clearly believes that previous work is unbiased; otherwise, that
group would hypothesize no equal probability that a new result
might appear better or worse than previous ones. And a much
broader modeling community, in the 2007 ‘‘Fourth Assessment
Report’’ of the United Nations Intergovernmental Panel on Climate
Change, makes a clear claim that modeled and observed climate
changes over the last 50 years are very similar:

It is likely that there has been significant anthropogenic warm-
ing over the past 50 years averaged over each continent
except Antarctica. . . . The observed patterns of warming,
including greater warming over land than over the ocean,
and their changes over time, are only simulated by models
that include anthropogenic forcing.

Consequently, both Battisti et al. in Massachusetts v. EPA and the
larger IPCC community feel that the models are unbiased and that
therefore new results have an equal probability of making the future
appear ‘‘worse’’ or ‘‘better.’’

Obviously, the publications of the scientific community that are
selected by Science and Nature have considerable influence and pene-
tration into the media. So why is the news almost always bad? It’s
not necessarily because of journalistic bias. Instead, their primary
source—the science itself—is biased in one direction.

The demonstrable existence of publication bias in global warming
science has several consequences, especially when one tries to ‘‘sum-
marize’’ climate science.

Repeated attempts have been made to use the primary literature
to form such overall summaries or ‘‘state of the science’’ reports.
The highly cited 2007 IPCC ‘‘assessment’’ of climate change is a
primary example. Periodically, the U.S. National Research Council
addresses climate change issues, and those reports largely rely on
the published literature. (As noted earlier, a recent Council report
served as much of the basis for Battisti et al.) There is never a
discussion of the possibility of publication bias in these reports.

The consequences of synergy between publication bias, public
perception, and scientific ‘‘consensus’’ and policy are very disturb-
ing. If the results shown for Science and Nature are in fact a general
character of the scientific literature concerning global warming, our
climate-related policies are based upon a directionally biased stream

208

Pervasive Bias and Climate Extremism

of information, rather than one that has a roughly equiprobable
distribution of altering existing forecasts or implications of climate
change in a positive or negative direction. That bias exists despite
the stated belief of the climate research community that it does not.
Note that bias does not mean the exclusion of differing points of
view; it just means that, in the case of global warming, published
research papers saying that warming will be attenuated or that the
effects will be muted are likely to be fewer and further between.

To survive the minefield of publication bias, papers arguing for
less global warming or less impact of climate change that appear in
the refereed literature are likely to be compelling and relatively free
of the major flaws. Although the ratio of alarmist to ‘‘worse’’ to ‘‘not
so bad’’ is about 10 to 1, it is clear that there is a body of literature
out there that may argue very cogently against the current hysteria.

The Internet and Peer Review

The Internet is dramatically changing the sacrosanct nature of the
peer-reviewed literature. High-profile papers appearing in major
journals hit the blogs, and are sometimes found to be deeply flawed
after publication. Perhaps an example from something slightly less
incendiary than climate change—stem cell research—will illustrate
the process.

In March 2004, Science published an earthshaking paper by H. S.
Hwang and several coauthors who claimed to have obtained stem
cells from a cloned human embryo. It included several pictures.

Skeptical scientists detected something wrong with the images.
They noted that the individual pictures actually were overlapping
images of a single larger picture. Moreover, they had been seen
before. Another team of researchers had submitted them to the
journal Molecules and Cells before Hwang’s paper was sent to Science.
The earlier paper described them as cells that were created with-
out cloning.

The Boston Globe had been following this issue on the blogosphere.
It took the photos and sent them to several stem cell researchers,
who pronounced them as identical.

Eventually, and with great embarrassment, Science retracted the
paper. Donald Kennedy, the editor of Science, defended the review
process, saying that reviewers could not be expected to detect delib-
erate falsehoods. Further, Kennedy noted that reviewers can demand
more data if they are suspicious.

209

CLIMATE OF EXTREMES

Rather than let the reviewers (and Science) try to slide off the hook
on the stem cell fiasco, Kennedy should have thanked the Internet!
Steve McIntyre, himself a tireless and meticulous investigator of
global temperature histories, cites the ‘‘Hwang Affair’’ as (yet
another) example of poor peer review. In his blog Climate Audit
(http://www.climateaudit.org), he noted that

While Western scientists were still supporting Hwang, young
Korean scientists posting on a blog . . . are credited with
actually looking through the details of his work and finding
evidence for fabrication.

For whatever reason that the peer review process failed, the
Internet-as-peer-reviewer helped to dig out the truth. The Internet
will certainly not prevent publication bias, but it is a powerful
medium that can communicate almost immediate correction of
errors so obvious that they should have been detected before
publication.

Involvement of Professional Organizations

It seems that every profession, including science, has its Washing-
ton lobby. The American Association for the Advancement of Science
(AAAS) sits on prime real estate at 1100 New York Avenue NW
and can be thought of as America’s science lobby. Do organizations
such as that publicize unbalanced or even wrong information about
climate change without peer review?

On its website, AAAS states, among other activities, that it ‘‘spear-
heads programs that raise the bar of understanding for science
worldwide.’’ One such ‘‘program’’ was on June 15, 2004, when
AAAS convened what it called an ‘‘all-star’’ panel of U.S. climate
scientists to discuss climate change.

The AAAS ‘‘all-star’’ panel including, Daniel Schrag and Michael
Oppenheimer, who had appeared less than three weeks earlier on-
stage with Al Gore during the ultra-Leftist MoveOn.org’s kick-off
for the climate fiction movie The Day After Tomorrow.

Though Schrag and Oppenheimer didn’t embrace the nonscience
of the movie (e.g., an instantaneous ice age), they did embrace its
sentimentality and message, an indication that they believe gross
exaggeration and scientifically impossible scenarios are permissible,
if they will draw attention to the issue of global climate change.

210

Pervasive Bias and Climate Extremism

Other panelists were Sherwood Rowland, Richard Alley, Gerald
Meehl, Joyce Penner, and Lonnie Thompson, all prominent in public
discussions of climate change.

Reuters’ health and science correspondent Maggie Fox noted that
the panel expressed frustration that the U.S. government and public
were not more concerned with what the panelists saw as the risks
associated with global warming.

The panelists were correct in their assertion about public percep-
tion of global warming. At that time, a Gallup poll revealed that a
plurality of Americans believed that news reports exaggerated the
seriousness of global warming. The poll asked this question: ‘‘Think-
ing about what is said in the news, in your view is the seriousness
of global warming—generally exaggerated, generally correct, or is
it generally underestimated?’’

Gallup reported that 38 percent of us thought it is ‘‘generally
exaggerated,’’ compared with 25 percent who thought it is ‘‘gener-
ally correct.’’

Maggie Fox then reported, ‘‘[The AAAS panelists] said even as
sea levels rise and crop yields fall, officials argue over whether
climate change is real and Americans continue to drive fuel-
guzzling SUVs.’’

The statement about crop yields was astoundingly wrong. Figure
7.2 is the history of yields from two important U.S. crops, corn and
wheat. There has been a dramatic rise since the late 1940s. The year
2003 saw record-high wheat yields and 2004 saw record-high corn
yields. In fact, according to data from the U.S. Department of Agricul-
ture, 14 of the 20 major crops grown here have set record-high yields
within the past 10 years. To state that crop yields are falling is at
best misleading, and at worse an outright falsehood, given that the
U.S. produces almost half of the entire world’s corn supply.

Panelist Michael Oppenheimer (Environmental Defense Fund and
Princeton University), told the audience: ‘‘The sea-level rise over
the past century appears greater than what the model says it should
be. The [Greenland and Antarctic] ice sheets may be contributing
more than the models predict.’’

Such a statement showed little regard for the latest scientific evi-
dence at that time. For example, published just days before, in the
journal Geophysical Research Letters, were the results of a sea-level
rise study conducted by Cambridge University’s Peter Wadhams,

211

CLIMATE OF EXTREMES

U.S. YIELDS OF CORN AND WHEAT, 1900–2007

Figure 7.2

SOURCE: U.S. Department of Agriculture 2008. http://www.nass.usda.gov/
QuickStats/.

along with Scripps Institution of Oceanography’s Walter Munk.
Those researchers carefully calculated the known contributions to
sea-level rise (ocean warming, Greenland and Antarctic ice sheets,
and midlatitude glaciers) over the 20th century and concluded, ‘‘We
do obtain a total rise which is at the lower end of the range estimated
by the IPCC—exactly the opposite of what Oppenheimer told the
AAAS audience.’’

Publication Bias: Creating the Political Climate

The notion that publication bias is responsible for an unbalanced
scientific literature requires that there be some incentive. Obviously,
personal and professional advancement are incentives, but what is
it that could enable this?

In a word, funding. Government funding. Consequently, a thesis
of publication bias has to be supported by a notion that the funding
stream is in fact predicated upon the belief in dire climatic change,
rather than a more moderate view of the subject.

212

Pervasive Bias and Climate Extremism

Congress, of course, is the source for federal science funding. How
can scientists lead it to a biased point of view? How can powerful
scientific organizations such as the IPCC have an influence?

Simple. Make sure that future projections of climate change
include a range of estimates, with no particular likelihood ascribed
to any value within the range. Any organization that does so can
be sure that the most extreme values will be featured in political
discussion.

The IPCC is fully aware that its extreme values will be the ones
that are quoted for political purposes. In its 2001 assessment, IPCC
indicated a prospective 21st century warming of 1.4°C to 5.8°C
(2.5°F to 10.4°F). Inevitably, as we have seen, that becomes a state-
ment about warming ‘‘as much as 10.4°F.’’

These high-end estimates are then misused in policy arguments.
Note how the environmental organization Bluewater Network, a
part of the radical Friends of the Earth, used extreme values when
making a successful political argument.

In March 2004, Bluewater Network took credit for inspiring Sena-
tors John McCain (R-AZ) and Ernest Hollings (D-SC) to ask the U.S.
General Accounting Office (GAO) to investigate potential impacts
of climate change on public lands and waters. Being involved for a
long time in environmental politics, Bluewater knew that any such
report would be luridly biased.

Bluewater network claimed that GAO did so in response to their
2002 publication Scorched Earth, which ‘‘examined’’ the consequences
of potential climate change on U.S. public lands and waters. Scorched
Earth is a typical example of publications from environmental organi-
zations that rely on extreme scenarios and misstated science to sug-
gest that the climate and ecosystems of the United States will be
rendered unrecognizable as a result of anthropogenic emissions of
greenhouse gases.

The report’s Executive Summary contains a paragraph that epito-
mizes the gloom-and-doom predictions. Given that Senator McCain
had become, soon after the GAO report, a principal advocate among
Republicans for limits on carbon dioxide emissions, it’s worth exam-
ining Scorched Earth’s assertions in detail.

According to Bluewater,

Over the past 100 years, emissions of greenhouse gas pollu-
tion have led to increased global temperatures of more than

213

CLIMATE OF EXTREMES

1°F, which is unprecedented in the past 1,000 years. Scientists
worldwide predict that the pace of global climate change
will accelerate over the next century and impact ecosystems
with increasingly dramatic results. Average global tempera-
tures could increase by up to 10.4°F, a change unprecedented
over the past 10,000 years. This temperature increase is pro-
jected to result in reduced water availability, increased cata-
strophic wildfires and storms, and habitat impacts that could
wipe out entire species and ecosystems. Scientists predict a
rise in sea level of up to 2.89 feet as a result of projected
global temperature increases. Coupled with increasingly
severe storm events, a sea-level rise of this magnitude will
reshape coastlines and submerge low-elevation islands
entirely in both the U.S. and abroad. These global climate
change impacts will occur so rapidly that many plant and
wildlife species will not survive.

Here’s some of their assertions in perspective:

‘‘. . . increased global temperatures of more than 1°F . . . .’’ True, but
misleading. Yes, there has been a global temperature rise of about
‘‘more than 1°F’’ in the past 100 years. But about half that rise
occurred prior to the mid-1940s—a period before there were large
anthropogenic emissions of greenhouse gases. Between the mid-
1940s and mid-1970s, global temperatures even declined somewhat,
before beginning to rise again from the late 1970s to 1998. A human
fingerprint on the more recent temperature increase is probable,
because, as predicted by theory, the warming tends to take place in
the coldest air of the winter, mainly in Siberia. But it’s inaccurate
and misleading to state that the temperature rise that began more
than 100 years ago has resulted from human-enhanced levels of
greenhouse gases in the atmosphere.

‘‘. . . scientists worldwide predict that the pace of global climate change
will accelerate over the next century . . . .’’ This results from the coupling
of ‘‘as much as 10.4°F,’’ which is the extreme value from the IPCC,
with the previous statement about the observed rate of warming.
Because the IPCC is composed of ‘‘scientists worldwide’’ and the
two rates are obviously different, then ‘‘scientists worldwide’’ must
predict an increasing rate of warming.

In fact, as shown in Figure 1.5 (see insert), the ‘‘consensus’’ rate
of warming for their ‘‘midrange’’ emissions scenario is constant,

214

Pervasive Bias and Climate Extremism

not increasing. But the coupling of observed and (extreme) forecast
warming creates a totally different impression.

‘‘. . . a rise in sea level of up to 2.89 feet . . . .’’ Again, an extreme
value is used. The IPCC ‘‘Third Assessment Report’’ (the latest one
at the time of the publication of Scorched Earth) gives the range of
sea-level rise by 2100 resulting from anthropogenic greenhouse gas
increases as 0.30 to 2.89 feet. Again, Bluewater emphasizes only the
highest end of the range. The low end of this range results in impacts
no greater than those we observed during the 20th century, minor
changes to which we fully adapted.

‘‘. . . increasingly severe storm events . . . .’’ A common promise, but
as we have demonstrated repeatedly in this book, one that is at best
controversial, and that certainly is not supported by a thorough
reading of the scientific literature.

The Executive Summary of Bluewater Network’s Scorched Earth is
accurate in one respect: It correctly characterizes the misconceptions
contained throughout the rest of the report. So much for the publica-
tion said to have inspired McCain and Hollings’s GAO request.

The GAO’s response

Suffice it to say that the GAO took Bluewater’s lure hook, line,
and sinker, providing one of the most profound examples of publica-
tion bias. We’ll just excerpt one paragraph, a summary of the biologi-
cal effects of climate change:

Biological effects of climate change include increases in insect
and disease infestations, shifts in species distribution, coral
bleaching, and changes in the timing of natural events,
among others. For example, warmer temperatures and
reduced precipitation associated with climate change have
contributed to insect outbreaks in some areas, as illustrated
at the Chugach National Forest in Alaska. According to an
FS [Forest Service] official at the forest, a spruce bark beetle
outbreak has led to high mortality rates for certain types of
spruce trees on over 400,000 acres of the Chugach. In the
Kenai Peninsula, Alaska, on which part of the forest is
located, about 1 million acres have been affected by the bee-
tles. Officials at the Chugach indicated that continued
increases in temperature and decreases in precipitation could
further change vegetation composition and structure, and
increase the incidence and severity of future insect outbreaks.
Similarly, in the Mojave Desert near the BLM Kingman Field

215

CLIMATE OF EXTREMES

Office, invasive grasses, combined with drought caused, at
least in part, by climate change, have increased the frequency
and severity of wildland fires, destroying native plants
and transforming some desert communities into annual
grasslands. Prolonged drought weakens the natural plant
communities and then, in periods of wetness, invasive spe-
cies—particularly grasses—fill the gaps between native veg-
etation. These invasive grasses can spread and grow faster
than native species; the thicker and less evenly spaced vegeta-
tion leads to increased fire danger. If a fire starts, it burns
much hotter due to the invasive grasses. Native plant com-
munities, such as saguaro cacti and Joshua trees, are dam-
aged, which provides further environment for invasive spe-
cies and increased fire danger. According to experts, this shift
in ecosystems from desert to grassland is likely to continue as
the climate changes, which will in turn result in a loss of
species diversity in these areas.

Note, not one positive effect of climate change is mentioned. There’s
no discussion of the fact that carbon dioxide itself has been known
to make plants grow better for 100 years.

Consider the work of Ramakrishna Nemani and colleagues, who
studied two decades’ worth of satellite observations of large-scale
plant growth patterns across the world, and published their results
in Science. They reported a remarkable enhancement of global vege-
tation during that time. Nemani et al. concluded that the enhanced
growth resulted from a combination of two major influences: the
increased fertilization effect from growing concentrations of atmo-
spheric carbon dioxide and the patterns of change in the earth’s
climate during the study period. That finding is strong evidence of
a global-scale benefit of uniform greenhouse gas enhancement—
and stands in stark contrast to a picture of ecological destruction.
Nor is there any mention of increased crop production as a direct
result of carbon dioxide–induced growth stimulation and as an indirect
effect of increasing growing seasons and midlatitude precipitation.

What could be a finer example of the workings of publication bias
that ultimately influences policy? A lobbying group purposefully
ignores anything but the most lurid estimates of the IPCC, then sells
their report to two important senators, who commission the GAO
to produce a completely one-sided review.

216

Pervasive Bias and Climate Extremism

The Hockey Stick Controversy

A prominent example of publication bias via lax reviews in
modern climate science is the acceptance, by Nature, of a 1,000-
year temperature ‘‘history’’ derived from a mathematical
combination of large numbers of ‘‘paleolclimatic’’ records—
including tree rings, corals, and ice cores, all of which provide
some annual information on climate. (As an example, tree rings
are thicker in warm, wet summers than they are in cold, dry
ones.) It was originally a 600-year record published in Geophysi-
cal Research Letters, but the methodology for both the 600-year
and the more prominent Nature paper was the same.

This was discussed briefly earlier in this chapter in the con-
text of reviewer bias as noted in the Wegman report. The work
in question is the same ‘‘Hockey Stick’’ (Figure 7.3; see insert),
which figured prominently in the widely cited ‘‘Third Assess-
ment Report’’ on climate change published by the IPCC in
2001, and became a poster child for global warming enthusiasts
worldwide.

Steve McIntyre, a retired mining executive (and mathemati-
cian) from Canada became interested in how this result was
obtained and, along with University of Guelph Economist Ross
McKitrick, doggedly pursued Mann’s original data and then
applied the same mathematics to it that Mann had done.

Both were inherently suspicious. In 2005, economist McKi-
trick wrote, ‘‘After the dot-com boom, however, many business
people cringe when they see a hockey-stick graph.’’ One of
those, of course, was McIntyre.

McIntyre and McKitrick plowed through the mathematical
details of the original paper, which McKitrick characterized as
‘‘written in grandiose yet disorganized prose and omit[ting]
the mathematical equations that would allow readers to attain
an unambiguous understanding of what was done.’’

McIntyre had hypothesized that the method used to refer-
ence the data to the 1902–80 period (rather than the 600-year
average), combined with the complicated mathematics, would
preferentially create hockey sticks.

(continued on next page)

217

CLIMATE OF EXTREMES

(continued)

The trials and tribulations they went through to obtain the
original data and do their analyses is legendary and detailed
elsewhere, such as in chapter 2 of the book Shattered Consensus,
written by Ross McKitrick.

In a section of the Shattered Consensus chapter titled ‘‘How to
Make a Hockey Stick,’’ McKitrick shows the difference between
what happens when the mathematics is based on the average
of an entire (600-year) sample of random numbers vs. when it
is based on the average of the last 78 years (Figure 7.4).

MATHEMATICAL SIMULATIONS OF HISTORICAL

Figure 7.4

TEMPERATURES, BASED ON THE AVERAGE OF A 600-YEAR

SAMPLE OF RANDOM NUMBERS (TOP); AND THE AVERAGE OF

THE PAST 78 YEARS PLUS RANDOM NUMBERS (BOTTOM)

SOURCE: McKitrick 2005.
MBH98 ⳱ ‘‘Mann, Bradley and Hughes, 1998’’ method, which used
the last 78 years of observed temperatures for calculation of the global
temperature index (‘‘hockey stick’’).

In Shattered Consensus, McKitrick blamed the Hockey Stick
on lack of rigorous peer review. He and McIntyre learned
this first-hand when, after sending their results to Nature, the
reviewers stated that they were not capable of determining
whether Mann or McIntyre and McKitrick were correct. From
that, McKitrick concluded:

(continued on next page)

218

Pervasive Bias and Climate Extremism

(continued)

We are quite confident that Nature’s peer reviewers
for the original publication did not examine the data
or the program used to produce Mann’s hockey stick
or carry out any audit level due diligence.

When the IPCC published Mann’s hockey stick prominently
in the Technical Summary of the ‘‘Third Assesssment Report,’’
there was obviously no attempt to question Mann’s findings;
they relied only on the fact that it was published in Nature and
as a result it was obviously beyond question.

Publication bias doesn’t mean that only alarming global warming
papers are published, but rather that they are likely to comprise the
majority. Further, it means that the minority of nonalarmist (or anti-
alarmist) publications are likely to have undergone a very rigor-
ous review.

The causes are manifold, including the file drawer problem, per-
sonal incentives, and the tendency for extreme results to be used
for political effect. We are sure that this panoply is only one set of
factors that has led to the ‘‘climate of extremes,’’ but it has certainly
resulted in an ocean of dire findings dotted by islands of moderation.

219

8. Balancing Act: A Modest Proposal

The last chapter demonstrated that there are substantial institu-
tional biases in climate science that result in a preponderance of
gloom-and-doom findings. Yet the world prospers, despite global
warming almost always being ‘‘worse than we thought.’’

In the peer review process, there are clear incentives to keep global
warming ‘‘hot,’’ because the resulting headlines will keep political
(and federal funding) attention on it. There are also obvious disincen-
tives to publish much of the other side of the story.

The results are interesting. Horror stories that have received
shoddy review are easy to tear up in public. More moderate work
that has received stringent review holds up well under scrutiny.
Even though the ratio of horror stories to bedtime stories on global
warming is around 10 to 1, the more moderate paradigm seems to
possess remarkable internal consistency. Put simply, the earth exhib-
its a modest warming trend; computer models indicate that trend
will continue; and that means there is plenty of time—a century or
so—for technological development that will be more efficient and
emit far less carbon dioxide. Of course, the way to delay that happy
ending is to institute policies immediately that are based on the
horror stories. All that does is suck investment capital out of the
system—capital that could have been used to create a more effi-
cient future.

We see an example of an irrational political response to the horror
stories every day now, at the grocery store. President Bush’s response
to the clamor for global warming policy was to ask for legislation
mandating ethanol production. In 2005, a Republican congress sent
a Republican president an energy bill that resulted in 7 billion gallons
of ethanol in 2007 being produced as a replacement for fossil fuel. The
primary feedstock is corn. Put another way, the primary feedstock is
food. In 2007, we diverted a quarter of our corn crop from food
to ethanol.

Never mind that this doesn’t do a thing about global warming.
As shown in 2008 by Princeton’s Timothy Searchinger et al. in Science

221

CLIMATE OF EXTREMES

magazine, a study of the entire life cycle of biofuels demonstrates that
they produce more carbon dioxide than they save. That should surprise no
one. Agriculture often requires the cutting down of an original forest.
Corn requires fertilizer—a fossil fuel–based product—to achieve
high yields. Tractors run on gas or diesel. In the fermentation process,
the concentration of ethyl alcohol (ethanol) has to remain below
about 20 percent, or the fermenting yeasts will die. So, to create pure
ethanol, 80 percent of the liquid has to be boiled away, almost always
using fossil fuel–derived heat. Ethanol is a loser—for everyone
except agriculture, which benefits enormously from federal subsi-
dies for its production.

In 2007, President Bush proposed that 20 percent of our current
gasoline consumption be displaced by ethanol in 2020. If all our
corn became ethanol, only 12 percent of our gasoline consumption
would be displaced. Meeting this target requires another source of
ethanol and new technologies to digest cellulose, making so-called
‘‘cellulosic’’ ethanol. The crop of choice would likely be a tall peren-
nial weed called switchgrass. One problem: cellulosic ethanol has
never been produced in an economically viable fashion.

Searchinger noted:

We found that corn-based ethanol, instead of producing a
20 percent savings [of carbon dioxide], nearly doubles green-
house emissions over 30 years and increases greenhouse
gases for 167 years.

Cellulosic ethanol doesn’t work, either. If grown on corn lands,
Searchinger calculates that it would raise emissions by 50 percent.
Food prices are skyrocketing. Land that would normally be used
to grow soybeans or wheat is diverted to corn for ethanol. Conse-
quently, the price of wheat tripled less than three years after the 2005
Energy Bill. Because just about everything (except fresh vegetables)
in the grocery store is either a product of primary feedstocks (wheat ⳱
bread), or a result of their use (soybean and corn meal ⳱ hogs ⳱
pork), the price of just about everything is going up.

This creates grumpiness in the United States, and it also fosters
riots in poorer places, such as Indonesia and China in early 2008.
That’s just one example of how the climate of extremes on global
warming can lead to outrageous policy decisions or legal instruments.
Another outrage was the Kyoto Protocol to the UN Framework
Convention on Climate Change, a dismal failure. It was supposed

222

Balancing Act: A Modest Proposal

to reduce global carbon emissions to about 5 percent below 1990
levels, with the burden falling mainly on the industrialized, devel-
oped world (China and India were exempt). Global compliance with
Kyoto never happened. Instead, emissions from countries that were
supposed to cut emissions went up. The clear reason is that meeting
Kyoto was too expensive; if that were not a problem, it would have
been implemented easily. Still, even if it were fully complied with,
there would be no net detectable reduction in global warming for
nearly a century.

So, what can be done to modify the climate of extremes?
Certainly a part of it results from publication bias. If a preponder-
ance of scientific literature says ‘‘it’s worse than we thought,’’ that
will be the consensus position. We demonstrated that there was
certainly a bias toward ‘‘worse than we thought’’ articles in Science
and Nature, whatever the cause.

Our modest proposal to reduce publication bias is to eliminate
anonymous peer review. The massive expansion of cyberspace
makes that possible.

We propose that each of the major journals post on the Web every
article that is submitted along with the authors’ names. If the journal
simply rejected the submission without sending it out for review,
that should be noted. If it was sent for review, the journal should
post the reviews along with the names of the reviewers.

That would dramatically change peer review. The entire commu-
nity becomes privy to submitted manuscripts. There’s little chance
that this plan would lead to plagiarism or theft of concepts because
there’s a traceable timeline. More important, the wider scientific
community can now see what articles are accepted or rejected, and
what the critical reviews were.

The net result will be the opening of science. Papers accepted for
publication that are highly flawed would probably have undergone
very cursory review, which will be duly noted in the science blo-
gosphere. Similarly, those that are rejected without sufficient
grounds will also be made known. Remember, it was the blogo-
sphere that first exposed the fraud in the stem cell paper of Woo
Suk Hwang discussed in the last chapter.

This is much different from an experiment in ‘‘Open Peer Review’’
that was conducted by Nature in 2006. Nature asked authors if they
would allow anyone to review a submitted manuscript. Reviewers

223

CLIMATE OF EXTREMES

had to identify themselves. On a parallel track, Nature also sent each
article out for traditional review, where the journal holds the names
of the reviewers in confidence. As might be expected, most of the
‘‘public’’ comments were inconsequential, and, in fact, there were
surprisingly few in toto.

In an accompanying editorial, ironically, Nature noted that the
furor caused by Science’s publication of the Hwang paper resulted
in the creation of an independent review committee to investigate
the incident, and that the committee recommended that ‘‘journals
apply additional scrutiny and risk assessment to papers that are
likely to have a significant public impact, such as those with direct
implications for policy, public health or climate change.’’

We think our suggestion would ensure precisely what the commit-

tee recommended.

Although publishing signed peer reviews will hardly end publica-
tion bias, it certainly might ensure that highly flawed papers that
are published because of light reviews are ‘‘outed’’ in public, and
further, it should strengthen the review process itself. However, this
proposal will not stop bias resulting from the file-drawer problem
or the sticky nature of paradigm-driven science.

In this book, we attempted to show that there is a consistent body
of literature—albiet smaller in size than its counterpart—that argues
for the existence of climatic change but against its more dramatic
and apocalyptic interpretations.

The overall picture seems quite clear. Humans are implicated in
the planetary warming that began around 1975. Greenhouse gases
are likely to be one cause, probably a considerable one, largely
because the warming is accentuated at high-latitude land areas in
the Northern Hemisphere, and because it is more prevalent in winter
than in summer. A stratospheric cooling trend is also consistent with
greenhouse warming as well as stratospheric ozone depletion.

Counterfactual is the observation of no net warming (and probably
a cooling) of Antarctica, and very conflicting data on Antarctic snow-
fall, which should increase as a result of warming of the surrounding
ocean. Another problem is that there is clearly ‘‘nonclimatic’’ warm-
ing in the temperature history, owing to local site and regional
and national factors. In general, destitute nations will not make
maintaining a high-quality weather or climate network a high prior-
ity. It is interesting that when the UN’s surface temperature histories

224

Balancing Act: A Modest Proposal

are adjusted for this, the frequency distribution of very far above-
normal months is modified, and that it much more resembles the
satellite data (which are insensitive to local economic or land-use
influence). The main change is there are fewer months that are very
far above normal.

As far as ‘‘iconic’’ climate change is concerned, the picture is very
ambiguous. This is particularly true for hurricanes. The most severe
storms in the Atlantic and Western Pacific have become about as
frequent as they were in the 1940s to the 1960s, long before the
second warming of the 20th century began. Actual observations of
hurricane strength and frequency in these two regions are probably
reliable back to when ‘‘hurricane hunter’’ aircraft first took to the
air. But other ‘‘proxy’’ records of storms, such as cave stalagmites
or datable sediments from periodic overwashes, go back hundreds
and even thousands of years, indicating nothing unusual in the
current regime.

Historical temperatures turn out to be much more problematic
than once thought. The three major records (surface thermometers,
weather balloons, and satellites) have undergone major revisions,
which create ‘‘more’’ warming out of the same initial data. This is
like flipping a coin and getting all heads or tails, as presumably it
is equally possible that each record would suffer from methodologi-
cal or technical flaws that would give an equal probability of either
raising or lowering the temperature trend when revised. At any rate,
the probability for the records to be initially unbiased but to change
in one direction for two revisions is 0.016, or less than 1 in 50. What’s
happened is certainly possible, but it is not very probable.

With regard to the ice and sea-level rise, again we find conflicting
data, even though there has been a tremendous amount of press
coverage about the demise of Greenland. The most recent decade
was certainly no warmer than several in the early 20th century, and
long-standing temperature records even hint that it may have been
just as warm in the late 18th century. The big to-do about the discov-
ery of ‘‘Warming Island’’ turns out to be a farce. It’s shown as an
island in a map accompanying a book by aerial photographer Ernst
Hofer, published in 1957, near the end of several decades of warm
temperatures. Greenland then cooled down and extended an ice
bridge to the ‘‘island,’’ which was uncovered again in 2005.

Arctic temperatures are going up—and beginning to exceed those
observed in the 1930s. Still, there’s some pretty strong evidence from

225

CLIMATE OF EXTREMES

the tundra of Siberia and Scandinavia that conditions were much
warmer for millennia after the end of the last ice age. If the summer
sea ice is receded now, it was probably gone then (despite that, the
polar bear and the Inuit survived).

Satellite-sensing of sea ice extent began in 1979, when the Arctic
was at the end of its coldest period since the early 1920s. Conse-
quently, ice there had to have expanded when that record began,
and much of the early decline was merely a return to normal. Since
then, late-summer ice has continued to decline because of increasing
temperature. But on the same planet, what is to be made of the fact
that summer sea-ice extent in the Southern Hemisphere reached
record levels in 2007–08, that most ‘‘warming’’ in Alaska is explained
as a one-year jump in 1976–77, or that the glaciers of Kilimanjaro
were receding when the planet was cooling in the mid-20th century?
The IPCC expresses ‘‘low confidence’’ in any estimate of future
temperate-latitude storms. That’s probably because, despite a jillion
stories in the Euro tabloids, there’s no evidence for any long-term
trends in storminess there. Here in the States, rainfall is increasing,
but the proportion from heavy rainstorms remains the same.

Politicians blame wildfires in Southern California on global warm-
ing, with absolutely no supporting evidence from the local climate
history. Recent, highly publicized drought in the Pacific Southwest
pales when compared with a whopper in the 12th century. Despite
a warming trend, satellites show us no global trend whatsoever in
fire frequency or extent.

It turns out that Europe’s killer heat wave of 2003 was a small
atmospheric bubble embedded in a summer known worldwide for
its relative moderation. Nonetheless, the more frequent heat waves
become, the fewer people die. It’s called adaptation, which is physio-
logical as well as political. Consequently, when a similar heat wave
hit three years later, there were fewer deaths than would have nor-
mally been generated by such temperatures. Speaking of a real catas-
trophe, it turns out that the North Atlantic’s circulation has been
quite stable, quashing scare stories of an imminent ice age in Europe.
But perhaps the biggest piece of science that has been kept out
of public view is the tremendous number of lives that have effectively
been saved by the technology powering and developed by our fossil
fuel–driven society. When life expectancy doubles, as it has in the
industrialized world in the last 110 years, that’s equivalent to saving

226

Balancing Act: A Modest Proposal

one of every two lives. No one will ever know the number of people
who would have otherwise died, but somewhere around a billion
of us is a reasonable estimate.

We hope that our readers have enjoyed learning about the real
science on global warming that has received so little attention. It
paints a compelling picture of a warming planet that steadfastly
ignores Cassandra, as people live longer, more prosperous lives.

227

Preface

time 

to 

In 2005 I followed a link from a British political blog to Steve McIntyre’s
Climate Audit site, then the newest addition to the blogosphere. While some
of the statistics were over my head, there was plenty to interest a lay reader
with  an  interest  in  sceptical  arguments  against  the  global  warming
hypothesis.  While  I  was  never  a  daily  reader  of  the  site,  I  found  myself
returning  regularly,  learning  more  and  more  each  time,  until  I  eventually
found I could follow most of the postings without difficulty.

From  time  to  time,  new  visitors  to  Climate  Audit  would  plead  for  an
introduction to the site and while there were some excellent primers, like
Ross  McKitrick’s  What  is  the  Hockey  Stick  Debate  About?,  there  was
nothing that explained the story in the level of detail that I felt was required
to  enable  the  newbie  to  get  fully  up  to  speed  on  the  intricacies  of  the
science,  and  from 
if  my  newly-found
understanding of the debate would enable me to take on the task myself.1

time  I  wondered 

It wasn’t until the story of Caspar Ammann’s purported replication of the
Hockey Stick came to light during 2008 that I finally decided to take the
plunge. The antics involved in keeping Ammann’s paper alive, despite the
catastrophic  failure  of  its  verification  statistics,  was  so  extraordinary,  it
seemed almost to be a public duty to make the story more widely known.
Over  the  course  of  the  next  two  or  three  days,  I  summarised  a  series  of
Climate Audit postings into a long article on my blog. Caspar and the Jesus
Paper, as I chose to call the story, briefly turned my sleepy and relatively
obscure website – my daily visitor count was probably a couple of hundred
a day at the time – into a hive of activity, with thirty thousand hits being
received  over  the  following  three  days  alone.  To  move  from  ten  hits  per
hour to ten per second was something of a shock.

Many  commentators  have  described  Caspar  and  the  Jesus  Paper  as  a
history of the Hockey Stick, but in truth it covers only a small part of the
tale, reproduced here in Chapters 8 and 12. There was so much more to tell.
I was spurred into telling the full story by the sight of the Hockey Stick in
the  manuscript  of  a  new  science  textbook  that  crossed  my  desk  one
afternoon. Two years after it had been discredited the Hockey Stick was still
being used as the basis of a programme of environmental propaganda for
schools. What made it worse was that the author was using the stick in its

‘unofficial’ guise, the twentieth century instrumental record grafted onto the
end, the separate datasets not revealed to the reader. By the autumn of 2008
I was immersed in telling the tale from beginning to end.

With  only  one  or  two  minor  exceptions,  there  are  no  new  revelations
here. Every part of the debate between McIntyre and Mann has been fully
documented  on  their  competing  blogs  and  elsewhere,  and  to  some  extent
my task as a chronicler has been merely to sort their postings into coherent
order and to distil the essence of the statistical arguments into something
comprehensible by a lay reader. The reader can decide for themselves if I
have been successful in doing this.

I am grateful to several people who provided help and assistance along
the  way.  Steve  McIntyre,  Ross  McKitrick  and  Roger  Pielke  Jnr  read  the
manuscript and provided perceptive reviews. Steve and Ross also provided
some source materials that I was unable to locate elsewhere. David Holland
sent  me  some  unpublished  details  of  his  search  for  the  IPCC  review
comments  and  Eduardo  Zorita  allowed  me  to  identify  him  as  the  second
reviewer of the Nature submission. Dr Angela Montford harrassed me over
my grammar and spelling and asked many searching questions. Dr Lesley
Montford  also  read  the  text  and  made  sure  I  stopped  work  from  time  to
time.

Preface to the revised edition

When I set out to write the Hockey Stick Illusion in 2008, I thought I was
writing a book that would probably be read by only a handful of people – a
few newcomers to Climate Audit and some of the more obsessive climate
science geeks I would joke with my wife about how we would sell a dozen
copies at most. Two significant factors changed all that. First was the risk
taken by Tom Stacey in agreeing to take the book on for the ‘Independent
Minds’ series, cutting out the ability of the upholders of the global warming
hypothesis  to  ignore  the  book  entirely  –  to  Tom  I  am  eternally  grateful.
Second,  of  course,  was  the  advent  of  Climategate,  just  weeks  before  the
book’s publication. Strokes of luck like that do not come along very often
and  2010  was,  in  many  ways,  an  annus  mirabilis  for  me,  plucked  from
obscurity  to  become  something  of  a  spokesman  for  those  who  question
aspects of global warming science.

Special mention must be also made of Matt Ridley, who championed the
book in the media, and Christopher Booker, who kept finding a reason to
mention  it  in  his  Sunday  Telegraph  column.  Their  efforts  apart,  the
mainstream  media  have  maintained  a  determined  silence  about  the  book,
but in the end this may have mattered little because of impact of the blogs
and word of mouth. There have been many champions of the Hockey Stick
Illusion  –  readers  who  lent  it  to  family  members  and  others  who  bought
bundles  of  copies  to  distribute  to  friends  and  colleagues.  To  all  these
supporters I send my heartfelt thanks.
AWM
Milnathort
2011

Notes on Usages

There  is  considerable  discussion  in  the  text  of  the  rather  frightening
sounding  ‘Pearson’s  squared  correlation  coefficient’  (its  meaning  and
importance, which are relatively straightforward, are explained in Chapter
2). In different fields of study this measure is signified by either R2 or r2, the
former being more common in the social sciences, the latter in the physical
sciences. Throughout the text I have preferred the usage R2, since this is the
style adopted by Steve McIntyre.

There is also much discussion in the text of a statistical technique called
principal components analysis. The technique is described, hopefully in a
non-threatening manner, in Chapter 2,  but  it  is  worth  explaining  here  the
particular terminology I have chosen to use. The technique of PC analysis,
as  I  will  refer  to  it,  comes  in  one  widely  used  ‘vanilla  flavour’  plus  a
number of rarely used ones. Much of the story revolves around the use of a
novel variant which we will refer to as short-centred PC analysis, although
as we will see that its classification as a form of PC analysis is not generally
accepted.  Elsewhere,  short  centring  has  often  been  referred  to  as
‘decentred’ PC analysis, but I use the former style as I think it gives a better
idea of the what has happened to the underlying data.

Much of the debate over the Hockey Stick has taken place online, on the
blogs  of  the  participants.  It  is  therefore  inevitable  that  much  of  the
argumentation does not involve the checking of spelling and grammar that
was normal in the past in print-based disputes. Rather than excuse myself of
every  error  by  appending  a  ‘sic’,  I  have  preferred  to  correct  each  one,
except  in  one  or  two  cases  where  a  mistake  impinged  directly  upon  the
story.

Quoting as I do, directly from blog postings, I have had to make many
simplifications,  both  for  the  benefit  of  a  non-technical  readership  and  for
reasons of space. All such changes are marked by brackets and/or ellipses.

Throughout the text, I will use ‘bristlecones’ to refer to both bristlecone
pines and foxtails, two closely related species that are critical in the story of
the Hockey Stick. The two species are found on adjacent mountain ranges
in the USA and, in fact, are so closely related that they interbreed.

1     The Hockey Stick

And  thus  Bureaucracy,  the  giant  power  wielded  by  pygmies,  came  into  the  world.
(Honoré de Balzac)

The Hockey Stick was a long time in the making. The idea that manmade
emissions of carbon dioxide might cause the Earth to heat up can be traced
back to the French scientist, Joseph Fourier, who worked at the start of the
nineteenth century.a Fourier is probably better known for his mathematical
studies, but in a seminal paper of 1824, he also described how atmospheric
gases  might  be  capable  of  warming  the  atmosphere.  In  the  1850s  John
Tyndall,  the  Irish  head  of  the  Royal  Institution,  built  on  Fourier’s  work,
performing a number of experiments that demonstrated the effect in action.
The  term  ‘greenhouse  effect’  was  not  itself  used  until  the  end  of  the
nineteenth century. The expression was coined by the Swedish physicist and
Nobel  Prize  winner,  Svante  Arrhenius.  Arrhenius  was  the  first  to  attempt
quantitative work on the warming produced by the atmosphere, and was the
first to raise the question of whether manmade emissions of carbon dioxide
could actually alter the temperature of the Earth. However, Arrhenius, far
from being concerned about this possibility, thought that if man’s activities
caused  a  rise  in  temperature,  the  effects  on  humankind  would  be  entirely
beneficial. Warmer temperatures, he explained, would lead to higher crop
yields and so to fewer hungry mouths, an issue which was of great public
concern  at  that  time  as  the  population  of  the  planet  continued  to  grow.
Arrhenius also put forward a theory that carbon dioxide might be behind the
cycle  of  ice  ages  and  warmings  that  scientists  had  perceived  in  the
geological  record,  and  even  went  so  far  as  to  suggest  that  increasing  the
levels of carbon dioxide in the atmosphere could actually prevent the Earth
from slipping into another ice age, again demonstrating a rather different set
of concerns to those of many people today.

While Arrhenius’s theory attracted the attention of his fellow scientists
and a certain amount of controversy at the time, it soon disappeared from
the mainstream of scientific life. The theory made a brief reappearance in
the  early  twentieth  century  when  a  British  engineer  and  amateur
meteorologist called Guy Callendar wrote a number of papers expanding on

In  the  1950s,  the  global  warming  hypothesis  received  a  boost  when
accurate measurements of atmospheric carbon dioxide levels started to be
recorded  by  the  observatory  on  Mauna  Loa  in  Hawaii.  Until  then  it  had
been widely assumed that any carbon dioxide emitted into the atmosphere
would simply be absorbed by the oceans, but the Mauna Loa results showed
a clear and steady upward trend, and scientists started to dust off the work
of  Callendar  and  Arrhenius  to  work  out  what  this  might  mean  for  the
climate.

Work continued quietly but steadily in the background. Then in 1977 the
pace  started  to  quicken.  The  impulse  was  provided  by  the  creation  of  a
separate  climate  bureaucracy  under 
the  World
Meteorological Organisation (WMO). The WMO had organised the first World
Climate Conference, which was held in Geneva two years later, and it is to
that first meeting that the beginnings of the global warming movement can
be traced.

the  auspices  of 

Arrhenius’s work, but the subject remained a scientific backwater until after
the Second World War.

The  conference  was  instructed  to  review  the  state  of  knowledge  of
climatic  change  and  variability,  due  both  to  natural  and  anthropogenic
causes, and also to assess what this meant for humankind. In the way that
bureaucracies sometimes do, however, the scientists actually did something
slightly  but  tellingly  different  to  what  they  had  been  asked  to  do.  Rather
than  simply  assess  the  state  of  scientific  knowledge  and  consider  what
might  happen  in  the  future,  they  set  out  the  steps  they  thought  policy
makers should take in a ‘Call to Nations’ that was issued at the end of the
conference. This statement called for full advantage to be taken of man’s
knowledge of climate, for steps to be taken to improve that knowledge, and
for  potential  manmade  changes  to  climate  to  be  foreseen  and  prevented.
This then was not merely a call for more research, but also a demand for a
particular  policy  outcome  –  prevention  rather  than  adaptation.  One  can
almost detect the germ of a idea forming in the minds of the scientists and
bureaucrats assembled in Geneva: here, potentially, was a source of funding
and influence without end. Where might it lead?

A couple of years later there was, to coin a phrase, something of a shift
in the climate. James Hansen, a physicist from NASA’s Goddard Institute for
Space  Studies,  and  a  man  who  has  been  central  to  the  whole  global
warming movement, published a breathtaking paper in Science in which he

Another climate conference, this time held in Villach, Austria in 1985,
upped  the  ante  even  further.  This  meeting  has  been  described  as  the  first
time  that  a  scientific  ‘consensus’  emerged  on  the  issue  of  manmade  or
‘anthropogenic’ global warming and the conclusions of the conference were
certainly  more  outspoken  than  its  predecessor.  Predictably,  the  delegates
called for more scientific research, but again went rather further than would
have been expected from a scientific conference. They also demanded that
policymakers fund research into the economic, social and political impacts
of climate change and consider what steps could be taken to mitigate any
future  changes.  Climatology  was  moving  quickly  from  being  an  obscure
backwater of scientific research to being an area of study which could shape
policy in almost every conceivable area and affect the lives of millions of
people around the world. The man in the street might not know it yet, but
there were to be some big changes coming.

The  first  breakthrough  in  bringing  the  global  warming  hypothesis  to
public  notice  came  in  1988,  when  Hansen  went  to  the  US  Congress  to
explain how the release of carbon dioxide into the atmosphere was likely to
affect the climate in coming years. Fortuitously, or perhaps by design, the
hearing  was  held  in  midsummer  on  a  swelteringly  hot  day.  The  baking
temperatures  outside  may  well  have  affected  the  views  of  the  assembled
congressmen  anyway,  but  Hansen  was  certainly  not  pulling  his  punches
either. He told the Senate Committee on Energy and Natural Resources that
the Earth was hotter in 1988 than at any time in the history of instrumental
measurements, and that it was possible to point the finger of blame at the
greenhouse  effect.4  His  models,  Hansen  explained,  predicted  violent
extremes of weather including, coincidentally, summer heatwaves.

claimed  that  global  warming  was  going  to  start  happening  much  sooner
than had previously been expected and that temperature records would start
to be broken by the 1990s.3

This no-holds-barred warning seemed to have had the desired effect and
it was reported around the world. With headlines secured around the world,
1988 turned into a pivotal year for the global warming hypothesis. A few
months  later,  Margaret  Thatcher  gave  a  speech  to  the  Royal  Society  in
which she is quoted as having said that ‘we may have unwittingly begun a
massive  experiment  with  the  system  of  the  planet  itself’.5  Thatcher’s
conversion to the green cause is credited to her ambassador at the United
Nations, Sir Crispin Tickell, although Hansen may also have played a part –

Thatcher  is  said  to  have  read  his  congressional  testimony  and  he  is  also
believed  to  have  made  a  presentation  to  her  on  his  findings.  The  Royal
Society speech was not the only time that she spoke out on global warming
either. In the heady atmosphere following the finalisation of the Montreal
Protocol  to  ban  CFCs  and  save  the  ozone  layer,  the  environment  was  the
political buzzword du jour, and Thatcher was able to add global warming to
the list of green issues she outlined in an address to the  UN the following
year:

What we are now doing to the world, by degrading the land surfaces, by polluting the
waters and by adding greenhouse gases to the air at an unprecedented rate – all this is
new in the experience of the earth. It is mankind and his activities that are changing
the environment of our planet in damaging and dangerous ways.6

The floodgates were open. Politicians were leaping onto the bandwagon and
soon the political momentum of the issue would be all but unstoppable as
global warming found its way onto front pages and into election speeches
around the world. The final step was the formation of a permanent climate
bureaucracy and in the same year, 1988, the WMO and the UN together set up
the  Intergovernmental  Panel  on  Climate  Change  (IPCC),  a  scientific  body
that would report on the state of climate science, advising policy makers on
what was known about global warming and what should be done about it.
Everything  the  climatologists  had  demanded  just  three  years  before  at
Villach had been granted to them.
Climate science
In its First Assessment Report (FAR), the IPCC was rather circumspect in its
conclusions  about  what  was  happening  to  the  Earth’s  climate  and  the
reasons for any change that might be perceived.7 Despite what Hansen had
said  in  his  congressional  testimony  about  there  being  a  high  degree  of
confidence  in  the  causal  relationship  between  carbon  dioxide  and  recent
temperature rises, climatology was, and to a large extent remains, a science
in its infancy. In the executive summary, the report’s authors commented:

We conclude that despite great limitations in the quantity and quality of the available
historical  temperature  data,  the  evidence  points  consistently  to  a  real  but  irregular
warming over the last century. A global warming of larger size has almost certainly
occurred  at  least  once  since  the  end  of  the  last  glaciation  without  any  appreciable
increase in greenhouse gases. Because we do not understand the reasons for these past

warming events, it is not yet possible to attribute a specific proportion of the recent,
smaller warming to an increase of greenhouse gases.7

Their words, and particularly the closing sentence, show the problems that
the  global  warming  movement  faced.  If  they  were  going  to  persuade
policymakers  to  vote  them  still  more  funds  and  to  take  drastic  action  in
terms of changing the workings of the economy and the way people lived, it
was  going  to  be  necessary  to  persuade  the  public  as  well,  and  the  public
were  unlikely  to  be  convinced  by  science  that  was  sparse  and  limited  in
quality.

There  was  a  bigger  problem  too.  The  report  included  a  chart  showing
how  global  temperatures  had  varied  in  previous  ages,  according  to  the
scientific understanding of the time. This was something of a dampener for
the argument for catastrophic global warming because it suggested that past
temperatures had been warmer than today in a long period lasting from the
eleventh to the fifteenth centuries. This period had been followed by two or
three  hundred  years  of  much  cooler  temperatures,  lasting  until  the
eighteenth  century.  Since  then  warming  had  recommenced,  but  current
temperatures were still thought to be well short of those reached during the
medieval warming. This then was a huge problem for those promoting the
idea of global warming – how would they convince anyone that a rise of a
fraction of a degree in temperature portended something dangerous when
the climate had been much warmer in the past?

At  the  time,  the  FAR  graph  was  pretty  much  a  representation  of  what
might have been considered common knowledge. The so-called ‘Medieval
Warm Period’ was extremely well represented in medieval annals and other
documentary sources and it had come to have at least some impact on the
public  imagination.  Every  schoolboy  knew  that  the  Vikings  had  taken
advantage of warmer temperatures to colonise Iceland and Greenland at the
end of the first millennium, and historians had also discovered that grapes
had been grown commercially in England at the time. There was a wealth
of evidence that the medieval period had been an age of warmth, plenty and
a flourishing of culture. The ‘Little Ice Age’, meanwhile, was equally well
known  –  the  Viking  evacuation  of  Greenland  at  the  start  of  the  fifteenth
century, the freezing of the Golden Horn in the seventeenth, stories of ice-
fairs on the Thames and the winter paintings of Breughel the Elder had all
created a strong public perception of years of biting cold winters. So at the

start of the 1990s nobody was going to take issue with the story that the
IPCC was telling – of Medieval Warm Period giving way to Little Ice Age
before another gentle warming was ushered in.

FIGURE  1.1:  The  Medieval  Warm  Period  as  shown  in  the  IPCC  First
Assessment Report in 1990

The exact origins of the chart presented by the  IPCC  were,  at  the  time,
obscure;  rather  strangely,  the  report  did  not  contain  a  citation  or  other
indication  of  its  authorship.  Although  it  appeared  to  be  a  schematic  or
cartoon rather than a proper graph, it must have had some basis in scientific
research, but quite what this basis was was not discovered until many years
later  when  it  was  shown  to  be  derived  from  the  work  of  a  British
climatologist called Hubert Lamb.8 Lamb, while an important scientist, was
born in 1913 and the chart turns out to have been based on work he did in
the 1960s. The relative antiquity of this climate history might explain the
reluctance  of  the  IPCC  to  explain  its  provenance.  What  was  still  more
surprising  was  that  Lamb’s  work  turned  out  to  be  largely  based  on  the
Central  England  Temperature  Record,  a  long  series  of  instrumental
readings, which dated back to the mid-seventeenth century. In other words,
the understanding of world climate history propagated to the public by the
IPCC  was  based,  not  on  any  understanding  of  global  climate,  but  on  the
records for just one part of England: an odd situation to say the least.b
The Medieval Warm Period becomes less warm
In 1994 a pair of tree-ring researchers called Malcolm Hughes and Henry
Diaz  co-authored  a  journal  review  which  struck  a  major  blow  at  Lamb’s

view of climate history.9 The two men surveyed the evidence supporting the
existence of the Medieval Warm Period, considering all the different types
of  data  that  had  been  used  to  reconstruct  past  temperatures  since  Lamb’s
time.  Their  conclusions  were  that  temperatures  had  been  higher  in  some
parts of the world – they singled out Scandinavia, China, the Sierra Nevada
in  California,  the  Canadian  Rockies  and  Tasmania.  However,  they
emphasised  that  these  warm  periods  seemed  to  have  happened  at  slightly
different  times  in  different  places.  This  suggested  that  the  warmings  had
probably had different causes. They also claimed that there was no evidence
for any abnormal medieval warming at all in the southeast United States,
southern Europe along the Mediterranean, and parts of South America. If
they  were  right,  then  it  would  only  be  possible  to  conclude  that  the
Medieval Warm Period was, at most, a series of regional warmings.

On  its  own,  these  findings  might  look  interesting  but  otherwise
unremarkable. But put in the context of the temperature history of the last
thousand years their impact on the climate debate was potentially explosive.
Anecdotally at least, the Medieval Warm Period, represented by the bump
upwards in temperatures at the left hand side of the IPCC 1990 graph, was
being slowly flattened out. And as it flattened, the current warming started
to look more and more significant – if current temperatures were in excess
of  anything  seen  in  previous  times,  it  would  be  powerful  evidence  that
manmade global warming had already had a serious and deleterious effect
on the world’s climate. The flatter the representation of the medieval period
in the temperature reconstructions, the scarier were the conclusions.

This was one paper in a single volume of review articles. It would take
more than that to overturn an well-embedded paradigm. However, behind
the scenes climatologists were busy, and a short time after the Hughes and
Diaz  paper  was  released,  the  public  got  a  brief  glimpse  of  what  was
happening. It was not at all as it should have been.
The Deming affair
David Deming was a geoscientist from the University of Oklahoma, whose
expertise was in boreholes. From these holes, drilled deep into the Earth’s
surface, it was possible to extract a profile of the temperatures within the
rocks all the way down. This profile was a direct record of what the surface
temperature  had  been  in  the  past.  The  deeper  you  went,  the  older  the
temperature record you could get. Of course it wasn’t as simple as that –

there  were  all  sorts  of  confounding  factors  affecting  the  reliability  of  the
results but it was one of the approaches being tried as a way of discerning
the history of the Earth’s climate.

Deming  had  recently  created  a  temperature  reconstruction  for  the  last
150 years, based on boreholes in North America. In his study, he concluded
that  North  America  had  warmed  somewhat  in  the  period  since  1850,  but
had little to say beyond that. This was good, solid science but not the stuff
of  newspaper  headlines.  His  findings  were,  however,  considered  highly
important in climate science circles. With the expectation that temperatures
were being driven upwards by carbon dioxide emissions, the Deming study
seemed  like  good  evidence  to  support  the  hypothesis.  Because  of  this
interest, Deming was able to get his work published in one of the world’s
most  important  journals,  Science.10  And  with  a  storyline  of  rising
temperatures published in such a prestigious publication, he also attracted
the  notice  of  some  of  the  most  influential  people  in  the  global  warming
industry,  who  thought  they  saw  in  Deming  a  valuable  new  recruit  to  the
cause. Deming explained what happened in a later article:

With the publication of the article in  Science,  I  gained  significant  credibility  in  the
community of scientists working on climate change. They thought I was one of them,
someone who would pervert science in the service of social and political causes. So
one of them let his guard down. A major person working in the area of climate change
and global warming sent me an astonishing email that said ‘We have to get rid of the
Medieval Warm Period.’11

This sudden flash of light on a particularly murky shadow of climatological
practice is probably unique. Suddenly it was possible to see that the Hughes
and Diaz retake on the Medieval Warm Period was not considered enough.
The  aim  was  to  erase  it  from  the  climatological  record  in  its  entirety.
Although  Deming  himself  did  not  identify  the  email’s  author,  Richard
Lindzen of MIT has confirmed internet rumours that the email was written
by  Jonathan  Overpeck  of  the  University  of  Arizona.12  It  was  evident  to
anyone  who  was  watching  that,  in  some  quarters  at  least,  there  was  a
concerted effort to rewrite the Earth’s climate history so that the Medieval
Warm Period disappeared. Unfortunately, few people were watching. Those
who  noticed  what  Deming  was  saying,  and  tried  to  raise  the  alarm,  were
ignored by the media.

Deming had another interesting story to tell too. A couple of years after
the publication of his Science article he had been the reviewer for another
borehole study, this time written by Shaopeng Huang of the University of
Michigan.  Huang’s  results  had  shown  a  pronounced  worldwide  Medieval
Warm Period, something that was anathema to those in the global warming
mainstream. In fact the study suggested that medieval temperatures might
have been well in excess of those in modern times. Deming explained what
happened next:

The Huang et al. (1997) study was originally submitted to Nature. I was one of the
reviewers of the manuscript. I told the Nature editors that the article would surely be
one of the most important papers they published that year. But it never appeared in
print. Nature asked the authors to revise the paper twice and then, after a long delay,
ended up rejecting it.11

This difficulty in getting into print any result that went against the idea of
catastrophic  global  warming  was  to  be  a  consistent  complaint  among
sceptics,  and  readers  may  like  to  note  Nature’s  treatment  of  Huang  and
compare it to later events in this story.

A  few  months  after  Deming’s  revelations  about  the  fate  of  Huang’s
paper, the second IPCC report picked up on the changing attitudes towards
the Medieval Warm Period. The report’s authors noted that:

Based on the incomplete observations and paleoclimatic evidence available, it seems
unlikely that global mean temperatures have increased by 1°C or more in a century at
any time during the last 10,000 years.

and went on,

The limited available evidence from proxy climate indicators suggests that the 20th
century global mean temperature is at least as warm as any other century since at least
1400 AD. Data prior to 1400 are too sparse to allow the reliable estimation of global
mean temperature.13

This represented a significant change in emphasis by the IPCC. The story in
the  FAR,  of  a  pronounced  Medieval  Warm  Period  with  temperatures
exceeding modern ones, had been replaced by a new narrative, in which it
was said that modern warmth was probably unprecedented – or at least as
high as anything seen in the last six hundred years. And if anyone were to
question  how  all  the  historical  records  of  warm  temperatures  in  the

medieval period could be wrong, it was explained that these were a regional
phenomenon and that overall, the globe appeared to have been no warmer
back then than it was at present.

There  was  one  major  problem  with  the  case  for  the  Medieval  Warm
Period having been an insignificant regional phenomenon though. This was
the  paucity  of  hard  data  to  support  the  case  –  the  ‘limited  available
evidence’ referred to above. It was simple for critics to point out that any
conclusions  drawn  from  this  data  would  have  to  be  highly  speculative  at
best. Climate science wanted big funding and big political action and that
was  going  to  require  definitive  evidence.  In  order  to  strengthen  the
arguments for the current warming being unprecedented, there was going to
have  to  be  a  major  study,  presenting  unimpeachable  evidence  that  the
Medieval Warm Period was a chimera.

Enter the Hockey Stick.

The paper
The Hockey Stick paper made its grand entrance in an article published in
Nature on 23 April 1998.14 Its main author was a hitherto relatively obscure
scientist based at the University of Massachusetts (UMass) called Michael
E.  Mann,  and  it  went  by  the  distinctly  unmemorable  title,  ‘Global  scale
temperature  patterns  and  climate  forcing  over  the  past  six  centuries’.
Despite  this  unpromising  opening  and  a  style  of  writing  that  has  been
politely described as ‘rather obscure’,15 it was to become one of the most
cited scientific papers of that year or indeed of any other year. In fact, when
the  controversy  was  at  its  height,  one  investigator  discovered  that  it  had
been  cited  twice  as  often  as  was  normal  for  a  scientific  paper,  and  years
after  its  publication  it  was  still  being  referenced  at  a  startling  rate  in  the
scientific literature.

Mann was just starting out on his scientific career, receiving his PhD in
1998 at the age of 33. At the time of the paper’s publication he was still
only an adjunct member of faculty at UMass. Mann may have been a late
developer, but he was ambitious and self-confident and the reception for his
paper suggested he was destined for great things.

Apart from Mann, the Hockey Stick paper had two secondary authors:
the  first  was  Ray  Bradley,  a  colleague  of  Mann’s  from  the  University  of
Massachusetts, while the other was Malcolm Hughes of the University of
Arizona,  whom  we  have  already  met  as  one  of  the  authors  of  the  first

serious attempt to ‘get rid of the Medieval Warm Period’. In the years since
its publication the paper has become known by the initials of its authors’
names and we will be referring to it as MBH98 from here on.

MBH98 was novel on a number of levels. Firstly, it had been based on a
much greater volume of raw data than earlier studies. Mann, Bradley and
Hughes  had  trawled  the  archives  for  anything  from  which  they  might
extract  a  temperature  signal  and  had  come  up  with  a  network  of  112
‘indicators’, as they termed them. These are more normally referred to as
‘proxies’  (see  Chapter  2).  Although  the  majority  of  the  indicators  didn’t
extend back to the critical medieval period, the MBH98 dataset represented a
significant  advance  and  struck  a  blow  at  those  critics  who  had  rejected
earlier studies as lacking sufficient data to be reliable. In fact, some of the
indicators were actually summaries of larger networks of proxies, so there
was even more data backing up their reconstruction than was suggested by
the reported number of 112 series. This summarising had been done using a
statistical procedure called principal components analysis (PC analysis) and
this  first  application  of  the  technique  to  temperature  reconstructions  gave
the study an air of great technical sophistication, which would again render
it  much  harder  to  criticise.  With  a  large  dataset  and  state  of  the  art
methodology in place, the authors wanted their readers to be in no doubt as
to  how  good  their  results  were,  speaking  of  its  ‘highly  significant
reconstructive skill’. This suggested a study that was going to be hard to
refute.

What then of the findings? The abstract of the paper explained that Mann
and his team had been able to reconstruct temperatures since the year 1400
and that recent temperatures were warmer than any other year since the start
of  their  records.  In  the  remainder  of  the  paper,  they  went  on  to  assess
possible reasons for the dramatic change in temperatures by testing how the
graph of their reconstruction correlated against possible causes (‘forcings’
in  the  jargon),  such  as  atmospheric  dust,  solar  irradiance  and  carbon
dioxide. It will be no surprise to anyone that their conclusion was that the
only potential culprit was carbon dioxide. The implications were once again
clear: mankind was warming the globe. Here then was the beginning of the
end of the process of getting rid of the Medieval Warm Period. All that was
lacking was a degree of publicity, something that was to be dealt with in
fairly short order, as we will see.

The  key  graphic  in  the  paper  was  a  chart  of  the  reconstruction  of
Northern  Hemisphere  temperatures  for  the  full  length  of  the  record  from
1400 right through to 1980. The picture presented was crystal clear. From
the very beginning of the series the temperature line meandered gently, first
a little warmer, then a little cooler, never varying more than half a degree or
so from peak to trough. This was the 500-year long handle of the Hockey
Stick,  a  sort  of  steady  state  that  had  apparently  reigned,  unchanging,
throughout most of recorded history. Then suddenly, the blade of the stick
appeared at the start of the twentieth century, shooting upwards in an almost
straight line.c It was a startling change and it was this that made the Hockey
Stick  such  an  effective  promotional  tool,  although  to  watching  scientists,
the remarkable thing about the Hockey Stick was not what was happening
in the twentieth century portion – that temperatures were rising was clear
from the instrumental record – but the long flat handle. The Medieval Warm
Period  had  completely  vanished.  Even  the  previously  acknowledged
‘regional effect’ now left no trace in the record. The conclusions were stark:
current temperatures were unprecedented.
The splice
As presented in Nature, the Hockey Stick chart was a dreadful example of
scientific  graphics,  with  the  authors  managing  to  cram  no  less  than  four
lines onto the same chart, making it hard for even the attentive reader to see
exactly  what  it  was  they  were  looking  at  (see  Figure  1.2).  One  should
charitably  point  to  the  space  restrictions  necessary  for  publication  in
Nature,  and  the  difficulties  of  presenting  information  in  black  and  white
diagrams.

Presented  on  top  of  each  other  were  four  sets  of  numbers:  the
reconstructed  temperature,  a  smoothed  version  of  these  figures,  the  error
bars, and at the right hand side (and easy to miss for the inattentive), the
thermometer  record  for  the  twentieth  century.  The  inclusion  of  the
instrumental  record  was  instantly  controversial,  with  global  warming
sceptics  accusing  Mann  of  having  spliced  two  entirely  different  datasets.
The effect of this scientifically dubious presentation was, they said, to make
the  twentieth  century  portion  look  more  frightening  than  the  underlying
data would warrant. Mann’s counter-argument to these accusations was that
the data was not spliced, but overlaid, and that its inclusion was justified in

order to extend the reconstruction from 1980, which was when most of the
underlying data ended, right up to the present day.

FIGURE 1.2: The Hockey Stick in MBH98

While it is difficult to see exactly what had been done from the black and
white graph in Nature, later versions of the Hockey Stick were much clearer
about the splice/overlay, using colour to distinguish the different datasets,
although  some  well-known  users  of  Mann’s  work  did  forget  to  make  the
distinction,  as  we  will  see  later  in  the  story.  The  IPCC’s  version  of  the
Hockey  Stick  is  shown  on  the  back  cover,  with  the  instrumental  overlay
shown in red at the right hand side. Clearly, a large proportion of the blade
of the stick was not from the same dataset as the handle, although there is
undoubtedly a rise in the reconstructed temperatures too. Opinions on the
issue  remained  divided;  the  first  Hockey  Stick  controversy  was  off  and
running.
Reactions
The paper was clearly expected to be of huge public interest, and a press
release was issued by UMass, timed to coincide with its publication:

Climatologists  at  the  University  of  Massachusetts  have  reconstructed  the  global
temperature over the past 600 years, determining that three recent years, 1997, 1995,
and 1990, were the warmest years since at least AD 1400.

The  researchers  were  able  to  estimate  temperatures  over  more  than  half  the
surface  of  the  globe,  pinpointing  average  yearly  temperatures  in  the  northern

hemisphere to within a fraction of a degree, going back to AD 1400. The study places
in  a  new  context  long-standing  controversy  over  the  relative  roles  of  human  and
natural changes in the climate of past centuries, according to Mann.

Advanced statistical techniques were used to translate the proxy information into
surface  temperature  patterns,  so  that  past  centuries  could  be  compared  with  the
twentieth century.16

With the press release so unequivocal, it is hardly surprising that the media
found  the  story  irresistible.  Just  five  days  after  its  publication  in  Nature,
Mann was given the honour of an article in the New York Times, announcing
the  results  of  his  study  to  the  world.17  This  was  a  truly  remarkable
achievement  for  Mann,  who,  as  we  have  seen,  had  only  just  finished  his
PhD,  although  it  should  be  noted  in  fairness  that  he  had  been  active  in
paleoclimate for some years previously. The story was penned by the New
York Times’ science reporter, William K. Stevens and its headline echoed
the press release’s certainty about the findings:

NEW EVIDENCE FINDS THIS IS WARMEST CENTURY IN 600 YEARS

Interestingly, beneath the headline, much of the article was actually taken
up with discussing doubts about the reliability of the study. One scientist
quoted in the New York Times article wondered if it would ever be possible
to  get  a  temperature  reconstruction  that  was  reliable  enough  to  tell  if  the
current warming was unprecedented or not. Even Mann himself was quoted
as  saying  that  there  was  quite  a  bit  of  work  to  be  done  in  reducing  the
uncertainties.  However,  the  headline  and  another  scientist  quoted  in  the
study left no doubt that this was expected to be a very significant piece of
work.

USA Today was much less equivocal though:

90S WERE WARMEST YEARS IN CENTURIES

The 20th century has been warmer than the five centuries that preceded it, and 1997,
1995  and  1990  were  the  warmest  years  since  1400,  says  the  latest  study  to  relate
global climate change to the burning of fossil fuels.18

Elsewhere  it  was  the  same:  NBC  website  told  its  readers,  ‘Millennium
ending  with  record  heat’,19  while  Time  magazine  went  for  the  jauntier
headline, ‘It hasn’t been this sizzling in centuries’.20

MBH99
Buoyed by the success of their first paper, Mann, Bradley and Hughes set
about extending the study back to the start of the millennium, publishing
their new results in Geophysical Research Letters.21 ‘Northern Hemisphere
temperatures  during  the  last  millennium:  inferences,  uncertainties  and
limitations’ (which we will refer to as MBH99) was, as the title suggests, a
much more cagily worded article. There was so little data available for the
first four centuries of the reconstruction that any conclusions could only be
extremely tentative. That said, the conclusions were broadly similar – that
the  twentieth  century  appeared  to  be  anomalous  compared  to  any  other
period  in  the  last  1,000  years.  The  global  warming  bandwagon  was  on  a
roll.

There  was  no  let  up  on  the  public  relations  front  either.  Once  again,
UMass made sure that the paper received maximum publicity, with a press
release that concentrated on the scary bits. Under the headline ‘1998 was
warmest year of millennium, UMass Amherst climate researchers report’,
they quoted Bradley as saying, ‘Temperatures in the latter half of the 20th
century were unprecedented’.22 Those who read further might have noticed
Mann discussing the uncertainties and the sparseness of the data, but this
was clearly not the key message, and most media outlets chose not to dwell
on  the  uncertainties  when  they  reported  the  results  to  their  readers.  The
newspaper headlines were all written in terms which left no room for any
doubt.
IPCC: The Third Assessment Report
The two Hockey Stick papers were good for Mann. Within months of the
first  paper’s  publication,  he  found  himself  advancing  rapidly  through  the
academic ranks with a speed that was simply breathtaking. In 2000, John
Daly,  a  prominent  global  warming  sceptic  explained  just  how  dramatic
Mann’s rise to fame had been, and how influential he had now become in
the climatology community.23

At  the  time  he  published  his  ‘Hockey  Stick’  paper,  Michael  Mann  held  an  adjunct
faculty position at the University of Massachusetts, in the Department of Geosciences.
He received his PhD in 1998, and a year later was promoted to Assistant Professor at
the University of Virginia, in the Department of Environmental Sciences, at the age of
34.

He is now the Lead Author of the ‘Observed Climate Variability and Change’
chapter of the IPCC Third Assessment Report (TAR2000), and a contributing author on
several other chapters of that report. The Technical Summary of the report, echoing
Mann’s  paper,  said:  ‘The  1990s  are  likely  to  have  been  the  warmest  decade  of  the
millennium, and 1998 is likely to have been the warmest year.’

Mann  is  also  now  on  the  editorial  board  of  the  Journal  of  Climate  and  was  a
guest  editor  for  a  special  issue  of  Climatic  Change.  He  is  also  a  referee  for  the
journals Nature, Science, Climatic Change, Geophysical Research Letters, Journal of
Climate,  JGR-Oceans,  JGR-Atmospheres,  Paleooceanography,  Eos,  International
Journal  of  Climatology,  and  NSF,  NOAA,  and  DOE  grant  programs.  (In  the  ‘peer
review’ system of science, the role of anonymous referee confers the power to reject
papers that are deemed, in the opinion of the referee, not to meet scientific standards).
He  was  appointed  as  a  ‘Scientific  Adviser’  to  the  U.S.  Government  (White

House OSTP) on climate change issues.

Mann lists his ‘popular media exposure’ as including – ‘CBS, NBC, ABC, CNN,
CNN headline news, BBC, NPR, PBS (NOVA/Frontline), WCBS, Time,  Newsweek,
Life,  US  News  &  World  Report,  Economist,  Scientific  American,  Science  News,
Science,  Rolling  Stone,  Popular  Science,  USA  Today,  New  York  Times,  New  York
Times (Science Times), Washington Post, Boston Globe, London Times, Irish Times,
AP, UPI, Reuters, and numerous other television/print media.’23

As time went on, prizes and titles flowed his way too, with papers he had
written lauded on all sides. In 2002 Scientific American selected him as one
of  the  ‘50  leading  visionaries  in  science’;  all  the  work  that  went  in  to
preparing the Hockey Stick certainly seemed to have been worthwhile.

As Daly had noted, one of Mann’s most most significant accolades after
the triumph of the Hockey Stick was his appointment as the lead author of
the  paleoclimate  chapter  in  the  IPCC’s  Third  Assessment  Report  of  2001.
Again,  we  can  only  stand  back  in  admiration  that  someone  who  had
published his PhD a matter of a year or so earlier could be invited to head
the  team  writing  one  of  the  most  critical  chapters  in  one  of  the  most
important scientific reports written for decades. Mann had certainly made
an impact in the climate world.

Mann’s  position  as  lead  author  did  present  an  apparent  problem,
however,  since  in  that  position  he  had  a  clear  conflict  of  interest  in
assessing the published literature – he was going to be considering his own
work. It is unfortunate then that the Hockey Stick was given extraordinary
prominence  in  the  Third  Assessment  Report,  particularly  in  Mann’s  own
chapter on paleoclimate. In fact the whole IPCC report started to look like a
locker room, it was so full of hockey sticks. As one observer noted:

[The  Hockey  Stick]  appears  as  Figure  1b  in  the  Working  Group  1  Summary  for
Policymakers, Figure 5 in the Technical Summary, twice in Chapter 2 (Figures 2-20
and  2-21)  of  the  main  report,  and  Figures  2-3  and  9-1B  in  the  Synthesis  Report.
Referring to this figure, the IPCC Summary for Policymakers (p. 3) claimed it is likely
‘that  the  1990s  has  been  the  warmest  decade  and  1998  the  warmest  year  of  the
millennium’ for the Northern Hemisphere.1

The  IPCC  report  also  ‘bigged  up’  the  paper’s  claims  to  statistical
sophistication,  stating  that  the  reconstruction  had  ‘significant  skill  in
independent cross-validation tests’. Whenever the Hockey Stick appeared,
it was larger, bolder and more colourful than any other temperature series
presented. Mann must have been thrilled with the report. The final icing on
the cake was when the IPCC chairman, Sir John Houghton, announcing the
publication of the report, sat in front of an enormous blow-up of the Hockey
Stick itself. This was Mann’s moment of triumph: 1998 was officially the
warmest year of the millennium, a stunning recognition of his work.

In  the  years  that  followed  more  and  more  interest  was  focused  on  the
Hockey Stick. In particular, it was one of the key arguments used to support
the need for the Kyoto treaty. Citations of Mann’s work flooded in and its
influence and importance grew without restraint, until it came to symbolise
the very idea of manmade global warming. As one BBC reporter put it, ‘it
is hard to overestimate how influential this study has been’.24 Every home
in Canada was sent a leaflet quoting the paper’s conclusions and warning of
the dangers of climate change. School books told children that the Hockey
Stick meant that the world had to change. Politicians told voters that only
they  could  save  people  from  the  threat  it  demonstrated.  Insurers,
newspapers and magazines, pamphlets and websites were all in thrall to its
message; the Hockey Stick swept all before it.

a  The early history of the science of global warming was ably documented by Spencer Weart, on

whose work much of this section of the story is based.2

b  We should note in passing that the caption to the original FAR graph was unequivocal that it was a

representation of global temperatures.

c  Readers outside North America may wonder why a straight upward line on the end of a long flat
handle should make the graph look anything like a hockey stick: it is, of course, and with delicious
irony, an ice hockey stick.

2     Science

Torture numbers, and they’ll confess to anything.

Gregg Easterbrook

We  saw  in  the  last  chapter  how  Mann  and  his  team  created  the  Hockey
Stick and the impact it had on the world. Before we go on to tell the story of
how his work was undone, we need to learn some paleoclimatology and a
little  statistics  (really,  just  a  little!),  so  that  you  can  follow  what  the
arguments were about.
Paleoclimatology
So how do you actually go about measuring the temperature of the past?
For recent centuries, it’s relatively straightforward. Thermometer records go
back  at  least  a  hundred  and  fifty  years,  and  in  some  places,  even  further
than that. In principle, all you need to do is to take all your thermometer
readings and work out an average. Of course, many parts of the world are
not  covered  by  a  thermometer  record,  and  many  of  the  records  may  be
unreliable,  and  ways  have  to  be  found  to  deal  with  these  issues.  Another
problem is that as you go back into the nineteenth century, the thermometer
coverage of the globe becomes thinner and thinner. However, compared to
the situation in earlier centuries, the twentieth century and the second half
of the nineteenth can be said to be fairly well understood.

Before  about  1850  though,  there  are  very  few  instrumental  records  to
speak  of,  and  scientists  have  to  find  some  other  way  of  assessing  the
temperature.  We  saw  briefly  in  the  last  chapter  how  attempts  have  been
made  to  directly  measure  past  temperatures  from  boreholes.  This  is  a
procedure which is fraught with difficulty and there are many confounding
factors, although the approach is not necessarily worse than any of the other
ways  we  are  going  to  examine.  Mostly  though,  historic  temperatures  are
estimated  indirectly  using  proxies.  A  proxy  is  simply  some  quantity  that
varies with temperature and which leaves some trace after the event that can
be sampled and measured. There are lots of different kinds of proxies and
we  will  meet  many  of  these  in  the  course  of  this  story,  but  the  most
common ones, and the ones which are of most relevance to the rest of the
story, are tree rings.

The basis of the theory of tree rings as a proxy for temperature is that if
you pick the correct tree, it can be seen to grow more in a warm year than in
a  cold  one.  The  annual  growth  rings  will  be  wider  and  the  wood  will  be
denser. So by taking a core through the tree from the outside towards the
middle,  it  should  be  possible  to  extract  what  is  effectively  a  record  of
temperatures throughout the lifetime of the tree.

Not  all  trees  respond  to  temperature  in  this  way  though.  A  tree  at  the
edge of a desert will not grow more when it gets hotter because it can’t get
enough  water  –  scientists  say  that  it  is  precipitation  limited.  Other  trees
might be limited by a lack of nutrients or by competition with other species.
But,  the  theory  goes,  there  are  some  trees  which  are  indeed  limited  by
temperature. These are trees that are located on the upper tree lines on the
sides  of  mountains  –  where  the  forest  gives  way  to  rock  and  grass  –  or
perhaps those that are at the northern limit of their geographical range.a To a
paleoclimatologist,  these  special  trees  are  a  kind  of  thermometer.  By
examining  the  width  (and  also  the  wood  density)  of  the  rings  of  these
particular trees, it is thought that you can get an estimate of how warm it
was at any point in time in the past – as long as the tree was alive then.

temperature 

It goes without saying that you wouldn’t want to base your temperature
estimate on a single tree, which might be affected by the conditions in its
immediate  vicinity,  or  by  insect  infestation  or  some  other  unidentifiable
problem. Any of these factors could affect the record in such a way as to
completely 
this,
dendroclimatologists,  as  the  scientists  who  collect  tree  ring  samples  are
called, put together the ring samples from a number of trees at a particular
site into what they call a chronology, which shows the average picture of
tree ring behaviour there. The idea is that all the small variations – issues
with insects and so on – average out once you sample enough trees, leaving
just  the  temperature  signal  behind.  And  from  this  you  can,  in  theory,
measure the temperature of the past.

reconstruction.  Because  of 

ruin 

the 

In  fact  it  is  rarely  this  simple.  There  are  very  large  numbers  of
confounding factors, not least the fact that even if you were to keep a tree at
a  constant  temperature  it  wouldn’t  actually  grow  the  same  amount  each
year.  Researchers  have  discovered  that  trees  tend  to  grow  quickly  at  first
and then gradually less and less each year, a fact that, if uncorrected, would
produce a matching slow decline in the temperatures reconstructed from the
rings.  To  deal  with  this,  the  chronologies  have  to  be  standardised.  This

involves  working  out  an  expected  growth  curve  for  the  trees  in  question,
and then expressing the ring width for any given year as a percentage of the
expected  ring  width,  essentially  leaving  only  the  temperature-related
information – or so the researchers hope.

Even then there are other factors that may destroy the effectiveness of
the  studies.  For  example,  it  has  been  noted  by  one  researcher  that  trees
within a single site can show completely opposite growth patterns – some
grow more in times of higher temperatures, but others grow less.25 If this
tendency  is  widely  replicated  then  the  whole  approach  of  calculating
historic temperatures from tree rings is thrown into doubt.

While most of the proxies used in paleoclimate reconstructions are tree
ring series, there are other types as well. Ice cores are much used, as well as
speleothems  (more  normally  known  as  stalagmites  and  stalactites  and
similar  rock  formations).  Ocean  sediments  and  corals  are  also  used  in  a
similar way. The extraction of a temperature record from these other proxy
types  mostly  involves  analysis  of  relative  proportions  of  the  isotopes  of
certain atoms in the proxy. The theory is that, for one reason or another, the
isotope ratio will be different in a hot year to a cold one. Find the ratio in a
sample of your stalagmite and you can work out the temperature. So long as
you can also date the sample correctly then you can create a valid proxy
series.

the 

increasingly 

trend  has  been 

For  a  long  time,  temperature  reconstructions  were  created  using  single
proxies,  but 
towards  multi-proxy
reconstructions,  using  a  mixture  of  different  proxy  types,  and  MBH98  fell
into this latter category. As we will see, while using a multiproxy approach
means  that  much  more  data  is  available,  this  advantage  is  offset  by  the
complex statistics required, but that tale will have to wait until later in the
story.
Methodology
Once  you  have  collected  together  your  proxies,  how  exactly  do  you  go
about reconstructing the temperatures of the past? There are different ways
of doing this, but they all fall under one overall framework, which is what
we need to describe next.

In  very  simple  terms,  we  can  derive  temperatures  of  the  past  by
calculating  the  mathematical  relationship  between  tree  ring  width  and
temperature in the recent past. Once this relationship has been determined,

it  is  quite  simple  to  reverse  it,  enabling  researchers  to  work  out
temperatures in the distant past from the widths of the rings of ancient trees.
That’s the simple explanation. Let’s take a look in a bit more detail.

Paleoclimatologists have proxy records stretching back into the past for
several hundred years. Let’s say that we have a tree ring chronology which
goes back to 1400. We can divide this 600-year period into three parts: the
calibration period, the verification period and the reconstruction period (see
Figure 2.1). In our example, the calibration period stretches from 1900 to
2000 and the verification period from 1850 to 1900. In these two periods
we already know the temperatures because we have instrumental records.
The  objective  of  the  exercise  is  to  estimate  temperatures  in  the
reconstruction period, which runs from 1400 to 1850.

FIGURE 2.1: The periods in a temperature reconstruction
The first step of the process is called calibration. All this means is that the
mathematical  relationship  between  the  proxy  values  and  the  temperature
values is calculated, using only the data from the calibration period – i.e.
the twentieth century proxy values and the twentieth century instrumental
records. By way of a simple hypothetical example, we might work out that
ring width in millimetres times ten is equal to average annual temperature
in  degrees  centigrade.  In  other  words  a  temperature  rise  of  1°C  gives  an
extra 0.1 mm of ring width. Of course, in reality, it’s not that simple, but
this is all that is meant by calibration. The relationship is established by a
statistical technique called regression analysis, which simply comes up with
the  linear  relationship  between  ring  width  and  temperature  that  gives  the
least errors, which is to say the one that best matches the actual data in the
calibration period.

One problem that occurs in calibration is that any relationship we might
have  been  able  to  calculate  could  have  arisen  purely  by  chance.  In  other
words, just because tree ring width multiplied by ten happened to be equal
to temperature in the twentieth century, that doesn’t mean that it was always
that way. For example, twentieth century ring widths might have been lower
than normal, say because of insect infestation in that particular set of trees,
or maybe even in trees in general. Another possibility is that the tree isn’t
actually responding to temperature at all, but to something else. If any of
these issues were really affecting tree growth, it might be that the normal
relationship  is  actually  an  extra  0.15  mm  of  growth  from  a  rise  in
temperature of 1°C.

Assessing  whether  this  kind  of  problem  exists  is  what  the  verification
period is for. We have to take the tree ring widths in the verification period
(the  second  half  of  the  nineteenth  century  in  our  example)  and  use  the
mathematical  relationship  we  have  just  worked  out  to  calculate  what
temperatures were at that time. In our example, the calibration exercise told
us that 10°C gives us 1 mm of ring width. Let’s now say that in 1850, the
average ring width was 1.25 mm. According to our maths, this means that
the  average  temperature  should  have  been  12.5°C.  We  can  now  go  and
check  the  value  we  have  calculated  against  the  thermometer  records  and
find out if we were right or not. We can also repeat the exercise for all the
years  in  the  verification  period,  and  then  by  comparing  the  calculated
temperatures to the actual ones we can work out how good our algorithm is.
If the insect infestation we talked about above had actually occurred in the
twentieth  century  (and  remember,  we  probably  wouldn’t  know  that  this
period  was  abnormal)  then  the  results  could  be  wildly  different.  If  the
results of the verification exercise were no good, then we would have no
choice but to start again.

However,  assuming  that  the  verification  procedures  show  that  the
algorithm  is  still  effective  in  the  verification  period,  we  can  repeat  the
process  on  the  reconstruction  period,  taking  the  proxy  values  for  these
earlier centuries and working backwards using the algorithm to get to the
equivalent temperatures.
How many proxies?
In the simple example above, we have considered a single tree ring series
and have shown how this can be used to reconstruct the temperature. But
this  isn’t  the  only  way  to  go  about  a  reconstruction.  Multiproxy

reconstructions, in which a variety of different proxies are used, have been
increasingly popular in paleoclimate. Once you have more than one proxy,
the mathematics involved becomes considerably more complicated, but for
the purposes of this story it is not necessary to go into this. The essence of
what is done is that the calibration in a multiproxy reconstruction produces
a weighting for each proxy series. Put the series and the weightings together
and you have a multiproxy reconstruction.

If you have a large number of proxies, there are two main ways in which
you can go about the calibration.26 The first of these has been described as
‘the Schweingruber method’, or composite-plus-scale (CPS), and involves
taking  proxies  that  are  expected  to  be  temperature  sensitive,  calibrating
them against local temperatures and essentially taking an average. The other
way, the ‘Fritts method’, or climate field reconstruction, involves taking lots
of  proxy  series,  which  are  sometimes  not  even  responding  to  their  local
temperatures,  and  seeing  if  some  sort  of  correlation  can  be  found  with
temperature measurements somewhere in the wider vicinity. What emerges
from this latter method is essentially a weighted average of the full proxy
set,  with  the  temperature  sensitive  proxies  having  a  much  higher  weight
than the non-temperature-sensitive ones. It’s this Fritts method that is the
relevant one for our story. The Fritts method involves a certain leap of faith
to trust that trees that are not responding to their own local temperature can
nevertheless  detect  a  signal  in  a  wider  temperature  index.  You  have  to
believe  in  the  existence  of  something  called  ‘teleconnections’,  whereby
temperatures in a possibly distant part of the world affect the climate in the
locale of the tree in such a way as to affect its growth, and in a consistent
manner. If this sounds implausible to you, then you are not alone. However,
the  reality  of  the  mechanism  is  accepted  by  the  paleoclimate  community
and for the purposes of our story that’s what you need to know.
Principal components analysis
History of principal components analysis
We mentioned in the last chapter that Michael Mann used a technique called
principal  components  analysis  in  MBH98  to  summarise  some  of  the  proxy
records, and it’s this that we need to look at next.

Principal  components  analysis  is  not  a  new  technique,  and  it  may
surprise  many  readers  to  read  that  much  of  the  debate  over  the  Hockey
Stick was about a process that, far from being cutting edge statistics, was

actually invented over a hundred years ago. The technique was developed
by  the  English  statistician  Karl  Pearson  in  the  first  few  years  of  the
twentieth century, and it has been in regular and uncontroversial use ever
since; in recent years it has found new applications in facial recognition and
image compression.

So,  averages  can  hide  interesting  patterns.  This  is  where  PC  analysis
comes  in:  PC  analysis  will  extract  from  the  database  the  most  important
pattern in the data, which is called the first principal component or PC1. A

Pearson was a giant of the early development of mathematical statistics,
doing groundbreaking work on regression and correlation analysis. In fact,
he is often credited with turning statistics into a true science and he was the
founder of the world’s first university statistics department at the University
of London. It is perhaps only his aggressive support for eugenics that has
lead  to  his  relative  obscurity  in  modern  times,  at  least  outside  his  own
specialism; he was for a time the holder of the Galton chair of Eugenics at
the University of London.
What principal components analysis does
Principal components analysis (which we will refer to as PC analysis for the
rest of the book) sounds complicated, and if you’re a layman and you are
presented with a page of matrix algebra showing how it works, it certainly
looks  scary  as  well.  The  good  news,  for  this  story  at  least,  is  that  the
essence of what it does is actually rather simple.

PC  analysis  simply  extracts  key  patterns  from  the  data  in  a  database.
Imagine you have lots and lots of data series, let’s say a database of tree
ring chronologies. Let’s also say there are a hundred chronologies, each one
being 400 years long. The first thing to notice is that there is a lot of data.
How do you get your head round what is going on in there? One possible
approach would be simply to average all the data. This is fine, and useful
and  will  give  you  an  overall  picture,  but  it’s  quite  possible  that  there  are
interesting things going on in the data that will be obscured by averaging.
Let’s  say  that  half  of  the  trees  have  a  sharp  uptick  in  ring  widths  in  the
twentieth  century  while  most  of  the  others  show  a  gentle  decline.  In  the
average, you might well see pretty much nothing at all, maybe a gentle rise
in  the  twentieth  century,  but  nothing  that  will  grab  your  attention.  You
would almost certainly miss the important information about the different
behaviour  in  different  trees.  As  we  saw  above,  this  is  a  real  issue  in
paleoclimate.

principal component is somewhat analogous to a stock exchange index. The
FTSE or Dow Jones indices are simply weighted averages of the underlying
share  values,  and  stand  in  for  those  underlying  datasets.  In  just  the  same
way, a principal component is simply a weighted average of the underlying
proxy  data  series,  the  weights  calculated  in  such  a  way  as  to  explain  as
much of the underlying variability as possible. When the PC1 is calculated,
as well as getting the pattern, you also get a set of weightings that explain
the relationship between each data series and the pattern. In the example we
have just looked at, the  PC1 might show the twentieth century uptick, and
the  weighting  for  each  underlying  proxy  series  showing  such  an  uptick
would  be  positive.  The  weighting  for  series  showing  a  twentieth  century
downtick  would  be  negative,  indicating  that  they  were,  broadly  speaking,
mirror images of the PC1.

Once you have got the PC1 out, you can go on to extract the second most
important pattern, the PC2, and this would be accompanied by another set of
weighting  coefficients  explaining  how  each  proxy  series  related  to  that
pattern. In fact, you can go on extracting more and more patterns (and more
and more weightings) from the data, right up to 100 PCs; the PC3 might show
a relatively small rise in ring widths in the seventeenth century for example,
the PC4 would be something different again. However it is important to note
that each extra PC that you extract from the database explains less and less
of the total variance in the underlying data. So while the PC1 might explain
60% of the total variance, by the time you get to PC4, you might be talking
about only 6 or 7%. In other words, the PC4 is not telling you anything of
much significance at all, in this example at least. How many PCs you want
to extract and retain depends on how much variance each one explains and
the exact nature of the data you are analysing.

The  PCs  are  often  described  as  being  like  the  shadow  cast  by  a  three-
dimensional object. Imagine you are holding an object, say a comb, up to
the sunlight, and it is casting a shadow on the table in front of you. There
are  lots  of  ways  you  could  hold  the  comb,  each  of  which  would  cast  a
different shadow onto the table, but the one which tells you the most about
the object is when you expose the face of the comb to the light. When you
do this, the sun passes between the teeth and you can see all the individual
points. You can tell from the shadow that what is being held up is a comb.
This shadow is analogous to the first  PC. Now rotate the comb through a
right angle,b so that you are pointing the long edge of the comb to the sun.

If you do this, the shadow cast is just a long thin line. You can see from the
shadow that you are holding a long thin object, but it could be just about
anything.  This  would  be  the  second  PC.  It  tells  us  something  about  the
object, but not as much as the first PC. You can rotate through a right angle
again  and  let  the  sunlight  fall  on  the  short  edge  of  the  comb.  Here  the
shadow is almost meaningless. You can tell that something is being held up,
but it’s impossible to draw any meaningful conclusions from it. This then, is
the third PC.

FIGURE 2.2: Raw series
Centring the data
I said earlier that the way  PC analysis works is difficult for the layman to
follow, and this is true. Nevertheless, if you are going to follow the story
you will need to know just a couple of things about the actual mechanics of
performing PC analysis. Again, these are not terribly complicated, so bear
with me. But make sure you understand these next few paragraphs because
they are critical to the later story. Here goes.

We have our database of tree ring series. Each column in the database
represents a single chronology, the average of all the trees sampled in that
particular  site.  Let’s  say  we  have  a  hundred  chronologies  and  therefore  a
hundred  columns.  Each  row  represents  the  years;  let’s  say  we  have  400
years and therefore 400 rows. Each cell contains the average ring width for
that chronology in that particular year.

Because  of  the  way  the  underlying  mathematics  of  the  PC  calculation
works, before you can start crunching the numbers, every chronology in the

database of tree ring chronologies has to be adjusted to a mean of zero – a
process called ‘centring’. This is mathematically quite simple – high school
level  maths  in  fact.  Take  your  first  chronology.  First,  you  calculate  the
average tree ring width for that chronology. Then one by one, you go down
each of the 400 ring measurements for the chronology, and you take away
the  series  average  you  have  just  calculated.  And  that’s  it.  See?  That  was
easy wasn’t it? Now repeat the process for the other 99 series and you are
ready to start the PC calculation.

Let’s look at it what happens to the tree ring data on some graphs so we
can  see  the  effect  of  the  centring.  We  will  look  at  two  dummy  tree  ring
series  (they’re  nothing  like  real  tree  rings  at  all,  but  the  effect  of  the
centring is clearer if you do it this way). Figure 2.2 shows the uncentred
data. There are two series, with Series A having a slightly higher mean than
Series B – it’s higher up the chart.

The effect of taking away the relevant series average from each value is
to slide both lines down the page until they hover above the x-axis. In other
words, the two resulting ‘centred’ series have an average of zero. Let’s see
the effect on our two tree ring series (Figure 2.3).

What you should now see is that the effect of centring the data is simply
to  move  all  the  series  down  the  chart  until  they  are  varying  around  an
average of zero.

FIGURE 2.3: Centred series

And that’s it: that’s all you need to know. Throughout the months and
years of bitter argument over Mann’s Hockey Stick, this simple step was the
only part of the PC analysis that was in dispute. For the purposes of the story

it  is  not  really  necessary  to  understand  anything  else  about  how  the
subsequent calculations work. You can think of PC analysis as a big black
box which takes the centred data and churns out as many patterns as are felt
necessary. It is, however, useful to understand just a little of the detail of
what happens to the centred data, as it will help explain just why centring is
so important.

As it processes each proxy series, the PC algorithm calculates the square
of each value in the series and sums the resulting figures down the column.
The resulting number, the ‘sum of squares’ has a very specific meaning: it
is a multiple of the series variance. This is a very important result because,
of course, we are interested in series with large variances: they are the ones
which will form the basis of the dominant patterns in the dataset. In other
words, by centring the data, the PC algorithm points us automatically to the
most important series. It is important to realise, however, that this result is
only  achieved  if  the  data  is  first  centred.  Because  of  this,  centring  is
considered an integral part of PC analysis.

a  The northern limit applies to trees in the Northern Hemisphere, the majority of trees studied in this
way. Temperature-limited trees in the Southern Hemisphere are found at the southern geographical
limit.

b  PCs are all at right angles to each other; they are ‘orthogonal’, in the jargon. In a database of 100
chronologies, you can have 100 PCs, all at right angles to each other, in a 100-dimensional space.
I’ve explained this in a footnote for fear of some of my readers’ heads exploding at the idea of a
space with 100 dimensions.

3     Face-off

Man is in error throughout his strife.

Johann Wolfgang von Goethe

Climate Skeptics
At the start of 2003 a short comment was posted to an Internet forum for
global  warming  sceptics  by  a  Dutchman  called  Hans  Erren.  Erren  had
developed an interest in Mann’s 1999 update to the Hockey Stick and had
decided  to  dig  into  the  study  in  a  little  more  detail.  It  quickly  became
apparent to him that it was not a simple task. Mann’s paper was opaque and
difficult to understand. Also, some of what Erren read seemed a little odd;
he could make little sense of the way the proxies were calibrated against
temperature,  for  example.  So,  as  people  in  the  sceptic  community  would
often do in these circumstances, he threw the question open to the forum to
see if anyone could help or shed any light on the problem.

The readers at Climate Skeptics were a diverse bunch. People from all
over  the  world  and  from  very  different  backgrounds  used  the  forum  to
exchange news and views on the development of the global warming scare,
from both scientific and political viewpoints. Teachers and engineers were
common on the site, and there were other interested amateurs from a huge
variety  of  other  specialisms.  There  were  some  eccentrics,  of  course,  but
there were also climatologists and meteorologists, and even the editors of a
couple  of  scientific  journals.  There  was  a  steady  stream  of  postings
covering  subjects  as  diverse  as  urban  heat  islands,  radiative  physics  and
historic sea levels in Tasmania, all discussed in the mixture of erudition and
outrage that you can find on any Internet site.

There had been a great deal of excitement on the forum in recent months.
A  new  study  by  two  Harvard  astrophysicists,  Willie  Soon  and  Sallie
Baliunas,  had  just  been  published  and  its  appearance  had  caused  a  huge
furore in the world of paleoclimate.27  Soon  and  Baliunas  had  reviewed  a
large dataset of paleoclimate proxies to see how many showed the Medieval
Warm  Period,  the  Little  Ice  Age  and  the  modern  warming.  They  had
concluded  that  the  Medieval  Warm  Period  was  in  fact  a  real,  significant
feature  of  climate  history.  The  paper  had  been  extremely  controversial,

contradicting  the  mainstream  consensus  that  the  Medieval  Warm  Period
was probably only a regional phenomenon. Climatologists from around the
world  had  fallen  over  themselves  to  attack  the  Soon  and  Baliunas  paper,
mainly  on  the  grounds  that  many  of  the  proxies  used  in  the  study  were
precipitation  proxies  rather  than  temperature  proxies.  So  great  was  the
uproar, in fact, that several scientists resigned from the editorial board of
Climate  Research,  the  journal  which  had  published  the  paper  in  the  first
place. In the face of all this opposition, the paper had gained little traction
in terms of changing mainstream scientific opinion on the existence of the
Medieval Warm Period. It had been a huge disappointment for the sceptic
community.

Another  topic  which  had  been  visited  and  revisited  by  the  forum  over
previous  months,  was,  of  course,  Mann’s  Hockey  Stick  and  what  the
climate sceptics saw as its use as a ‘sales tool’ in the IPCC report – pushing
the idea of dangerous global warming in the face of what otherwise would
have amounted to a distinct lack of hard evidence. There was no doubt in
the  minds  of  anyone  posting  there  that  it  was  the  Hockey  Stick  that  was
driving the relentless momentum of the global warming movement. It was
this centrality in the debate that had attracted the interest of Erren.
McIntyre
It’s perhaps a little surprising then that Erren’s posting received just a single
response. Perhaps the subject had been debated too hard already; maybe it
was just a quiet day, but the only person who had anything to add was Steve
McIntyre,  a  semi-retired  mining  consultant  from  Toronto.  His  response
though was rather encouraging:

Hans  –  Just  to  note  that  I  think  the  dissection  of  MBH1999  (also  1998)  is  very
important. I’ve spent a fair bit of time on this and intend to reply to your posts, but
plan to spend some attention on radiative physics for a little while.

McIntyre  was  one  of  the  mainstays  of  the  Climate  Skeptics  site,  posting
comments on a wide array of subjects. In recent weeks he’d spent a great
deal of time discussing radiative physics, trying to understand how the IPCC
came up with an expected temperature rise of 2.5°C every time atmospheric
carbon dioxide doubled. He’d not really got anywhere with it so far (and in
fact it remains a mystery to this day), but he was far from giving up hope. If
there was an explanation to be had, he fully expected to find it.

McIntyre had always been a talented mathematician. At high school in
his native Ontario, he’d been a prize winner, going on to come top in the
maths exams at provincial and national level. It was pretty much a given
that he’d continue his studies at university – a goal which he achieved when
he graduated in pure mathematics from the University of Toronto in 1969.
Postponing  an  opportunity  to  study  mathematical  economics  at  MIT,  he
decided  to  broaden  his  horizons  and  head  for  England,  where  he  spent  a
year studying PPE (Philosophy, Politics and Economics) at Oxford.

With  family  commitments  preventing  him  from  taking  up  the  MIT
position when his Oxford studies were complete, he had embarked upon a
career  in  the  mining  industry,  where  he  was  to  spend  the  next  thirty-odd
years.  He’d  mainly  been 
involved  with  small  mineral  exploration
companies,  his  work 
listing  prospectuses,
analysing prospects for acquisitions and the like. This work involved a great
deal  of  heavy  duty  data  analysis,  and  gave  McIntyre  a  facility  with
statistical  tools  which  was  to  stand  him  in  good  stead  during  the  later
controversies over climatology.

involving  preparation  of 

By 2002, McIntyre was in comfortable semi-retirement in Toronto, the
easy  routine  of  children  and  grandchildren  and  visits  to  the  squash  court
only interrupted by occasional scraps of consultancy work for old contacts
in  the  mining  industry.  Climatology  was  not  even  on  his  radar.  He  knew
nothing of the IPCC or the Hockey Stick or Michael Mann. But 2002 was to
be the year in which all that began to change. We have seen that in the drive
to promote the Kyoto treaty, every home in Canada was sent a leaflet about
the risks of global warming. When McIntyre read in his copy that it was the
warmest year in the last millennium, instead of shrugging his shoulders as
most  of  us  would  do,  he  found  himself  wondering  just  how  it  was  that
scientists  knew  this.  Canadians  were  brought  up  on  stories  of  the  Viking
explorations  during  the  Medieval  Warm  Period  and  the  opening  up  of
Canada by fur traders during the Little Ice Age. The claims in the leaflet
seemed to be overturning a very well-embedded paradigm.

Within  a  matter  of  days,  McIntyre  was  absorbed  in  the  world  of
climatology, his reading quickly taking him to the IPCC’s Third Assessment
Report with its prominent display of the Hockey Stick. To someone with
long experience of mining promotions it immediately struck McIntyre that
someone  had  spent  a  great  deal  of  time  and  money  making  an  effective
sales  tool  out  of  the  Mann’s  graph.  This  observation  seemed  even  more

pertinent  when  McIntyre  saw  for  the  first  time  the  confusing  way  the
Hockey Stick had been presented in the original Nature paper. Improving
the presentation of the graph for inclusion in the IPCC report didn’t make it
wrong, of course, but it was clearly intended to be persuasive.

While  McIntyre’s  readings  in  climatology  broadened,  he  also  began
discussing  the  IPCC’s  claims  of  unprecedented  warmth  with  friends  and
acquaintances.  His  contacts  in  the  mining  industry  were  particularly
interesting on the subject. Familiar as they were with the long-term history
of the Earth, many of the geologists McIntyre spoke to had strong opinions
on  claims  that  recent  temperatures  were  unprecedented  and  most  were
highly  sceptical  of  the  idea.  When  it  came  to  the  Hockey  Stick  itself,
mining  people  –  geologists,  lawyers  and  accountants  –  were  openly
contemptuous.  Hockey  sticks  were  a  well  known  phenomenon  in  the
business  world,  and  McIntyre’s  contacts  had  seen  far  too  many  dubious
mining  promotions  and  dotcom  revenue  projections  to  take  such  a  thing
seriously. The contrasting reactions to the Hockey Stick of politicians and
business people – on the one hand doom-laded predictions of catastrophe
and  on  the  other  open  ridicule  –  acted  as  a  spur  to  McIntyre,  who  flung
himself headlong into the world of climatology.
Proxies
By the time Erren posted up his initial questions in April 2003, McIntyre
was therefore already thinking far ahead of the Dutchman. Like Erren, he
had discovered that getting to the bottom of the Hockey Stick was no easy
task. McIntyre was used to the corporate world, where tight regulation and
nervous investors meant that clear explanations of methods and results were
an absolute necessity. In the mining industry, a garbled explanation of what
a drill core contained would send potential investors running for the door,
so  McIntyre  had  been  completely  bemused  by  the  obscure  language  and
vague allusions that littered Mann’s papers. It was very hard for an outsider
to get any purchase on the slippery slope of Mann’s narrative. Working out
what he had done was clearly going to take a great deal of effort.

Despite  the  difficulties,  McIntyre  had  made  a  reasonable  start  on
understanding  the  two  Mann  papers.  He  could  see  that  Mann  had  used  a
network of 112 proxy series, and in fact behind the scenes there was even
more  data  than  this.  For  some  parts  of  the  world,  particularly  North
America, the archive of tree ring data was very large and if it had been used

directly  in  the  calculations,  these  parts  of  the  world  would  have  been
grossly over-represented. So in order to avoid this problem, Mann had used
PC analysis to distil these series down into a few key patterns. Therefore,
while some of the 112 series that went into the final calculation were single
site chronologies, others were  PC series, representing a summary of many
sites.  This  was  unobjectionable  and  must  have  seemed  an  eminently
reasonable step.

MBH98 was a multiproxy study, using a variety of different proxy records
to  reconstruct  temperature.  As  we  saw  in  Chapter  2,  there  are  two  main
approaches  to  temperature  reconstruction:  either  find  the  relationship
between the individual proxies and their local temperature and calculate an
average  (the  Schweingruber  method)  or  find  the  relationship  between  the
full set of proxies and some regional temperature index (the Fritts method).
Mann had taken the Fritts approach although he referred to it in the paper as
a ‘climate field reconstruction’.

The proxy series used were a real mixture. The majority were tree-rings,
but among the 112 there were also ice cores, corals and ice melt records,
together  with  a  few  oddities.  Several  of  his  series  were  rainfall  records,
included presumably because of a possible correlation between rainfall and
temperature. This was surprising in view of the furore over the Soon and
Baliunas  paper.  With  so  many  voices  of  outrage  having  been  raised  at
Soon’s  use  of  precipitation  records,  it  is  amazing  that  nobody  in  the
paleoclimate mainstream seemed to have spotted that Mann had done the
same thing a few years earlier. In fact, at the time of the Soon controversy,
Mann  and  the  group  of  colleagues  who  would  later  be  known  as  the
‘Hockey Team’a had themselves written a critique of Soon and Baliunas in
which they said,

In  drawing  inferences  regarding  past  regional  temperature  changes  from  proxy
records, it is essential to assess proxy data for actual sensitivity to past temperature
variability . . .

. . . and went on to note that it was ‘patently invalid’ to fail to do so.29

As well as using proxies which he had previously deemed invalid, Mann
had also used some ‘proxies’ that weren’t actually proxies at all, such as the
Central  England  Temperature  Record.  This,  as  we  saw  in  Chapter 1,  is  a
long  series  of  thermometer  readings  extending  back  to  the  seventeenth
century. Sceptics unkindly noted that the eccentric mix of proxies, some of

which had an unproven relationship with temperature, was hardly a reliable
methodology  –  merely  throwing  everything  bar  the  kitchen  sink  into  the
database was not necessarily a recipe for success. In fact, quite the opposite.
Temperature series
As we have seen, the first step in a temperature reconstruction is to calibrate
the  proxies  against  instrumental  temperatures.  The  most  reliable  surface
temperature record is agreed by many researchers to be the series prepared
by  Professor  Phil  Jones  of  the  Climatic  Research  Unit  (CRU)  at  the
University of East Anglia; the record is generally known by the acronym
HADCRUT.b  This  database  consists  of  figures  compiled  from  a  host  of
weather stations and other sources of temperature data such as ocean buoys
and  ships,  all  put  together  in  tabular  form.  Imagine  a  huge  spreadsheet,
rather like the one for the proxies. Each column represents a single weather
station  and  each  row  represents  a  period  of  time.  In  each  cell  is  the
temperature for that period for each station. It’s pretty simple stuff. There
are  plenty  of  problems  with  the  temperature  data,  but  all  the  raw  figures
were cleaned and adjusted for known issues. The data that Mann used was
the CRU’s best stab at what the actual temperatures had been for the previous
150-odd years, and as we’ve noted, CRU’s data was reckoned to be the best.
Whether  these  adjustments  were  correct,  or  valid,  is  another  question
entirely, and it was one which had already attracted the attention of climate
sceptics, but it is not one with which we need concern ourselves as it does
not bear directly on this story.

Taking all the temperature data, Mann had used PC analysis to summarise
the  records  into  its  key  patterns,  reducing  1000  or  more  individual
temperature  series  into  just  16  PCs.  In  other  words,  PC  analysis  was  used
twice in the study – once on the proxy records and once on the instrumental
temperature  records.  The  temperature  PCs  were  the  climate  patterns,  or
‘climate fields’ as Mann called them, that he was going to try to recreate in
the  reconstruction  by  calculating  the  mathematical  relationship  between
them and the tree ring data. Having recreated the temperature PCs he would
then be able to reverse the PC analysis using a procedure called expansion,
taking the PCs and their respective weightings to recreate an approximation
of  how  the  underlying  temperature  data  would  have  looked  in  the  past.
From  there  it  was  a  relatively  simple  step  to  recreate  the  northern
hemisphere temperature average for the last 600 years.

Regression
With his data ready, Mann’s next task was to calibrate the proxies against
the temperatures to establish the mathematical relationship between them.
As we saw in Chapter 2, this is done using regression analysis.

In  the  simple  case  of  performing  a  regression  on  a  single  set  of  data
points, this is a relatively straightforward exercise in fitting a line through a
cloud  of  points.  However,  MBH98  involved  a  multivariate  calibration  –  in
other words there were multiple sets of data needing to be calibrated: the
112  proxies  and  the  16  temperature  PCs.  But  quite  how  Mann  had  gone
about this much more complex process was a mystery that was not revealed
in the text of the paper. That would have to wait for another day.
McIntyre’s conclusions on the first analysis
So, after spending a good few hours perusing Mann’s papers McIntyre had
a  sense  of  how  the  studies  had  been  performed,  at  least  at  a  perfunctory
level. But already questions were emerging that suggested that there might
be more to MBH98 than met the eye. Something was not quite right with the
Hockey Stick papers. As he said to the sceptics:

I am not able to comment at present on his methodology, but my sense is that there are
weaknesses to it, which deserve careful auditing.

On this score at least, McIntyre was not mistaken.
First look at divergence and Briffa
Meanwhile,  there  was  another  issue  that  was  nagging  at  the  back  of
McIntyre’s  mind.  This  concerned  some  comments  made  by  another
prominent paleoclimatologist called Keith Briffa about tree ring records in
the twentieth century. McIntyre couldn’t recall the reference but distinctly
remembered that Briffa had said that twentieth century ring widths hadn’t
gone  up  alongside  the  warming  that  had  been  seen  in  the  instrumental
record.  But  if  this  was  the  case,  then  how  had  Mann  got  a  sharp  rise  in
twentieth century temperatures from a study which was dominated by tree
rings? And if tree rings and temperatures didn’t move in tandem, wasn’t the
very basis of paleoclimatology thrown into doubt?

The more he looked at the paper, the more McIntyre found questions that
were  not  answered  in  the  text  and  issues  that  needed  to  be  checked  or
clarified.  But  even  at  this  point,  the  Hockey  Stick  remained  a  secondary

consideration  for  the  Canadian.  Over  the  next  couple  of  months  he
continued  to  post  comments  at  Climate  Skeptics  on  a  range  of  different
topics – climate models, radiative physics and so on – and very little about
the Hockey Stick. He even commented that he felt that the best contribution
that  the  sceptic  community  could  make  would  be  to  concentrate  on
publicising the lack  of  any  adequate  disclosure  from  the  IPCC.  But  in  the
background he was still working away at Mann’s paper. He had spent quite
a  lot  of  time  looking  at  some  of  the  available  proxy  records,  particularly
those not based on tree rings. So far as he could see, none of these showed
the uptick that would have been expected from rising temperatures in the
twentieth century. On the tree ring front, moreover, he had located all the
Briffa  papers  that  discussed  the  divergence  of  tree  ring  growth  and
temperatures, and their conclusions were just as he had recalled them:

Averaged  around  the  Northern  Hemisphere,  early  tree  growth  .  .  .  can  be  seen  to
follow . . . trends in recorded summer temperatures, tracking the rise to the relatively
high levels of the 1930s and 1940s and the subsequent fall in the 1950s. However,
although  temperatures  rose  again  after  the  mid-1960s  and  reached  unprecedentedly
high recorded levels by the late 1980s, hemispheric tree growth fell consistently after
1940, and in the late 1970s and 1980s reached levels as low as those attained in the
cool 1880s . . . The reason for this increasingly apparent and widespread phenomenon
is not known but any one, or a combination, of several factors might be involved.30

in 

the 

This  was  a  rather  remarkable  finding,  given  the  prominence  accorded  to
tree  ring  reconstructions 
IPCC’s  report.  It  seemed  simply
inconceivable  that  a  ‘widespread  phenomenon’  –  and  one  that  could
potentially  undermine  much  of  the  science  of  paleoclimatology  –  should
barely warrant a mentionc in the main study in which the findings of climate
science  were  reported  to  the  public.  As  McIntyre  observed,  Mann  should
presumably have observed this divergence effect too, but if he had, it was
not  mentioned  anywhere  in  his  papers.  His  failure  to  observe  the
divergence,  or  worse,  if  he  had  failed  to  report  it  at  all,  must  seriously
undermine the credibility of the Hockey Stick papers.
McIntyre starts to concentrate on paleoclimate
By  the  start  of  April,  McIntyre  was  starting  to  rein  back  on  his  other
climatological  interests  in  order  to  concentrate  his  efforts  on  the
paleoclimate papers, and in particular the Hockey Stick. He announced this
intention to the Climate Skeptics forum. There was, he said, ‘an opportunity

for some quite provocative analyses’. At the time though, the reaction from
his fellow sceptics was only one of mild interest. There was no great sense
of expectation or excitement. There was no inkling of the controversy that
was to be unleashed in the coming months and years.
First detailed analysis of Mann’s work
Within  a  matter  of  days  of  his  announcement,  McIntyre  was  posting
findings  to  the  Climate  Skeptics  forum.  He  had  now  worked  through
Mann’s  explanation  of  his  methodology  and  he  had  soldiered  his  way
through the matrix algebra. It was still very strange. The use of PC analysis
was new in the realm of paleoclimate and Mann had made no attempt to
prove  the  validity  of  the  technique  in  the  field,  instead  relying  on  a  bold
assertion that it was better than the alternatives. In view of this and given
the  surprising  results  –  with  no  Medieval  Warm  Period  or  Little  Ice  Age
visible in the reconstruction – one might have expected that experts in the
field would have questioned whether Mann’s novel procedures might have
been a factor in his anomalous results. But despite a thorough search of the
literature, there was no sign that anyone else had seen fit to probe the issue
further. Nor had any other researchers adopted Mann’s methodology in the
five years since his paper had been published. Given how often the Hockey
Stick had been cited in the scientific literature, these were very surprising
observations,  which  seemed  to  suggest  that  paleoclimatologists  liked
Mann’s results rather more than they liked his methodology.
Lies, damned lies and calibration statistics
Another  issue  was  also  attracting  McIntyre’s  attention.  During  his
calibration  exercise,  Mann  had  assessed  how  well  the  temperature  data
matched up against the proxies by calculating various statistical measures –
in other words, numbers that acted as a score of how good the match was.
The main way he did this was using a measure that he called the beta (ß),
which  he  described  as  being  ‘a  quite  rigorous  measure  of  the  similarity
between two variables’.14

This  was  a  somewhat  surprising  choice  since  the  beta  statistic  was
virtually unheard of outside climatology circles. (It also goes by the names
of the ‘resolved variance statistic’ or the ‘reduction of error (RE) statistic’ –
the latter being the term we will use to refer to it henceforward.) With his
experience in statistics, McIntyre was aware that there was great danger in

using novel measures like these, whose mathematical behaviour hadn’t been
thoroughly  researched  and  documented  by  statisticians.  The  statistical
literature  was  littered  with  examples  where  particular  statistical  measures
gave results which misled in certain circumstances. Mann had left no clue
as to why he had preferred the RE rather than the more normal measures of
correlation, such as the correlation (r), the correlation squared (R2) or the CE
statistic.  The  behaviour  of  all  of  these  measures  under  a  wide  range  of
scenarios  was  well  documented,  so  McIntyre  was  surprised  not  to  see  an
explanation.

Mann indicated in the paper that the r and R2 had also been calculated,
which might have provided some reassurance to McIntyre but for the fact
that the results of these calculations were not presented for the calibration
step  anywhere  in  the  paper  or  in  the  online  supplementary  information.
However, by now McIntyre had got hold of the data for the second Hockey
Stick paper, MBH99 – the extension back to the year 1000 – so he was able to
start  to  make  some  significant  progress  in  answering  some  of  these
questions. Because the number of proxies used in MBH99 was so small (there
being very few proxies that extended so far into the past) it was a relatively
straightforward  task  for  McIntyre  to  recreate  Mann’s  calibration  and  to
calculate some of the correlation statistics for himself. The results were eye-
opening, to say the least. As he reported to the climate sceptics:

The R2 . . . ranges from –0.006 to 0.454; on this basis, only 2 of 13 proxies have R2
adjusted over 0.25, and 7 of 13 have values under 0.1 . . .

To put this in perspective, R2 will normally vary between 0 and 1. A score
of  0  indicates  that  there  is  no  correlation  at  all,  and  1  indicates  perfect
correlation.  So  what  McIntyre  was  seeing  was  that  the  proxies  and  the
temperature PCs didn’t really match up very well, according to a standard
measure of correlation. The best among them were not even halfway good,
and some simply showed no correlation at all. Could this explain why Mann
was so enthusiastic about the RE statistic, the climatologists’ own measure
of correlation?

Struck  by  this  result,  McIntyre  repeated  the  calculation  in  a  slightly
different  way.  Previously  he  had  measured  the  correlation  of  each  proxy
against  the  full  set  of  temperature  PCs  –  sixteen  in  all.  This  time  he
restricted  himself  to  only  the  temperature  PC1,  which  was  the  only

temperature PC that was used to recreate temperatures in the early part of
the  MBH99  reconstruction  –  it  was  the  only  one  that  really  mattered.  And
when McIntyre saw the results, they turned out to be even worse, with only
one proxy achieving an R2 score of more than 0.2. As McIntyre noted:

The  low  correlations  against  [the  temperature]  PC1  need  to  be  carefully  noted.
Some/most of the datasets are essentially uncorrelated to [it]. There is no mention in
MBH98 or MBH99 of these low correlations.

Many people might have sat back and stopped at this point but McIntyre
decided to take his analysis a step further. He started to swap the proxies for
some completely unrelated datasets in order to see what sort of correlation
scores  he  could  get.  For  example,  79  consecutive  values  from  a  table  of
Eurodollar six month interest rates achieved an R2 of 0.595, far in excess of
the  proxies.  Concentrations  of  potassium  from  an  ice  core  dug  out  of  a
glacier on Mount Everest scored 0.444. He also tried regressing nineteenth
century  proxy  data  against  twentieth  century  temperatures  and  found  no
great difference in the  R2 score to those achieved when the correct proxy
data  was  used.  The  conclusion  was  clear:  if  you  could  get  such  high
correlations from obviously unrelated data, what meaning could there be in
the  proxies,  whose  scores  were  so  much  lower?  Taken  literally,  the
implication was that it would be better to recreate historical temperatures
with Eurodollar interest rates than with the proxies, a conclusion which was
obviously nonsense.

As we saw in Chapter 2, after calibrating the proxies and the temperature
records, it is necessary to demonstrate that any correlation between the two
is  real  rather  than  spurious  by  checking  the  temperature  reconstruction
against historic temperatures in the verification period. McIntyre explained
the details to the sceptics:

Mann’s verification was to show that he could get similar correlations in a withheld
period 1854–1901. However, since all of the correlations are at rather low levels and
similar  correlation  levels  are  obtained  with  completely  spurious  series  or  against
unrelated time periods, it is not clear to me that this verification exercise shows that
statistical  significance  has  been  achieved,  or  alternatively  and  perhaps  more
importantly, that there has been any material narrowing of error bands.

In  essence  then,  a  whole  new  set  of  correlation  statistics  had  been
generated, just like the RE statistic mentioned above, this time measuring the

the  actual 

temperatures  and 

correlation  between 
the  reconstructed
temperatures generated by Mann’s mathematical model in the period 1854
to  1901.  What  McIntyre  was  pointing  out  though,  was  that  if  the
correlations were insignificant during calibration (according to the standard
R2 measure), what was the point of even following the analysis through to
verification?
MBH99 data
On an even simpler level, there was a great deal about the data used in the
MBH99 reconstruction that was peculiar. Of the 13 proxy series which had
generated  the  reconstruction  for  the  period  between  the  years  1000  and
1399, four were ice cores from a single small ice cap area of Peru, called
Quelccaya, while a further three were the first three PCs from a PC analysis
of tree rings in the southwest of the USA.

You will remember that in the first Hockey Stick paper, Mann had noted
that his roster of proxy series was too heavily weighted to the USA, and so
he had distilled down the data using PC analysis, creating a summary of the
main  patterns  in  that  area.  Here,  in  the  second  paper,  exactly  the  same
problem seemed to exist, with 4 of 13 series being from a single location.
Why then, had he not summarised the data in the same way? If having too
many series in a single area was a problem in the first Hockey Stick paper,
why was it not a problem for ice cores in the second?

The  southwest  USA  tree  rings  also  didn’t  seem  quite  right.  Why  had
Mann  retained  the  first  three  PCs  from  this  analysis?  Surely  if  you  were
trying  to  recreate  only  the  first  temperature  PC  (the  Northern  Hemisphere
mean temperature) you would only need the first proxy PC1? Remember, the
tree  growth  was  supposed  to  respond  to  temperature,  so  the  temperature
signal should have been right there in the first PC.

McIntyre  decided  to  examine  what  would  have  happened  if  Mann  had
prepared  his  proxy  data  along  the  more  logical  lines  this  analysis  was
suggesting.  He  prepared  a  PC  analysis  of  the  Quelccaya  ice  cores  and
eliminated the second and third PCs of the south-west USA tree rings. This
left him with eight series, which he regressed against the temperature  PC,
creating a completely new calibration. By now he wasn’t expecting there to
be any significant correlation between the temperatures and the proxies, and
he  wasn’t  disappointed.  The  R2  score  reached  a  measly  0.385,  virtually
indistinguishable from what you would get from random numbers. In fact,

the proxies didn’t even exhibit any sort of correlation with each other – in
other words they were all wiggling up and down apparently independently,
making  a  nonsense  of  the  idea  of  extracting  some  sort  of  a  common
temperature signal from them.
Reaction from Climate Skeptics
The  surge  of  alarming  results  from  McIntyre’s  analysis  was  starting  to
attract the attention of the other members of Climate Skeptics. There was a
flurry  of  comments  from  the  regular  readers,  some  of  whom  started
pressing  McIntyre  to  submit  his  findings  for  publication.  One  less
enthusiastic commenter pointed out that there were at least five independent
studies that had arrived at the same conclusions as Mann and that on the
face  of  it,  it  seemed  extremely  unlikely  that  they  had  all  used  flawed
approaches. However McIntyre had already been thinking about this and, as
he pointed out, there seemed to be a certain amount of commonality of the
data  used,  his  point  being  that  perhaps  the  independent  analyses  weren’t
quite  as  independent  as  they  seemed.  But  in  the  meantime,  he  was
beavering away, collecting the data from the other papers, ready to see just
what a careful study of their findings might reveal.

With  his  long  experience  of  the  mining  industry,  McIntyre  was  well
equipped  to  get  to  the  underlying  truth  of  a  compelling  graphic  like  the
Hockey  Stick.  In  a  posting  on  Climate  Skeptics,  he  pondered  some
similarities between the work of a mining analyst and a climate auditor.

[A]n individual time-series has much the same function as a drill-hole. Where there is
an ore-body (i.e. a significant ‘signal’), the information in the individual drill-holes is
not subtle. Any analyst recommending a mining stock has to look at the drill holes –
not just the compilations. The application of valid statistical methods to invalid data
can result in fiascos like Bre-X [a famous mining scandal in which drill-hole results
had been ‘improved’]. ‘Adjustments’ are always something to be suspected.

Mining promoters would often come up with carefully manipulated sets of
data  which  they  would  summarise  in  a  compelling  graph  in  order  to
convince  potential  investors  to  part  with  their  money.  In  his  mining  days
McIntyre had dealt with this kind of promotional graphic by turning to the
raw data in graphical form to ‘get a feel for the numbers’. Only by looking
at raw data could he be sure that he was seeing what the rocks were saying
rather than the results of some statistical shenanigans overlaid on the raw
data specifically to fox the unwary.

As  he  got  his  hands  on  more  and  more  proxy  data,  McIntyre  became
frustrated by the fact that most of the proxies stopped at around 1980. This
meant  that  the  dramatic  warming  of  the  1980s  and  1990s,  which  should
have vastly inflated the ring widths, couldn’t be seen. As he tartly observed:

If the IPCC were a feasibility study for a mere $1 billion investment in a factory or a
mine, you can be sure that the engineers would bring all this type of data up to date.
The casualness of the IPCC process in respect to not bringing the data up to date (but
relying on it for sales presentations) is really quite awe-inspiring.

However,  where  there  were  up-to-date  numbers,  it  became  increasingly
clear  that  the  raw  proxies  actually  showed  twentieth  century  trends  that
were,  broadly  speaking,  absolutely  normal.  There  was  no  sign  of  the
proxies  breaking  new  ground.  How  was  it  then,  that  the  Hockey  Stick
showed  twentieth  century  temperatures  that  were  unprecedented  in  a
thousand  years,  when  they  were  based  on  the  same  proxy  data?  Was  it
something to do with the particular proxies that Mann had used? Or was it
perhaps an artefact of the PC methodology? Only time would tell.
Making contact
At the same time as doing this work on  MBH99 McIntyre had also made a
start on the Hockey Stick proper – the original MBH98 paper. His first step
was to contact Mann directly in order to get hold of the proxy data. On 8
April 2003 he wrote the first of what was to be many emails to Mann.

Dear Dr Mann,
I have been studying MBH98 and 99. I located datasets for the 13 series used in [MBH99 .
. .] and was interested in locating similar information on the 112 proxies referred to in
MBH98 . . . (the listing at [the official website] is for 390 datasets, and I gather/presume
that many of these listed datasets have been condensed into PCs, as mentioned in the
paper itself). Thank you for your attention.
Yours truly,
Stephen McIntyre, Toronto, Canada31

Mann’s response was almost immediate, but while it was quite courteous, it
also contained something of a surprise.

Dear Mr McIntyre, These data are available on an anonymous FTP site we have set up.
I’ve forgotten the exact location, but I’ve asked my colleague Dr Scott Rutherford if
he can provide you with that information.
Best regards,
Mike Mann 31

So apparently, the author of one of the most important scientific papers in
recent decades didn’t know where the data he had used in that study was
located. This seemed a little odd, but the quick reply and its businesslike
tone boded well.

A couple of days later, on 11 April, Mann’s assistant, Scott Rutherford,
had finished his search for the data and emailed McIntyre to tell him what
he had come up with. Again the response was not what would have been
expected:

Steve,
The proxies aren’t actually all in one FTP site (at least not to my knowledge). I can get
them together if you give me a few days. Do you want the raw 300+ proxies or the
112 that were used in the MBH98 reconstruction?
Scott31

So,  according  to  Rutherford,  and  somewhat  contrary  to  what  Mann  had
said, the data wasn’t even in one place. Stranger and stranger. The fact that
the  data  had  never  been  compiled  into  a  single  record  also  strongly
suggested that nobody had ever asked to see the figures before. Nobody had
ever  tried  to  replicate  Mann’s  study.  However,  McIntyre  didn’t  raise  the
question with Rutherford or Mann, instead indicating that the 112 distilled
proxies  would  suffice,  and  offering  to  organise  the  data  to  make  things
easier for anyone who might want to use it in future. (The 300+ series that
Rutherford was referring to were the raw proxy data, some of which would
be summarised down using PC analysis, leaving just 112 to be put into the
calibration.)
Arrival of the data
After a couple of weeks, and following a few gentle reminders, an email
from  Scott  Rutherford  popped  into  McIntyre’s  inbox,  indicating  that  the
proxy data was now available on Rutherford’s FTP site at the University of
Virginia.

Steve,
OK,  I  think  I  have  it  all  straight  now.  You  can  get  the  data  via  anonymous  FTP  at
holocene.evsc.virginia.edu/pub/sdr
Regards,
Scott 31

The  data  took  the  form  of  a  simple  but  fairly  large  text  file  called
pcproxy.txt which contained the values for the 112 proxies for each of the
600 years of the MBH98 paper: each column of the file represented the values
for one proxy, and the rows were the years. At the topmost rows of the file
were  the  data  from  the  oldest  proxies,  the  first  starting  in  the  year  1400.
They  ran  down  through  the  centuries,  with  the  most  recent  figures  being
from 1980. Rutherford had helpfully attached a list of descriptions of each
series and provided a link to a website that would enable McIntyre to work
out what each one was. At last having the actual data in his hands, McIntyre
was ready to start the audit.
A strange shortage of hockey sticks
With all the amazing findings from his work on MBH99 he must have been
intrigued as to what he would find in the MBH98 data. Once again, the reality
was  to  be  every  bit  as  surprising  as  he  had  expected,  although  the  sheer
number of issues must have taken even him aback.

twentieth  century  warming 

After  loading  up  the  data  into  a  spreadsheet  McIntyre  plotted  all  112
series  in  separate  graphs.  In  this  way  he  hoped  to  be  able  to  see  clearly
which  series  were  driving 
the
reconstruction. He quickly noticed that the more prominent anomalies were
coming, not from the individual proxy series, but from the PC series – where
multiple proxy records had been summarised to stop their geographic area
being overrepresented. This didn’t seem quite right: if the output from the
PC analyses showed significant twentieth century warming (i.e. wider tree
rings) then the tree ring series that went in as raw proxies must have shown
the same warming too. But why then did the other raw proxy series – the
ones that hadn’t been put through a PC analysis – show nothing of the sort?
Qualitatively  then,  there  was  a  problem,  but  McIntyre  needed  to  get  a
firmer grasp on the scope of the issue. He needed to define which series had
a  hockey  stick  shape.  In  order  to  do  this,  he  had  to  define  ‘hockey  stick
shaped’ in mathematical terms and the definition he came up with was this:
any series where the value in the year 1975 was greater than one standard
deviation from the series average. This was a crude measure, but it would
help  to  make  sense  of  the  full  roster  of  proxies.  Armed  with  his  new
definition  he  analysed  the  full  dataset  and  discovered  that  hockey  stick
shaped series constituted just 13 of the original 112. In other words, most of
what  went  into  the  reconstruction  was  essentially  just  noise  and  had  no

the 

in 

effect on the final result. The shape of Mann’s temperature reconstruction
emanated from a short list of hockey sticks, which is shown in Table 3.1.
TABLE 3.1: Hockey stick shaped series in MBH98

SERIES NO
53
96
65
17
84
93
91
58
54
85
60
94
61

NAME
Gaspé
Australia PC1
Mongolia, Tarvagatny Pass
West Greenland Ice Melt
North America PC1
South America PC1
North America PC8
Coppermine River, Canada
Arrigetech
North America PC2
Churchhill Canada
South America PC2
Castle Penin, Canada

1975 EXCURSION

3.05
2.47
1.50
1.44
1.39
1.35
1.28
1.28
1.22
1.18
1.17
1.14
1.14

The 1975 excursion is the 1975 distance from the series mean measured in standard deviations.
All but one of the series were derived from tree rings, and no less than six
were PC series. This raised the question of why was it, if all of the proxies
could carry a temperature signal, it only seemed to be the tree rings which
were picking up the rapid warming seen in the twentieth century? Were the
non-tree-ring series actually temperature proxies at all?

FIGURE  3.1:  The  Gaspé  series,  showing  the  dramatic  twentieth  century
uptick in ring widths.

At the top of the list, the enormous deviation from the mean of Series 53
was striking, but here, once again, McIntyre’s mining background proved to
be surprisingly useful. As he pointed out to the climate sceptics:

Series  53  is  from  Gaspé,  Québec.  At  the  time  that  this  series  was  making  its  big
excursion,  I  happened  to  be  working  for  the  Canadian  company  which  owned  the
Gaspé. It’s certainly hard to think of a reason why trees in Gaspé were making a 3
[standard deviation] excursion in the 1970s.

McIntyre smelt a rat, and it wasn’t to be the last time either. Might not the
database be infested with them?
Principal components
The  trouble  started  while  McIntyre  was  trying  to  replicate  one  of  the  PC
analyses.  As  we  have  seen,  because  certain  parts  of  the  world  were
overrepresented  in  the  tree  ring  archives,  Mann  had  used  PC  analysis  to
reduce  this  mass  of  records  to  a  few  key  patterns.  He  had  identified  five
regions  as  needing  to  be  distilled  down  in  this  way:  Texas–Oklahoma,
Texas–Mexico, International Tree Ring Database (ITRDB US), South America
and Australia– New Zealand. Even without considering the underlying data
or calculations, these groupings looked a trifle strange. Why should Texas–
Oklahoma and Texas–Mexico be worthy of a  PC compilation in their own
right  when  there  was  a  US-wide  compilation  available  –  the  ITRDB  US
series?  Surely  it  would  have  made  more  sense  just  to  lump  them  all
together? There may of course have been a rational explanation for this, but
Mann’s paper was silent on the subject.

Meanwhile  McIntyre’s  roving  eye  alighted  on  one  of  the  individual
proxies, series 106, which Mann had given the designation MEXI001. As
the name suggested, this was a series from Mexico. The question this raised
in  McIntyre’s  mind  was  why  Mann  had  not  included  this  series  in  the
Texas–Mexico  PC  compilation?  It  made  no  sense.  He  investigated  further
and it turned out that this wasn’t an isolated issue either. Series 49–64 were
all from North America, 46 and 47 were South American, while 43 and 45
were from the Australia–New Zealand region. It would have made far more
sense  to  put  all  of  these  into  the  relevant  PC  compilations.  Again,  Mann
gave no rationale in the paper for what appeared to be an entirely illogical
approach to the question of data compilation.

Mann  had  indicated  that  the  individual  sites  for  each  of  these  regions
could be found in his online Supplementary Information. However, it turned
out that this information wasn’t actually complete although, with only one
or two exceptions, McIntyre was able to identify the missing sites by other
means.

While  he  was  suspicious  of  some  of  the  individual  proxies  used,
McIntyre  was  expecting  the  PC  replication  to  be  straightforward,  but  it
turned out that it was far from simple. After downloading clean data from
the World Data Center for Paleoclimatology, and having centred it ready for
analysis, McIntyre immediately ran into problems in replicating what Mann
had done. When he looked at the Australia–New Zealand series, he had data
going back to 1625, but Mann’s  PC series only started in 1750. Why was
that? And worse, in Texas–Mexico, the data only went back to 1760 and yet
Mann had PC calculations going back to 1400. From data which went back
220 years, Mann had extracted a main pattern that went back nearly 600.
Now this really was a mystery; PC analysis will not work if the series used
have missing data – its default reaction to a missing value would be for the
algorithm to fail. So if these were the series that Mann had actually used,
how had he been able to get his PC analysis to work? He had obviously done
something  –  used  alternative  versions  of  the  data  perhaps  –  but  what?
Again, the paper was no help. Scratching his head somewhat, McIntyre put
the problem aside and decided to take a look at the data for the 112 series
used in the calibration, that is, the figures as they were after the PC analysis.
Perhaps seeing some actual Mann data – some cold hard numbers – would
shed some light on the issue.
Dodgy data
To a lay reader, the columns of proxy series are pretty much indecipherable
– rows of numbers, columns of numbers, like so many grains of sand. But
to an experienced eye, used to picking out patterns from dense screeds of
data, certain things can jump out and demand to be examined more closely.
So when McIntyre started to study Mann’s proxy data series, it wasn’t long
before he noticed something odd.

The  Texas–Mexico  chronologies  had  been  reduced  to  nine  PCs,  which
appeared as proxy numbers 72 to 80 out of the 112. What McIntyre noticed
was  that  for  the  year  1980,  the  values  for  each  of  these  series  were  the
same.  This  wasn’t  a  case  of  rounding  making  them  appear  the  same;  the

value was identical, to seven decimal places: 0.0230304. This simply could
not be correct. It looked almost as if someone had copied the data from one
series and pasted it over the others.

When auditors of companies’ financial statements find errors, they have
no alternative but to extend their testing and see if what they have found is
an isolated error, or whether they are scratching at the surface of something
more serious. McIntyre’s climate audit was no different, and he commenced
a careful examination of all of the 112 proxy series. It wasn’t long before
more  and  more  oddities  of  the  same  kind  were  tumbling  out  of  the
woodwork. The copying of 1980 values that had infected the Texas–Mexico
PCs was also seen in the three PC proxy series known as the Vaganov PCs, as
well as four of the nine PC series derived from the International Tree Ring
Database (ITRDB). Again, each one was identical to seven decimal places.
Another strange feature was also observed in these  PC series. All but two
started in a year ending either in 99 or 49. For example, Series 73 started in
the year 1499, 74 and 75 in 1599 and 76 to 80 in 1699. The PC series were
meant to have started either on the century or half-century, so it looked as if
what had happened was a simple clerical error – some of the data appeared
to have been copied into the file at the wrong row and then the missing data
at the bottom of the column had been infilled by copying from an adjacent
series.

These three groups of affected records – Vaganov, Texas– Mexico and
ITRDB – amounted to a total of 16 series. Assuming one in each group was
actually correct (that is to say, the one from which the infilled value had
been copied) then that left thirteen which were incorrect, or more than ten
percent of the series used in the reconstruction.
Infilling
More  digging  into  the  proxy  records  turned  up  a  different  kind  of  error:
proxy number 45 had the same value in every year from 1978 to 1982.

Series 46, on the other hand, was identical from 1974 to 1980. And as
McIntyre looked across the columns he saw similar problems in still more
of the series. Series 51 was the same. So was 52. And 54, 56 and 58 as well.
It looked as though some of the numbers had been missing from the series,
and rather than discard the proxy or locate the missing data, someone had
infilled the missing numbers with the final available figure. Further across
the  file,  the  same  thing  could  be  seen  in  another  sequence  of  series,

numbers  93,  94,  95,  96,  97,  98  and  99.  And  number  6  too.  All  of  these
series had their final values infilled in the same way: by simple means of
copying  the  last  available  value  into  the  empty  cells.  Elsewhere  though,
Series 53 was infilled for four years at the beginning, and Series 3 showed
every sign of having been infilled for a period during the 1950s.

The pièce de resistance though, was Series 50. Here, the values for the
entire period from 1962 to 1982 were copied from Series 49. With a little
digging, McIntyre was able to work out that, although Mann had attributed
both 49 and 50 to a study by Fritts and Shao, Series 49 was in fact derived
from an entirely different study, by Keith Briffa.

Most of the infilling was happening, as you can see, during the modern
era,  which  is  when  you  would  expect  it  to  be  easiest  to  obtain  complete
data, but more importantly it was during the calibration. Inaccurate results
here would have a direct knock-on effect on the reconstruction of historical
temperatures. To be fair to Mann, he had said in the online supplementary
information to the original paper that there had been some infilling in the
data:

Small gaps have been interpolated. If records terminate slightly before the end of the
1902–1980 training interval, they are extended by persistence to 1980.14

but  it  must  also  be  said  that  this  didn’t  cover  the  copying  of  data  from
adjacent  series  and  nor  did  it  really  give  the  reader  a  sense  of  the  sheer
amount of infilling that had seemingly gone on. In all, more than a third of
the series had been affected in this way, and for Mann to have been fair to
the reader this should have been disclosed, together with some assessment
of the potential impact on the reconstruction.
Where did the data come from?
It got worse. Series 10 and 11 were two instrumental records – the Central
England Temperature Record (CETR)  and  the  Central  Europe  Temperature
Record,  (you  will  remember  that  not  all  of  the  inputs  into  the  regression
were  proxies  –  some  were  actual  temperature  readings).  When  McIntyre
checked these back to the original publicly archived data he found that the
figures didn’t match. Where was Mann getting his numbers from? With a
little  digging  the  answer  turned  out  to  be  that  the  figures  were  actually
based on the average of June, July and August for each year, rather than the
full  year  average.  The  problem  with  this  was  that  Mann  was  trying  to

the  average 

recreate  an  annual  average  temperature,  not  a  summer  average,  so  why
choose  the  summer  figures?  All  the  other  instrumental  records  were  full
year averages, so why should CETR be different?

CETR  is  the  one  of  the  oldest  uninterrupted  temperature  records  in  the
world.  It  measures 
temperature  for  an  area  roughly
corresponding  to  the  English  Midlands,  but  also  includes  areas  which  an
Englishman would normally consider ‘the North’. It was started in the year
1659, giving it the best part of 350 years of uninterrupted measurements. It
is hard then to understand why Mann should have truncated the record at
1730,  reducing  the  length  of  the  series  to  250  years.  Cynical  observers
might, however, have noticed that the late seventeenth century numbers for
CETR  were  distinctly  cold,  so  the  effect  of  this  truncation  may  well  have
been to flatten out the Little Ice Age.

When McIntyre transferred his attentions to the Central Europe series, he
came across a similar problem – the data had been truncated at 1550, when
the  full  series  actually  went  back  to  1525.  Here  the  warmest  part  of  the
record  was  removed  from  the  series,  and  the  effect  was  presumably  to
flatten  the  Medieval  Warm  Period  somewhat.  In  neither  case  were  these
truncations disclosed or justified.
Mislocations
The MBH98 reconstruction included 11 precipitation (rainfall) series, which
Mann had referenced to a paper by Jones and Bradley 1992 (the same Phil
Jones who prepared the temperature data).d However, when McIntyre tried
to  check  the  precipitation  numbers  back  to  original  data  in  the  public
archive  he  immediately  ran  into  problems.  He  was  able  to  check  the
matches  en  masse,  by  calculating  the  correlations  between  the  archive
version and Mann’s version of the same series. His best scores were above
0.9, indicating a close but not exact match, but many of the proxy series
barely matched at all, with correlation scores of less than 0.5. Where had
Mann got this data?

There were more problems with these series too. Series 37 was identified
as being the rainfall records for Paris, France, and the numbers Mann had
used had a high correlation with the archive figures. The start dates of the
two sets of numbers were the same as well. On the face of it, this looked to
be correct. However, in Mann’s reconstruction the series had been located at
42.5N, 72.5W, which is just outside Boston, Massachusetts, an error which

prompted McIntyre to quip cruelly that ‘The rain in Maine falls mainly in
the Seine’, much to the amusement of the sceptic community.

Two other precipitation series were located in India according to Mann’s
paper, but the authors of the study which Mann had quoted as his source,
Jones and Bradley, didn’t actually have any Indian series in their paper, so it
simply  couldn’t  be  correctly  located.  Certainly  the  figures  didn’t  match
actual Indian rainfall figures, and the best match McIntyre could find in the
archived  precipitation  records  turned  out  to  be  Philadelphia,  although  not
with a high enough correlation to make the identification definitive, or even
likely.

The rest of the precipitation series were either unarchived or were from
unreported sources or had been manipulated in some way prior to use in the
reconstruction, otherwise they would have been identifiable by correlation
analysis.

And  last,  but  not  least,  Series  20,  an  ice  core  from  Greenland,  was
materially  mislocated,  and  the  locations  of  Series  46  and  47  had  been
swapped. It was all fairly amazing, but if McIntyre thought that was the end
of the story, it was an idea of which he was shortly to be disabused.
Old data
McIntyre’s  comparison  of  the  data  Mann  had  used  to  the  figures  in  the
public archives, which had identified the origins of the precipitation figures,
also  revealed  another  puzzling  aspect  of  MBH98.  When  McIntyre  checked
Series 51 to 61 to the archive, it turned out that all of these series had more
up-to-date figures available – Mann had been using old versions of the data.
Of  course,  it  was  quite  possible  that  some  of  these  might  have  been  the
current versions at the time MBH98 was originally written, so McIntyre made
some  enquiries  at  the  World  Data  Center  for  Paleoclimatology  who
maintained  many  of  the  records.  Their  response  was  that  the  updated
figures had been available since 1991 and 1992, more than six years before
the publication of MBH98.

In all, there were 24 series where more up-to-date figures were available
in the public archive, and some of the differences between the two versions
were far from trivial. One in particular was astonishing: Series 56, was a
tree  ring-width  chronology  called  Twisted  Tree,  Heartrot  Hill  (see  Figure
3.2).  Mann  had  used  an  old  version  of  the  data,  which  ended  in  1975.
Needing  data  to  run  up  to  1980,  he  had  therefore  infilled  up  to  1980  by

simply repeating the 1975 value for the final five years of the series – this
can  be  seen  as  the  tiny  plateau  at  the  right  hand  end  of  the  record.  The
overall trend in Mann’s data was upwards. However, the updated version in
the archive now included figures right through to 1992, and these showed
that during the 1980s the trend in the ring widths had dramatically reversed,
with all the gains from earlier years being lost.

FIGURE 3.2: Twisted Tree, Heartrot Hill
Top: Mann version; Bottom: Up-to-date version

This decline confirmed exactly what Briffa had said about the divergence
between tree ring widths and temperature in the modern era; it is therefore
not  surprising  that  McIntyre  found  that  this  divergence  was  not  unique
among those series where updated figures were available, with Series 51, 54
and 59 all showing declining ring widths while the versions used by Mann
showed increases.
More on PCs
Having finally exhausted, for the time being, the possibilities for error in
the  MBH98  database,  McIntyre  returned  to  the  subject  of  the  principal
components calculation. Where Mann had got a  PC  that  extended  back  in
time further than the underlying data, nothing could be done until McIntyre
was  able  to  unravel  the  mystery  of  how  the  PC  calculation  was  made  to
work with sections of data missing. But, as we’ve seen, this situation didn’t
apply to the Australia–New Zealand PC analysis, where the raw data went
back  to  1625  but  Mann’s  calculations  had  run  only  from  1750.  McIntyre
had all the data, so there was nothing to stop him replicating at least this
small step – there were only 16 proxies in the compilation, after all.

FIGURE 3.3: The Australia PC1
Top: Mann version; Bottom: McIntyre’s version

With  a  statistics  add-in  for  his  spreadsheet,  running  a  PC  analysis  was
very  straightforward,  particularly  with  such  a  small  dataset.  The  results,
which are shown in Figure 3.3, were once again spectacular; they told an
entirely different story to the one that Mann had apparently read from the
same data.

The top chart shows Mann’s figures for the first PC from the Australia–
New Zealand compilation, with a sharp twentieth century uptick at the right
hand  side,  suggesting  widening  tree  rings,  apparently  the  result  of  global
warming.  The  bottom  chart  is  the  equivalent  from  McIntyre’s  analysis  of
the  same  dataset,  calculated  using  a  standard  PC  algorithm  and  showing,
well, not much of anything.

This was starting to look like dynamite: Mann had got his PC calculation
wrong,  and  in  the  Antipodean  proxy  compilation  at  least,  the  effect  of
putting it right was to make the hockey stick shape disappear completely.
Reconstruction
How  would  this  pan  out  in  the  Northern  Hemisphere  reconstruction?
Remember,  the  tree  ring  PC  calculation  distils  groups  of  raw  tree  ring
proxies down into their main patterns. You then have to calibrate these tree
ring PCs (and the individual proxies) against temperature PCs (‘This tree ring
pattern behaved like that temperature pattern during 1902– 1980’), validate
them (‘My tree ring pattern still behaved like the same temperature pattern
in 1850–1901’) and then use the tree ring PCs to recreate temperature PCs of
the past (‘Since I know my how my tree rings were behaving in 1400 to
1850, I can work out what the temperature was in the same period’). It was
quite possible that the Australia–New Zealand series could be completely

trendless and yet the Northern Hemisphere reconstruction could still show a
hockey  stick,  driven  by  another  series.  The  reconstruction,  remember,
would be driven by those proxies which correlated best against temperature
in the calibration period.

Now  that  the  proxy  database  had  been  cleaned  up,  all  the  mistakes
corrected, and up-to-date data collected, putting together a recalculation of
the  reconstructed  temperature  should  have  been  easy,  but  with  Mann’s
description of his methods being so vague, it was still a hard task to work
out exactly what he’d done.

Making  little  headway,  McIntyre  emailed  Mann  again  to  try  to  get
clarification of some of the ambiguous calculation steps, but received little
useful information in return. Without Mann’s input it was almost impossible
to  get  an  exact  replication  of  the  Hockey  Stick,  and  without  an  exact
replication  it  was  hard  to  be  certain  how  correcting  the  errors  in  the
database would affect the final result. However, if his findings were ever
going  to  see  the  light  of  day,  he  was  going  to  have  to  reconcile  his  own
work to Mann’s. There was nothing for it except to use trial and error to try
to discover the exact combination of methodological steps that Mann had
used.

Over the next few weeks, McIntyre laboured at his task. Returning again
and again to the text of the paper and using what further clues there were in
the  online  Supplementary  Information,  he  threw  every  conceivable
methodological  permutation  at  the  data,  noting  anything  that  brought  his
own results closer to Mann’s. The problem was that small changes in the
methodology could have a dramatic effect on the outcome, a finding that
suggested Mann’s results were far from robust. However, slowly but surely
McIntyre’s  take  on  the  Hockey  Stick  edged  closer  and  closer  to  Mann’s,
until  by  September  he  had  something  that  he  felt  was  good  enough.  It
wasn’t exact – something was still not quite right – but if he hadn’t arrived
at Mann’s algorithm, he certainly had something that looked very like it.

With a good approximation of Mann’s methodology at hand, McIntyre
now reached the moment of truth. How would correcting all the errors in
the  database  affect  the  results?  McIntyre  pointed  the  program  at  the
corrected  data  and  set  the  calculations  in  train.  In  a  minute  he  had  the
answer:  when  he  saw  the  results,  it  was  clear  that  the  hunches  he’d  had
when  looking  through  the  graphs  of  the  proxies  were  entirely  borne  out.
With the database corrected, the handle of the Hockey Stick was warped –

that is to say, there was a pronounced Medieval Warm Period. In fact the
temperatures of the reconstructed fifteenth century were even higher than
those reached in the twentieth century.

Now McIntyre really had something. The Hockey Stick looked as though
it was bent. But he can have had little inkling as to how long it would be
before it would be broken once and for all.

a    It  has  been  suggested  that  the  use  of  the  expression  ‘Hockey  Team’  to  describe  Mann  and  the
group  of  climatologists  associated  with  him  is  derogatory  and  amounts  to  accusation  of  a
conspiracy. Its earliest use in this context appears, however, to have been due to Mann himself.28

b    The  acronym  is  derived  from  the  names  of  the  land  temperature  series  (CRUTEM)  and  the  sea

temperature series (HADSST) used in its preparation.

c  The section on tree rings mentioned Briffa’s work, together with a possible explanation for it from
another researcher, but given the criticality of the question, two sentences seem rather inadequate.

d  See page 61.

4     Energy and Environment

It has been said that though God cannot alter the past, historians can; it is perhaps
because they can be useful to Him in this respect that He tolerates their existence.

Samuel Butler

The audit team
McIntyre’s  Hockey  Stick  postings  on  the  Climate  Skeptics  forum  had
garnered him a great deal of support and encouragement, including many
kind  words  from  some  of  the  professional  scientists  in  the  sceptical
community,  such  as  physicist  David  Douglass  and  geologist  Bob  Carter.
Two other academics who were supportive of McIntyre’s work went on to
play significant parts in the story that followed.

Ross McKitrick is a professor of economics at the University of Guelph,
not far from McIntyre’s own home in Toronto. A prominent global warming
sceptic, he had already written books and articles critical of the scientific
basis for the theory.32–34 He also hung out at Climate Skeptics from time to
time  and  in  the  middle  of  July  2003,  McIntyre  sent  him  a  short  email
suggesting that they get together to discuss his work on MBH98. McKitrick
explained in his reply that he was en route to a summer holiday in British
Columbia but suggested that they should meet up in early September, when
he  expected  to  have  some  free  time.  Ahead  of  the  meeting,  however,
McIntyre sent through his notes, together with an explanation of how his
thinking  had  developed  and  the  kinds  of  calculations  he  was  doing.  As
McKitrick  pored  over  the  intricacies  of  the  mathematics  he  realised  that
McIntyre  had  made  discoveries  that  were  of  undeniable  importance.
However,  it  was  one  thing  to  make  a  scientific  step  forward,  but  quite
another to turn it into a paper fit for publication. McIntyre had a lot to do if
he was to show the world what he had found.

At  around  the  same  time,  Sonia  Boehmer-Christiansen,  a  geographer
based  at  the  University  of  Hull,  also  got  in  touch.  Boehmer-Christiansen
was  the  editor  of  a  controversial  journal  called  Energy  and  Environment.
Energy and Environment  was,  and  remains,  a  rather  obscure  journal  with
(rightly  or  wrongly,  depending  on  your  point  of  view)  a  reputation  for
publishing  climate  science  papers  that  more  mainstream  journals  would

rather not touch (for valid or invalid reasons, again depending on your point
of view). Its circulation was tiny, and it could probably be best described as
a social science journal, specialising as it did in policy matters rather than
physical sciences. However, it did publish occasional scientific papers, and
since  it  adhered  to  the  scientific  norm  of  peer  review  it  was  almost
impossible  for  mainstream  climatologists  to  ignore  something  published
there.

In  the  middle  of  2003,  Boehmer-Christiansen  was  planning  a  special
issue of the journal dealing with climate issues and, aware from the Climate
Skeptics forum that McIntyre had made some interesting discoveries about
MBH98, invited him to submit an article. This was something of a surprise
for  McIntyre  who  had  never  published  anything  in  an  academic  journal
before,  and  he  admitted  to  being  rather  flattered  that  anyone  should
approach him in this way. He accepted the invitation and set to work on a
draft,  although  time  was  very  short:  the  special  issue  was  due  to  be
published in the autumn, just a few months away.
Writing the paper
By  early  September,  McIntyre  had  completed  a  draft  of  the  paper  and
arranged  to  meet  McKitrick  on  19  September  at  a  small  restaurant  near
Toronto’s  Pearson  airport,  midway  between  their  homes.  This  was  to  be
their first meeting, no suitable occasion having arisen since they had first
mooted the idea back in the summer. Over lunch, McKitrick explained that
he was impressed by the content of McIntyre’s draft paper but not by how it
was presented. He later said of the paper that ‘the discussion was unfocused
and  the  conclusions  unclear’,35  and  the  two  men  spent  a  long  time
considering how it might be tightened up and clarified. With McKitrick’s
experience  of  drafting  academic  papers  and  his  detailed  knowledge  of
statistics,  McIntyre  realised  that  the  economist  was  a  very  useful  man  to
know. Before the day was over, the two had agreed to continue their work
together.

The paper was clearly going to be very critical of Mann; McIntyre had
found too many errors in  MBH98 for a reader to come away with anything
other than an unfavourable impression. The big worry was that something
essential  had  been  missed.  It  would  be  humiliating  if  they  published  a
damning  criticism  that  turned  out  to  be  flawed  in  some  way.  Having
checked and rechecked their work, they were certain that they had made no

mistakes. This almost certainly meant that the mistakes they had identified
were real and the critique they had prepared was valid. However, there was
one other possibility. Could it be that somehow they had been working with
the  wrong  data?  Of  course,  they  had  got  the  numbers  from  Mann’s
colleague,  Scott  Rutherford,  but  it  was  just  possible  that  an  inadvertent
mistake had been made and that the data he had supplied was not the same
as  the  data  Mann  had  used.  It  seemed  best  to  check  out  this  possibility
before  they  went  to  print  and  it  was  agreed  that  McIntyre  would  draft  a
letter to Mann to confirm they had indeed received the right dataset. At the
same time, he could probe the nagging issue of how Mann had got his PC
calculations to work with the gaps in the data.

Dear Prof Mann
Here is the pcproxy.txt file sent to me last April by Scott Rutherford at your direction.
It  contains  some  missing  data  after  1971.  Your  1998  paper  does  not  describe  how
missing data in this period is treated and I wanted to verify that it is the correct file.
How did you handle missing data in this period? In earlier periods, it looks like you
changed the roster of proxies in each of the periods described in the Supplementary
Information using only proxies available throughout the entire period. I have obtained
quite  close  replication  of  the  [reconstructed  PCs]  in  the  20th  century  by  calculating
coefficients for the proxies and then calculating the [reconstructed PCs] using the . . .
procedures  described  in  MBH98  and  the  .  .  .  Supplementary  Information.  The
reconstruction  is  less  close  in  earlier  periods  .  .  .  The  description  in  MBH98  was
necessarily very terse and is still very terse in the Supplementary Information; is there
any more detailed description of the reconstruction methodology to help me resolve
this? Thank you for your attention.
Yours truly,
Steve McIntyre,
Toronto, Canada31

The reply from Mann was brief, but evasive on the questions:

Dear Mr. McIntyre,
A  few  of  the  series  terminate  prior  to  the  nominal  1980  termination  date  of  the
calibration period (the earliest such instance, as you note, is 1971). In such cases, the
data were continued to the 1980 boundary by persistence of the final available value.
These  details  in  fact,  were  provided  in  the  supplementary  information  that
accompanied the Nature article . . .
          The  results,  incidentally,  are  insensitive  to  this  step;  essentially  the  same
reconstruction  is  achieved  if  a  calibration  period  terminating  in  1970  (prior  to  the
termination of any of the proxy series) was used instead.31

Mann  also  seemed  very  keen  to  end  any  further  enquiries  into  his  work
(brackets in original):

Owing  to  numerous  demands  on  my  time,  I  will  not  be  able  to  respond  to  further
inquiries. Other researchers have successfully implemented our methodology based on
the information provided in our articles [see e.g. Zorita, et al 1998]
I  trust,  therefore,  that  you  will  find  (as  in  this  case)  that  all  necessary  details  are
provided  in  the  papers  we  have  published  or  the  supplementary  information  links
provided by those papers.
Best of luck with your work.
Sincerely, Michael E. Mann 31

Mann’s reference to the Zorita paper was not particularly helpful, since the
authors  of  this  paper  had  actually  applied  a  variation  on  the  MBH98
methodology to a completely different dataset. With Mann giving nothing
more away, McIntyre and McKitrick had little choice but to go ahead with
the data and methodological information they had already.
Publication
McIntyre’s findings were clearly going to be highly controversial and it was
therefore  important  to  make  the  paper  scientifically  watertight.  To  make
sure  nothing  had  been  missed,  he  and  McKitrick  recruited  a  number  of
external  reviewers  to  examine  the  findings  and  the  draft  paper.  Then,  to
check that their work was unassailable from a statistical perspective, they
also  commissioned  a  review  from  a  professional  statistician  with
paleoclimate expertise. All of this input greatly improved the paper, but it
also  took  time  and  there  was  precious  little  of  that  left.  The  deadline  for
Energy and Environment was 30 September 2003, now just a matter of days
away. Work continued on the final drafts at a furious pace, McIntyre and
McKitrick  exchanging  emails  and  drafts  on  an  almost  hourly  basis.  Even
then, it looked as if their efforts were not going to be enough to have the
paper  ready  on  time,  but  fortunately  Boehmer-Christiansen  was  prevailed
upon to extend the deadline by a few valuable days.

Finally, at the start of October, McIntyre and McKitrick were ready to go
and sent the paper, with its provocative title, ‘Errors and defects in Mann et
al. (1998) proxy data and temperature history’, on its way to Energy  and
Environment.  With  the  publication  deadline  already  passed,  Boehmer-
Christiansen had been struggling to get the paper included, but by dint of
persuading the peer reviewers to do their work in a fraction of the normal
timea  and  with  McIntyre  and  McKitrick  responding  equally  quickly,  she
was able to scrape the paper home just in time to go to press at the end of

October. MM03, as the paper became known (after its authors’ initials and
the year of publication), was about to make a splash.

The  paper  was  published  in  Volume  14,  Issue  6  of  Energy  and
Environment on 27 October 2003 with a revised title of ‘Corrections to the
Mann  et  al.  (1998)  proxy  data  base  and  northern  hemisphere  average
temperature series’.37 As it was being printed, it was also posted online on
the Energy and Environment website and, unusually, because of its political
importance it was made freely available. Publication was accompanied by
the  launch  of  a  dedicated  website  called  Climate2003,  which  contained
background information on McIntyre and McKitrick and more details of the
findings.  The  website  also  contained  all  of  the  data  and  code  used  in
McIntyre’s research. This was done quite deliberately, mindful of possible
accusations of hypocrisy, as the two authors had been very critical of Mann
and his team for failing to make all of their data and methods available. To
fail  to  have  their  own  supporting  material  available  at  the  time  of
publication would have left them looking very foolish.

News of McIntyre and McKitrick’s findings hit the media within hours
of publication. First out of the blocks was USA Today which declared on 28
October, just 24 hours after publication:

An important new paper in the journal Energy and Environment upsets a key scientific
claim  about  climate  change.  If  it  withstands  scrutiny,  the  collective  scientific
understanding of recent global warming might need an overhaul.38

Mann’s mouthpiece
Even more startling was the fact that Mann managed to shoot back almost
as  quickly.  The  very  next  day,  through  the  website  of  a  sympathetic
journalist  called  David  Appell,  he  fed  an  extraordinary  story  about  why
McIntyre and McKitrick’s results were so different from his own.

Mann’s first claim was, almost predictably, that McIntyre had used the

wrong data. Appell reported that what had happened was this:

[McIntyre and McKitrick] asked an associate of Mann to supply them with the Mann
et al. proxy data in an Excel spreadsheet, even though the raw data is available [on
Mann’s University of Virginia  FTP site]. An error was made in preparing this Excel
file, in which the early series were successively overprinted by later and later series,
and this is the data [McIntyre and McKitrick] used.39

McIntyre and McKitrick were taken aback. Mann’s explanation of what had
happened  bore  no  resemblance  to  what  had  actually  happened.  McIntyre
had certainly made no request for the data to be delivered in spreadsheet
format and when the data was eventually delivered, it was as a text file.b As
for the rest of the claims, it was a mystery how Appell and Mann expected
McIntyre to have checked the data to the FTP site.

This was the first time McIntyre had even heard of the site’s existence,
let alone that it contained a data repository. There was certainly no link to
the site on Mann’s homepage. Neither Mann nor Rutherford had made any
mention  of  it  in  their  correspondence  with  McIntyre.  A  few  days  after
McIntyre’s original request, Mann had said he didn’t know where the data
was. Rutherford had said that he would have to compile the figures from
different locations, suggesting that he too was also unaware of the FTP site’s
existence. If Mann didn’t know where the data was and Rutherford didn’t
know  that  a  single  compilation  of  the  data  existed  at  all,  why  should
McIntyre be expected to know about it?

In a second posting later that day Appell reproduced McIntyre’s original
email request for the data, together with Mann’s response that he would get
Scott Rutherford to look up the FTP location. Appell didn’t seem to notice
that  this  first  email  made  no  mention  of  a  spreadsheet  and  therefore
contradicted the story he had posted a few hours earlier. However, he also
posted some new details of Mann’s side of the story.

Mann says that the crux of [McIntyre and McKitrick]’s error is their use of a Excel
dataset with only 112 columns (where each column represents one set of proxy data–
tree  rings,  ice  cores,  historical  temperature  data,  etc.),  when  in  fact  the  full
paleoclimatic data series requires 159 to be used properly. 40 [Emphasis added]

If  the  story  about  a  spreadsheet  was  a  surprise,  this  new  claim  was  truly
bizarre. Appell explained to his readers that McIntyre was aware that there
were 159 series used in  MBH98 rather than 112, and pointed his readers to
McIntyre’s  original  email  to  Mann.c40  The  problem  was  that  this  email
referred specifically to ‘the 112 series’ (the same figure that was mentioned
in the original paper) and to the 390 raw series, which had been summarised
down using PC analysis. The figure of 159 series was completely out of the
blue,  appearing  nowhere  in  either  of  Mann’s  papers  or  the  online
supplementary information that went with them, nor could mention of it be

found  in  Rutherford  or  Mann’s  correspondence  with  McIntyre,  or  in  any
other scientific papers which referred to Mann’s work.

Appell continued:

I have asked McIntyre and McKitrick if they had checked the data they received from
Mann and associate against [Mann’s] raw data, as you’d think you would if you were
truly trying to double-and triple-check an important established scientific conclusion
(especially if you were going to seriously slam it), but haven’t received a reply.40

Appell’s idea that McIntyre should have checked the data back to another
dataset run and controlled by the same research group was odd. The validity
of the data could only be checked by matching it to the original sources in
the scientific archives and this was something which McIntyre had done; he
had set out the results in all their gory detail in his Energy and Environment
paper. He had also contacted Mann to check if he had supplied the correct
data, but Mann had failed to direct him to the FTP site and in fact rebuffed
him in no uncertain terms. How could he now complain that McIntyre had
failed to check their dataset sufficiently?

McIntyre  and  McKitrick  couldn’t  allow  Appell’s  story  to  stand
unchallenged and decided to make a considered reply. They explained that
they were reluctant to engage the argument in this way since a full Mann
response  was  apparently  on  its  way,  but  they  set  about  the  task  with  a
certain relish anyway.

To  his  credit,  Appell  posted  another  article  the  next  day,  pointing  to
McIntyre’s response, which had been posted on the Climate2003 website.41
It is probably fair to say that Appell was taken aback by the ease with which
McIntyre was able to rebut Mann’s story since his tone was considerably
milder than in his earlier postings, and he seemed to avoid exploring their
responses in detail.

It  was  relatively  easy  for  McIntyre  to  refute  the  idea  that  he  had
requested  a  spreadsheet,  simply  by  pointing  to  his  correspondence  with
Mann. Likewise, it was simple to cite MBH98 itself, where the text referred
to ‘the full multiproxy network of 112 indicators’.14 Mann and Appell had
also  claimed  that  the  data  McIntyre  used  contained  meaningless  splices
from the earlier and later centuries. Clearly, since Mann had supplied this
data,  the  splices  couldn’t  be  McIntyre’s  fault,  but  were  presumably
attributable to Scott Rutherford, who had compiled the numbers on Mann’s
behalf. But, as McIntyre pointed out, he and McKitrick had checked each

series  to  the  archives  and  while  there  were  plenty  of  errors,  there  was
absolutely no sign of the kind of splicing errors Mann described. Perhaps
Mann would consider telling them which particular series were affected in
this way?

As McIntyre’s response went on, it became worse and worse for Mann,

whose accusations were simply opening more avenues of enquiry:

Why did the data file have to be assembled from scratch? Did he not have a copy for
his own work? Has no one ever asked for it before? Is he accusing his associate, Scott
Rutherford,  of  inserting  all  the  fills?  And  if  what  we  received  was  ‘a  complete
distortion’,  and  bears  ‘no  relation’  to  the  dataset  he  used,  how  were  we  able  to
replicate his original results so closely?
      While the claim implicit in Professor Mann’s defence is that he actually did work
from  correctly  collated  data  file  in  his  1998  paper,  this  still  fails  to  address  the
substantial problems of obsolete series, mislabelled locations, truncation of sources,
extrapolations of missing data, use of [summer] data where annual are available etc.41

The  point  about  having  been  able  to  replicate  Mann’s  work  using  the
allegedly erroneous data was key. This meant that Mann’s claims that the
MM03 result of a pronounced Medieval Warm Period had been due to the use
of incorrect data couldn’t be true. It was when McIntyre had used clean, up-
to-date data direct from the scientific archives that he had got a Medieval
Warm  Period.  When  he  used  the  erroneous  data  that  Rutherford  had
supplied, he was able to replicate Mann’s Hockey Stick closely. Mann had
used the erroneous data, or at least something that looked very much like it.
Investigating the FTP site
As well as preparing their response to Mann, McIntyre and McKitrick were
busy  checking  out  some  of  his  mysterious  claims.  Mann’s  attempted
rebuttal  was  the  first  time  either  McIntyre  or  McKitrick  had  heard  of
Mann’s FTP site at the University of Virginia, which should not be confused
with Rutherford’s FTP site, from which McIntyre had first downloaded the
proxy data back in April. The obvious step was to examine the site to see if
the information it contained in any way supported Mann’s defence of his
work.

After checking that the data they had originally downloaded was still up
on Rutherford’s site, McIntyre visited Mann’s website and downloaded the
copy  of  pcproxy.txt,  the  equivalent  of  the  original  data  file  they  had
received from Rutherford. After verifying that the data on Rutherford’s site
was unchanged and noting that the file carried a date of 8 August 2002, he

checked off each series against the Mann version and discovered that the
files were identical. The file creation date being in 2002 meant that the data
they had been sent must have been prepared well before his request for it in
April 2003.

As  well  as  examining  Mann’s  FTP  site,  however,  McIntyre  also  spent
some  time  looking  at  Rutherford’s  website  and  here  he  chanced  upon
further evidence that the pcproxy.txt file had been around for several years.
On  a  graph  comparing  different  temperature  reconstructions,  Rutherford
had made reference to the pcproxy.txt file, and when McIntyre traced the
heritage of the web page using the Wayback Machine,d he discovered that
Rutherford had originally posted this page in 2001, well before he had even
started looking at Mann’s paper, let alone requested the data. This and the
file  creation  date  together  amounted  to  definitive  evidence  that  the  file
hadn’t been prepared especially for him, as Mann was now claiming.

McIntyre also discovered that even if he had been aware of the existence
of the directory on Mann’s FTP site, he wouldn’t have known which of the
proxy series there were to be used. You will remember that in Mann’s initial
response through Appell’s website, the number of proxy series used in the
calibration had suddenly changed from 112, as reported in MBH98, into 159.
In the directory on Mann’s FTP site there were 430 raw proxy series, some
of  which  would  have  been  summarised  down  using  PC  analysis,  while
others  were  non-PC  series  which  would  have  gone  into  the  calibration  as
they were. These latter series would presumably have been quite simple to
identify, simply by comparing the values in pcproxy.txt to the files on the
FTP site. However this would then leave the task of working out how the
remaining series were put through the PC calculations. Which PC rosters did
they go into? In which periods? And at the end of the day, how many PCs
were  to  be  extracted  from  each  PC  calculation?  McIntyre  was  working
towards  a  total  of  112  (81  non-PC  series  plus  31  PCs),  but  according  to
Mann, he should have been trying to get to 159 and should therefore have
produced 78  PCs on top of the 81 non-PC  series.  The  task  was  essentially
impossible.

The  visits  of  McIntyre  and  McKitrick  to  the  websites  of  Mann  and
Rutherford didn’t go unnoticed. A few days later, things started to get very
strange. Without warning, the copy of pcproxy.txt on Mann’s FTP site was
deleted. Fortunately, a vigilant member of the sceptic community noticed
the  deletion,  and  on  8  November  emailed  McIntyre  to  tell  him  what  had

happened.  Since  the  data  had  been  there  on  the  FTP  site  shortly  after  the
publication of  MM03, the deletion must have occurred in the previous two
weeks.

A few days later, it got stranger still. As we saw in Chapter 1,e shortly
after the publication of MBH98 in 1998, Mann had left UMass and had taken
up a postion at the University of Virginia. However, some of the data that
McIntyre  had  used  in  MM03  was  still  located  on  Mann’s  old  website  at
UMass.  On  12  November,  just  four  days  after  the  disappearance  of
pcproxy.txt,  the  entire  MBH98  directory  at  UMass  was  suddenly  deleted,
again without notice. What was worse was that the deletion of the data had
happened  before  McIntyre  and  McKitrick  had  a  chance  to  copy  the  full
contents.  Fortunately,  the  disappearance  of  the  data  was  again  quickly
picked  up  in  the  sceptic  community  and  an  email  was  dispatched  to  the
webmaster responsible, requesting the restoration of the data from backups.
By a stroke of good fortune, he was obliging and the data was restored and
made secure again. The deletion, the webmaster alleged, had been made in
order to save server space and the timing was, apparently, coincidental.

This  was  not  the  end  of  the  deletions  though.  A  few  days  later,  all
reference  to  pcproxy.txt  was  systematically  removed  from  Rutherford’s
website.  It  now  appeared  that  these  simultaneous  deletions  were  no
coincidence:  could  it  really  be  that  the  evidence  that  the  file  sent  to
McIntyre had contained the original MBH98 data was being carefully erased?
In  some  ways  this  would  have  been  futile,  because,  as  noted  above,
McIntyre had been able to replicate the Hockey Stick using the data he’d
been sent and the methods Mann had described, which was strong evidence
that they had both used the same data. Nevertheless, the disappearance of
the  data  made  it  pretty  clear  that  Mann  was  intending  to  argue  on
regardless.
Mann’s reply
Mann’s formal reply was published online a few days later at the website of
Tim  Osborn,  a  colleague  of  Phil  Jones  at  Britain’s  Climatic  Research
Unit.42 As expected, it was an aggressive defence of his position. After an
initial shot across the bows of McIntyre and McKitrick for failing to allow
him  to  review  MM03  before  publication,  Mann  got  to  the  meat  of  his
arguments. His first line of defence was based around an alleged failure by
McIntyre and McKitrick to include all the data in their calculations.

It  seems  clear  that  [McIntyre  and  McKitrick]  have  made  critical  errors  in  their
analysis  that  have  the  effect  of  grossly  distorting  the  reconstruction  of  MBH98. Key
indicators of the original  MBH98  network  appear  to  have  been  omitted  for  the  early
period  1400–1600,  with  major  consequences  for  the  character  of  the  [MM03]
reconstruction of Northern Hemisphere temperatures over that interval.42

Mann’s claim that McIntyre and McKitrick had missed out key data from
the  early  part  of  the  reconstruction  was  two-pronged.  Firstly  he  was
disputing the validity of the corrections. He pointed first to Twisted Tree,
Heartrot Hill, which you may remember from Chapter 3 had been used in
MBH98 with an obsolete version.f McIntyre had shown in MM03 that a more
up-to-date version had declining temperatures in the late twentieth century.
Mann’s objection was that the more up-to-date version only started in 1530,
while  the  obsolete  one  went  right  back  to  1459.  He  was  arguing  that
McIntyre had effectively thrown away over 70 years of data. Of course, it
hadn’t been McIntyre who had thrown away the data at all, but the scientists
who had entered the revised data onto the archive. Assuming they hadn’t
made an error, these researchers had presumably removed the early years
because the data failed quality control measures in some way. Whatever the
reason, McIntyre was quite happy to stand on a position of using the most
up-to-date  numbers  available,  and  leave  it  to  Mann  to  explain  why  he
thought the older version was more valid.

Mann’s second line of attack was to accuse McIntyre and McKitrick of
missing out data by not following the same procedures that had been used
in MBH98. Mann explained that, as you went back in time, fewer and fewer
proxy series were available in the pcproxy database. As we saw in Chapter
3,g standard PC analysis will fail if there are missing values, and of course,
as series dropped out of the MBH98 record in the earlier centuries, there were
more  and  more  gaps.  In  order  to  get  round  this,  Mann  explained,  he  had
adopted a ‘stepwise’ procedure. He first reconstructed the temperature for
1850 to 1980 using the full roster of proxies. Then he repeated the process
for 1800 to 1980 using only those series that were available for the full 180
year period. Continuing in this vein, he could calculate 1750 to 1980, 1700
to  1980  and  so  on,  right  back  to  1400.  Obviously,  he  now  had  several
reconstructions for each period, each one based on a smaller set of proxies
than  the  last.  It  was  therefore  necessary  to  take  the  most  reliable
reconstruction for each period – the one with the most proxies in its roster –
and splice it to the most reliable reconstruction for the previous period. So

the final Hockey Stick was actually a patchwork of ‘steps’ or sections of
several different reconstructions that had been spliced together.

This stepwise process was how he was able to avoid any failures of the
PC algorithm, Mann said, and because McIntyre had failed to use the same
procedure, great swathes of data had been dropped from the calculations –
in particular the Stahle PC1 and the North American PC1. (This was the series
referred to in Chapter 3 as IRTDB US, but it is usually known as ‘NOAMER’
and this is the way we will refer to it from now on.) This failure explained
much of the discrepancy between his results and McIntyre’s.

The problem with Mann’s argument was that there was no word of this
kind of stepwise procedure having been used anywhere in MBH98 or in the
online Supplementary Information, although we can see that McIntyre had
been guessing that this might have been the case from his email to Mann
shortly before publication. In fact, Mann had stated in  MBH98 that he had
used ‘conventional principal components analysis’ and it was a moot point
as  to  whether  what  he  had  done  was  actually  ‘conventional’  at  all.  And
because Mann had refused to answer his questions, it was impossible for
McIntyre to have ascertained what had in fact been done.

Mann may well have felt that he had done enough to fend off McIntyre’s
criticisms but McIntyre’s perspective was quite different. Without realising
that  he’d  done  it,  Mann  had  inadvertently  shone  a  little  light  on  another
murky  corner  of  his  famous  paper.  To  McIntyre,  what  made  Mann’s
response  most  interesting  was  not  the  fact  that  Mann  had  used  an
undisclosed  methodology,  but  the  fact  that  if  you  left  out  just  two  of  the
proxy series – the Stahle and NOAMER PC1s – you got a completely different
result – the Medieval Warm Period magically reappeared and suddenly the
modern warming didn’t look quite so frightening. What this meant was that
Mann’s result – that the Medieval Warm Period didn’t exist – seemed to rest
on just a tiny fraction of his data. The rest of the series were just ‘noise’.
Mann may well have been justified in using a stepwise procedure, but if his
conclusions  depended  on  just  two  PC  series,  then  they  could  hardly  be
considered robust.
McIntyre and McKitrick shoot back
A  few  days  later,  McIntyre  and  McKitrick  responded  with  their  own
broadside,  a  formal  response  to  Mann.43  As  well  as  detailing  the  issues
around when the FTP site became available, and repeating the responses they

had made to Appell, a couple of other points were addressed. Firstly, they
noted  in  an  aside  that  they  had  discovered,  buried  deep  within  the  site’s
directory  structure,  a  subdirectory  with  a  rather  surprising  name:
BACKTO_1400-CENSORED. To what purpose or in what way this directory had
been ‘censored’ was not clear but, the two Canadians noted, with apparently
straight faces,

In  light  of  the  identified  sensitivity  of  early  15th  century  values  to  very  slight
variations in proxy indicators and the evidence elsewhere of truncation (censoring?) of
important  temperature  series,  we  believe  that  disclosure  of  the  censoring  process
would be helpful.43

Although they didn’t realise it at the time, the  CENSORED directory was to
play an important part in the subsequent story.

The  response  to  Mann  also  included  a  long  appendix,  looking  at  the
failure by the Hockey Stick authors to disclose materials and methods in an
adequate fashion. The gaps Mann had left had raised a whole raft of new
questions  about  the  decisions  he  had  taken  in  designing  the  study.  For
example,  although  he  had  now  revealed  that  he  had  used  a  stepwise
approach to the proxy  PC calculations, it was still impossible to work out
exactly  which  proxies  had  been  used  in  which  steps  and  how  many  PCs
were retained from each calculation. Another question was why so many of
the 159 (or perhaps 112) tree ring series were derived from North America
– more than half of the total. Surely the proxies sampled should have had a
more even spread across the globe? It was also unclear why the Australian
series  were  only  used  in  the  PC  calculation  from  1750,  when  they  were
actually available from 1625. Why was the South American PC1 not used for
its full extent either? The Central England Temperature Record too? It was
a shambles and McIntyre was not inclined to give Mann the benefit of the
doubt. With a paper like MBH98, which was of such huge public importance,
nothing other than full disclosure was acceptable.

Regardless  of  the  merits  of  their  methodology,  until  MBH  provide  the  long  overdue
public disclosure of their PC rosters, one is still involved in an ongoing guessing game,
which  is  completely  unedifying  for  a  paper  on  which  there  is  considerable  public
reliance. The disclosure within MBH98 and the Supplementary Information to MBH98 is
inadequate  and  further  disclosure  was  not  given  upon  private  request.  Material
differences may result from a reconstruction using stepwise PC calculation rather than
conventional PC calculation. The non-disclosure in  MBH98 of the use of stepwise  PC
methods  is  accordingly  a  material  non-disclosure.  More  adequate  disclosure  by

[Mann, Bradley and Hughes] in  MBH98 may well have resulted in a more searching
examination of their methodology by statistical specialists long before now.43

A new approach to Mann
It was now clear to McIntyre and McKitrick that the only way they were
ever  going  to  get  to  the  bottom  of  MBH98  was  to  get  hold  of  the  actual
computer code Mann had used and to obtain complete details of the data,
including the crucial information about which series were used in which PC
calculations and in which periods. A decision was taken to approach Mann
once more and on 11 November McIntyre wrote an email as follows:

You  have  claimed  that  we  used  the  wrong  data  and  the  wrong  computational
methodology. We would like to reconcile our results to actual data and methodology
used in MBH98. We would therefore appreciate copies of the computer programs you
actually used to read in data (the 159 data series referred to in your recent comments)
and construct the temperature index shown in Nature (1998) (‘MBH98’), either through
email or, preferably through public FTP or web posting.44

Mann’s reply, however, was unresponsive. He made no mention of the code
and pointed again to the FTP site as the location of the data:

To reiterate one last time, the original data that you requested before and now request
again are all on the indicated FTP site, in the indicated directories, and have been there
since at least 2002. I therefore trust you should have no problem acquiring the data
you now seek.45

Still  without  the  details  of  the  proxy  rosters  and  the  computer  code,
McIntyre  decided  to  press  the  point  once  more,  but  unfortunately  for
everyone, Mann chose to bring the correspondence to an end:

I am far too busy to be answering the same question over and over for you again, so
this will be our final email exchange.46

This particular line of enquiry, at least, looked as though it had gone cold.

a  Mann’s supporters later questioned whether the peer review can have been adequate because the
process  appeared  to  be  very  brief.  The  review  appears  to  have  taken  between  three  and  four
weeks.36

b  Readers may like to refer to McIntyre’s original email request on page 72.
c  See page 72.
d  The Wayback Machine is a website that archives the whole of the Internet. See www.archive.org.

e  See page 37.
f  See page 83.
g  See page 78.

5     Line Brawl

What a book a devil’s chaplain might write on the clumsy, wasteful, blundering, low,
and horribly cruel works of nature!

Charles Darwin

More investigations
The possibility of getting more information from Michael Mann might have
come  up  against  something  of  a  dead-end,  but  there  was  still  plenty  to
explore on the FTP site, and also the intriguing question of the significance
of the CENSORED directory.

McIntyre had been unable to work out from the data repository on the
FTP site which series had been used in which steps of which PC calculations.
Without  this  information,  he  was  certain  that  he  would  never  be  able  to
produce an exact replication of Mann’s study. However, as he explored the
rest  of  the  site,  he  suddenly  made  a  breakthrough  when  he  discovered  a
number of files which contained the unspliced PCs – essentially the results
of the individual steps which would then have been spliced to give the final
reconstruction. Being able to see these intermediate steps in the calculation
was enough to finally enable him to work out how the whole thing had been
put together.
Discovery of the PC code

to  data  from 

McIntyre was still chipping away at the apparently intractable problem
of trying to replicate Mann’s PC calculations. It seemed that no matter what
he did, he just couldn’t produce exactly the same results as Mann – he was
close, but there was still something missing. Mann had said that McIntyre’s
attempt at replication had failed because he hadn’t used stepwise methods,
leading 
the
reconstruction: the NOAMER and Stahle PC1s and the Twisted Tree, Heartrot
Hill  series.  We  also  saw  that  this  claim  set  the  alarm  bells  ringing  for
McIntyre, suggesting as it did that these series were key to the whole study.
Knowing they were key was one thing, but being able to do anything about
it was another. That was until the release of the FTP site. When this became
available, many of the mysteries which had bamboozled McIntyre finally
started to unravel.

three  key  ‘indicators’  dropping  out  of 

Having  made  this  massive  step  forward,  McIntyre  thought  that  he  would
finally be able to replicate the Hockey Stick but to his surprise he found he
still couldn’t get the same answer as Mann. In desperation, he started to go
systematically  through  Mann’s  FTP  site  to  see  if  by  chance  there  was
something,  anything,  which  might  provide  even  a  small  clue  to  what  the
missing step was. File after file was examined and checked for clues until
finally, after days of searching, he found what he was looking for. Buried
deep in the directory structure of the site, he chanced upon a small fragment
of a Fortran computer program, which turned out to be key to the whole
Hockey Stick reconstruction.

For  those  familiar  with  computer  programming  languages,  Fortran  is
generally  considered  rather  antediluvian  –  something  that  no  serious
programmer  would  use  these  days,  there  being  much  more  efficient  and
powerful alternatives available. However, it still appears from time to time
in  legacy  applications  and  programmers  can  therefore  still  chance  upon
examples of the code ‘in the wild’. McIntyre was therefore pretty surprised
to find Fortran in use in Mann’s group, but he had studied the language in
his university days and, with a certain amount of brushing up of his skills,
he was able to decipher the code and work out what it did. As he worked his
way through the dense text, he realised that he had found just what he was
looking for.

The code fragment turned out to be a copy of the actual program used in
the tree ring PC calculations. Having grasped this fact, McIntyre set about
working out exactly what it did, transcribing the whole thing, one line at a
time, into a more modern language. By that time, McIntyre had long since
abandoned  his  use  of  a  spreadsheet,  which  had  proved  too  clumsy  for
manipulating the large datasets he had now collated. Instead, he was using a
specialist  statistical  programming  language  called  simply  ‘R’.  R  had  a
number  of  huge  advantages  for  someone  like  McIntyre  –  it  could  drill
through the data in any direction, it allowed him to easily chop and change
between different versions of the data series in his endless search for the
correct  identity  of  the  159  series,  and  it  had  a  dizzying  array  of  statistic
functionality built in as standard, including PC analysis. This was a hugely
powerful tool, available as freeware, and in widespread use by statisticians
all  over  the  world.  Quite  why  Mann  would  be  using  something  as
antiquated as Fortran when he could have used R or an equivalent package

was something of a mystery, particularly because, with Fortran, you had to
program in PC functionality from scratch. It just seemed hugely amateurish.
As  he  processed  the  Fortran  program  into  R,  slowly  building  his
understanding of its workings, McIntyre finally came across a handful of
lines of code that looked as if they might be . . . not quite right. You will
remember  from  Chapter  2  that  before  performing  the  PC  calculation,  you
have  to  centre  the  data  by  subtracting  the  series  average  from  each  data
point.a  That  is  what  happens  in  conventional  PC  analysis.  It  turned  out,
however, that Mann had done something different. Only slightly different,
mind you, but the effect looked significant: instead of subtracting the mean
of the whole series from each data point, he had subtracted the mean of the
calibration period. Then, he had ‘standardised’ by dividing the answer by
the standard deviation of the calibration period and then standardised them
again in a slightly different way.

With a possible answer finally in their hands, McIntyre and McKitrick
worked furiously at deciphering the meaning of what they had found. The
standardisation steps were odd, to say the least. Back in Chapter 2 we saw
that before tree ring data is archived it has already been standardised.b This
involves expressing each ring measurement as the ratio of its width to the
expected  width  for  a  tree  of  that  age.  Standardisation  in  this  way  makes
every tree ring chronology directly comparable to every other one.

The  standardisation  process  that  Mann  had  adopted  –  dividing  by  the
standard  deviation  –  is  normally  used  when  data  series  are  not  directly
comparable,  for  example  if  the  data  is  recorded  in  different  units.  It
therefore  appeared  to  be  an  entirely  superfluous  step,  given  that  the  data
series  were  already  directly  comparable.  Why  he  should  choose  to
standardise twice more was a real mystery, particularly because each time
he  did  so  he  potentially  removed  the  very  variance  that  he  was  trying  to
observe in the dataset.

But while the restandardisations looked odd to the two Canadians, it was
Mann’s decision to centre his data using only the twentieth century mean
that looked the most intriguing. The step was wrong, of that there was not a
shadow of a doubt, but what was the effect of this error on the final Hockey
Stick result? Was it this that was causing the Mann methodology to produce
a hockey stick? The more they looked at it, the more certain McIntyre and
McKitrick  became  that  they  had  the  answer.  Mann’s  incorrect  method  of
centring (which we will refer to as short centring) created a bias towards

hockey  stick  shaped  series  –  any  series  with  either  a  twentieth  century
uptick (or a downtick) would be heavily weighted in the PC1, forcing it into
the  same  shape.  Although  this  may  seem  a  little  counterintuitive,  it  is
actually extremely simple to show how this happens. The critical step is the
subtraction  of  the  twentieth  century  average  from  each  data  point,  rather
than the series mean, and this effect is shown in Figures 5.1–5.3.

Figure 5.1 shows two dummy tree ring series. Series A, the black line,
has an uptick in the nineteenth century, while Series B, the grey line, has an
identical uptick in the twentieth century. Now see what happens when you
centre these series the correct way – by subtracting the full series average
from each data point, just like we saw in Chapter 2. This is shown in Figure
5.2.

FIGURE 5.1: Two raw tree ring series

FIGURE 5.2: The same series correctly centred

FIGURE 5.3: The series under Mann’s short-centring
As you can see, the effect on the two series is identical, shifting them both down so that they are
‘centred’ around a zero mean – part of each series is above the zero line and part below. Remember
that PC analysis will determine how much weight to give each series by calculating the sum of the
squares of each variance from the mean. Since both the series here are centred on the mean, the sum
of squares is not a large number for either series, both of which should receive approximately equal
weightings in the final reckoning.
Figure  5.3,  on  the  other  hand,  shows  the  effect  on  the  same  series  of
Mannian  short  centring.  You  can  see  that  Series  B,  with  its  twentieth
century  uptick,  has  been  shifted  down  below  zero  –  in  the  jargon,  it  has
been ‘decentred’. Now, almost every data point has a big variance from the
mean,  and  the  sum  of  squares  rapidly  inflates  to  a  very  large  number.
Because  of  this,  the  weight  it  receives  in  the  first  PC  is  extremely  high
compared to Series A. The short centring regime was effectively ‘mining’
the database for series where the twentieth century diverged from the long-
term mean – hockey sticks in other words – and was then loading all the
weight onto them in the final result.
Implications of short-centred PCs
With the answer in their hands, McIntyre and McKitrick needed to set out
the  problem  and  its  implications  for  everyone  to  see  in  a  clear  and
unassailable manner and after giving it some consideration, they came up
with a three-pointed plan of attack. Firstly McIntyre contrasted the effect of
the short centring on some actual MBH98 data. He picked two series from the
NOAMER proxy roster known as Sheep Mountain and Mayberry Slough (see
Figure 5.4). Sheep Mountain is a hockey stick shaped series with a sharp

twentieth  century  uptick.  Mayberry  Slough,  on  the  other  hand,  had  its
growth peak in the early nineteenth century.

Remember  that  PC  analysis  assigns  weights  to  each  component  of  the
dataset, so that those series accounting for the most variance get the most
weight. Both of these series show a spike in growth, although at different
times, rather like the artificial example we have just looked at. Standard PC
analysis  would  be  expected  to  give  them  similar  weights  in  the  first  PC.
However,  the  effect  of  short-centred  standardisation  was  to  grossly
overweight Sheep Mountain, with its recent growth spurt, over Mayberry
Slough. In fact, in the final calculation, Sheep Mountain had 390 times the
weighting of Mayberry Slough, making it look vastly more significant.

FIGURE 5.4: Sheep Mountain and Mayberry Slough

In order to reinforce the point, the second line of attack was to prepare
some simulations of what Mann-style standardisation would do to random
data.  When  we  talk  about  random  numbers  we  are  usually  referring  to  a
particular  kind  of  randomness  called  ‘white  noise’.  In  white  noise,  each
additional  data  point  is  independent.  The  throw  of  a  dice  is  a  familiar
example: if you get a six with the first throw, it makes no difference to your
chances of getting a six with your second. The probability is just the same –
one in six. There are, however different kinds of randomness and the one
relevant to the story of the Hockey Stick is called ‘red noise’. Red noise is
distinguished by the fact that each data point is not independent of the last
one.  An  example  of  a  red  noise  process,  in  slightly  more  mathematical
terms,  would  be  one  where  the  value  is  given  by  ‘the  last  point  plus  or
minus a random amount’. Red noise is best described as a ‘random walk’,
which can be envisaged on a graph as a line which wiggles up and down
without  ever  going  anywhere  in  particular.  It  might  wander  off  in  one
direction  for  a  while,  but  eventually  it  will  turn  round  and  head  back

Red  noise  processes  appear  to  be  very  common  in  nature,  and  in
particular  are  observed  in  weather  and  climate  systems  and  in  biological
processes.  So  in  order  to  test  Mann’s  algorithm,  it  was  necessary  to  see
what  it  would  do  to  red  noise  series  rather  than  to  white  noise.  To  make
absolutely certain he had headed off any potential objections, McIntyre was
careful to ensure that the red noise series had exactly the same statistical
characteristics  as  the  noise  in  the  tree  ring  series  actually  used  in  MBH98.
And  when  he  fed  the  results  into  the  Mann  PC  routines  –  bingo!  Hockey
sticks appeared. You could feed pretty much any group of red noise series
into Mann’s algorithm and, provided there was a rising or falling trend in
the twentieth century it would give you a  PC1 shaped like a hockey stick.
The  short  centring  was  simply  overweighting  any  series  with  twentieth
century  upticks.  Meanwhile,  any  with  twentieth  century  downticks  were
given large negative weightings, effectively flipping them over and lining
them up with upticks. Figure 5.5 shows the result from processing the same
red  noise  series  using  conventional  centring  (top)  and  short  centring
(bottom), the latter with a distinct hockey stick shape.

McIntyre repeated this process ten times and every time he got a hockey
stick. If even random numbers would give you a hockey stick shaped PC1,
there could be no doubt that Mann’s methods were fundamentally flawed.

towards  the  mean.  White  noise  on  the  other  hand  would  look  just  like  a
mess of dots.

There is a subtlety to this result which needs to be understood because
there was considerable confusion at a later date. McIntyre and McKitrick
were  not  suggesting  that  the  short  centring  algorithm  produced  hockey
sticks from nothing – it couldn’t conjure hockey sticks out of white noise,
for  example.  But  if  there  was  even  one  hockey  stick  shaped  series  in  a
database containing dozens of series without any significant trend, it would
overweight it, yielding a PC1 that suggested that a hockey stick shape was
the dominant pattern in the data.

Another  possibility  was  that  a  database  really  did  have  a  hockey  stick
shape as its dominant pattern. But even then the short centring algorithm
had  to  be  treated  with  immense  care.  Since  short  centring  could  produce
hockey sticks from red noise, it would have to produce a hockey stick with
a much more pronounced blade from tree rings before the result could be
seen  as  statistically  meaningful.  This  was  an  issue  that  was  to  cause
considerable controversy later in our story.

The  third  plank  of  the  argument  was  to  demonstrate  the  effect  of  the
short  centring  on  the  NOAMER  PC1.  This,  if  you  like,  was  the  pièce  de
resistance,  a  reconciliation  of  his  work  with  Mann’s.  In  essence,  the
significant differences between MBH98 and MM03 boiled down to just a few
series:  NOAMER,  with  its  problematic  centring,  the  Stahle  series,  and  one
which Mann had not flagged up: Gaspé.

FIGURE 5.5: The effect of short-centring on red noise
We  met  the  Gaspé  series  in  Chapter 3  where  we  saw  its  extraordinary  hockey  stick  shape.  Since
making  that  observation,  McIntyre  had  discovered  that  the  series  had  actually  been  used  twice in
Mann’s paper. It appeared once in the NOAMER PC series and once as a single proxy. According to the
record in the archive, the series extended back to the year 1404. Strangely, however, when used as a
single proxy, Mann had managed to get the data to go back to 1400. It turned out that the value for
the year 1404 had been repeated in the three earlier years, a step that was not disclosed in the paper
and was unique in the  MBH98 dataset. While this might seem a relatively innocuous procedure, its
impact was significant. With the data now extending back to the start of the fifteenth century, the
Gaspé series, with its dramatic hockey stick shape, could be incorporated into the critical AD 1400
step where it would push down the Medieval Warm Period.
Nature paper
With these amazing findings in the bag, McIntyre and McKitrick could at
last  make  a  full  response  to  the  critics  whose  denunciations  were  still
echoing loudly around the Internet. The obvious step was to submit their
new work to Nature,  which  had  published  the  original  MBH98 paper. This
had  one  major  disadvantage  in  that  until  the  findings  were  in  print  they
could  not  make  any  public  response  to  their  Internet  opponents  –  the
findings would have to remain under embargo. However, this appeared to
be a price worth paying and so, resigning themselves to having to endure
the barbs of the critics, they set to writing once more.

The immediate problem the two men faced was that the information they
wanted to convey didn’t fit neatly into the categories of article that Nature
usually accepted. ‘Letters’ and ‘Articles’ were categories reserved for the

publication  of  new  work,  while  the  final  possibility,  ‘Communications
Arising’, was for short comments on other articles. The problem was that
there  was  a  lot  of  information  to  convey  and  the  word  limit  for  a
Communication Arising was likely to be too short. They decided however
to go with this category since it was reserved for ‘exceptionally interesting
or  important  comments  and  clarifications  on  original  research  papers’,
which seemed to be a pretty good description of their new paper.

The paper was submitted on 14 January 2004, together with a covering
letter  explaining  its  importance  and  also  outlining  their  difficulties  with
categorising  the  content  and  asking  for  editorial  advice  on  how  best  to
proceed.47 The paper, they said, was longer than a normal Communication
Arising, but shorter than a normal Letter. They also asked that when peer
reviewers were appointed, at least one of them should be an expert in  PC
analysis, with no connection to the climate field.

When a critical comment like McIntyre and McKitrick’s is received, a
scientific  journal  will  normally  approach  the  targets  of  the  critique  for  a
response.  Then  both  the  comment  and  the  response  can  be  published
alongside  each  other.  Towards  the  end  of  January,  Nature  contacted
McIntyre to indicate that this process was underway. There were, however,
some worrying signs in the email:

You will also see from our guidelines that Communications Arising are only published
as  such  if  they  represent  a  scientific  advance  over  the  original  paper;  critical
comments  such  as  yours  may  instead  be  addressed  in  the  form  of  a  published
correction/clarification  from  the  criticised  authors,  if  this  is  recommended  by  our
referees. In this event, your contribution is likely to be acknowledged.48

As  McIntyre  noted  in  his  reply,  the  words  ‘scientific  advance’  were  not
actually  mentioned  in  Nature’s  published  policies  on  Communications
Arising,  this  requirement  only  being  relevant  for  Articles.  Besides,  he
added,  eliminating  a  material  error  in  a  hugely  important  scientific  paper
must surely count as a scientific advance. He went on:

We have put a lot of time and effort into this investigation, and along the way have
taken  a  great  deal  of  public  criticism  on  earlier  findings  published  elsewhere,
including  some  from  high-profile  scientists  and  commentators.  The  present
submission is a substantial advance from our earlier findings and serves as a complete
response to these criticisms. Although we would have obviously liked to have already
responded to these attacks, doing so would have required divulging the contents of our
submission to Nature. Thus, at some personal sacrifice, we have let some attacks go

unchallenged, so as to allow the Nature review process to operate confidentially. If our
findings are correct, the originating authors not only applied incorrect methodology,
but  their  disclosure  of  methods  and  data  to  the  editors,  reviewers  and  readers  of
Nature  was  inaccurate,  and  substantially  influenced  how  the  paper  was  received.  It
would  be  inequitable  if  our  findings  are  upheld,  but  the  originating  authors  are
allowed  to  present  a  ‘clarification’,  without  simultaneous  presentation  of  our
findings.49

Favourable revise and resubmit
At the start of March, Nature emailed to say that they had now received the
reply from Mann that would be published alongside their article if it was
accepted,  together  with  comments  from  the  two  external  peer  reviewers.
The comments from the reviewers were overall quite complimentary, and
the paper had received what is known as a ‘favourable revise and resubmit’
– in other words, some amendments were required but both the paper and
Mann’s  reply  to  it  were  recommended  for  publication.  McIntyre  and
McKitrick therefore set to work to incorporate the amendments asked for
within the two week deadline set by the journal.
Mann’s reply – another mystery
The request for a resubmission meant that everyone involved had to repeat
the whole review process again. This gave McIntyre and McKitrick as well
as the Hockey Team a chance to hone their arguments in the light of their
contributions the first time round, as well as addressing themselves to the
points made by the peer reviewers.

that 

Mann’s reply was just as forthright as his previous pronouncements on
McIntyre and McKitrick’s work. He took issue with almost all of the paper,
saying that the two Canadians were ‘incorrect with respect to each major
point’ 
their  work  did  ‘not  demand  serious
consideration’. However, he quietly conceded a great deal.50

they  raised  and 

that  McIntyre’s  data  and  methods  gave  rise 

Much of his response reiterated the position he had taken in response to
to  a
MM03,  namely 
reconstruction that eliminated much of the data in the early periods – the
three  key  indicators.  He  went  on  to  point  out  that  when  normal
standardisation was used, the reconstruction scored very poorly on the  RE
statistic, which he said showed that McIntyre’s reconstruction was spurious.
This  was  rather  misleading,  as  McIntyre  and  McKitrick  never  claimed  to
have  produced  a  reconstruction  of  their  own  –  only  to  have  shown  that
Mann’s was not robust. He also dismissed McIntyre’s red noise simulations

out of hand, saying that the statistical properties of the actual series were
not  emulated  by  red  noise,  and  that  it  was  not  therefore  an  appropriate
model to use.

On  some  of  the  data  issues,  like  Stahle,  Mann  argued  that  even  if
McIntyre was right, it was immaterial to his results. As far as the Stahle PCs
were  concerned,  McIntyre  was  happy  to  accept  the  argument  that  their
inclusion  was  irrelevant  to  the  fifteenth  century  results,  and  he  indicated
that he would amend the paper accordingly, but noted that it was actually
Mann  who  had  described  it  as  a  ‘key  indicator’  in  his  early  responses  to
MM03. It was a bit rich of him now to claim it as immaterial.

Meanwhile, on Twisted Tree, Heartrot Hill,c Mann put up no substantive
defence  of  his  use  of  an  obsolete  data  version,  while  still  apparently
objecting  to  McIntyre’s  preference  for  the  up-to-date  data.  His  position
appeared to be that a longer record was better than a shorter one, without
addressing McIntyre’s argument that the early part of the record had been
eliminated  from  the  archives  for  a  good  reason.  In  fact,  by  the  time  the
revised  paper  was  ready  to  be  resubmitted,  McIntyre  had  discovered  that
the early part of the series was based on only a single tree, which appeared
to be the reason this part of the record had been removed.

Mann also tried to dismiss the extrapolation of the Gaspé record back to
the start of the fifteenth century, saying that is was a mere ‘technicality’. In
his reply, McIntyre noted that ‘This is hardly correct’, a rather polite choice
of phrase in the circumstances.51 As the only series in the MBH98 corpus that
was  extrapolated  in  this  way,  and  with  the  fact  that  the  extrapolation
allowed its inclusion in the AD 1400 roster, it was surely deserving of some
discussion. And as McIntyre noted, its inclusion had a dramatic impact on
the Medieval Warm Period. This being the case it was incumbent on Mann
to explain to the readers the reasons for his adjustment.
Bristlecones
Much  of  the  argument  though  was  over  the  first  two  PCs  of  the  North
American tree ring database, NOAMER. Of the nine NOAMER PCs, only these
two  were  used  in  every  reconstruction  step  back  to  the  start  of  the
reconstruction,  including  the  critical  AD  1400–1450  step,  where  the
Medieval Warm Period would have been expected to appear. McIntyre and
McKitrick  had  set  out  the  effect  of  Mann’s  short  centring  on  the  results,
noting  the  overweighting  given  to  Sheep  Mountain,  which  has  been

described above. Mann, however, objected to McIntyre’s focus on this one
series; there were, he said, lots of other series contributing to the NOAMER
PC1.  If  McIntyre  had  previously  thought  he  had  got  to  the  bottom  of  the
Hockey Stick, he was now set right, because this apparently simple claim
was about to open yet another can of worms that would keep him busy for
years to come.

Intrigued  to  discover  exactly  which  series  were  involved,  McIntyre
prepared  a  table  outlining  the  sixteen  top-weighted  series  in  the  NOAMER
PC1. This table was eventually included in the letter which accompanied the
resubmission McIntyre and McKitrick sent to Nature and it is reproduced
here as Table 5.1.52

As McIntyre explained,

It immediately stands out that 15 of the 16 sites are high-altitude sites due to Donald
Graybill. We identified a presentation of 12 of these sites in Graybill and Idso (1993)
(which was also discussed in Mann et al. (1999)).52

The  implications  of  this  discovery  were  enormous.  Not  only  did  Mann’s
Northern  Hemisphere  reconstruction  depend  largely  on  just  one  of  its  PC
series,  namely  the  first  PC  from  the  NOAMER  network,  but  the  shape  of
NOAMER itself depended on just a tiny selection of trees from one corner of
the western USA. They were all from one of two closely related species –
bristlecone  pines  and  foxtails  –  and  all  but  one  had  been  collected  by  a
single researcher, Donald Graybill. These records all had a distinct hockey
stic  shape  with  a  dramatic  growth  spurt  in  the  twentieth  century  but
unremarkable growth levels in the fifteenth century.

TABLE 5.1: Sixteen heavily weighted series in the NOAMER PC1

Name

Species Elevation Author

ID

 

 

 

 
ca528 Flower

Lake

ca529 Timber

 

 
PIBA

PIBA

(m)

 
3291

3261

 

 
D.A.
Graybill
D.A.

Graybill- Exclusion
Idso
(1993)
 

MBH98

Censored
TRUE

13

14

TRUE

Gap Upper

ca530 Cirque

Peak

ca533 Campito
Mountain

PIBA

PILO

3505

3400

ca534 Sheep

Mountain

PILO

3475

ca555 Yolla Bolly PIBA

2460

co523 Windy
Ridge

co524 Almagre
Mountain

co525 Hermit

Lake

PIAR

PIAR

PIAR

3570

3536

3660

co535 Frosty Park PILF

3218

co545 Niwot
Ridge

nv510 Charleston

Peak

nv511 Mount

Jefferson
nv512 Pearl Peak

nv513 Mount

Washington

nv514 Spruce

Mountain

PILF

PILO

PILF

PILO

PILO

PILO

3169

3425

3300

3170

3415

3110

12

5

11

4

1

3

6

7

9

8

Graybill
D.A.
Graybill
Graybill
&
Lamarche
D.A.
Graybill
B.
Buckley
D.A.
Graybill
D.A.
Graybill
D.A.
Graybill
D.A.
Graybill
D.A.
Graybill
D.A.
Graybill
D.A.
Graybill
D.A.
Graybill
D.A.
Graybill
D.A.
Graybill

 

 

 

 

TRUE

TRUE

TRUE

FALSE

TRUE

TRUE

TRUE

TRUE

TRUE

TRUE

TRUE

TRUE

TRUE

TRUE

PILO: Pinus longaeva (Intermountain bristlecone pine); PIFL:

Pinusflexilis (Limber pine);

PIAR: Pinus aristata (Bristlecone pine); PIBA:Pinus balfouriana (Foxtail

pine)

As we have seen, the short-centring routine picks up any deviation from
the norm in the twentieth century and vastly overweights it in the PC1. The
hockey stick shape of the bristlecones was therefore imprinted on the PC1 as
apparently  being  the  dominant  pattern  in  the  database.  Then,  during
calibration, when the PC1 was matched up against the temperature records,
the blade of the stick – the twentieth century uptick – correlated well with
the  instrumental  uptick  and  so  the  PC1  was  highly  weighted  again  in  the
reconstruction. This effectively passed the hockey stick shape onwards from
the PC1 to the final temperature reconstruction and lo and behold, no more
Medieval Warm Period.

What  made  it  all  much  more  problematic  was  that  Graybill  had  stated
that the twentieth century growth spurt in these trees had nothing to do with
temperature changes. Graybill’s co-author, Sherwood Idso, was a prominent
global warming sceptic who had been seeking evidence that any twentieth
century  increase  in  ring  widths  was  due  to  carbon  dioxide  fertilisation  –
increased growth due to higher levels of carbon dioxide. He and Graybill
had  sampled  these  particular  trees  because  they  thought  that  bristlecones
would be responding to carbon dioxide and not temperature. In other words,
it  was  going  to  be  impossible  to  use  these  trees  as  temperature  proxies
because the growth pattern was being contaminated by the effects of carbon
dioxide. As McIntyre explained:

Graybill and Idso specifically stated that the 20th century growth in these sites were
not accounted for by local or regional temperature and hypothesised that these trees . .
. contained signals of direct 20th century CO2 fertilization.52

Indeed,  even  the  Hockey  Team  had  agreed  that  the  bristlecones  were  not
indicators  for  temperature.  In  an  article  in  2003,  Malcolm  Hughes  had
described  their  twentieth  century  growth  spurt  as  ‘a  mystery’,53  and  the
Team had apparently gone on to adjust for the effect in the second Hockey
Stick paper, MBH99.

There was more though. You will rememberd that McIntyre had found a
directory on Mann’s FTP site called BACKTO_1400-CENSORED. In this, he had
discovered  that  Mann  had  created  a  new  set  of  NOAMER  calculations:  a
sensitivity analysis that excluded certain proxy series. Now he was finally
able  to  see  the  significance  of  the  data  in  this  directory.  In  their
resubmission  letter,  he  and  McKitrick  explained  to  the  editors  and
reviewers, just how important the sensitivity analysis was:

We also analyzed the ‘censored’ version of the NOAMER PC calculations at the MBH98
FTP site to determine which sites were excluded (finding out in the process that the PC1
was virtually identical to our own calculations). We found that 19 of the 20 sites so
‘censored’ were Graybill sites and that all of the above 16 sites were so censored. We
believe that this analysis sheds a great deal of light on the critical  NOAMER  PC1 and
also on how much significance can be placed on RE and other verification statistics,
and have added a discussion of this effect.52

In other words, Mann had created a revised  NOAMER PC calculation which
excluded all sixteen of the Graybill sites listed in Table 5.1 (together with a
handful  of  others).  By  doing  this,  Mann  would  have  removed  the  few
hockey stick shaped records from the database. The rest of his data series,
however, amounted to little more than noise, which meant that they would
not  be  picked  up  by  the  short-centring  algorithm.  As  a  result  the  hockey
stick  shape  disappeared  from  the  NOAMER  PC1  and  in  turn  from  the  final
temperature  reconstruction.  So  Mann’s  revised  reconstruction  would  have
had  a  new  PC1  looking  just  like  McIntyre’s,  with  an  elevated  Medieval
Warm Period. But he hadn’t reported these findings in the paper. McIntyre’s
discovery demonstrated conclusively that the Hockey Team had been aware
that their result – of flat fifteenth century temperatures – depended on just
20  mostly  related  series  out  of  the  400-odd  that  were  fed  into  the
calculations. And these were series that were known to be contaminated by
carbon dioxide fertilisation as well. This was, to say the least, at odds with
the statement they made in MBH98:

On  the  other  hand,  the  long-term  trend  in  [the  Northern  Hemisphere]  is  relatively
robust  to  the  inclusion  of  dendroclimatic  indicators  in  the  network,  suggesting  that
potential tree growth trend biases are not influential.14

In other words, Mann had claimed that you could take out any trees you
liked  and  it  wouldn’t  make  much  difference  to  the  reconstruction.  It  was

now clear that this was not the case.
Mann’s reply: PCs
Much of the rest of Mann’s argument was an attempt to defend his use of
short-centred  standardisation.  In  it,  he  provided  a  recalculation  of  the
Hockey Stick in which all of the series in the NOAMER PC1 were artificially
given  an  equal  weight.  In  essence  he  simply  bypassed  the  PC  calculation
entirely and fed all the NOAMER series one by one into the calibration. This,
he showed, still gave hockey stick shaped results, with no Medieval Warm
Period.  However,  even  in  the  new  scenario,  the  carbon  dioxide-induced
spurt in bristlecone ring widths was still going to match up well against the
temperature records and so would dominate the final reconstruction. So the
hockey stick shape would still be imprinted on the final reconstruction even
though this shape was not due to temperature changes. This wasn’t the only
problem  with  Mann’s  proposal.  As  McIntyre  pointed  out  in  his  reply  to
Nature,  what  Mann  had  done  merely  changed  a  situation  in  which  the
Graybill  sites  were  two  out  of  22  series  used  to  reconstruct  the  early
fifteenth  century,  to  one  in  which  they  were  20  out  of  95.  The  PC
methodology  had  been 
to  prevent  particular
geographical  areas  being  overrepresented.  If  he  was  to  discard  the
procedure in this way, Mann would also be throwing out one of the putative
strengths of his original study.
Peer review
The comments of the two external peer reviewers were of course much less
adversarial.54 Both were anonymous, as is normal for peer review, although
they  both  inadvertently  left  clues  to  who  they  were,  and  over  subsequent
years  their  identities  have  been  established  with  some  certainty.  In  his
comments,  the  first  reviewer  had  identified  himself  as  an  expert  on  PC
analysis,  suggesting  that  the  Nature  editors  had  taken  up  McIntyre’s
suggestion that they use someone with expertise in this area. It was widely
assumed afterwards that he was Professor Ian Jolliffe, emeritus professor of
statistics at the University of Aberdeen, and a man who could fairly claim
to be one of the world’s leading experts on PC analysis. In fact Jolliffe has
since publicly acknowledged that this supposition is correct.

introduced 

in  order 

From  his  comments,  Jolliffe  appeared  to  be  favourably  disposed  to
McIntyre and McKitrick’s arguments although he was clearly being even-

handed. His initial comments had certainly been complimentary to the two
Canadians:

I  find  merit  in  the  arguments  of  both  protagonists,  though  [Mann]  is  much  more
difficult  to  read  than  McIntyre  and  McKitrick  .  .  .  [Mann  and  his  colleagues’]
explanations are (at least superficially) less clear and they cram too many things onto
the same diagram.54

On  the  subject  of  the  PC  procedures  though,  he  had  been  somewhat
bemused.

[PC analysis] is an area where I have expertise . . . [I am] uneasy about applying a
standardisation based on a small segment of the series to the whole series, if that is
what is being done.54

It  was,  of  course,  exactly  what  had  been  done,  and  McIntyre  took  the
opportunity  to  confirm  this  to  Jolliffe  in  the  covering  letter  which
accompanied his response. Jolliffe had also criticised Mann and his team for
attempting to reject McIntyre’s work on the basis of the RE statistic, and for
dismissing the red noise simulations. Surely, he wondered, Mann must have
been slightly concerned that the algorithm produced hockey sticks from red
noise when none would have been expected, regardless of whether it was
precisely the correct statistical model?

Jolliffe had gone on to address some of the issues over the quality of the

data in the early periods:

I am not qualified to say much on [data quality] but it seems to be the crucial point.
Both sets of authors agree that the omission of some [medieval] data changes the early
reconstruction  considerably.  [Mann  et  al]  say  that  the  omitted  data  are  reliable;
[McIntyre  and  McKitrick]  say  they  are  not.  Does  anyone  know  who  is  correct?  If
there  is  disagreement  among  experts,  then  the  true  behaviour  of  the  series  must  be
very uncertain.54

This  was  certainly  something  that  McIntyre  and  McKitrick  could  agree
with.

The second reviewer was Eduardo Zorita, the head of the department of
paleoclimate at the Institute for Coastal Research in Geesthacht, Germany,
and the scientist whom Mann had indicated had independently implemented
the  Hockey  Stick  methodology.  But  while  he  was  familiar  with  Mann’s
work, Zorita had never worked with Mann and was not associated with the

Hockey Team in any way. In many ways he was an ideal candidate as the
second peer reviewer.

Zorita had also had difficulties in assessing the paper and explained that
to  do  so  really  required  an  in-depth  look  at  the  data  and  code,  perhaps
unaware that Mann was refusing access to the code and that there was no
definitive  description  of  the  dataset.  But  his  overall  impression  was
favourable:

In general terms I found the criticisms raised by McIntyre and McKitrick worthy of
being  taken  seriously.  They  have  made  an  in-depth  analysis  of  the  MBH98
reconstructions  and  they  have  found  several  technical  errors  that  are  only  partially
addressed in the reply by Mann et al.54

In the rest of his comments, Zorita made relatively minor criticisms of both
McIntyre  and  McKitrick  and  of  Mann  and  his  team,  and  suggested  a
number of amendments that he thought should be made to the paper before
publication. In essence he was saying that the answer to the question of who
was right would only be reached by the scientific community examining the
arguments  of  both  sides  in  more  detail,  something  that  a  peer  reviewer
couldn’t  be  expected  to  do.  Providing  some  amendments  were  made,
publication of McIntyre’s comment should go ahead alongside a reply from
Mann.
Nature problems
With the amendments demanded by the peer reviewers all incorporated and
the  manuscript  safely  resubmitted,55  McIntyre  might  have  expected  a
trouble-free  path  to  publication,  but  as  was  starting  to  be  a  habit,  the
Hockey Stick had a way of screwing things up. On 26 March 2004, Nature
editor, Rosalind Cotter, emailed McIntyre out of the blue, asking for further
changes to be made to the manuscript.

Before we can proceed further, I am afraid that it will be necessary for you to shorten
your manuscript substantially, in accordance with our author guidelines . . . You will
have seen that submissions to this section of the journal have a strict length limit (up
to  800  words,  with  one  multipanelled  figure  and  no  more  than  15  references),
although you are welcome to include supplementary material for reviewing purposes
only.56

This was strange indeed. The manuscript had been submitted once already
and  had  passed  muster  on  its  word  count  at  that  point.  What  could  have

possessed  Nature  to  suddenly  declare  the  manuscript  was  too  long?
McIntyre’s paper was of pressing scientific and political importance and it
is hard to credit the idea that Cotter can have been unaware of this fact. It
would  be  hard  not  to  start  to  become  a  little  paranoid  in  the  face  of  this
sudden volte-face. However, there was little alternative but to comply and
without  further  ado,  McIntyre  and  McKitrick  set  about  the  almost
impossible task of distilling their paper down to just 800 words.

The revised paper, which was submitted just two weeks later, was terse
to the point of bluntness, with all erudite discussion cast aside in favour of
direct  presentation  of  the  facts,  but  somehow  they  managed  to  scrape  in
under  the  word  limit  with  their  central  arguments  –  short  centring,
bristlecones  and  Gaspé  –  all  intact.57  It  may  not  have  been  the  most
elegantly  worded  paper  in  history  but  it  remained  one  of  the  most
important. Now, surely, they had crossed the final hurdle.

A  month  passed  without  any  word  from  Cotter  beyond  an  automated
acknowledgement  of  the  revised  submission.  May  likewise  passed  by
without a word from Nature. Mann was busy, meanwhile, publishing a new
paper which said that McIntyre’s findings in MM03 should be ‘dismissed as
spurious’.58  He  also  cited  another  paper  he  was  to  publish  shortly  in
Climatic  Change,  which  he  said  demonstrated  that  there  were  ‘critical
flaws’  in  McIntyre’s  work.  Mann  was  clearly  determined  to  keep  the
pressure up.

By  mid-June  McIntyre  was  becoming  increasingly  frustrated  with  the
lack  of  progress  and  emailed  Nature  again  to  enquire  after  the  paper’s
whereabouts.  In  the  wake  of  Mann’s  new  paper,  he  had  been  receiving
questions from a number of media outlets, including New Scientist, asking
after the Nature submission. The problem was that while the paper was still
going through the publication process, he was unable to discuss its contents
with anyone.

When Cotter finally provided a few words of explanation, she said that
not only had the journal had to get another comment from Mann but they
had felt obliged to add a third reviewer, because McIntyre had raised the
issue of the suitability of tree ring data in the revised submission. This was
indeed  the  case  –  it  had  been  suggested  by  the  peer  reviewers  in  their
comments.  She  hoped,  however,  that  matters  would  be  resolved  in  short
order.59

than  110  days.  Cotter 

Three more weeks passed and another McIntyre email was fired across to
Cotter, wondering how the review of a single 145-word paragraph (which is
all  the  discussion  of  data  quality  that  had  been  added)  could  have  taken
more 
remained  apologetic  but  otherwise
unresponsive. It would all have to wait until the reviewers reported back.
They had only received, she said, one set of comments so far.
Rejection
The coup de grace was delivered ten days later, when McIntyre received an
email from Nature.

Thank you for your revised comment on the contribution by Mann et al., which I am
afraid we must decline to publish. As is our policy on these occasions, we showed
your revised comment to the earlier authors, and their response is enclosed. We also
sent the exchange to 3 referees, whose comments are attached.
     In the light of this detailed advice, we have regretfully decided that publication of
this  debate  in  our  Brief  Communications  Arising  section  is  not  justified.  This  is
principally  because  the  discussion  cannot  be  condensed  into  our  500-word/1  figure
formatf  (as  you  probably  realise,  supplementary  information  is  only  for  review
purposes because Brief Communications Arising are published online) and relies on
technicalities that do not bring a clear resolution of the underlying issues.60

Perhaps  this  rejection  should  have  been  seen  as  inevitable,  with  all  the
delays and obfuscation from the Nature editors. It may also be enlightening
to compare McIntyre’s treatment at the hands of the journal to the way they
had dealt with Huang’s borehole study a few years earlier.g

letter,  did  not  meet 

the  standards  for  publication 

Mann was still holding nothing back. McIntyre’s comment, he said in his
covering 
in  a
‘Communication  Arising’.  Their  work  was  ‘specious’  and  ‘spurious’  and
their claims were ‘false’.61 As McIntyre and McKitrick looked through the
review comments it was easy to identify Jolliffe and Zorita again.60 Jolliffe
was  still  striving  to  be  even-handed  but  he  was  struggling  with  Mann’s
rhetorical style.

I  started  my  original  review  by  saying  that  I  found  merit  in  the  arguments  of  both
[Mann et al and those of McIntyre and McKitrick]. To rewrite this, I believe that some
of the criticisms raised by each group of the other’s work are valid, but not all. I am
particularly  unimpressed  by  [Mann’s]  style  of  ‘shouting  louder  and  longer  so  [he]
must be right’.60

Jolliffe  agreed  that  Mann-style  standardisation  was  not  PC  analysis,  but
didn’t want to venture an opinion on whether it was a useful technique or
not without having worked through the calculations himself. Neither would
he  be  drawn  on  a  claim  by  Mann  that  he  had  been  unable  to  reproduce
McIntyre’s  ‘hockey  stick  from  red  noise’  experiment.  Without  seeing  the
code and testing it for himself, Jolliffe couldn’t tell who was right, and this
was  something  that  a  peer  reviewer  just  didn’t  have  time  to  do.  He
concluded:

Regarding publication, I think it is all or nothing. Either you publish neither, or both.
In the latter case, the main thing that would be achieved is to highlight that a serious
disagreement  exists.  Only  a  reader  with  several  days  to  spare  (longer  if  they  are
unfamiliar with the area), to chase references and probably the authors, could hope to
come close to a full understanding of the arguments.60

Zorita’s  position  had,  meanwhile,  shifted  away  from  McIntyre  and
McKitrick,  as  he  felt  the  manuscript  had  weakened  ‘considerably’.  The
focus  of  his  concerns  was  the  fact  that  both  Mann’s  reconstruction  and
McIntyre’s  corrected  version  of  it  scored  poorly  on  their  verification
statistics:

[A] reader of these manuscripts will be led to think that both reconstructions are not
trustworthy . . . This . . . conclusion seems to me rather weak for a manuscript.60

In some ways this rather missed the point, as this was exactly the argument
that McIntyre was trying to make. He had never claimed to be making an
alternative  reconstruction  –  he  was  merely  demonstrating  that  Mann’s
wasn’t robust. That the verification statistics were poor when the data was
properly centred didn’t in any way detract from the case that they were poor
when  short  centring  was  used.  However,  as  a  climatologist,  Zorita  was
familiar  with  the  RE  statistic,  which  was  much  used  in  the  field,  and  he
declared  that  McIntyre  required  strong  justification  for  preferring  R2.  Of
course, with the absurd word limit now imposed by Nature, there was no
room  for  this  kind  of  discussion  and  with  the  paper  already  having  been
rejected,  the  point  was  moot  anyway.  As  we  will  see  in  later  chapters,
however, the reviewers’ comments on the verification statistics opened up
another extraordinary line of enquiry.

Concluding, Zorita said

In summary, judging from the present version of the manuscript and the response by
[Mann],  I  now  think  that  basis  for  [the  critique  of  McIntyre  and  McKitrick]  has
wavered and that further work, or further convincing evidence, would be needed to
present a more solid case.60

The new reviewer had some strong opinions, and indeed said that he felt
McIntyre and McKitrick had some preconceived notions that affected their
audit,  although  he  didn’t  explain  what  these  were.  He  did,  however,  note
that  this  didn’t  mean  they  were  wrong.  His  conclusions  were  generally
unfavourable:

Generally, I believe that the technical issues addressed in the comment and the reply
are quite difficult to understand and not necessarily of interest to the wide readership
of  the  Brief  Communications  section  of  Nature.  I  do  not  see  a  way  to  make  this
communication  much  clearer,  particularly  with  the  space  requirements,  as  this
comment is largely related to technical details.60

As McIntyre noted, it was odd that the readers of a leading scientific journal
might not be interested in technical details but observations like this would
get him nowhere. The Nature paper was finished.
Materials complaint
At the same time as the Communication Arising was crawling through the
submission process at Nature, McIntyre was also having to deal with Nature
on another front. With Mann’s blunt refusal to release his computer code
still ringing in their ears, he and McKitrick had little choice but to try other
means: they were going to have to approach Nature, who had published the
original article, and ask them to obtain the data on their behalf.

Many scientific journals have a policy on data archiving, either requiring
authors to make data and methods information available to interested third
parties on request, or to place it in a public archive prior to publication. In
some  of  the  more  politicised  social  sciences  like  econometrics,  even
stronger measures are in place, with authors routinely required to submit all
of  their  data  and  code  with  the  manuscript  they  are  submitting  for
publication.

Nature’s policy at the time was for authors to make data and materials
available  to  interested  parties  on  request.  In  theory  at  least,  if  an  author
failed  to  make  the  requested  information  available,  the  journal  could
withdraw  the  article,  although  this  was  unlikely  to  happen  in  practice.

However,  Nature  still  had  strong  powers  of  moral  force  as  well  as  a
contracted right to the data and code, so it was reasonable to think that they
might  be  able  to  extract  the  information  McIntyre  needed.  With  this  in
mind, McIntyre and McKitrick penned a joint letter to Nature, telling the
whole sorry story of their work on MBH98: the truncations, the duplicate use
of  series,  the  obsolete  data,  the  discrepancies  between  what  Mann  had
actually done and what had been reported in the paper, and Mann’s repeated
refusals  to  make  data  and  code  available.  They  also  refused  to  shy  away
from the issue of the file deletions from Mann’s websites, calling it ‘very
disquieting’. Concluding the complaint, they stated:

Under the circumstances, we believe that the full data set and accompanying programs
for MBH98 should now be included in the Nature Supplementary Information, along
with an accounting of any discrepancies between what has been listed at nature.com to
date and what was actually used in MBH98.62

In early December, the editorial staff at Nature replied in positive manner:

. . . we have already been in touch with Professor Mann’s group, who have indicated
their willingness to supply us with the various materials pertaining to your complaint.
Once we have these in hand, we intend to seek external independent advice on the
issues that you raise; and on the basis of such advice, we will decide on any actions
that need to be taken.63

The second materials complaint
This was immensely encouraging, and so, in a follow-up request, McIntyre
and McKitrick decided to spell out in more detail exactly what it was they
were  after.64  They  also  took  the  opportunity  to  ask  for  the  disclosure  of
futher information relating to the calculation of the confidence intervals R2
and RE statistics.

Again,  a  prompt  reply  was  returned  by  Nature  and  it  was  just  as

encouraging:

We are putting the points that you raise here to Professor Mann (as we did with those
from your original communication) and will await his response. I hope that you will
understand that, given both the seriousness of your concerns and the time of the year
(our office being closed for several days over [Christmas]), it may take us longer than
normal to bring this matter to a conclusion. But we are nevertheless anxious to do so,
and I hope that you will bear with us.65

The Hockey Team’s purposes
As February drew to a close, the long-awaited reply from Mann and his co-
authors  arrived  at  the  London  offices  of  Nature,  and  was  forwarded  to
McIntyre  by  Heike  Langenburg,  the  editor  responsible  for  handling  the
complaint.  As  might  have  been  expected,  it  conceded  very  little  and  was
aggressive in its defence of the Hockey Stick.66

McIntyre  and  McKitrick  had  raised  several  new  issues  that  they  had
discovered  since  the  publication  of  the  MM03,  some  of  which  were  rather
surprising. During their examination of Mann’s  FTP site, the two men had
spent a considerable time trawling through the directories trying to discover
which tree ring series had been used as inputs into which  PC  calculations
and  in  which  periods.  The  problem  was  that  there  were  over  400  series
archived on the site and these didn’t match the series that Mann claimed he
had used originally. For example, when McIntyre looked at the series for
the  South  America  PC  calculation,  he  discovered  that  there  were  18  sites
listed in the Supplementary Information for MBH98 but only eleven appeared
at the FTP site – and these were what Mann now said had actually been used.
It was incredibly frustrating. However, while they were struggling to match
claim with reality, they had, during their searches, chanced upon the text of
an  unusual  email,  inadvertently  saved  among  a  mass  of  data  files.  It  was
written by Mann’s co-author Malcolm Hughes and was addressed to Mann
himself. (The email is reproduced below, with emphasis added.)

Mike – the only one of the new S.American chronologies I just sent you that already
appears in the ITRDB sets you already have is [ARGE030]. You should remove this from
the two  ITRDB data sets, as the new version should be different (and  better  for  our
purposes).
Cheers,
Malcolm

It  was  possible  that  there  was  an  innocent  explanation  for  the  use  of  the
expression ‘better for our purposes’, but McIntyre can hardly be blamed for
wondering  exactly  what  ‘purposes’  the  Hockey  Stick  authors  were
pursuing.  A  cynic  might  be  concerned  that  the  phrase  actually  had
something  to  do  with  ‘getting  rid  of  the  Medieval  Warm  Period’.  And  if
Hughes meant ‘more reliable’, why hadn’t he just said so? By any stretch of
the imagination, it was a strange choice of words.

The  existence  of  the  email  was  too  important  to  withhold  from  the
journal, even if McIntyre had felt that there was no nefarious intent. He had

they  had 

then  been 

inadvertently 

included 

in 

therefore concluded his remarks to Nature on the data issues by saying that
there was ‘evidence of intentional exclusion of a disclosed South American
site’.

In his reply to Nature, Mann was apparently outraged by the suggestion
that  the  two  records  had  been  swapped  for  anything  other  than  valid
scientific  reasons.  It  was,  he  said,  ‘distasteful’  and  ‘deeply  offensive’.66
What  had  happened,  he  explained,  was  that  Hughes  had  been  using  a
screening process to weed out proxy series that weren’t of adequate quality.
This  process  involved  looking  at  the  mean  segment  length  (the  average
number of rings in the series – remember that a series will be an average of
many  trees)  and  replication  of  the  chronologies  (whether  the  trees  on  the
site were all telling the same story) as well as some other criteria. The series
involved,  he  claimed,  had  simply  been  excluded  on  this  basis,  but
unfortunately, 
the
Supplementary  Information,  thus  leading  to  the  confusion  over  what  was
actually used in the final calculations. Mann also pointed to a subsequent
paper he had written, which discussed the screening process and its use in
MBH98.

Mann’s  explanation  did  rather  concede  the  point  –  namely  that  the
Supplementary Information and the FTP site didn’t match up – but it didn’t
really  explain  series  ARGE030.  In  the  email,  Hughes  had  said  that  Mann
should remove the chronology from the database because the revised one
should  be  ‘different’  and  better  for  their  ‘purposes’.  But  when  a  new
version  of  ARGE030  was  received,  it  might  have  been  expected  that  they
would have included it as a matter of course, the newer data presumably
being more up-to-date? In fact, though, it appeared likely that they didn’t do
this as a matter of course since, as we have seen, McIntyre’s audit of the
MBH98 database had uncovered many series that were obsolete. But if they
had  been  screening  new  series  using  the  method  Mann  described,  why
wouldn’t Hughes have told Mann to remove the old data because the new
data was better, full stop? Why tell him to do it because the new data was
‘different’  and  better  for  their  ‘purposes’?  In  the  event,  the  series  didn’t
actually  seem  to  have  been  used  in  the  final  calculations,  despite  Mann
having  listed  it  in  the  MBH98  Supplementary  Information  as  having  been
included. It was all very strange.

Regardless of these apparent weaknesses in Mann’s story, McIntyre and
McKitrick  set  about  examining  Mann’s  quality  control  procedures  and

testing  to  see  how  they  would  apply  to  the  MBH98  data  series  in  practice.
Assuming the process was valid (and this appeared reasonably likely, given
Mann’s reference to a discussion of it in another paper) McIntyre expected
to  be  able  to  reproduce  the  discrepancies  between  the  FTP  site  and  the
Supplementary  Information.  The  differences  should  be  only  those  series
which failed the screening tests.

By  now,  readers  will  probably  not  be  surprised  that  the  screening  test
didn’t  appear  to  explain  the  discrepancies  at  all.  For  example,  one  of
Mann’s tests was that the series should have commenced by the year 1626.
But of the series in the FTP site, there were no less than 39 which didn’t pass
this test. By 1680, a series had to have at least 8 trees in it to be considered
valid;  22  series  failed  this  test.  171  sites  that  had  gone  into  the  final
database  failed  a  test  of  the  minimum  correlation  between  the  individual
trees  and  the  site  average.  In  fact,  one  of  these  sites  failed  the  test  so
spectacularly  that  McIntyre  emailed  the  author  of  the  original  study,
Professor  Rosanne  D’Arrigo,  who  discovered 
the  wrong  site
chronology  had  been  archived.  If  Mann  had  actually  applied  the  tests  he
claimed to have used, McIntyre asked, how was this failure not picked up
earlier?
A wild guess
There  had  been  a  second  Hughes  email  too,  this  time  relating  to  the
Vaganov  PCs,h  a  set  of  tree  ring  chronologies  from  Siberia.  In  the  email,
Hughes had explained to Mann a little bit about the format of the data and
some of its oddities, before going on to discuss which of the records he felt
they should use. He listed four that he felt should be excluded straight off,
as they were better covered by another series already in their possession.
His next statement was, however, rather odd.

that 

Now, we do not know what the internal replication is, but, at a wild guess, I would
hope that series starting by 1570 should be reasonably replicated by 1625. I would
include these, and their file numbers are: 26, 6, 31, 32, 41, 10, 11, 12, 15, 42, 44, 46,
47, 49, 50, 51, 52, 53, 16, 17, 22, 23, 25, 55, 56, 57, 59, 61.
For the present (1625 on) exercise I would exclude all the others. Cheers,
Malcolm

The  problem  with  this  was  that  a  ‘wild  guess’  is  not  a  scientific  way  of
deciding if a series should be included or not. What was even more strange,
the series that he said should be excluded didn’t seem to have been deleted

anyway.  Again,  there  may  have  been  less  to  this  than  met  the  eye,  but
together with the earlier email, it did start to present a somewhat alarming
picture  of  the  way  the  study  had  been  put  together.  Mann  had  little  of
substance to say in his defence, beyond claiming that the procedure was as
objective  and  rigorous  as  possible,  and  accusing  McIntyre  of  quoting
Hughes  out  of  context  (although  McIntyre  had  reproduced  the  email  in
full).
Truncations
Mann’s tactic in defending the truncations of the proxy series was to fire off
an  aggressive  denial,  but  then  effectively  to  concede  the  point,  while
explaining it away. He said that each of McIntyre’s claims was ‘either false
or  disingenuous’  and  the  accusations  were  ‘distasteful’.  For  example,
McIntyre  had  complained  that  the  Central  England  Temperature  Record
(which, you may remember, was used as one of the 112, or perhaps 159,
proxy series), was deleted for its first 70 years without notice to the reader.i
In his response, Mann adopted the politician’s trick of ignoring the actual
accusation  and  mounting  a  defence  against  another  charge  altogether:  he
subtly  changed  the  wording  of  McIntyre’s  points,  suggesting  that  the
accusation was one of unjustifiably eliminating this data and then merely set
about  justifying  it.  Once  this  is  seen,  it  is  clear  that  he  was  effectively
conceding the point – he didn’t deny that the data was deleted, or that he
had  not  provided  notice  of  this  fact,  but  tried  to  rationalise  it  away  by
saying that other scientists did the same thing.
Duplicate versions
McIntyre and McKitrick had pointed out to Nature the fact that many of the
series were used more than once in MBH98, among them Gaspé, which as we
have seen had also been extrapolated to allow its inclusion in the AD 1400
proxy roster. In their complaint, the two men had provided an appendix in
which they listed all the series used more than once, pointing out that Mann
and his team should have explained why they did this (if indeed there was a
rational  explanation).  The  response  from  the  Hockey  Stick  authors  was
blunt:  ‘this  claim  is  incorrect  with  one  exception’.  That  exception  was
Gaspé.  Mann  went  on  to  say  that  if  you  left  Gaspé  out  of  the  North
American PC series, it made no material difference to the final results. Alert
readers may wonder though what would happen if the other copy of Gaspé,

the one used as a single series, were removed instead. This is a story that
will be told in a later chapter.
Obsolete data
McIntyre had claimed that many data versions used were obsolete when the
paper was published, an accusation that Mann in his response called absurd.

We  listed  the  specific  data  used  by  us  (albeit  with  some  typos,  and  incorrect
references, as noted) in the supplementary information, and provided all of the data on
our data site. We did not indicate there, or elsewhere, that all of the tree ring data used
were available in the NOAA databank.

Again, he was denying something slightly different to the actual accusation.
The  specific  allegation  McIntyre  had  made  was  that  the  versions  were
obsolete.  Mann  was  claiming  that  the  versions  he  had  received  were
donated by fellow researchers and therefore not to be found in an archive.
This  was  a  classic  rhetorical  sleight  of  hand  by  Mann.  McIntyre  had  not
said that the proxy versions Mann had used were not in the archive, he was
saying that they were in the archive but that they were obsolete: there were
more up-to-date versions available.
Principal components
With  the  new  paper  still  wending  its  way  through  Nature’s  peer  review
process at the time, McIntyre and McKitrick had to be a little careful what
they said about the problems with Mann’s use of PC analysis. It was enough
to point out that, contrary to what Mann had said in MBH98, the PC analysis
was  not  ‘conventional’.  Mann  had  not  only  used  an  unconventional  tree
ring  data  standardisation  –  the  short  centring  discussed  earlier  –  but  had
also done something odd when he dealt with the temperature PCs: the raw
data series had many gaps in the record and so conventional PC calculations
should  have  ‘fallen  over’.  Mann  would  had  to  have  done  something  to
overcome  this  problem,  but  had  not  dislosed  either  the  existence  of  the
problem or the steps he had taken to overcome it.

Once  again,  Mann  responded  to  McIntyre’s  claims  with  bravado  and
counter-accusation  while  tacitly  conceding  the  point.  With  respect  to  the
missing  data  in  the  temperature  records,  he  said  that  McIntyre  and
McKitrick had made a fundamental mistake in using a different version of
the temperature data to that used in MBH98. With this fighting talk out of the
way, he went on to explain how the missing data had been infilled: he had

interpolated from known to missing data points, thus confirming that there
had been a genuine problem and a hitherto undisclosed procedure.

When it came to the tree rings, Mann dismissed the whole of McIntyre’s
claims  as  incorrect,  and  now,  almost  predictably,  went  on  to  answer  a
different point to that originally made. In response to McIntyre’s claim that
there  had  been  an  unconventional  standardisation  of  the  data  (short
centring, in other words) he said that McIntyre and McKitrick had failed to
implement the stepwise procedure. This, as we’ve seen is a whole different
ballgame and one which represents a story in its own right, but it was no
defence to an accusation of standardising the data in an invalid way.
Draft Corrigendum
While Mann had made fighting defences of his work, the editors at Nature
seemed  to  have  their  doubts  about  what  he  was  saying:  at  the  end  of
February  Langenburg  emailed  to  say  that  the  journal  was  going  to  ask
Mann, Bradley and Hughes to issue a corrigendum.67 A corrigendum is a
published correction to a scientific paper, which is required when serious
errors  are  uncovered.  Nature’s  publishing  policies  explained  that  it  was
‘notification  of  an  important  error  made  by  the  authors  that  affects  the
publication record or the scientific integrity of the paper, or the reputation
of the authors or the journal’ (Nature’s emphasis). Clearly then, this would
necessitate a major admission by Mann and his colleagues that there were
indeed serious flaws in their papers. Or would it? Only time would tell.

In the meantime, McIntyre and McKitrick realised that being given space
in the pages of Nature, even for a corrigendum, would give Mann and his
team the opportunity to fire some more shots in the ongoing war of words.
McIntyre therefore sent off an email to Langenburg seeking assurances that
this  would  not  be  permitted,  and  she  quickly  confirmed  that  this  would
indeed be the case. Here at least was one concern put to rest.

Towards the middle of March, the editors at Nature sent McIntyre and
McKitrick the draft text of the Corrigendum. When McIntyre cast his eye
over  the  wording  though,  he  immediately  realised  that  this  was  not  the
capitulation they might have hoped it would be. It failed to address many of
the  errors  uncovered  by  McIntyre  and  although  it  was  exceedingly  short,
still managed to include a whole host of new mistakes.68

As  well  as  publishing  the  Corrigendum,  Mann  was  to  prepare  a  new
Supplementary Information website to accompany it. When he discovered

this, McIntyre immediately requested that he be able to view its contents,
but  was  swiftly  rebuffed  by  Nature,  who  declared  that  they  did  not  edit
supplementary  information.  This  was  something  of  a  surprise  as,  on  the
surface,  it  seemed  to  contradict  their  declared  policy  of  including
supplementary information in the peer review process.

McIntyre’s  biggest  concern  with  the  Corrigendum  itself  was  what  was
missing  from  it.  Anyone  reading  it  would  have  come  away  with  the
impression that there were a few issues with data citations and not much
else. For example, there was no explanation of why the original paper had
claimed  that  112  series  were  used  when  the  real  number  had  apparently
been 159. There was no mention of the use of decentred PCs and there was
not a word about the stepwise application of  PC analysis either. As far as
Nature’s  publication  record  was  concerned  (and  their  policies  suggested
they  were  keen  to  protect  this),  Mann  had  used  ‘conventional  principal
components analysis’, the explanation given in the original paper, despite
both Mann and McIntyre having agreed that a nonstandard stepwise process
had been used.

Many  of  the  data  collation  errors  that  McIntyre  had  uncovered  were
simply ignored in Mann’s Corrigendum. As far as Mann and Nature were
concerned the rain in Maine was still falling in the Seine – the geographical
errors  in  the  precipitation  series  remained  uncorrected.  Even  simple  data
citation errors went only half-fixed, with a vague reference to an alternative
location  given  the  nod  by  Nature  as  an  adequate  response.  And  Mann’s
approach to explaining the infilling and truncation of tree ring series were
terse statements that left the reader unaware of exactly what had been done.
For example, on the Gaspé series, where the years from 1400–03 had been
infilled by copying the value from 1404 back into the earlier years, Mann’s
simply said that:

For  one  of  the  12  ‘Northern  Treeline’  records  of  Jacoby  et  al.  used  in  ref.  1  (the
[Gaspé] series), the values used for AD 1400–03 were equal to the value for the first
available year (AD 1404).69

There  was  absolutely  no  indication  of  the  importance  of  this  seemingly
trivial adjustment. As we’ve seen before, this allowed Gaspé, a series with a
dramatic  hockey  stick  shape,  to  be  used  in  the  early  years  of  the
reconstruction. For such an important change Mann should, by rights, have
discussed the impact and the reasoning for it.

For the deletion of the first 25 years of the Central Europe temperature

series, Mann’s Corrigendum explanation was merely to state:

The start year for the ‘Central Europe’ series of ref. 1 is AD 1525.69

So once again the reader was left in the dark as to the fact that values prior
to the year 1550 had been removed, let alone the reasons for doing so or the
effect on the final reconstruction. Meanwhile the similar truncation in the
Central England Temperature Record wasn’t mentioned at all.

All these failings and many others were outlined in a long email which
McIntyre sent off to Nature. As he pointed out, unless all the issues were
resolved satisfactorily, nobody would be able to replicate what Mann had
done. He presented a list of all the information that Mann needed to file on
the new Supplementary Information website in order that this fundamental
step  in  the  scientific  process  could  be  met.  This  included  the  series
identities, the results of the screening tests, the number of PCs retained from
each  PC  calculation,  details  of  the  stepwise  PC  calculations,  the  actual
temperature  series  used  for  the  temperature  PCs  and  above  all  the  actual
computer code used. With this last piece of information, a huge amount of
misunderstanding and bickering would be avoided, because anyone trying
to replicate Mann’s work would be able to see exactly what he had done.
Path to issuing of the Corrigendum
Once again, Nature’s reaction was not unfavourable and they agreed to take
the  Corrigendum  out  of  production  while  McIntyre’s  criticisms  were
digested. However as emails were exchanged over the next few weeks, it
became  clear  that  Nature  were  backtracking  somewhat.  On  26  March
Langenburg emailed again with an amazing set of statements. McIntyre’s
criticisms of the misleading claims Mann had made on PC analysis – namely
that they were ‘conventional’ – were ruled out of order. The consistency of
the  methods  used,  she  said,  was  ‘not  the  subject  of  a  corrigendum’.  She
went on:

You also make a number of additional comments to the Corrigendum, but for reasons
of space constraints, we insist that such publications are as concise as possible. We
feel that the current version, together with the Supplementary Information explicitly
listing the data sets and methods used, clearly establishes which data were used in the
paper.70

Nature  was  allowing  Mann  to  have  his  way.  The  Corrigendum  would  be
about data issues alone.

By the middle of June and with the Corrigendum still not in print, there
were some new developments. Mann had been just as busy as McIntyre and
McKitrick,  and  two  new  papers  were  working  their  way  through  the
publication  process.  The  first  of  these  was  the  submission  to  the  journal
Climatic  Change,  which  was  mentioned  previously.  McIntyre  had  been
asked  to  act  as  a  reviewer  of  this  paper  by  Climatic  Change’s  editors,
enabling him to see the content pre-publication, and he had picked up on a
reference  to  a  new  MBH98  page  on  Mann’s  website  in  the  manuscript.  It
looked as though this was the new Supplementary Information that was to
sit alongside the Corrigendum. As might have been expected, the contents
still failed to provide sufficient information to enable someone to reproduce
the  study  –  for  example  the  159  series  remained  unidentified,  the  data
citations  were  still  inadequate  and  the  computer  code  was  still  a  closely
guarded  secret.  However,  there  were  some  surprises.  There  was  a  new
description of the  PC  methodology  which  confirmed  the  short  centring  of
the tree ring series, thus vindicating the claims made in McIntyre’s Nature
submission. Given that this information didn’t appear in the text of the draft
Corrigendum, it was clear that Nature were going to allow Mann to retain
as much face as possible. The correction was to be kept out of sight in the
Supplementary Information, the journal itself remaining uncorrected.

There was another kick in the teeth for McIntyre in the Supplementary
Information:  after  explaining  the  short  centring  methodology,  Mann  had
added  the  words  ‘The  results  are  not  sensitive  to  this  step’,  citing  the
forthcoming  Climatic  Change  paper  as  evidence.  Discussing  the  PC
methodology in this way also spoke directly against McIntyre’s own Nature
submission,  which  Mann  had  seen  as  a  reviewer.  This  appeared  to  be  in
direct  contravention  of  Nature’s  policies  for  reviewers,  which  required
Mann  to  keep  McIntyre’s  paper  entirely  confidential  and  not  to  use  the
contents  for  his  own  purposes.  It  also  appeared  to  breach  Langenburg’s
undertaking that Mann would not be permitted to use the Corrigendum as
an  opportunity  to  attack  McIntyre.  McIntyre  immediately  issued  a
complaint  to  Karl  Ziemelis,  the  physical  sciences  editor  at  Nature,  but  it
was all too late. The decision was already made.
The Corrigendum

The  Corrigendum  was  published  on  1  July  2004,  and  even  after  over  six
months of email exchanges, claim and counterclaim, there were still some
surprises.69  Nature  had  quietly  decided  to  allow  Mann  to  make  a  late
change to the Corrigendum, namely the addition of a sentence at the end
stating  ‘None  of  these  errors  affect  our  previously  published  results’.  So
Mann’s  claim  had  been  carried  forward  from  the  online  Supplementary
Information  to  the  main  body  of  the  Corrigendum  in  the  printed  journal.
McIntyre  had  shown  the  journal  that  the  claim  was  false,  a  position
confirmed by the peer reviewers’ who had accepted that the errors mattered
in  their  written  reports.  And  yet  Nature,  the  world’s  premier  scientific
journal published Mann’s claim regardless.

This was pretty outrageous, and McIntyre sent off yet another email to
Nature the same day, protesting formally about the breach of Nature’s own
policies and the undertakings made by Langenburg.

In  March  2004,  we  were  shown  page  proofs  of  the  Corrigendum,  which  did  not
contain  this  sentence.  It  appears  that  it  was  inserted  after  the  review  process  had
closed.  Like  the  above  sentence  on  the  [Supplementary  Information]  web  site,  we
have shown in our [Nature Submission] that it is untrue. By publishing it while the
Communications  Arising  is  under  embargo,  Professor  Mann  has  attempted  to  pre-
empt  our  submission  and  has  breached  the  embargo  under  which  we  continue  to
withhold our own material. This sentence is not merely incidental; it is already being
cited and circulated by Professor Bradley and perhaps others.71

In response to these points, however, Nature were not inclined to be helpful.

In  your  message,  you  also  draw  our  attention  to  some  points  concerning  the
Corrigendum published on 1 July by Mann et al. First, the published phrase ‘Mann et
al., in review’ does not constitute a break of Nature’s embargo policy because it does
not  specify  the  journal  involved;  neither  does  it  pre-empt  your  Communication
Arising  which,  in  the  event  that  it  is  accepted  for  publication,  will  be  published
alongside a reply from Mann et al.72

This  was  a  very  peculiar  statement  because  it  clearly  did  pre-empt  the
publication of McIntyre’s paper (and as we have seen, Nature managed to
avoid  publishing  that  document  anyway).  The  policy  at  issue,  which
McIntyre  had  quoted  in  his  email,  didn’t  mention  the  specification  of
journals at all. It merely stated that the comments should not be used for
any purpose other than making a response:

The responders [i.e. Mann] must keep the comment confidential and must not use it
for their own research or for any other purpose apart from replying to the comment,
nor can they distribute it without first obtaining Nature’s permission.

Yet  here  was  Mann  using  the  comment  not  only  in  the  Corrigendum  but
also apparently in the submission to Climatic Change. The only suggestion
from Nature’s editors was that McIntyre should add discussion of Mann’s
statement to his own submission, an idea which must have provided little
comfort since that paper had been in publishing limbo for over three months
at the time. McIntyre made some last despairing attempts to get Nature to
withdraw the critical sentence but his emails went unanswered. It was pretty
clear that once again, the journal stood between McIntyre and his search for
the truth. This battle had been lost.
Mann’s bulldog – an interlude
In 2008, several years after the rejection of McIntyre’s Nature submission,
there  was  another  equally  strange  attempt  to  defend  short-centred  PCs.  A
scientist  supporter  of  Mann,  known  only  as  ‘Tamino’  (although  he  also
styled himself ‘Mann’s bulldog’), had been trying to knock down the idea
that full-period centring was a critical part of PC analysis and to promote the
idea that short-centred standardisation was a valid alternative. He claimed
that the Mannian approach was supported in the statistical literature and he
invoked in his support none other than Ian Jolliffe.

Centering is the usual custom, but other choices are still valid; we can perfectly well
define PCs based on variation from any ‘origin’ rather than from the average. It fact it
has distinct advantages if the origin has particular relevance to the issue at hand. You
shouldn’t just take my word for it, but you should take the word of Ian Jolliffe, one of
the worlds foremost experts on [PC analysis], author of a seminal book on the subject.
He takes an interesting look at the centering issue in this presentation . . .73

Tamino must therefore have been completely mortified when the following
appeared on his website:

IAN  JOLLIFFE:  It  has  recently  come  to  my  notice  that  .  .  .  my  views  have  been
misrepresented, and I would therefore like to correct any wrong impression that has
been given. . . . An apology from the person who wrote the page would be nice.74

Jolliffe went to to explain that his presentation in no way supported the idea
of short centring,j and also rather surprisingly said that until the second half

of 2008 he had had no idea of exactly what Mann had done in MBH98. Up
until then he had been labouring under the misapprehension that Mann had
used  a  technique  called  uncentred  PC  analysis,  which  is  essentially  PC
analysis with no centring at all. Despite having reviewed McIntyre’s Nature
submission, he had apparently still not realised that the technique used by
Mann  was  short  centring.  He  even  went  so  far  as  to  say  he  had  doubts
whether even standard PC analysis was a suitable technique for temperature
reconstructions.

Jolliffe had killed off the idea of short centring, but this exchange didn’t
take  place  until  2008.  Back  in  2004,  McIntyre  was  to  have  a  long  fight
ahead  of  him  to  win  an  argument  that  someone  with  Jolliffe’s  authority
could have settled in minutes, if only he had noticed what Mann had done
when he reviewed McIntyre’s Nature submission.

a  See page 52.
b  See page 43.
c  See page 83.
d  See page 103.
e  See page 92.
f  The change from 800 words in Cotter’s email of 26 March to 500 as shown here is odd, as there
appears to have been no communication of a further reduction in word count to McIntyre. It may
have been a typing mistake. McIntyre has pointed, however, to a subsequent comment published
by Nature where the author was allowed 1500 words and two figures.

g  See page 29.
h  The collection is named after Eugene Vaganov, the researcher who collected the data.
i  See page 81.
j  Jolliffe referred to it as ‘decentred’.

6     Fighting Back

Universities incline wits to sophistry and affectation.

Francis Bacon

Although Nature had declined to publish McIntyre and McKitrick’s critique
of MBH98, it was clear to both sides that the debate was not over and that
they would attempt to have their work published elsewhere. There would
have  been  a  great  deal  of  cachet  in  appearing  in  Nature,  who,  as  the
publishers of the original study, should also have published the correction.
But  so  long  as  his  arguments  got  into  print  somewhere,  McIntyre  was
reasonably content.
Two papers and twelve hockey sticks
As he and McKitrick pondered how best to proceed, the idea came to them
to publish not one but two revised papers. The issues around MBH98 were so
complex that there was no shortage of material and the extra space would
allow  them  to  develop  their  arguments  as  fully  as  was  necessary.
Eventually, it was decided that they would submit one paper to Geophysical
Research  Letters  (GRL)  examining  Michael  Mann’s  incorrect  use  of  PC
analysis and the question of the verification statistics, while a longer paper,
looking at the sensitivity of Mann’s reconstructions to various changes in
the data and methods, would be submitted to Energy and Environment. To
outsiders, the outlet for the second paper was something of a surprise, as
many critics had tried to dismiss McIntyre’s work in MM03 on the fallacious
grounds that it had been published in such an obscure journal. But McIntyre
and McKitrick felt that they owed Sonia Boehmer-Christiansen a favour – it
was she who had taken a risk on publishing them in the first place and now
they  were  attracting  so  much  attention,  it  only  seemed  fair  to  give  her  a
small payback.

Work  on  the  new  papers  continued  throughout  the  summer  of  2004,
McIntyre and McKitrick developing and extending the arguments they had
presented in their Nature  submission  in  order  to  cover  all  of  the  flaws  in
Mann’s  papers.  The  new  critique  had  to  be  watertight  and  McIntyre  was
kept  busy  developing  a  whole  new  series  of  simulations  of  the  Hockey

Stick  –  firing  red-noise  at  the  short-centred  standardisation  routine  and
analysing the results. His hope was to develop a much more sophisticated
analysis of the effect of short centring on the data. With the expectation that
more space would be available to him in GRL, McIntyre wanted to explain
and  quantify  exactly  what  was  going  on.  By  the  time  he  had  performed
10,000 simulations he had some very damning evidence indeed. In fact, the
Mann algorithm managed to deliver a hockey stick from these random data
series over 99% of the time.

Towards the end of the year, McIntyre was invited to present some of his
results in a poster at the Fall Meeting of the American Geophysical Union
(AGU) in San Francisco, and part of his presentation was a graphic showing
some of these simulations. This is shown in Figure 6.1. The chart shows 12
hockey stick plots, all of which were generated using short centring. Only
one of them, however, is based on real proxy data – it was the Hockey Stick
itself – while the other eleven are generated from red noise. The game was
to  guess  which  hockey  stick  was  the  real  one,  and  of  course  none  of  the
visitors to McIntyre’s poster at the AGU could do it.
The GRL paper
The new papers were ready by the October 2004 and spent the following
winter tied up in the usual to and fro as reviewers demanded revisions and
clarifications. GRL had decided not to appoint Mann as a reviewer, so there
was to be a much easier time ahead for McIntyre and McKitrick.

As we have seen, the GRL paper (which will henceforward be referred to
as  ‘  MM05(GRL)’)  focussed  on  PCs  and  the  verification  statistics.75  The
central  theme  of  the  PC  argument  was  unchanged  –  Mann’s  novel  short-
centring procedure was biased and therefore produced hockey stick shaped
graphs.  However,  the  arguments  could  now  be  made  much  more
persuasively.  Armed  with  the  battery  of  new  simulations,  McIntyre  had
categorised  each  of  the  10,000  dummy  hockey  sticks  according  to  a  new
‘hockey stick index’. In essence the more the twentieth century portion of
the graph deviated from the long-term average, the higher its hockey stick
score.  If  the  twentieth  century  excursion  of  the  first  PC  was  less  than  the
standard deviation (which, you may remember, can be thought of as ‘how
far  the  line  normally  deviates  from  the  mean’),  then  it  was  allocated  a
hockey  stick  score  of  0.  If  the  excursion  was  more  than  one  standard
deviation it scored 1, two standard deviations scored 2 and so on.

FIGURE 6.1: Twelve hockey sticks

Using this simple methodology, McIntyre was able to demonstrate that
Mannian short centring would almost never produce anything other than a
hockey stick. In fact, three-quarters of the time it would produce a hockey
stick with a score of 1.5 – in other words a stick with a big blade.

Having  demonstrated  the  effect  of  short  centring  on  random  data,
McIntyre  went  on  to  discuss  its  effect  on  the  NOAMER  PC1.  With  short
centring, the bristlecone pines completely dominated the PC1 – in fact 93%
of the variance could be shown to be due to 15 bristlecone pine series. But
with standard centring, the bristlecones didn’t appear in the  PC1 at all, but
were relegated to the PC4 – the fourth most significant pattern in the data. In
other words short centring made the hockey stick shape of the bristlecones
appear  to  be  the  dominant  pattern  in  North  American  tree  rings.  Correct
centring  relegated  them  to  what  they  were  –  the  growth  pattern  of  an
obscure species of tree from one small area of the western USA.
Preisendorfer
The  theme  was  taken  up  again  in  the  Energy  and  Environment  paper
(henceforward ‘MM05(EE)’). In his final reply to Nature, to which McIntyre
and  McKitrick  had  been  unable  to  respond,  it  being  the  final  round  of
submissions,  Mann  had  invoked  a  new  argument  to  defend  his  short-

centring methodology. In essence he argued that whether the methodology
was biased or not was irrelevant: the final reconstruction you got with short
centring was the same as the one you got with correctly centred data. He
agreed that when correctly centred, the bristlecones remained stuck in the
PC4. In the original Hockey Stick paper, he had only used the PC1 and the PC2
of the NOAMER network in the reconstruction of the early fifteenth century.
The  PC4  was  not  carried  forward  to  the  calibration  and  hence  did  not
influence the final reconstruction. Now, however, he was claiming that the
first  five  PCs  should  be  used  and  hence  the  PC4  would  still  be  carried
forward. Once there, it didn’t matter that it was only a  PC4 that explained
only a small fraction of the total variance in the dataset. This is because, as
we  have  seen,  the  weighting  a  series  receives  in  the  final  reconstruction
depends  only  on  how  well  it  correlates  to  the  temperature  PC  during
calibration. The hockey stick shape of the bristlecones correlated well with
temperatures, so it didn’t matter to Mann which PC the hockey stick signal
was in, so long as it was one of the ones retained and carried forward to
calibration. Once in the calibration, the hockey stick shape would imprint
itself on the reconstruction.

if 

The  argument  implicit  in  Mann’s  claims  was  that  bristlecones  in  the
White  Mountains  of  California  were  somehow  picking  up  a  temperature
signal of the whole Northern Hemisphere that was absent from the rest of
his  dataset.  An  objective  observer  would  surely  question 
the
reconstruction could be considered robust and reliable if its findings stood
or fell on the inclusion of a PC4, representing one type of tree in one small
area of the USA.

The number of PCs that is retained from a PC calculation is not something
that is set in stone. As we saw in Chapter 2a it depends very much on the
particular data set being examined. Mann, however, had said he had used
‘the  standard  rule’  for  determining  such  things.  The  rule  to  which  he
referred,  which  goes  by  the  slightly  forbidding  name  of  ‘Preisendorfer’s
Rule  N’,  is  actually  one  of  a  number  of  possible  approaches  that
statisticians use for working out how many PCs are significant and should be
carried  forward  to  subsequent  calculations.  McIntyre  later  cited  one
statistical  authority  who  had  listed  a  whole  host  of  exotic-sounding
alternatives,  with  names  like  the  Bootstrapped  Kaiser–Guttman  Criterion,
the Broken Stick, and Bartlett’s Test of the Equality of Variance.76 So there
are  no  hard  and  fast  rules  in  this  area,  and  Preisendorfer’s  Rule  N  is  no

more  than  one  rule  of  thumb  among  many,  despite  Mann’s  claims  that  it
was the ‘standard rule’.

That said, the original MBH98 paper did refer to the use of Preisendorfer’s
Rule N, but unfortunately for Mann’s case, this was only in the context of
the  retention  of  temperature  PCs.  There  was  no  mention  at  all  of  any
retention policy for tree ring PCs.  Of  course,  Mann  might  well  argue  that
Rule N was used for tree rings too, but his problem here was that there was
no sign that he had. Wildly different numbers of PCs were being retained in
the different  PC  calculations.  For  example,  Mann  had  retained  two  PCs  in
one of the Vaganov network calculations and nine in one of the Stahle ones.
Whatever  policy  he  was  using  it  appeared  to  be  neither  rational  nor
consistent  and,  to  this  day,  it  remains  one  of  the  unsolved  mysteries  of
MBH98.
Verification statistics
The story of the verification statistics plays an important part in the rest of
the Hockey Stick story, and it is probably worthwhile recapping how they
fit into the overall scheme of things. In Chapter 2 we showed how, once the
paleoclimatologist has developed his mathematical model of how tree ring
widths relate to temperature, he has to assess whether he got the answer by
chance or whether he has in fact come up with something he can rely on for
a full-scale temperature reconstruction. To do this, he keeps back a portion
of instrumental temperature data (the verification period) and then tries to
recreate  those  temperatures  using  only  the  ring  widths.  The  verification
statistics simply measure how well these reconstructed temperatures match
up against the instrumental ones. If they are close enough (‘significant’, in
the jargon) then he will go ahead and perform the rest of the reconstruction.
There  are  a  number  of  approaches  to  measuring  how  well  two  sets  of
numbers  match  up  against  each  other.  As  we’ve  seen,  Mann  had
concentrated on the Reduction of Error (RE) statistic, citing a paper by one
of  his  associates,  Ed  Cook,  in  his  support.  Although  Cook  was  a
paleoclimatologist rather than a statistician, he was regarded as something
of  an  authority  on  statistical  matters  in  climatological  circles.  However,
unfortunately for Mann, the Cook paper he cited in favour of the RE statistic
actually stated that a suite of verification statistics should be used. Cook’s
list of suggestions was headed by McIntyre’s own preferred measure, the

R2, alongside the RE and such exotica as the CE statistic, the sign test and the
product mean test.

We also saw in Chapter 3b that for many sciences the  R2 is the default
choice for measuring the correlation between two sets of numbers. The RE
statistic, on the other hand, is little used outside climatology and, as a result,
has not been studied by theoretical statisticians. This means that the way it
behaves in different situations and the circumstances in which it is safe to
rely on it are not fully understood. However, despite these drawbacks, the
RE is widely used in climatology and it was not going to help McIntyre’s
case to dispute Mann’s application of it in MBH98. The R2 was not without its
difficulties either and Mann had made much of these. When responding to
McIntyre’s  Nature  submission,  Mann  had  positively  raged  about  its
unsuitability  for  climate  reconstructions,  calling  R2  scores  ‘inappropriate
measures of forecasting or reconstructive skill’.61 These remarks had been
picked up by Ian Jolliffe in his review comments:

The advocacy of RE in preference to [R2] by MBH is a bit extreme.c [R2] certainly has
drawbacks,  but  no  verification  measure  is  perfect,  and  I  see  no  evidence  in  the
verification literature . . . that RE is the standard preferred measure. Indeed the only
one of the 3 references . . . cited in the revised response that was available to me is
somewhat critical of RE. My preference would be not to rely on a single measure . .
.60

. . . which, as we saw above, is precisely the position of Mann’s own quoted
authority,  Cook.  The  whole  argument  was  somewhat  strange  anyway
because,  despite  what  he  was  now  saying  about  R2 being ‘inappropriate’,
according to what he had reported in MBH98, Mann had calculated the R2, as
well as its close variant, r:

[RE] is a quite rigorous measure of the similarity between two variables, measuring
their correspondence not only in terms of the relative departures from mean values (as
does the correlation coefficient r) but also in terms of the means and absolute variance
of the two series. For comparison, correlation (r) and squared-correlation [R2] statistics
are also determined.14

In fact,  MBH98  included  a  colour-coded  map  of  the  verification  R2 for the
1854–1901 step of the reconstruction, so there can be little doubt that the

figures  had  been  calculated.  Tellingly,  the  equivalent  figures  for  earlier
periods, including the critical AD 1400 step, were nowhere to be seen.

McIntyre’s  Nature  submission  had  been  almost  silent  on  verification
statistics;  he  and  McKitrick  were  quite  clear  that  they  were  not  creating
their  own  reconstruction  of  past  temperatures,  merely  demonstrating  that
Mann’s  was  not  reliable.  Any  discussion  of  verification  statistics  had
therefore  seemed  pointless.  However,  when  he  read  the  reviewers’
comments,  the  inordinate  length  at  which  they  considered  the  issue
surprised  McIntyre.  It  was  out  of  all  proportion  to  the  weight  he  and
McKitrick  had  given  it  in  the  paper  itself;  it  all  seemed  rather  odd.  But
when  he  put  this  fact  together  with  Mann’s  fulminations  against  the  R2
statistic, the realisation dawned that he might have unearthed a whole new
problem with Mann’s paper. Could it be that the MBH98 reconstruction had
actually failed the R2 completely? When the reviewers had asked McIntyre
to  produce  his  own  verification  statistics,  he  had  noticed  that  the  R2  was
extremely low, but he just hadn’t seen what a can of worms he, Jolliffe and
Zorita had stumbled across. If the  RE was high, but the  R2 was low, there
was a real possibility that the MBH98 result was entirely spurious.
Spurious significance
Statisticians have understood for many years that just because your chosen
statistical  measure  indicates  that  the  result  is  significant,  it  doesn’t  mean
that it actually is significant. In fact, it is entirely possible that the result is
entirely insignificant. This is just as true of the R2 as it is of the RE or any of
the other measures.

Spurious correlations, as these statistical foul-ups are known, have been
written about since the start of the twentieth century, the classic case being
documented  in  the  1920s  by  the  Scottish  statistician  Udny  Yule.  Yule
revealed the remarkable fact that there was an amazingly close correlation
between the proportion of marriages that took place within the Church of
England and the mortality rate, the apparent implication being that getting
married  in  the  Anglican  rite  would  kill  you.  Of  course,  this  was  patently
absurd but it is by no means an isolated case. Other examples include the
correlation  between  the  number  of  ordained  ministers  and  the  rate  of
alcoholism,  and  that  between  the  salaries  of  Presbyterian  ministers  in
Massachusetts and the price of rum in Havana. The correlation scores on
some of these classic studies can be extraordinarily high, and statisticians

have learned to take great care when they find a high correlation to ensure
that it is not in fact spurious – this was what Jolliffe was alluding to when
he  recommended  looking  at  more  than  one  statistical  measure.  McIntyre
noted  subsequently  that  Yule’s  ‘nonsense’  correlation  between  Church  of
England  marriages  and  mortality  would  certainly  have  passed  the  RE  test
with flying colours.

If  he  was  going  to  confirm  the  possibility  of  spurious  significance,
McIntyre  had  to  show  that  the  R2  of  the  Hockey  Stick  was  as  low  as  he
thought. In order to do this he had to persuade Mann to release either his R2
figures, particularly those from the critical AD 1400 step, or alternatively the
‘residuals’  from  which  he  had  calculated  them.  These  latter  figures  were
simply 
the  actual
temperatures  in  the  verification  period,  which  would  allow  McIntyre  to
recalculate the R2 himself.

the  differences  between 

the  reconstructed  and 

McIntyre  had  been  asking  Mann  for  the  residuals  since  way  back  in
2003, together with the source code for all of the MBH98 calculations. After
Mann’s initial refusal, he had ended up copying the request to the National
Science  Foundation  (NSF),  the  US  government  body  that  had  originally
funded  Mann’s  research.  This,  he  hoped  might  concentrate  Mann’s  mind
sufficiently to elicit some action. Remarkably though, the  NSF replied that
Mann was under no obligation to deliver up the code, which they said was
his personal property(!), a view which might have troubled any American
taxpayers within earshot.

In  2004,  McIntyre  had  made  another  attempt  to  force  the  data  from
Mann’s grasp. You may remember that McIntyre had been invited to peer
review a Mann submission to the journal Climatic Change that was highly
critical of MM03.d McIntyre had taken advantage of this invitation to request
the source code and residuals again, as part of his peer review. The response
from  Stephen  Schneider,  the  journal’s  editor,  was  very  instructive.  He
explained that in 28 years of editing journals, he had never had a request for
this  kind  of  information  as  part  of  a  peer  review.  This  admission
demonstrates something very profound about the nature of the peer review
process, which is a subject we will return to later in this story.

The request to Climatic Change for the residuals dragged on for a short
time,  before  eventually  Schneider  decided  to  cut  short  the  argument.  He
stated that reviewers were not expected to run code, but he did indicate that
the journal would be adopting a new policy on data and materials, which

would  require  authors  to  deliver  up  data  on  request.  Unwilling  to  be
thwarted this way, McIntyre again requested that Climatic  Change  obtain
the residuals on his behalf, pointing out that these figures were undoubtedly
‘data’ within the meaning of the policy, but again there was a point blank
refusal from Mann.

It is not our responsibility to provide [the residual series], we have neither the time nor
the  inclination  to  do  so.  These  can  be  readily  produced  by  anyone  seeking  to
reproduce our analysis, based on the data we have made available, and our method
which we have described in detail . . .77

Another dead end. McIntyre wrote his review, pointing out that Mann was
thumbing  his  nose  at  the  journal’s  policies,  and  should  therefore  be
automatically disqualified from publication. In the event, nothing more was
heard of Mann’s paper, which was quietly withdrawn, although not before it
had  been  cited  in  other  papers  attacking  McIntyre’s  work.58  It  did  rather
look as if Mann had withdrawn the paper rather than release the residuals.
Even then, McIntyre hadn’t quite cottoned on to the possible implications.

The Nature materials complaint, which we looked at in the last chapter,
offered another opportunity for McIntyre to force the issue. However, as we
have seen, Nature  was  not  inclined  to  make  Mann  and  his  colleagues  do
anything  very  much  and  the  draft  Corrigendum  did  not  include  any
reference  to  the  residuals.  Amazingly,  McIntyre  refused  to  give  up.  After
the issuing of the Corrigendum, he made a second request to Nature (which
received an extemporising reply) and then a third one, this time directly to
the physical sciences editor, who refused outright:

And with regard to the additional experimental results that you request, our view is
that this too goes beyond an obligation on the part of the authors, given that the full
listing of the source data and documentation of the procedures used to generate the
final findings are provided in the corrected Supplementary Information. (This is the
most that we would normally require of any author.)78

Benchmarking
Without the residuals, it was difficult for McIntyre to prove his point, but
there was another aspect to the verification statistics that was rather more
fruitful and this was the area of benchmarking.

As  we  have  seen,  all  parties  agreed  that  R2  is  not  perfect.  It  does,
however,  have  some  great  advantages.  Because  of  its  ubiquity  in  other

sciences,  its  behaviour  in  different  circumstances  is  well  understood,  in
direct contrast to RE. It is also possible to refer to look-up tables that will
show you how significant your R2 is for any given set of numbers. This is
because,  in  the  jargon,  it  has  a  theoretical  distribution.  RE,  on  the  other
hand,  has  no  theoretical  distribution  and  you  therefore  have  to  assess
whether  your  result  is  significant  by  other  means,  usually  by  creating  a
benchmark.

There  is  a  rule  of  thumb  for  benchmarking  RE,  which  says  that  any
positive  number  is  significant,  although  this  only  applies  to  simple  linear
situations,  rather  than  the  more  complex  (‘multivariate’)  model  of  the
Hockey Stick. Because of this, Mann had not simply used the rule of thumb
but  had  gone  to  the  trouble  of  constructing  some  justification  for  his
benchmark.  In  order  to  do  this,  he  used  what  is  called  a  Monte  Carlo
method.

The principle of a Monte Carlo method is to see what happens to lots of
random number series when you put them through your process. If the score
your  actual  data  achieves  comes  out  at  the  top  end  of  what  any  of  the
random number series scored, you can be pretty sure that what you got was
not  just  chance,  but  represents  a  real,  meaningful  result.  To  elaborate
slightly: you have to create simulations of the reconstruction process, but
using random ‘red noise’ data rather than real proxies. These dummy data
series  are  known  as  pseudoproxies  and  are  rather  like  the  ones  McIntyre
used to test the effect of short centring on the PC calculation.e As before, the
pseudoproxies are carefully designed so that their statistical characteristics
are of the same type as the actual data – in our case, the tree rings. You then
crunch the pseudoproxies through the PC calculation and the calibration and
then you see what RE score you get against the real temperature data in the
verification period. This is your first simulation. However, you need to have
lots  of  simulations  –  let’s  say  10,000.  So  you  repeat  the  process  10,000
times  and  then  you  list  the  10,000  separate  RE  scores,  highest  at  the  top,
lowest at the bottom. The scientist doing the study then has to decide how
confident he wants to be that his result is significant. Mann went for a high
confidence level of 99% This would mean that there would be only a 1%
probability that he had got his results by chance. Having selected this level,
all he had to do was to count down to the 100th simulation in his list (1% of
10,000  is  100)  and  read  off  its  RE  score.  This  RE  score  was  the  new

benchmark. Then, any reconstruction he performed on real data that had an
RE in excess of this score was deemed significant.

When Mann performed the Monte Carlo benchmarking, he came up with
a  score  of  zero.  In  other  words  any  verification  routine  with  an  RE  score
greater than zero would be accepted as having ‘statistical skill’, which is to
say that the reconstruction could be considered significant. In MBH98 he had
reported that the RE score in the AD 1400 step was 0.51, well in excess of the
benchmark, and was therefore able to claim a high degree of skill.
Mann’s benchmark
While  Mann  had  given  a  brief  summary  of  his  benchmarking  routine  in
MBH98,  the  paper  contained  little  of  the  detail  necessary  to  fully  replicate
what he had done. He had disclosed the particular statistical model used to
simulate  the  tree  ring  series;  this  was  a  standard  statistical  model  called
AR1. There were also some details of the parameters used for the model.

When you are creating this type of simulation, it is very important that
your simulated data has the same statistical properties as the real data and
also that you treat the simulations in exactly the same way as the real data.
Whether  this  had  happened  in  practice  was  hard  to  tell  from  the  text  of
MBH98.  Was  AR1  an  appropriate  model?  Had  the  simulated  data  been
through  all  of  the  same  steps  as  the  real  data?  This  latter  point  was
potentially  crucial.  In  the  MBH98  methodology,  the  short  centring  routine
was doing some very strange things to the data.

This insight prompted McIntyre to try to create new RE benchmarks from
scratch using the 10,000 simulations he’d created to demonstrate the effect
of short centring. Instead of feeding random data into the correlation and
the RE calculation, as per Mann, the idea was to feed it random data that had
been  processed  through  the  short  centring  routine  instead.  Getting  the
calculations  done  was  straightforward  and  the  results  were  immensely
gratifying. The correct benchmark level for the RE statistic turned out to be
0.59,  as  compared  to  Mann’s  zero.  In  other  words,  any  temperature
reconstruction that came out of Mann’s data needed to score over 0.59 to be
considered significant. As we have seen, the Hockey Stick itself had only
scored  0.51  on  the  AD  1400  step,  so  it  could  no  longer  be  considered
statistically significant, even on the somewhat dubious RE measure.

For McIntyre, what was even better about the simulations was that they
appeared to replicate very faithfully what was seen in the real data, namely

high RE scores and low R2. This was extremely strong evidence that the RE
was in fact entirely spurious. It looked very much like game, set and match
to McIntyre.
The Energy and Environment paper
The  submission  to  Energy  and  Environment,  MM05(EE),  was  to  be  an
altogether  broader  paper,  summarising  much  of  McIntyre’s  work  to  date,
looking at the differences between MBH98 and his attempts to recreate it, and
examining  the  sensitivity  of  the  reconstruction  to  various  changes  in  the
data  network,  particularly  Gaspé  and  NOAMER.79  McIntyre  and  McKitrick
decided that they would also examine these last two series in more detail,
including a lengthy discussion of their validity as proxies. To round things
off,  there  was  also  to  be  a  section  rebutting  each  of  the  main  counter-
arguments put forward by Mann and another on the broader implications of
the whole Hockey Stick affair.
Reconciling MBH98
As  his  understanding  of  exactly  what  Mann  had  done  had  improved,
McIntyre had been able to pin down the differences between MBH98 and his
emulation of it to just two main factors – Gaspé and  NOAMER. In order to
demonstrate how the reconstruction was influenced by these two series, just
a  fraction  of  the  total  data,  he  and  McKitrick  had  decided  to  present  a
sensitivity  analysis.  This  would  demonstrate  how  removing  the  series,  or
making  apparently  insignificant  changes  to  them,  would  dramatically
change the final reconstruction.

As we have seen, Gaspé was included twice in the proxy network and,
uniquely among the MBH98 proxies, was artificially extrapolated back to the
year 1400, allowing it to be included in the critical AD 1400 roster, where it
had the effect of depressing the reconstructed temperatures in the Medieval
Warm Period. The extrapolation was not disclosed in the original paper, but
Mann had been forced to acknowledge it in the Corrigendum. Of course,
the  Corrigendum  also  said  that  this  didn’t  materially  affect  the  MBH98
results, so to the reader, the extrapolation appeared, as Mann had put it, a
mere ‘technicality’.

McIntyre’s arguments about Gaspé had been relegated to a footnote in
the original Nature submission in an attempt to reduce the word count, but
now with room to explain it in full, he could afford to set down all of the
idiosyncrasies of the series and Mann’s treatment of it. For example, from

the start of the series in 1404, through to 1421, Gaspé was based on a single
tree,  and  there  were  only  two  in  the  subsequent  period  up  to  1447.  This
obviously raised enormous doubts over the reliability of this section of the
data. In fact, the original authors of the Gaspé study, Jacoby and D’Arrigo,
had not used the early portion of the series at all in their own reconstruction,
deeming  the  data  too  sparse.  Their  lead  had  been  followed  by  later
scientists: only Mann, it seemed, had ever used the dubious early part.

to  question.  Some, 

So there were a number of Mann’s decisions regarding Gaspé that were
open 
like  extrapolation,  might  have  appeared
insignificant, while others, like the small number of tree cores in the earlier
centuries, might have looked a little more troubling. McIntyre was able to
show,  however,  that  all  of  the  decisions  Mann  had  taken  had  a  huge
influence on the final shape of the reconstruction. For example, assuming
you  had  first  fixed  the  short  centring  routine,  if  you  removed  the  early
portion  of  the  Gaspé  record  (based  on  one  or  two  trees)  you  kept  the
Medieval  Warm  Period.  If  you  removed  the  individual  proxy  version  of
Gaspé (but not the NOAMER version), you kept it too. In fact, you could only
get rid of the Medieval Warm Period by using the Gaspé series twice and by
including the unreliable early portion, and by extending this highly dubious
data back to the start of the fifteenth century.

Mann  had  claimed  that  the  extrapolation  was  justified  by  the  need  to
keep a representative of the northern treeline series in the reconstruction of
the fifteenth century. This claim was also shown to be wrong on a number
of scores. For a start, Gaspé, which is located south of Québec, is nowhere
near the northern treeline. But for the purposes of the sensitivity analysis,
McIntyre  merely  replaced  Gaspé  with  another  northern  treeline  series,
Sheenjek  River.  This  series  was  based  on  a  higher  number  of  trees  and
should therefore have been more reliable, and McIntyre was able to show
that when this was done, the Medieval Warm Period appeared once more.

The  other  side  to  the  sensitivity  analysis  was  to  look  at  various
configurations of the  NOAMER PC  series.  Assuming  you  had  first  removed
the Gaspé extrapolation, you could get wildly different results depending on
how  the  NOAMER  PC  calculation  was  performed.  It  turned  out  that  the
Medieval Warm Period was eliminated from the reconstruction only if you
had  bristlecone  pines  in  your  proxy  database  and  a  short-centred  PC
algorithm  (assuming  you  retained  the  same  number  of  PCs  as  Mann  had
used  in  MBH98).  If  the  PCs  were  centred  you  lost  the  influence  of  the

bristlecones,  which  lingered  down  in  the  PC4.  Now,  of  course,  Mann  had
pointed out that if you retained the PC4 too, you could get the bristlecones
back into the reconstruction and ‘get rid of the Medieval Warm Period’, but
this was hard to justify, as we saw above, because the PC4 represented such
an obscure pattern in the data.

Even if Mann could have justified the use of the PC4, there was another
problem with NOAMER. Because the bristlecones were not reliable proxies in
the  first  place  –  as  we  have  seen  the  growth  pattern  was  thought  to  be
contaminated with a non-climatic signal – Mann shouldn’t have been using
them  in  his  dataset  anyway.  McIntyre  was  able  to  show  that  if  you
eliminated  the  bristlecones  from  the  proxy  roster,  it  didn’t  matter  which
centring convention you used, the Medieval Warm Period remained in the
record.
Flipping proxies
The sensitivity analysis gave some stark results, but there was to be one last
amazing  demonstration  of  the  perverse  results  that  had  been  caused  by
Mann’s  short-centred  PC  convention.  A  reader  of  McIntyre’s  website  had
written  in  to  ask  what  happened  if  the  ring  widths  of  the  non-bristlecone
sites were artificially inflated in the early fifteenth century. The idea was to
see  how  this  new  dummy  information  would  show  up  in  the  final  result.
Intrigued by the suggestion, McIntyre ran another simulation, using his best
approximation  of  Mann’s  methodology  and  data,  but  adjusted  in  the  way
that  the  reader  had  suggested.  The  results  were  simply  extraordinary.
Adding  extra  ring  width  in  the  early  fifteenth  century  caused  the
reconstructed  temperature  for  that  period  to  go  down.  In  other  words  the
algorithm was reading wider ring widths as evidence of lower temperatures.
This  simply  could  not  be  the  case.  If  it  were,  it  would  be  the  end  of
dendroclimatology as a science.

McIntyre observed, ‘these results are initially very counterintuitive and
have provoked some disbelief’79 and a section was added to the Energy and
Environment  paper  detailing  the  exact  computer  code  used  in  the
calculation so that doubters could test the result for themselves. This was a
stark  contrast  with  Mann,  who  still  firmly  refused  to  release  any  of  his
scripts. McIntyre went on to explain exactly what was happening.

This  rather  perverse  result  nicely  illustrates  a  problem  of  mechanically  applying  a
numerical algorithm like PC analysis without regard to whether it makes sense for the

underlying  physical  process.  PC  methods  are  indifferent  to  the  orientation  (up  or
down) of a series – the difference is merely the presence or absence of a negative sign
. . .
Under the MBH98 algorithm, the addition of the extra values in the first half of the 15th
century causes the algorithm to flip the series upside-down so that they match as well
as possible to the bristlecone pines, whose hockey stick pattern is imprinted on the
PC1.79

The point he was making is that PC methods see upticks and downticks as
the  same  thing  –  to  a  PC  calculation  they’re  all  just  ‘deviations  from  the
mean’.  Presented  with  a  twentieth  century  uptick  and  a  twentieth  century
downtick,  the  PC  algorithm  would  read  these  as  ‘big  twentieth  century
deviations’.  It  would  then  flip  one  of  the  series  over  when  providing  the
answer.  Which  one  it  flipped  would  depend  on  their  relative  sizes  and
depending on which one this was, the final result could look like a downtick
rather than an uptick (or vice versa).

If you think back to the metaphor of the shadow cast by a comb, with
which we explained PC analysis back in Chapter 2, the point may become
clearer: it doesn’t matter which way up the teeth are pointing, the shadow is
in essence the same. So if we have a set of combs rather than the single one
in  our  previous  example,  and  we  shine  a  light  at  them,  PC  analysis  will
combine all the shadows from all the different combs, some of which have
their teeth pointing upwards and some downwards, and return the answer
‘comb’.

So  when  McIntyre  had  added  some  extra  dummy  ring  width  to  proxy
records in the fifteenth century, it turned out that the PC algorithm had found
the  best  way  to  summarise  it  was  to  flip  the  whole  series  over  to  match
fifteenth century downticks in other series. Extra fifteenth century growth
led to reconstructed temperatures that were apparently lower.

In fact there was another remarkable aspect to this particular statistical
oddity. It turned out that when Mann had archived results from his second
hockey stick paper (MBH99) the  PC algorithm had been flipping again: the
final PC1 had ended up with a twentieth century downtick, so it was actually
upside-down. Mann had therefore had to flip it back again for presentation
purposes.
Robustness
Apart  from  the  sensitivity  analysis,  McIntyre  also  wanted  to  present  a
demonstration of just how it was that Gaspé and NOAMER could make such a

huge difference to the AD 1400 reconstruction. If you remember, once the PC
calculations  are  complete,  the  112  (or  perhaps  159)  PCs  and  individual
proxies are lined up against the temperature records and a weight is given to
each, depending on how good the correlation is. It was all very well having
a hockey stick shaped series or two, but McIntyre needed to make it clear
how these came to dominate the temperature reconstruction.

In order to do this he constructed a plot of the hockey stick index against
the calibration correlation. For each series, the hockey stick index simply
measured how big the blade of the stick was compared to the handle. The
calibration correlation was, as the name suggests, a measure of how well
the  series  matched  up  against  the  temperature  records  in  the  calibration
period.  Those  that  had  high  correlations  would  be  heavily  weighted  and
their shapes would therefore dominate the final reconstruction.

The plot is shown in Figure 6.2, with Gaspé and NOAMER marked in the
top  right  hand  corner.  The  implications  were  clear,  as  McIntyre  and
McKitrick explained:

Except  for  [Gaspé  NOAMER]  there  is  an  overall  negative  relationship  between  [the
hockey stick index] and the correlation with temperature: i.e. hockey stick series fit
the  temperature  data  relatively  poorly  in  the  calibration  interval.  But  the  [NOAMER]
and  Gaspé  series  are  such  influential  outliers  that  they  reverse  this  pattern  for  the
model as a whole.

FIGURE 6.2: Outliers in the dataset
In other words, the group of points on the left have profiles that are hockey
stick  shaped  to  various  degrees.  However,  the  more  hockey  stick  shaped
they  are,  the  less  well  they  correlate  with  temperature.  This  is  in  stark
contrast to Gaspé and NOAMER, the two outliers, which correlate better than
any of the other series in the database. Because of this their hockey stick
shape is imprinted on the final reconstruction.
Credibility of the critical series
The final piece in the jigsaw was a look at the credibility of Gaspé and the
NOAMER  bristlecones  as  proxies.  They  may  have  been  outliers,  but  it  was
conceivable  that  there  might  have  been  an  unusual  temperature  signal
affecting their growth. Although this wouldn’t justify the extrapolation of
Gaspé  or  the  short-centred  PC  calculation,  it  would  at  least  justify  their
inclusion in the network in the first place.

As we’ve seen, McIntyre knew from his own experience that there was
nothing going on in the climate of Québec in the late twentieth century that
would have caused such a dramatic growth spurt in Gaspé. We’ve also seen
that it was widely agreed that the growth spurt in the bristlecones was non-
climatic  too.  It  was  necessary,  however,  to  bring  all  this  information

together in one place so that any arguments that these two series were valid
could  finally  be  laid  to  rest.  This  was  even  more  important  in  the  1000–
1399 extension of the Hockey Stick in Mann’s second paper, MBH99, where
Gaspé and the bristlecones were even more dominant in the final network
because  of  the  reduced  number  of  proxy  series  that  were  available  for
earlier centuries.

McIntyre’s paper therefore laid out in copious detail the problems with
the two datasets. He showed that the bristlecone growth was anomalous by
comparing  NOAMER  to  an  earlier  Northern  Hemisphere  reconstruction  by
Keith  Briffa,  which  was  based  on  a  mixture  of  tree  species.80  Briffa’s
reconstruction  showed  nothing  like  the  same  shape  as  MBH98,  which  was
dominated  by  the  bristlecones.  In  fact,  the  twentieth  century  correlation
between the two reconstructions was precisely nil. As McIntyre, deadpan,
observed,  whatever  major  temperature  signal  Mann  had  captured  had
apparently escaped detection in Briffa’s paper.

The  scientific  literature  appeared  to  hold  the  explanation  for  this
discrepancy.  As  we  saw  in  the  last  chapter,  Donald  Graybill,  the  original
author  of  the  bristlecone  studies,  had  stated  that  the  twentieth  century
growth spurt in these trees could not be explained by temperature changes.
He believed growth was being affected directly by carbon dioxide levels, an
effect  known  as  ‘carbon  dioxide  fertilisation’.  So  when  it  came  to  the
bristlecones, the growth spurt in the twentieth century was probably due to
changes  in  both  carbon  dioxide  levels  and  temperature,  which  inevitably
made  it  very  hard  to  isolate  the  temperature  signal.  There  were  also  any
number of other possibilities that might have been at the root of the growth
spurt:  nitrogen  fertilisation,  a  change  in  the  ecosystem,  the  influence  of
sheep and of rainfall.

In fact, the equation was even more complex than this. Bristlecones often
exist in a strange ‘stripbark’ form, where the bark on one side of the tree
dies back. It turned out that Graybill had actively sought out stripbark trees
when he collected the samples that ended up in the NOAMER PC1, believing
that  these  would  be  more  susceptible  to  carbon  dioxide  fertilisation.
Although some authors had claimed that bristlecones could still be used for
temperature reconstructions because only the stripbark form was affected in
this way, it was simple enough to demonstrate that there were only stripbark
trees  in  Graybill’s  samples.  The  NOAMER  PC1  and  Mann’s  results  were
therefore inherently unreliable.

Each  and  any  of  these  factors  should  have  been  considered  and
eliminated in the original  MBH98 paper before it was published. That they
weren’t was something of an indictment of the system of peer review. That
a  paper  that  relied  on  these  unreliable  results  passed  the  subsequent  IPCC
review and reached a position of such importance in the case for manmade
global warming was remarkable.

It  was  fairly  clear  then  that  there  were  major  problems  with  the
credibility  of  the  bristlecones  in  NOAMER.  What  then  of  Gaspé?  McIntyre
explained in the Energy and Environment paper that the data was derived
from cedars. These trees, like the bristlecones, exist in strip bark and whole
bark forms, and are very slow growing. Cedar chronologies are not unique
in tree ring studies, but they are few and far between. Apart from Gaspé,
those  that  do  exist  show  either  no  twentieth  century  growth  spurt  or  a
negative relationship between growth and temperature. Gaspé seems to be
unique in having a hockey stick shape.

Even if this fact had not been enough to raise suspicions over the series’
reliability,  there  was  another  feature  of  cedar  chronologies  noted  in  the
literature that was potentially fatal to Mann’s thesis: the response of cedars
to changes in temperature was non-linear. In other words, it appeared that
the trees grew fastest in cool and wet conditions but in very hot or very cold
weather they slowed down. A chart of growth versus temperature for cedars
would therefore be an upside-down U shape. The possibility of a non-linear
growth  response  had  been  observed  in  other  species  too,81,82  and  as
McIntyre  pointed  out,  if  true  it  would  completely  undermine  the  whole
basis  of  paleoclimatology,  which  relies  on  the  relationship  between
temperature and ring width being linear. If the curve was an upside-down
U, as shown in Figure 6.3, any given ring width could imply one of two
different temperatures, with no way of knowing which one was right. In the
example  shown,  a  ring  width  of  0.25  mm  could  imply  a  temperature  of
either  10.5°C  or  13.0°C.  McIntyre  performed  a  test  of  the  correlation
between the Gaspé ring widths and the local temperature and there was no
relationship apparent, suggesting that this was indeed a major problem.

FIGURE 6.3: Upside-down U-shaped temperature response

FIGURE 6.4: Gaspé – original and updated series
The original series has a sharp twentieth century uptick, while the update
shows a sharp decline in ring widths starting in the 1960s.

McIntyre  had  even  more  startling  revelations  to  make  about  Gaspé
though. During the course of his research, an update of the Gaspé series had
come  into  his  possession  that  threw  further  doubt  on  its  reliability.  The
original study was based on samples taken in 1982, but it turned out that
new ring cores had been taken in 1991. The new data were astonishing –
they showed nothing like the same growth spurt seen in the earlier figures
(see Figure 6.4).

Extraordinarily,  Jacoby,  the  author  of  the  new  study,  had  refused  to
publish these revised figures or to archive the data, on the grounds that the
older  data  showed  the  temperature  better  and  because  their  research  was
‘mission-oriented’. 83 In fact he was not even willing or able to identify the
location  of  the  site  to  allow  re-sampling,  repeatedly  turning  down
McIntyre’s requests for the map coordinates. Eventually, once the request
for the location was put to him via the original journal, Jacoby claimed that
the  location  had  been  lost,  a  remarkable  fact  for  a  site  that  had  been
sampled twice. The team who had made the update had apparently never
found  the  original  location.  The  exact  whereabouts  of  the  site  remain  a
mystery to this day.84
Announcement
By  January,  with  the  papers  accepted  for  publication  and  nearing  print,
McIntyre was ready to tell the world that he and McKitrick were about to
start  their  fightback.  He  posted  up  an  article  on  his  Climate2003  website
announcing the publication of no less than three papers about the Hockey
Stick – in  GRL, in Energy and Environment and a third article in a Dutch
popular science magazine.

The trouble was only just beginning.

a  See page 49.
b  See page 66.
c  Jolliffe actually referred to r, which is a slightly different measure of correlation to R2, but they are

closely related, R2 being the square of r. I’ve changed it to R2 for the sake of ease of narrative.

d  See page 146.
e  See page 113.

7     Commentary

For  a  successful  technology,  reality  must  take  precedence  over  public  relations,  for
Nature cannot be fooled.

Richard Feynman

involved 

RealClimate
On  19  November  2004  a  new  Internet  domain,  ‘realclimate.org’,  was
registered by Betsy Ensley, an employee of Environmental Media Services
(EMS), a PR firm based in Washington DC. EMS was a pivotal organisation
in the green movement and Ensley was a committed environmental activist,
having  previously  been 
in  setting  up  such  campaigning
organisations as bushgreenwatch.org and womenagainstbush.org.

EMS itself was run by David Fenton, a powerful PR executive, as part of
his lobbying organisation, Fenton Communications. Fenton has been called
‘one  of  the  most  influential  PR  people  of  the  twentieth  century’,  a  claim
that  was  based  in  part  on  the  leading  role  he  played  in  promoting  the
notorious Alar scare in the 1980s, when apple growers across the USA were
ruined by an unsubstantiated claim that a pesticide they used caused cancer.
The setting up of RealClimate in November 2004 signalled the start of a
new  phase  in  the  war  of  the  Hockey  Stick  –  the  blog  campaign.  Blogs  –
Internet diaries and journals – were big news at the end of 2004, promising
to  revolutionise  the  Internet  by  making  web  publishing  simple  and
accessible to everyone. There had been an explosion of online activity, with
all  sorts  of  people  piling  onto  the  bandwagon  and  setting  down  their
opinions  and  their  most  intimate  thoughts  for  all  to  read  online.  Public
relations people like Fenton had rapidly cottoned on to the significance of
blogging as a campaigning tool; it was clear from the start that RealClimate
was  to  be  a  weapon  in  the  environmentalists’  fight  and  that  much  of  its
campaigning was to be directed at McIntyre and McKitrick.
New media war
Ever since the publication of MM03, and even before then, supporters from
both  sides  had  been  hurling  brickbats  at  each  other  on  the  Internet.  As
McIntyre  and  McKitrick’s  new  papers  in  Energy  and  Environment  and

Geophysical  Research  Letters  neared  publication,  the  war  of  words  was
beginning to be stepped up, and looked as though it would explode in the
fairly near future.

With experience suggesting that a torrent of criticism would shortly be
heading  their  way,  McIntyre  and  McKitrick  were  very  grateful  for  any
support  they  could  get,  and,  in  the  event,  some  of  it  appeared  from
surprising directions. A highly supportive article was published in the  MIT
Technology  Review  by  an  eminent  physicist  from  the  University  of
California,  called  Richard  Muller.85  The  headline  ‘Global  Warming
Bombshell – A prime piece of evidence linking human activity to climate
change turns out to be an artefact of poor mathematics’ gives something of
the flavour of the piece. Muller was highly critical of the way the scientific
community  had  treated  McIntyre  and  McKitrick,  and  in  particular  the
refusal of Nature to publish their critique of MBH98. It was, he said, ‘more
dangerous to have a phony hockey stick than a broken one’.

in  a  web 

Mann’s supporters saw Muller’s article as a gauntlet thrown down, and
within  days  of  its  publication,  it  was  coming  in  for  sustained  criticism
around the Internet. Mann later accused Muller of a ‘scurrilous parroting’ of
McIntyre and McKitrick’s arguments,86 but the initial reaction came from
one  of  Mann’s  supporters.  William  Connolley,  a  climate  modeller  and
sometime  green  politician,  commented 
forum  called
sci.environment that he thought McIntyre and McKitrick’s claims about the
effect  of  short  centring  were  ‘probably  wrong’.87  However,  when  he
examined  Mann’s  code,  the  best  he  could  come  up  with  was  a  less
forthright statement that the short centring looked ‘odd’, although he also
opined  that  it  wasn’t  obviously  harmful.  Connolley  also  thought  that  the
short centring would reduce the impact of series with a twentieth century
uptick  rather  than  exaggerate  them,  suggesting  that  his  conclusions  were
only based on a brief review of McIntyre’s work. Another climate modeller,
James  Annan,  commented  that  McIntyre  and  McKitrick  didn’t  know  the
difference between multiplication and division and said that he intended to
start looking into the issue in more detail.88 In the event, though, it was in
fact  Connolley  who  started  the  counterattack,  setting  up  a  web  page  to
‘audit  the  auditors’,  but  whether  he  eventually  worked  out  that  McIntyre
was in fact correct or he lost interest in the subject, he soon moved on to

other things.89 Annan also suddenly seemed to lose interest in McIntyre’s
work.

Annan and Connolley may have moved on, but their earlier thoughts had
been  picked  up  and  propagated  across  the  left-wing  blogosphere,  where
prominent voices such as those of Brad DeLong and Tim Lambert spread
the idea that McIntyre and McKitrick’s new papers were fatally flawed,90,91
a remarkable thing considering that they had not even been published at that
time.

It  may  have  been  the  wish  to  undermine  a  prominent  supporter  of
McIntyre’s  work  that  led  to  all  the  criticism  of  Richard  Muller,  but  his
position  as  one  of  America’s  most  eminent  scientists  also  attracted  the
interest  of  others  with  a  more  neutral  standpoint;  people  who  were  just
interested  and  came  with  no  preconceptions  of  McIntyre’s  work  or  of
Mann’s.  One  of  these  was  Marcel  Crok.  Crok  was  a  Dutch  science
journalist,  who  worked 
for  a  popular  science  magazine  called
Natuurwetenschap  &  Techniek  (which  translates  into  something  like
‘Natural Science and Technology’). Crok had read Muller’s article and was
intrigued that such an important scientist should have stuck his neck out so
far  in  order  to  support  two  outsiders  such  as  McIntyre  and  McKitrick.
Perhaps,  Crok  wondered,  there  was  more  to  their  claims  than  one  might
think from the barrage of criticism and derision that was emanating from
Mann’s supporters. He resolved to take a closer look.

Shortly afterwards, in November, the RealClimate website went live in
something of a blaze of publicity, the cheerleading led by none other than
McIntyre’s  old  friends  at  Nature,  who  welcomed  the  new  venture  with  a
supportive  editorial.92  Billing  itself  as  ‘a  commentary  site  on  climate
science  by  working  climate  scientists  for  the  interested  public  and
journalists’,  RealClimate  boasted  nine  prominent  climatologist/writers  led
by  Mann  himself  and  a  climate  modeller  called  Gavin  Schmidt,  who
worked at  NASA’s Goddard Institute. Also in the RealClimate lineup were
Ray Bradley, one of Mann’s co-authors on MBH98, and William Connolley,
the  environmentalist-cum-climate  modeller  who  had  been  criticising
McIntyre on sci.environment.
Posting on Rutherford
Within days of the launch, Mann was using this new platform to carry on
the  fight  against  McIntyre.  One  of  the  first  postings  on  the  site  was  a

summary of a forthcoming paper by none other than Mann’s assistant, Scott
Rutherford, whom we met at the start of Chapter 3, sending the data files to
McIntyre.  The  new  paper,  known  as  Rutherford  et  al  2005  (it  was  to  be
published in the new year), included several prominent members of what
was to become known as the Hockey Team – Mann, Bradley and Hughes,
plus three British scientists – Keith Briffa, Phil Jones and Tim Osborn93 –
all familar names from earlier in the story.

Rutherford et al 2005 completely discredited the claims of McIntyre and
McKitrick, at least according to Mann, who authored the blog posting.94 He
was  certainly  not  mincing  his  words  in  the  text  either,  talking  of  MM03’s
‘falsely reported putative errors’ and McIntyre’s ‘misunderstanding of the
methodology  used’  in  MBH98.  To  anyone  familiar  with  what  had  gone
before,  it  was  clear  that  Mann  was  largely  reiterating  the  arguments  he’d
used earlier in the year to attack the Nature submission – that McIntyre had
used the wrong dataset, that he hadn’t used a stepwise procedure and so on.
It was technically relatively simple for McIntyre to rebut Mann’s claims of
course, he had done this before, but it quickly became clear that as the new
papers neared publication, RealClimate and the rest of Mann’s web-based
supporters were going to step up the level of attacks. He was going to be
hard  pressed  to  keep  on  top  of  the  task  of  defending  himself  from  all  of
Mann’s supporters.

The  next  offensive  was  not  long  in  coming.  Two  new  Mann  postings
appeared on RealClimate on the same day in December. The first was an
excoriating  rant  with  the  catchy  title  of  ‘False  claims  by  McIntyre  and
McKitrick  regarding  the  Mann  et  al.  (1998)  reconstruction’.95  This  piece
was an attempt to rebut the new criticisms of the Hockey Stick (‘spurious
criticisms’,  in  Mann’s  words)  before  they  were  published,  and  largely
revisited the arguments of his replies to Nature – Preisendorfer’s Rule N,
the argument that if you used the NOAMER proxies without processing them
through the PC calculation you still got a hockey stick and so on. It was no
doubt very effective in persuading his readers.

The second posting was called ‘Myth vs fact regarding the hockey stick’
and  it  included  another  sustained  attack  on  McIntyre  and  McKitrick  (the
brackets are in the original):

False claims of the existence of errors in the Mann et al (1998) reconstruction can also
be traced to spurious allegations made by two individuals, McIntyre and McKitrick

(McIntyre works in the mining industry, while McKitrick is an economist). The false
claims were first made in an article . . . published in a non-scientific (social science)
journal ‘Energy and Environment’ and later, in a separate ‘Communications Arising’
comment that was rejected by Nature based on negative appraisals by reviewers and
editor [as a side note, we find it peculiar that the authors have argued elsewhere that
their submission was rejected due to ‘lack of space’].86

At this point, readers may care to refer to the final letter from Nature, which
can be found at page 130.
Climate Audit
As the attacks grew more numerous and more ferocious, and his work drew
more  and  more  attention,  McIntyre  found  himself  having  to  spend  an
increasing amount of his time explaining what he was doing. He therefore
started to post regular updates on his Climate2003 website in order to try to
fend off some of the more frequent critiques and questions. This site, now
defunct,a predated blogging and was an old-style flat webpage with a few
screens  of  information.  Because  of  this,  it  had  always  been  difficult  for
readers to use and newcomers would have found it very hard to find their
way  around  the  site.  As  the  volume  of  information  on  the  site  started  to
increase, this situation was becoming more confusing and it soon became
clear  that  Climate2003  was  no  longer  up  to  the  job.  Maintaining  the  site
was  becoming  a  daily  struggle  which  was  occupying  too  much  of
McIntyre’s time and distracting him from his research. As supporters were
pointing out to him, the Internet world was gravitating towards blogs and he
was going to have to tag along too or get left behind, his voice drowned out
in the climate cacophony. The decision was taken to move to a blog format.
McIntyre didn’t have a PR adviser to help him get his new venture off
the ground, but with the help of a supporter, a blog was set up to McIntyre’s
specifications and he was able to start posting articles towards the end of
2004.  Its  name  was  Climate  Audit  and  its  inception  was  to  be  a  turning
point in McIntyre’s fortunes.

The first few posts on Climate Audit merely reproduced earlier articles
from Climate2003, but with years of research behind him, stretching right
back to the Climate Skeptics days, there was no shortage of new material
and  McIntyre  was  soon  into  the  swing  of  things.  Postings,  visitors  and
comments  were  slow  at  first,  but  McIntyre  managed  to  keep  a  relatively
civilised tone to proceedings and even gave critics a friendly welcome. As
the  months  passed,  and  visitor  numbers  and  comments  rose,  a  regular

readership  developed,  with  a  remarkable  array  of  scientific  expertise,
particularly in areas that were directly relevant to McIntyre’s research, such
as statistics and signal processing. The Hockey Team were soon confronted
by a very well-equipped opposition – the Climate Auditors. While some of
the blog’s readers were not always welcoming to those opponents who left
comments  on  the  site,  those  opponents  were  at  least  allowed  to  do  so
without being censored, in stark contrast to RealClimate, which had quickly
developed a reputation for deleting or editing comments.
Publication of the new papers
As  the  final  days  counted  down  towards  the  official  appearance  of
McIntyre’s new papers, Mann started a new set of attacks. A week before
the publication date, he and Schmidt posted up a long two-part article on
RealClimate on the subject of peer review, outlining a litany of scientific
articles  that  had  passed  the  peer  review  process  but  had  subsequently
proved controversial or just plain wrong. In the words of the article’s title,
peer review was ‘a necessary, but not sufficient condition’.97 It was obvious
where  this  was  leading,  and  sure  enough,  the  second  part  of  the  article,
timed  to  coincide  with  the  publication  of  McIntyre’s  new  papers,  was  an
aggressive denunciation of McIntyre and McKitrick’s work. Their studies
were ‘flawed’, ‘deeply flawed’, ‘botched’, ‘bizarre’, and their claims were
‘false and specious’. The paper had ‘managed to slip through the imperfect
peer review filter of [Geophysical Research Letters]’.28

This  was  strong  stuff,  but  Mann  and  Schmidt  didn’t  just  throw  mud.
They went through the whole catechism of their earlier arguments against
McIntyre and McKitrick for the benefit of new readers, in a vain attempt to
try  to  stop  the  Climate  Auditors’s  story  gaining  any  media  momentum.
However, by now McIntyre had his own blog, from which he could shoot
back, and now there was mainstream media interest in his work as well.
Natuurwetenschap & Techniek
As we have seen, the Richard Muller piece in MIT Technology Review had
been  picked  up  by  the  Dutch  science  journalist,  Marcel  Crok.  Crok  had
been  unsure  what  to  make  of  the  story  at  first,  but  he  had  arranged  to
interview McIntyre in December 2004, and since then had done a great deal
of  investigative  work,  digging  into  the  background  of  the  story  and  also
seeking independent opinions from Dutch scientists on the statistical issues.

The more he continued his investigations, the more convinced he became
that there was a big story to tell.

As his interest grew, Crok decided to contact Mann in order to get the
other side of the story and to try to draw out responses to the allegations
that McIntyre had just made in the two new papers. Some of the questions
were also prompted by McIntyre himself as a way of pointing Crok to the
pertinent  issues  –  the  differences  in  their  positions  and  where  he  felt  the
weaknesses in Mann’s claims lay.

Mann’s reply, when it arrived, was a prime example of his unmistakeable
style. In a long email, he went to considerable pains to point out his low
opinion  of  the  Canadians  and  made  it  abundantly  clear  that  he  wanted
nothing  favourable  about  McIntyre  or  McKitrick  to  appear  in  the  Dutch
press:

I  hope  you  are  not  fooled  by  any  of  the  ‘myths’  about  the  hockey  stick  that  are
perpetuated  by  contrarians,  right-wing  think  tanks,  and  fossil  fuel  industry
disinformation . . . I must begin by emphasizing that McIntyre and McKitrick are not
taken  seriously  in  the  scientific  community.  Neither  are  scientists,  and  one
(McKitrick) is prone to publishing entirely invalid results apparently without apology.
‘New  Scientist’  considered  running  an  article  .  .  .  on  [McIntyre  and  McKitrick’s]
claims.  The  editor  decided  not  to  run  an  article,  concluding  that  their  claims  were
suspicious  and  spurious  after  interviews  with  numerous  experts  and  after  it  was
revealed that they had suspiciously close ties with the fossil fuel/energy industry.98

When  he  accused  McKitrick  of  publishing  invalid  results  ‘without
apology’,  Mann  was  referring  to  a  paper  that  McKitrick  had  published
earlier  in  2004.99  Shortly  after  publication,  the  paper  had  been  found  to
contain  an  errorb  and  a  correction  was  issued  shortly  afterwards.  It  is
probably fair to say, though, that a correction is not normally accompanied
by  an  ‘apology’,  so  Mann’s  complaint  seems  somewhat  overstated.  The
citing  of  this  error  has  been  a  regular  form  of  attack  on  McIntyre  and
McKitrick over the years,100,101 despite the fact that McKitrick’s correction
showed that error didn’t affect the paper’s conclusions.

Claims of McIntyre’s alleged closeness to big oil have also been made
regularly by the Hockey Team and their supporters, the ‘evidence’ usually
consisting  only  of  dark  mutterings  about  McIntyre’s  background  in  the
extractive  industries.  McIntyre  had  made  clear  statements  in  all  of  his
papers that he had received no financing for his work – indeed he had spent
thousands  of  dollars  of  his  own  money  pursuing  it  up  to  that  point.  The

most  definitive  accusation  made  by  the  doubters,  which  was  the  one
referred to by Mann, was a claim by the Environmental Defense Fund that
McIntyre  was  being  funded  by  ExxonMobil.  However,  the  evidence
appeared  to  consist  only  of  the  fact  that  McIntyre  had  once  written  an
article for a think tank that had received funding from Exxon.

Mann  clearly  felt,  however,  that  this  was  persuasive  evidence  and
advised  Crok  to  treat  McIntyre  with  ‘appropriate  suspicion’.  Instead  of
talking to McIntyre and McKitrick, he said, Crok should speak to people
like Jones, Briffa, or Jonathan Overpeck – all core members of the Hockey
Team (although that is not the way Mann described them).

Much of the substantive content of Mann’s email – where he discussed
the scientific controversies – merely went over old ground. However, he did
manage  to  come  up  with  some  new  information  on  the  verification
statistics. We have seen above that one of the chief criticisms of the Hockey
Stick was the fact that Mann had not published his verification R2 so that it
was impossible for anyone to gauge the reliability of the reconstruction.c In
his replies to McIntyre’s Nature submission, he had argued forcefully that
the R2 was a flawed statistic and that is was inappropriate to the particular
circumstances of the Hockey Stick. Since that time, however, McIntyre had
got hold of some useful new intelligence on the subject. Crok had posted
his  correspondence  with  Mann  on  the  Internet  and  this  revealed  an
interesting exchange on the subject of verification statistics.

CROK: There is a severe debate between you and [McIntyre and McKitrick] about the
skill of the calculation. You claim a high RE-statistic. [McIntyre and McKitrick] show
that  their  simulated  hockey  sticks  also  give  a  high  RE-statistic  but  a  very  low  R2
statistic.

MANN:  .  .  .  Our  reconstruction  passes  both  RE  and  [R2]  verification  statistics  if
calculated correctly. Wahl and Ammann (in press) reproduce our RE results (which are
twice as high as those estimated by [McIntyre and McKitrick]), and cannot reproduce
[their] results. There is little, if anything correct, in what [McIntyre and McKitrick]
have  published  or  claimed.  Again,  none  of  their  claims  have  passed  a  legitimate
scientific peer review process!98

So  Mann  was  admitting  that  he  had  calculated  both  RE  and  R2,  and  also
claiming that the Hockey Stick passed the R2 test. This made something of a
nonsense of his claim that R2 was a flawed statistic.

By the end of Crok’s investigations, he was in no doubt where the truth
lay, and his final article carried the title ‘Kyoto protocol based on flawed
statistics’  and  the  subtitle  ‘Proof  that  mankind  causes  climate  change
refuted’.102  In  the  ten  pages  of  the  article,  he  told  the  full  story  of
McIntyre’s  work  up  to  that  time  in  a  way  that  was  hard  to  ignore.  In
particular, Crok had discussed McIntyre’s work with a number of eminent
scientists,  all  of  who  supported  the  criticisms  of  MBH98.  For  example,
Eduardo Zorita confirmed that he had never heard the number of 159 proxy
series  that  Mann  now  claimed  were  used  in  MBH98.  In  Zorita’s  work  on
MBH98 the number was 112, just as it was in McIntyre’s original study, and
just  as  it  was  in  the  text  of  MBH98.  Crok  was  also  able  to  report  the
comments  of  two  important  scientists  both  of  whom  confirmed  that  the
Mann algorithm ‘mined’ the data for hockey sticks. One of these was Mia
Hubert, an expert in robust statistics. The other was Hans von Storch, an
eminent climatologist and a colleague of Zorita’s at the Institute of Coastal
Research.d Von Storch is one of the big names in climatology and had been
one  of  the  editors  who  had  resigned  from  the  board  of  Climate  Research
over  its  publication  of  the  Soon  and  Baliunas  paper,e  but  he  was  not  a
member of the Hockey Team either. Later though, he was seen as being in
competition with McIntyre to be the man who broke the Hockey Stick. It is
fair to say that he was, and remains, respected by both sides of the debate
and we will meet him again later in the story.f
Regalado in the Wall Street Journal
Marcel Crok’s article was just the start of it. In mid-February, and with the
papers in print, McIntyre made the front page of the Wall Street Journal, in
a long article which was very supportive of his work.

Since  it  was  published  four  years  ago  in  a  United  Nations  report,  hundreds  of
environmentalists,  scientists  and  policy  makers  have  used  the  hockey  stick  in
presentations and brochures to make the case that human activity in the industrial era
is causing dangerous global warming.
     But is the hockey stick true?
     According to a semiretired Toronto minerals consultant, it’s not.103

The  article  contained  some  interesting  revelations.  Its  author,  Antonio
Regalado,  had  asked  Mann  about  his  refusal  to  release  his  code  to
McIntyre. As Regalado explained:

Mr.  McIntyre  thinks  there  are  more  errors  [in  Mann’s  work]  but  says  his  audit  is
limited  because  he  still  doesn’t  know  the  exact  computer  code  Dr.  Mann  used  to
generate the graph. Dr. Mann refuses to release it. ‘Giving them the algorithm would
be giving in to the intimidation tactics that these people are engaged in’, he says . . .

.  .  .  which  was  a  remarkable  statement,  given  that  McIntyre  had  already
published  all  of  his  correspondence  with  Mann  on  the  old  Climate2003
website,  and  none  of 
it  could  even  remotely  be  construed  as
‘intimidation’.31

Regalado’s other scoop was the news that Mann had contacted the editor
of Geophysical Research Letters (GRL) in order to denigrate McIntyre’s new
paper.

The editor [of GRL], Steve Mackwell, says Dr. Mann contacted him to argue that the
Canadians’  work  was  deeply  flawed.  Dr.  Mann  then  put  a  critique  on  his  blog,
‘Realclimate.org’, calling the Canadians’ new paper ‘demonstrably specious’.

A few days later, McIntyre was in the Wall Street Journal again, this time as
the subject of an editorial.104 He was suddenly hitting the big time.
Media blitz
The coverage rapidly turned into a media blitz, although it was a long way
short  of  uniformly  favourable.  David  Appell,  who  had  been  one  of  the
earliest  critics  of  McIntyre,g  wrote  a  hagiography  of  Mann  for  Scientific
American,105 while Mann himself was treated to a primetime spot on BBC
radio, in which he was allowed to promote his side of the argument largely
unchallenged.106

McIntyre  was  picking  up  plenty  of  support  of  his  own.  Hendrik
Tennekes,  a  former  head  of  research  at 
the  Royal  Netherlands
Meteorological Institute emailed McIntyre to say that he thought Mann was
‘a disgrace to the profession’,107 and climatologist Kevin Vranes had this to
say about Mann’s withholding of data and code:

The [Wall Street Journal (WSJ)] highlights what Regaldo and McIntyre say is Mann’s
resistance or outright refusal to provide to inquiring minds his data, all details of his
statistical analysis, and his code. The WSJ’s anecdotal treatment of the subject goes
toward confirming what I’ve been hearing for years in climatology circles about not
just Mann, but others collecting original climate data . . .
          As  concerns  Mann  himself,  this  is  especially  curious  in  light  of  the  recent
RealClimate posts . . . in which Mann and Gavin Schmidt warn us about peer review

and the limits therein. Their point is essentially that peer review is limited and can be
much less than thorough. One assumes that they are talking about their own work as
well as McIntyre’s, although they never state this. . .
     Of their take on peer review, I couldn’t agree more. In my experience, peer review
is often cursory at best. So this is what I say to Dr. Mann and others expressing deep
concern  over  peer  review:  give  up  your  data,  methods  and  code  freely  and  with  a
smile on your face. That is real peer review . . .
     Your job is not to prevent your critics from checking your work and potentially
distorting it; your job is to continue to publish insightful, detailed analyses of the data
and  let  the  community  decide.  You  can  be  part  of  the  debate  without  seeming  to
hinder access to it.108

The trickle of support started to grow. It seemed as though McIntyre’s paper
was emboldening those scientists who doubted Mann’s findings. It was as if
the whole climatological community had been fearful of speaking out until
an outsider had pointed out the flaws in MBH98. Suddenly, the logjam burst
open  and  climatologists  outside  the  Hockey  Team  began  to  air  their
concerns in public for the first time. Ulrich Cubasch, an eminent German
researcher  and  IPCC  lead  author,  announced  that  his  team  were  also
examining  the  Hockey  Stick,  and  that  they  could  not  reproduce  Mann’s
results.  Moreover  he  said  that  they  had  found  ‘a  can  of  worms’.109 Even
von Storch was getting in on the party, referring to ‘Mann’s shoddiness’ in
the same article.110 When even mainstream scientists felt they could speak
out against the Hockey Stick, it was clear that something had changed. The
tide had started to turn.

a  It is, however, well indexed on the Wayback Machine for any readers who might want to see its

contents.96

simpler to check his work.

b    Practicing  what  he  preached,  McKitrick  had  posted  his  data  and  code  online,  making  it  much

c  See page 157.
d  By strange coincidence, Heike Langenburg, the Nature editor we met on page 135, used to be a
postdoctoral researcher in von Storch’s laboratory. I don’t imply any great significance to this fact
beyond an observation that climatology is a very small world.

e  See page 56.
f    Von  Storch  is  a  colourful  character  who  once  founded  a  club  to  defend  Donald  Duck  against
accusations of indecent behaviour, and for some years was the editor of a Donald Duck magazine,
Der Hamburger Donaldist.

g  See page 94.

8     Big Mac and the Two Whoppers

Criticism is prejudice made plausible.

A man’s a man for a’ that.

HL Mencken

Robert Burns

McIntyre  and  McKitrick’s  GRL  paper  attracted  no  less  than  four  formal
responses,  a  surprisingly  large  number.  As  we  have  seen,  when  formal
comments on a paper have been received, the journal will normally invite
the paper’s authors to review these submissions and to formulate a written
response.  With  four  comments  in  play,  McIntyre  might  have  been
concerned at the amount of work he was going to have to do to fend them
all  off,  but  as  he  looked  over  the  manuscripts,  he  knew  he  could  relax.
There was little in any of them that struck serious blows at his work.
Von Storch and Zorita
Since the publication of MM03, Hans von Storch and Eduardo Zorita had got
in on the act of investigating the Hockey Stick. In 2004 they had published
their  own  critique  of  MBH98,  in  which  they  had  concluded  that  Michael
Mann’s methodology was artificially reducing the size of the wiggles in the
Hockey Stick’s long ‘handle’.111

Von  Storch  now  picked  up  on  this  approach  in  the  comment  he  and
Zorita submitted on  MM05(GRL). He argued that, while the effect of short-
centred PCs was just as McIntyre and McKitrick had described, it was not
significant  in  the  final  MBH98  result.112  Von  Storch  had  reached  this
conclusion by using artificial tree ring series (‘pseudoproxies’), which were
created  by  taking  his  climate  model’s  temperature  output  for  a  particular
point on the Earth’s surface (a ‘gridcell’) and adding noise to it to make it
look more like a real tree ring series. Because the pseudoproxies had been
created under controlled circumstances, they were a kind of idealised tree
ring record, whose properties were understood exactly. The pseudoproxies
could  then  be  fed  into  Mann’s  algorithm  in  place  of  the  real  proxy  data,

theoretically  allowing  von  Storch  to  discover  exactly  what  the  effect  of
Mann’s procedures was.

Von Storch and Zorita’s approach was plausible, but in McIntyre’s view,
their implementation of it was problematic on at least two counts. Firstly,
von Storch and Zorita assumed for the sake of their calculations that there
was a reasonable correlation between temperature and pseudoproxy in the
gridcell,  in  other  words  that  the  trees  were  responding  to  their  local
temperature,  and  therefore  the  tree  ring  widths  would  be  a  simple
mathematical  function  of  that  temperature.  This  was  not  an  unreasonable
assumption because, as we saw in Chapter 2, paleoclimatologists pick trees
that are at the upper treelines in the belief that these trees are responding to
temperature and not to any of the other factors which can affect tree growth.
The problem with this approach, however, was that in the real MBH98 data
there  was  no  correlation  at  all  between  gridcell  temperature  and  the  tree
ring  widths.  Mann,  you  may  remember,  created  his  reconstruction  by
correlating temperature directly against temperature patterns covering much
larger areas rather than the immediate locale. He had argued that these trees
were not responding to their local temperature, but rather to temperatures
over  these  wider  areas,  by  means  of  the  ‘teleconnections’  that  we  met  in
Chapter 2.a

The  other  major  problem  with  von  Storch’s  approach  was  that  he
assumed the pseudoproxy data should consist only of a climatic trend plus
some  noise.  This  missed  a  fundamental  point  about  McIntyre  and
McKitrick’s  claims,  and  one  that  the  two  Canadians  had  made  again  and
again,  without  anyone  seeming  to  quite  get  the  message.  Their  argument
was not simply that the short centring would produce hockey sticks; it was
that it would pick out hockey sticks to the exclusion of everything else. The
point was subtly different, and emphasised the interaction between the short
centring and poor quality data – ‘a few bad apples’ as McIntyre was wont to
put  it.  Mann’s  algorithm  could  be  imagined  as  ‘scanning’  the  proxy
database for hockey stick shaped series – it was in essence an automated
method of ‘cherrypicking’ hockey stick shaped series.b If there was a bad
apple – a series whose hockey stick shape was of non-climatic origin (like
the  bristlecones)  –  the  algorithm  would  be  likely  to  declare  this  the
dominant pattern in the data, to the exclusion of anything else.

In their reply to von Storch and Zorita, McIntyre and McKitrick created
some new simulations that powerfully demonstrated this point.113 You will

remember that there were 16 bristlecone pine series in the NOAMER network.
Starting with the original MBH network of 112 series (i.e. including all of the
nonbristlecone data too), McIntyre removed one bristlecone series at a time,
measuring  how  much  of  a  hockey  stick  shape  the  resulting  PC1  had  after
each  step  (using  the  hockey  stick  index).c  For  effect,  he  kept  the  most
heavily  weighted  series,  Sheep  Mountain,  until  last.  The  effect  was
extraordinary. The hockey stick index of the PC1 started at something over
1.4 when all the bristlecone series were present. As each of the 16 series
was  removed,  the  index  remained  to  all  intents  and  purposes  entirely
unchanged.  Even  when  there  were  only  three  left,  the  algorithm  still
produced a virtually unchanged hockey stick. With two bristlecone series in
the  network,  the  shape  of  the  PC1  was  only  slightly  attenuated,  with  a
hockey stick index of 1.1. It wasn’t until bristlecone representation in the
network was pruned to just the Sheep Mountain series that the hockey stick
shape disappeared from the PC1. Even then it reappeared in the PC2, which
Mann would presumably argue could be carried forward to the calibration
under his supposed application of Preisendorfer’s Rule N.

This  then,  was  the  crux  of  McIntyre’s  argument.  If  you  had  a  hockey
stick shaped series the short centring would put it in the PC1. Then, because
hockey  stick  shaped  series  correlated  well  with  the  temperature  PCs,  the
final  temperature  reconstruction  would  have  a  hockey  stick  shape  too.  It
was, as he said, a complete answer to von Storch’s comment.
Huybers
Peter  Huybers’  was  one  of  the  more  challenging  responses  to  McIntyre’s
new  paper.  Huybers  was  a  post-doctoral  fellow  at  the  Woods  Hole
Oceanographic Institution in Massachusetts and he had contacted McIntyre
soon  after  the  publication  of  the  new  papers.  There  had  been  a  regular
exchange of emails since, which McIntyre describes as ‘mostly cordial’.d

Huybers’  correspondence  with  McIntyre  seemed  to  suggest  that  there
were many areas of agreement between them. For a start, he seemed quite
convinced that short centring would indeed bias the  PC calculation to find
hockey sticks, although when McIntyre tried to make this agreement clear
in his reply to Huybers’ comment,117 he was at first shot down by the GRL
editor, who felt that this was an attempt to divert attention from differences
of  opinion.  This  prompted  McIntyre  to  write  again  to  Huybers.  It  was
important,  he  said,  that  the  wider  climatology  community  understood  the

points  of  agreement  as  well  as  any  bones  of  contention.  When  Huybers
consented,  they  were  able  to  put  on  a  united  front,  and  the  journal
somewhat  reluctantly  agreed  that  McIntyre  could  state  in  his  reply  that
Huybers ‘concurred’ that Mann’s algorithm was biased.

From his correspondence, Huybers also seemed to agree with McIntyre
that  bristlecones  were  unsuitable  proxies.  However,  in  his  comment
Huybers  made  a  complete  volte-face  and  wrote  that  their  suitability  as
proxies should be assessed in later studies, a most peculiar position when
there  was  already  a  considerable  body  of  literature  suggesting  that  there
were  unidentified  nonclimatological  effects  distorting  the  temperature
signal.
Covariance matrix, correlation matrix
Huybers’ next criticism addressed a fairly obscure corner of the Mannian
algorithm. When the proxy series are summarised down to the PCs, the first
step, as we’ve seen, is to centre the data by taking away the series average.
In  the  next  step,  there  are  different  ways  of  performing  the  calculation:
Mann had used an approach called SVD,e which was generally agreed to
have made his methodology even more biased than it already was. Huybers,
however, was taking issue with McIntyre’s use of another method called the
covariance matrix, rather than a third method, the correlation matrix, saying
that  it  exaggerated  how  bad  Mann’s  method  was.  The  two  methods  –
covariance and correlation – are actually very similar; the only difference is
that in the correlation matrix the data is adjusted to put everything on the
same scale – simply by dividing by the standard deviation. At this point you
might  well  be  saying  to  yourself  ‘but  wait  a  minute,  isn’t  everything
directly  comparable  already?’  and  you’d  be  quite  right.  For  paleoclimate
reconstructions, standardisation is not necessary within the matrix because
the data has already been standardised prior to processing. If a correlation
matrix  is  used,  it  would  mean  that  the  data  would  in  effect  be  re-
standardised.  Because  of  this,  McIntyre  had  used  the  covariance  matrix
instead, an approach that he found was endorsed by most of the statistical
authorities he could find in the literature. However he didn’t rule out using
the alternative approach.

The use of a correlation matrix (i.e. re-normalizing) is certainly an option, but climate
history  should  not  stand  or  fall  on  this  choice.  The  bristlecones  do  get  promoted
higher with a correlation matrix than with a covariance matrix.114

This was a position with which Huybers seemed inclined to agree. In fact,
he explained to McIntyre:

It is . . . rather unsatisfying that answers are so sensitive to seemingly small changes in
technique.114

So when McIntyre read the submitted comment, he was surprised to see that
Huybers was now in effect arguing that the choice he and McKitrick had
made to use the covariance matrix was exaggerating the apparent bias of the
MBH98 methodology.

In  summary,  [McIntyre  shows]  that  the  normalization  employed  by  MBH98  tends  to
bias  results  toward  having  a  hockey-stick-like  shape,  but  the  scope  of  this  bias  is
exaggerated  by  the  choice  of  normalization  and  errors  in  the  RE  critical  value
estimate.117

In  order  to  demonstrate  this  alleged  exaggeration  Huybers  presented  a
graph that showed how each of the three methods – MBH98 (short-centred),
McIntyre’s  (with  a  covariance  matrix)  and  Huybers  (with  a  correlation
matrix) – when applied to the AD 1400 proxy roster, compared to the simple
average of the underlying data. As he presented it, there was a gap between
the result you got from McIntyre’s covariance PC1 and the simple average
for  most  of  the  length  of  the  record,  the  implication  being  that  the  way
McIntyre was standardising the data was making MBH98 look worse than it
really  was.  This  is  shown  in  Figure  8.1.  McIntyre’s  version,  using  the
covariance matrix, is shown at the top and for most of the series there is a
gap between the PC1 and the average. Huybers’ correlation matrix is at the
bottom, matching the average much better. Huybers’ point was somewhat
moot,  however,  because,  as  McIntyre  observed,  if  your  intention  was  to
recreate  the  average  of  the  data,  it  might  be  simpler  to  actually  use  the
average. As we saw in the simple examples in Chapter 2, the whole point of
PC analysis is to capture features that are hidden by simple averages.

McIntyre also noticed a neat device that Huybers had used to make his
claims more credible. When you present two graphs on the same axes in
this way, it is normal to try to make the whole length of the two lines match.
But  when  Huybers  had  put  the  series  average  up  against  the  PC1s  in  his
paper, he had not done this; he had only tried to match the twentieth century
portion of the graph. It was this change that had opened up the gap between

McIntyre’s  covariance  PC1  and  the  series  mean,  apparently  supporting
Huybers’  thesis  that  the  covariance  matrix  was  exaggerating  things.
However,  McIntyre  was  able  to  show  that  if  you  centred  the  two  lines
properly,  the  covariance  PC1  would  have  been  pushed  down  so  that  it
matched  the  average  for  most  of  its  length,  but  opening  up  a  small
divergence  in  the  twentieth  century.  This  supported  McIntyre’s  position,
which was originally Huybers’ too, at least in their correspondence, that the
issue was relatively trivial.

McIntyre also pointed out that when you did the same comparison on the
full  network,  rather  than  just  the  AD  1400  network,  with  its  very  small
number of series, the average was most like the result from the covariance
method.  This  suggested  that  it  was  Huybers’  correlation  matrix  that  was
biasing the results, not the covariance matrix.

Huybers also tried to find support for the use of the correlation matrix in
the  scientific  literature,  and  cited  texts  by  two  eminent  statisticians,
Preisendorfer  (of  Rule  N  fame)  and  Rencher.  Strangely,  though,  Huybers
did  not  include  any  page  numbers  with  his  references  to  these  experts’
work, as is normal when citing a specific part of a book. When McIntyre
referred to the texts himself, both authors turned out to be unequivocal in
their support for the use of the covariance matrix and not the correlation.f In
the face of what appeared to be a misrepresentation of what Preisendorfer
and  Rencher  had  said,  McIntyre  decided  to  take  the  point  up  with  GRL’s
editor, Jay Famiglietti.g

FIGURE 8.1: Correlation matrix versus covariance matrix
Thin black line: PC1; Thick grey line, series mean.

Both  citations  are  texts,  yet  no  chapter  or  page  citations  are  given.  The  reason,  I
suggest, is that it would be impossible for Huybers to provide a direct quotation from
either  authority  on  this  point  because  they  do  not  support  the  procedure  Huybers
proposes  .  .  .  In  fact,  their  explicit  statements  run  in  the  opposite  direction.  As  a
general matter, it is simply false that scaling to unit variance is a ‘standard practice in
[PC analysis]’ for data sets already standardised to dimensionless units with a common
mean. The onus is on Huybers to back up this claim. Neither of his authorities do so.
He  should  either  produce  explicit  support,  such  as  a  chapter  and  page  number  or
remove the claim. The onus should not be on us to use up our word limit providing
extensive quotations from his own sources to show that they do not support him.116

In their draft reply to Huybers’ comment, McIntyre and McKitrick had tried
to  address  these  issues,  but  were  advised  by  Famiglietti  that  they  should
remove these sections since they would be dealt with editorially – in other
words that the journal editors would speak to Huybers and have him amend
his submission. McIntyre moved on to the more substantive issues.
Huybers on standardisation
Huybers had also spoken of McIntyre and McKitrick having attempted to
‘remove  the  bias’  in  MBH98  by  applying  standardisation  based  on  the  full
length  of  the  series,  and  described  their  standardisation  procedure  as
‘questionable’,  referring  to  it  as  ‘MM  [i.e.  McIntyre  and  McKitrick]
normalization’. This seemed to be an unnecessary personalisation of what
was actually the text book approach to the issue. McIntyre was also irritated
because these statements appeared to imply, as so many critics of his work
had  done,  that  he  and  McKitrick  were  trying  to  create  an  alternative
temperature reconstruction to Mann’s and that they were advocating the use
of the covariance matrix to do this correctly. This was something they had
consistently stated was not their intention, since they did not believe that it
was possible to extract a meaningful temperature signal from tree rings by
any method. McIntyre was understandably irritated with Huybers and asked
Famiglietti to intervene but to no avail.
Huybers on verification statistics
Huybers’ points on PC methodologies may have been easily dealt with, but
he had some much more challenging points to make about the verification
statistics. Privately, Huybers conceded that MBH98 catastrophically failed the
R2, although it took a little persuasion to get him and the journal to allow
McIntyre and McKitrick to say so in their reply. He was not convinced that
it failed the  RE test however. His issue was with the benchmarking of the
RE. We’ve seen how Mann and McIntyre had used Monte Carlo methods –

generating red noise pseudoproxies and processing them through the MBH98
algorithms – to assess how high the RE needed to be to indicate a significant
result.  Huybers  said  in  his  comment  that  McIntyre  had  missed  out  an
important  part  of  the  MBH98  procedure  and  that  if  he  replicated  the  exact
steps of the paper, he would get a benchmark level of zero, just as Mann
had. The missing step was part of the calibration process, where the proxy
records are matched up against the temperature records in order to define
the mathematical relationship between them. Huybers had noticed that the
pseudoproxy variances (the size of the wiggles in the line) was less than the
variance  of  the  target  –  the  temperature  data  –  and  argued  that  the
pseudoproxies  should  be  adjusted  by  ‘rescaling’  them.  This,  he  said,  was
what Mann had done with the real data and Huybers claimed that it was a
critical step in the process, although he didn’t explain why.

There was no mention of this rescaling step in the original MBH98 paper,
but  Mann’s  recently  released  code  (see  Chapter  9)  showed  that  he  and
Bradley  and  Hughes  had  rescaled  their  variances,  although  in  a  rather
different way to that described by Huybers. McIntyre was able to recreate
the  rescaling  the  way  Huybers  had  performed  it  and  agreed  that,  if  the
calculation was done as Huybers described, then the benchmark should be
zero. However, the difference between Huybers’ method and Mann’s turned
out to be important.

In the AD 1400 step, Mann had taken the  NOAMER PC1 and combined it
with  the  21  other  proxy  series.  Then  he’d  gone  through  the  steps
(calibration,  verification,  reconstruction)  to  take  him  to  the  reconstructed
PC1. Then he had rescaled so as to make the variances of the reconstructed
PC1 and the temperature PC1 the same. Huybers, on the other hand, simply
took his pseudoproxy PC1s and inflated them to match the variance of the
observed  temperature  record.  So  the  difference  between  the  two  methods
was  that  in  Mann’s  procedure,  the  PC  was  mixed  with  other  data  before
rescaling,  whereas  in  Huybers’  it  wasn’t.  Both  procedures  inflated  the
variance by the required amount, but with rather different side effects.

In order to reveal the effects of this difference in methodology, McIntyre
created some new simulations where the pseudoproxy PC1s were combined
with 21 white-noise series representing the 21 other proxy series used in the
original  AD  1400  step.  These  were  processed 
the  whole
reconstruction  process,  including  the  newly  revealed  rescaling,  right
through to the reconstructed temperature  PC.  This  replicated  as  closely  as

through 

possible  the  exact  Mann  algorithm  as  it  was  currently  understood.  When
this  was  done  for  all  1000  simulations,  the  revised  RE  benchmark  was
calculated to be 0.54, rather than the 0.59 calculated in the original MBH98
paper. While this was a slight reduction, it was well in excess of the zero
score argued for by Mann and Huybers and still above the score achieved
by the Hockey Stick itself.

All  of  Huybers’  criticisms  had 

therefore  been  dealt  with.
Huybers’comment and the Climate Auditors’ response118 would go forward
for  publication,  but  in  reality  not  a  finger  had  been  laid  on  McIntyre’s
critique of MBH98.
The Hockey Team comments
The two remaining comments on MM05(GRL) were both from Hockey Team
members.

The  first  was  from  from  David  Ritson,  a  physicist  from  Stanford  and
sometime  guest  author  at  RealClimate.  Ritson’s  comment  was  rather
strange. McIntyre described it as ‘goofy’119 and when he pointed out some
of the problems to the editors they chose to drop it in its entirety rather than
ask him to reply formally. Strangely though, this was not the last that was
heard of Ritson’s comment.
Wahl and Ammann
The other, more important, Hockey Team contribution came from Caspar
Ammann  and  Eugene  Wahl.  Ammann,  who  was  the  lead  author  of  the
comment  on  McIntyre’s  paper,  had  been  a  PhD  student  of  Ray  Bradley’s
and  had  since  published  several  papers  with  Mann.  In  fact,  so  close  was
Ammann’s  association  with  Mann  that  Climate  Audit  readers  unkindly
referred to him as ‘mini-Mann’. He now worked at University Corporation
for  Atmospheric  Research  (UCAR),  a  major  US  centre  for  climate  change
research in Boulder, Colorado.h

McIntyre had first come across Ammann back in 2004 at the AGU Fall
Meeting, where he had been presenting the poster on his work and inviting
visitors to ‘guess which was the real Hockey Stick’.i When not speaking to
visitors to his poster, McIntyre had taken the chance to listen to some of the
main presentations, and one of these had featured Ammann explaining that
he was in the process of replicating the Hockey Stick. In fact, Ammann had
gone so far as to tell the newspapers that he could recreate Mann’s work

precisely.  This  had  sounded  rather  surprising  to  McIntyre,  who  was  still
unable  to  replicate  Mann’s  results  exactly.  He  had  therefore  written  to
Ammann  to  enquire  when  his  results  would  be  published  and  whether  it
would be possible see a draft of the paper. However, despite writing twice,
he never received a reply.
Press release
Nothing more was heard of Ammann’s replication of the Hockey Stick for
six months. Then, in May 2005, with the controversy over the papers raging
on all sides, McIntyre and McKitrick were invited to give a lecture on their
findings at a meeting of the Heritage Foundation, a political think tank in
Washington  DC.  On  the  morning  before  McIntyre  was  due  to  speak,
Ammann and Wahl suddenly issued a press release via the UCAR website. In
it  they  claimed  that  they  had  submitted  two  manuscripts  for  publication,
which together showed that the Hockey Stick could be exactly replicated,
confirmed its statistical underpinnings and demonstrated that McIntyre and
McKitrick’s criticisms were baseless.

[Caspar] Ammann and Eugene Wahl of Alfred University have analyzed the Mann-
Bradley-Hughes (MBH)  climate  field  reconstruction  and  reproduced  the  MBH results
using  their  own  computer  code.  They  found  the  MBH  method  is  robust  even  when
numerous  modifications  are  employed.  Their  results  appear  in  two  new  research
papers  submitted  for  review  to  the  journals  Geophysical  Research  Letters  and
Climatic Change. The authors invite researchers and others to use the code for their
own evaluation of the method.120

Ammann’s  submission  to  GRL  then,  was  the  last  of  the  comments  on
McIntyre’s paper.121 The circumstances of the press release caused several
observers  to  raise  eyebrows:  the  practice  of  using  a  press  release  to
announce  scientific  findings  was  dubious  in  itself,  but  some  people  also
noted that when announcements of this kind are made, they tend to be about
papers that have just been published, or at least accepted for publication. To
make  such  a  dramatic  statement  about  the  submission  of  a  paper  was
unusual  in  the  extreme.  McIntyre  was  certainly  unable  to  find  any  other
UCAR  press 
in  similar  circumstances.
Unfortunately, this sort of subtlety probably went unnoticed by the majority
of  readers,  and  if  any  of  the  journalists  who  wrote  about  Ammann  and
Wahl’s work did spot it, they failed to point it out to their readers.
The comment

releases  announcing  papers 

Ammann and Wahl made two main points of attack in the comment they
submitted to GRL (‘the GRL comment’), covering much of the same ground
as  Mann  was  making  in  his  comments  at  RealClimate.  Firstly,  they  took
issue with McIntyre’s arguments over PC analysis, which they attempted to
do  by  means  of  a  subtle  (or  not-so-subtle,  depending  on  how  close  the
reader was to the debate) misrepresentation of what McIntyre was saying.
While McIntyre and McKitrick had always said that short centring of the
PCs biased the algorithm so as to overweight the bristlecones, Ammann and
Wahl chose to paraphrase this position as follows:

[McIntyre  and  McKitrick]  claim  that  the  standardization  approach  chosen  by  MBH
biases the [tree ring] information towards a ‘hockey stick’ shape . . .121

which could be construed as suggesting that the Canadians believed that the
hockey stick shape was introduced by the algorithm (rather than being in
the mix already in the shape of the bristlecones). They made this insinuation
more plain a little later in the article:

[McIntyre  and  McKitrick]  emphasize  that  the  ‘hockey  stick’  shape  is  introduced
because the standardization is performed relative to a subsection rather than the full
series . . .121

and again, a little further on:

[T]he  MM-claim  that  a  ‘hockey  stick’  outcome  in  the  PCs  is  an  artifact  of  the  MBH
standardization procedure is incorrect . . .121

and yet again in their conclusions:

The  claim  by  [McIntyre  and  McKitrick]  that  a  spurious  ‘hockey  stick’  climate
reconstruction is introduced by data transformation is unfounded.121

Having set up their straw man, they then went on to discuss PC retention
policies but also claimed that in their second paper they had shown that no
matter  how  you  standardised  the  data,  you  would  still  get  a  hockey  stick
provided you retained enough PCs. This repetition of Mann’s Preisendorfer
argument  was  also  repeated  in  several  places  throughout  the  paper  –  the
text,  the  summary  and  the  abstract.  The  problem  with  this  argument  was
that McIntyre was unable to respond to it because Ammann’s other paper

had not been published. McIntyre penned a letter to the GRL editor, James
Saiers, complaining about what he called this ‘pyramid scheme’, and also
pointing  out  that  he  and  McKitrick  had  discussed  the  whole  issue  of  PC
retention  in  the  Energy  and  Environment  paper  and  not  the  GRL  one
anyway.119  They  could  hardly  be  expected  to  defend  their  Energy  and
Environment paper in GRL.

Apart from these issues, Ammann and Wahl had presented several results
as  if  they  were  revealing  them  for  the  first  time,  when  in  fact  they  were
merely  reiterating  points  made  by  McIntyre  and  McKitrick  in  their  own
papers.  This  failure  to  cite  the  Canadians  also  had  the  side  effect  of
implying  to  the  unwary  reader  that  there  had  been  an  oversight  in
McIntyre’s  work.  There  were  other  bones  of  contention  too.  In  fact,  the
appendix to McIntyre’s letter listing all the issues with Ammann and Wahl’s
GRL comment ran to nine pages. This exhaustive treatment did seem to have
the desired effect on Saiers though, and a couple of weeks later McIntyre
received  a  letter  informing  him  that  Ammann  and  Wahl’s  comment  had
been rejected. At this point, one might have expected UCAR to withdraw or
modify  their  press  release,  but  this  was  not  done,  allowing  Hockey  Stick
supporters to continue to claim that McIntyre’s work had been refuted.
The paper
Meanwhile  the  second,  longer  paper  (‘the  CC  paper’)  had  started  its
protracted path to publication at the journal Climatic Change.122 This paper
purported to be a replication of the Hockey Stick and confirmation of its
scientific validity.

All McIntyre’s previous attempts at creating an exact replication of the
Hockey  Stick  had  been  hampered  by  lack  of  access  to  Mann’s  computer
code.  Fortunately,  though,  at  the  time  they  had  issued  their  press  release,
Ammann had also published his own computer programs, which, given that
he was claiming that he had exactly replicated MBH98, must almost certainly
have been identical to Mann’s. McIntyre therefore set to work to analyse
Ammann’s code and was able to make very fast progress. In fact, by the
time  the  CC  paper  was  submitted  to  Climatic  Change,  McIntyre  had
reconciled Ammann’s work with his own.

Amazingly, Wahl and Ammann’s emulation of the Hockey Stick turned
out to be nearly exactly the same as his own. Therefore he could be quite
sure  that  the  CC  paper  suffered  from  exactly  the  same  problem  as  the

Hockey  Stick  itself:  despite  Mann’s  having  told  Marcel  Crok  that  the
Hockey Stick passed the R2 verification test, the R2 number was in fact so
low as to suggest that Mann’s creation had no meaning at all, although the
RE was relatively high. Because McIntyre’s replications of both Mann’s and
Ammann’s  papers  had  these  low  scores,  far  from  proving  the  scientific
integrity  of  the  Hockey  Stick,  the  CC  paper  actually  confirmed  one  of
McIntyre’s main criticisms of it.

As the CC paper was critical of his work, McIntyre was invited to be one
of  its  peer  reviewers,  and  shortly  after  accepting  the  appointment  he
received  a  short  letter  from  Ammann  and  Wahl.  This  letter  was  simply  a
formality, an invitation to McIntyre to contact them if he didn’t receive a
copy of the paper from the publisher.123 However, this seemed like a good
opportunity  to  break  the  ice  a  little,  so  McIntyre  wrote  a  long,  good-
humoured  reply,  which  brought  Ammann  up  to  date  on  the  attempt  to
reconcile his own code with Ammann’s, and outlining what he thought the
main  points  of  contention  were  going  to  be  in  the  final  reckoning.  If
McIntyre  thought  he  was  proffering  an  olive  branch,  Ammann  had  other
ideas, declining to acknowledge the Canadian for a third time.

Some  days  later  McIntyre  wrote  to  Climatic  Change  to  ask  them  to
obtain a full set of verification statistics from Wahl and Ammann – R2, RE
and  a  selection  of  others.  He  noted  in  his  email  that  Ammann  had
emphasised the importance of reporting these figures and had indicated that
they  would  be  available.123  Confirmation  that  the  R2  was  close  to  zero
would strike a serious blow at Ammann’s CC paper, and it appeared that his
two opponents understood this fact just as well as he did: their response was
an outright refusal to release any of the numbers McIntyre had requested,
and a suggestion that he might like to calculate the statistics himself using
the code they had made available.123 This was a clear flouting of Climatic
Change’s  rules,  and  moreover  directly  contravened  the  journal’s  stated
position  that  peer  reviewers  were  not  expected  to  run  code.j  As  a
justification  of  their  extraordinary  action,  Ammann  and  Wahl  gave  a
lengthy  exposition  of  the  superiority  of  RE  over  other  measures  and  also
argued  that,  in  their  forthcoming  GRL  comment,  they  would  rebut
McIntyre’s  criticisms  of  Mann’s  RE  benchmarking.123  This  was  a
remarkable  statement  because,  as  we’ve  seen,  the  GRL  comment  had
actually  been  rejected  by  the  journal  some  days  earlier,  and  besides,  the

availability  of  the  code  didn’t  absolve  Ammann  and  Wahl  of  the  duty  to
calculate and present their verification statistics.

At  the  end  of  June,  with  his  review  of  the  CC  paper  nearly  complete,
McIntyre  took  the  opportunity  to  tactfully  probe  this  point,  by  asking
Climatic Change to obtain the publication date and content of this alleged
rebuttal  from  Ammann.123  Perhaps  in  order  to  maintain  a  modicum  of
decorum,  the  Climatic  Change  editor,  Stephen  Schneider,  chose  not  to
respond to this letter. Two weeks passed before McIntyre decided that he
could  wait  no  longer  and  sent  off  his  review  comments.124  This  was  a
lengthy letter, running to ten pages, in which McIntyre laid all of his cards
on the table. The tone was one of frustration and exasperation that he was
being forced to deal with a paper which was such a travesty.

Climatic Change should reject this article. First, the errors and mischaracterizations
are so numerous and affect the central conclusions so severely that dealing with the
required corrections will require a completely new article and rejection of the present
article  is  mandated.  Secondly,  the  authors  have  flouted  a  Climatic  Change  policy
requiring  authors  to  provide  supporting  data  and  calculations  and  have  provided  a
highly  implausible  rationalization  for  their  position.  Finally  and  most  importantly
———————k  in  their  Response  Letter  by  citing  a  submission  they  knew  had
already been rejected, in support of a point it did not provide support for anyway.124

Twenty-four hours later, Schneider’s assistant emailed through a letter from
Ammann in which he admitted that the GRL comment had been rejected and
with  it  the  alleged  refutation  of  McIntyre’s  RE  benchmarking  work.
Ammann  had  gone  on  to  say  that  the  GRL  editors  had  rejected  the  paper
because  he  and  Wahl  had  covered  points  made  by  other  commenters,
although he stressed that they would be submitting the comment elsewhere.
With  the  replication  of  the  Hockey  Stick  in  tatters,  reasonable  people
might  have  expected  some  sort  of  pause  in  the  political  momentum.
Seasoned observers of the climate debate, however, will be unsurprised to
hear  that,  in  practice,  global  warming  promoters  acted  as  if  nothing  had
changed.  The  UCAR  press  release  was  not  withdrawn  and  key  global
warming  players  such  as  the  head  of  the  IPCC’s  scientific  assessment
working group, Sir John Houghton, and Mann continued to cite the Wahl
and  Ammann  papers  as  evidence  that  McIntyre  and  McKitrick  had  been
refuted.  In  testimony  before  the  US  Senate  in  July  2005,  Houghton  cited
Ammann’s two papers (which McIntyre referred to as the Big Whopper and
the  Little  Whopper)  in  order  to  counter  a  suggestion  that  there  was  a

problem with the Hockey Stick. He outlined Ammann’s arguments in both
the paper and the comment, stating that one was ‘in review’ and the other
‘in  press’.125  But  at  the  point  at  which  he  made  this  declaration,  the  CC
paper  was  in  publishing  limbo  and  the  GRL  comment  had  already  been
rejected more than a month earlier.
The coup at GRL
Events soon took another surprising turn. In August 2005, McIntyre read an
article in Environmental Science and Technology, a journal published by the
American Chemical Society. The article was the latest in the long line of
‘hit’ pieces that McIntyre had had to endure since the publication of his new
papers  in  Energy  and  Environment  and  GRL,  and  included  all  the  usual
attempts to connect him to oil companies, to question his credentials or to
otherwise denigrate his work. What was interesting about the article was a
short interview with the editor-in-chief of GRL, Jay Famiglietti. Famiglietti
told  the  interviewer  that  he  had  decided  to  replace  James  Saiers  as  the
editor-in-charge of the file for the McIntyre paper and its responses. In fact
he  was  taking  over  the  file  personally.  This  was  justified,  he  claimed,
because of the high number of responses – four – that it had received, a turn
of phrase that neatly avoided mention of the fact that two of those responses
had been rejected already. This was a concern for McIntyre. Famiglietti had
been quoted in the article as saying:

If I had a student come to me and say, ‘I found this one paper that proves that climate
change is hogwash,’ I’d say, ‘Well, that’s one paper out of how many? In science, you
never look at [only] one paper.’126

leaving the question in readers’ minds of whether it was McIntyre who had
suggested  something  so  scientifically  ridiculous.  It  looked  very  much  as
though Famiglietti might not be entirely evenhanded.

The worries over Famiglietti’s intentions were quickly realised when, at
the  end  of  September,  he  wrote  to  McIntyre  to  inform  him  that  David
Ritson’s comment, which Saiers had rejected out of hand, had not only been
readmitted for review  but  had  been  accepted.  This  would  appear  to  have
breached  the  journal’s  own  policies,  which  stated  that  a  comment  on
another  article  had  to  be  sent  out  for  peer  review  accompanied  by  a
response  from  the  author  of  the  original  article.  To  add  insult  to  injury,
Famiglietti now belatedly invited McIntyre to prepare a response.

McIntyre  was  understandably  annoyed  and  wrote  a  strongly  worded
letter  to  Famiglietti,  pointing  out  that,  in  the  light  of  his  comments  in
Environmental  Science  and  Technology  and  his  multiple  breaches  of  his
own journal’s policies, his impartiality could now be questioned. He asked
that Famiglietti hand over the file for McIntyre’s paper and its responses to
another editor. Famiglietti replied almost immediately, asking to discuss the
matter  by  telephone,  and  he  and  McIntyre  arranged  a  conference  call  in
which McKitrick would also take part. Famiglietti insisted, however, that he
would only discuss the matter if the conversation was off the record. The
only information that he would allow to be divulged was a statement that
his  comments  in  Environmental  Science  and  Technology  had  not  been
directed  at  McIntyre  and  McKitrick.  All  details  about  his  ‘coup’  and  his
treatment of McIntyre remain a secret to this day.

through 

If McIntyre had any suspicions about the implications of the goings-on
in the GRL editorial office, these must have been swept aside when, shortly
afterwards  on  29  September,  Mann  commented  on  his  RealClimate  blog
that both of Ammann’s papers were back in play. The CC paper and the GRL
comment, he said, were ‘pending final acceptance’, although how he knew
this is not clear. McIntyre checked the  UCAR webpage for Ammann’s  GRL
comment but there was no apparent change in its status. However, two days
later,  an  observant  Climate  Audit  reader  noticed  that  the  page  had  been
updated and now showed that the  GRL comment had been resubmitted on
the  25th.  As  McIntyre  acidly  observed,  Ammann’s  work  had  made
remarkable  progress 
its
resubmission  on  the  25  September  and  its  position  of  ‘pending  final
acceptance’ just four days later. Both of the Wahl and Ammann papers were
indeed back in play.
The IPCC submission deadline
As 2005 neared its end, two important events loomed large. The first was
the  year-end  deadline  for  submission  of  papers  for  the  IPCC’s  Fourth
Assessment Report. Prompted by a Climate Audit reader, a possible reason
for the goings-on at GRL gradually dawned on McIntyre and his supporters:
did the IPCC need to have the Wahl and Ammann papers in the report so that
they  could  continue  to  use  the  Hockey  Stick  to  maintain  the  political
pressure?  Could  this  have  been  the  reason  for  Famiglietti’s  coup  at  GRL?
Had  someone  put  pressure  on  the  journal  to  ensure  that  Wahl  and
Ammann’s papers remained on the record so that they could be used by the

the  peer  review  system  between 

IPCC? The suspicion remains that this was the case. It appears that the price
for  resurrecting  Ammann’s  GRL  comment  was  to  resurrect  the  ‘goofy’
Ritson  comment  too.  Then  the  whole  affair  could  be  presented,  however
unconvincingly, as a ‘clean start’. While this could be seen as ‘conspiracy
theorising’ we will see in Chapter 11 just how important Ammann’s work
was to the IPCC project.
AGU 2005
The  second  important  happening  was  the  Fall  Meeting  of  the  American
Geophysical Union, which would be attended by many of the big names in
paleoclimate and at which both McIntyre and Ammann would be making
presentations. McIntyre’s plan was to use the question and answer session
after Ammann’s talk to once again press for his verification statistics. His
question  was  to  be:  ‘What  is  the  value  of  the  cross-validation  R2  for  the
fifteenth  century  MBH98  reconstruction?’  Perhaps  now,  after  requests  to
Mann,  to  Bradley,  to  the  National  Science  Foundation,  to  Nature,  to
Climatic Change, and to Ammann himself, and even a demand from the US
Congress (see Chapter 9), the figure might finally see the light of day.

As  a  student  at  Oxford,  McIntyre  had  played  some  rugby  and  had
developed an admiration for the rugby players’ ability to enjoy a beer with
the same people they’d been pummelling shortly before. He often tried to
in  his  face-to-face  meetings  with  his
adopt 

the  same  approach 

Ammann’s AGU presentation was pretty much as expected – there was a
great  deal  of  criticism  of  McIntyre  and  little  new  science  to  add  to  the
record.  When  it  came  to  the  question  and  answer  session  McIntyre  was
finally  able  to  confront  Ammann  with  the  fateful  question.  So  what  then
was the R2 figure for the fifteenth century?

Ammann still wasn’t saying. When McIntyre put the question, Ammann
prevaricated at great length, presenting an extended argument as to why the
audience shouldn’t have the long-awaited figure. This was essentially the
same argument that he had given for his refusal to release the figures as part
of  the  review  process  of  the  CC  paper  –  that  R2  was  an  inferior  measure,
which didn’t capture important features of the data. His evasions didn’t go
unnoticed by the audience though, which included many of the big names in
climatology,  von  Storch  and  Zorita  among  them.  However,  Ammann
extended his reply sufficiently to talk out the session and it was not possible
for anyone to press him further.

climatological opponents. After the AGU session, therefore, he attempted to
clear the air by inviting Ammann out to lunch, and was gratified, if rather
surprised, when the younger man took him up on the offer.

McIntyre  later  explained  to  his  Climate  Audit  readers  what  had
happened.  Under  the  circumstances,  most  of  their  time  together  seems  to
have been spent relatively amicably, the two men exchanging small talk and
passing  the  time.  Inevitably  though,  the  discussion  had  turned  to  more
serious matters such as the need to disclose the verification statistics:

I urged him at lunch – in his own interests – to deal with the issue himself. In any
enterprise, dealing with the bad news is no fun, but you’ve got to do it and you’re
always better off dealing with it yourself, rather than having someone else hammering
you with it. I pointed this out to him in the nicest possible way. I told him that, if he
doesn’t, it will be awfully easy for me to excoriate him for withholding these adverse
statistics and that I would obviously do so. I asked him: why give me such an easy
target? He was relatively young; I was trying to coach him.127

If Ammann understood that McIntyre was trying to help him, he certainly
didn’t seem to take the advice on board. He launched into another attempt
to justify withholding the validation statistics, claiming that McIntyre had
not published the equivalent figures for ‘his reconstruction’ – an argument
which, apart from being fallacious, flew in the face of McIntyre’s repeated
statements  that  he  and  McKitrick  were  not  offering  up  an  alternative
reconstruction but were merely demonstrating that Mann’s was not robust.
McIntyre reiterated this for Ammann’s benefit, but the conversation seemed
to  be  going  nowhere,  and  eventually  McIntyre  threw  in  the  towel  and
suggested that they return to the conference. As they were getting ready to
leave,  however,  Ammann  returned  to  the  subject  of  their  professional
relationship,  complaining  vehemently  about  the  way  that  McIntyre  was
dealing with him and Wahl. McIntyre was not impressed:

As we were winding up, in fact, just as we were returning to AGU, Ammann screwed
up his nerve to complain about getting roughed up and my tactics in doing so, which
he  didn’t  like  very  much.  This  is  a  guy  who  had  used  UCAR  press  facilities  and
distribution to issue a national press release on the very day that we’re making a rare
public appearance, announcing his submission of two articles supposedly debunking
us and the horse we rode in on. This press release was then relied on by Houghton,
Mann and others in their evidence to the US Congress. Ammann had given newspaper
interviews and presented in Washington and he’s complaining about getting roughed
up.127

Unimpressed  as  he  was,  McIntyre  thought  he  saw  a  way  to  break  the
impasse of each side firing critical papers across at the other without any
final  resolution  of  their  differences.  He  suggested  to  Ammann  that  they
write a joint paper outlining where they agreed and where they differed and
setting  out  possible  approaches  to  resolving  those  differences.  Ammann,
however,  was  noncommittal.  It  would,  he  said,  interfere  with  his  career
advancement  to  be  so  closely  involved  with  McIntyre,  although  one
wonders if this linking of their names would have been quite as bad as the
alternative:  namely,  a  possible  reputation  for  withholding  adverse  results.
Regardless of this, a few days after returning to Canada, McIntyre wrote to
Ammann, formalising the proposal of a joint paper but attaching an expiry
date to the offer. Unfortunately for everyone though, Ammann set out his
stall very clearly by failing to reply, and a McIntyre reminder on the expiry
date  likewise  went  entirely  unacknowledged.  The  Hockey  Team  was
determined to continue the dispute.
The Climatic Change paper is resurrected
While the AGU was meeting in San Francisco, things started to move on
the  two  Wahl  and  Ammann  submissions.  On  9  December,  GRL  wrote  to
McIntyre,  informing  him  that  they  had  decided  to  move  forward  with
Ammann’s comment and advising him to prepare a reply. Then, just three
days  later,  it  was  announced  that  Ammann’s  CC  paper  had  been
‘provisionally  accepted’.  It  wasn’t  entirely  clear  what  these  provisions  of
acceptance were, but one possibility may have been that Wahl and Ammann
would be required to include their verification statistics. Another was that
the  editors  at  GRL  must  not  reject  the  comment  again,  because,  as  we’ve
seen, it contained the statistical arguments to support the assertions in the
CC  paper.  All  the  time,  the  year-end  deadline  for  submissions  to  the  IPCC
was  looming  large  and  there  was  now  precious  little  time  remaining  for
Ammann’s papers to meet it.

For  Schneider,  the  Climatic  Change  editor,  to  move  forward  with  the
paper was remarkable, given that he had been presented with pretty clear
evidence that Wahl and Ammann had misrepresented the status of their GRL
comment. Scheider still had a problem though, which was that McIntyre’s
review  comments  on  the  paper  were  likely  to  be  excoriating.  In  order  to
deal  with  this  he  sidestepped  the  issue  by  simple  dint  of  not  inviting  the
Canadian  to  review  the  second  draft.  It  is  perhaps  worth  remarking  that
Schneider has been in the forefront of efforts to bring global warming to

public attention and is also a man who once said that ‘Each of us has to
decide what the right balance is between being effective, and being honest. I
hope that means being both’.128 Sceptics wondered if this was one of those
occasions when Schneider’s hopes had been dashed.

This  then,  was  all  that  McIntyre  knew  of  the  CC  paper  until  the  New

Year.
Formal reply to the GRL comment
Meanwhile  there  was  the  revised  GRL  comment  to  deal  with.  The  new
version was almost identical to the first, with the exception of a new section
on verification statistics. In this, Wahl and Ammann claimed that there was
a problem with the benchmarking exercise that McIntyre had used to rebut
Peter  Huybers’  critique.  They  said  that  if  you  fixed  this  problem,  which
they said was due to the pseudoproxy  PCs  being  statistically  dissimilar  to
PCs generated by real data, it was possible to confirm that the correct 99%
benchmark for RE was zero.

By the end of January 2006, McIntyre and McKitrick had prepared a new
reply, which was duly submitted to GRL. It covered the same points that they
had  made  in  their  letter  to  James  Saiers  the  first  time  aroundl and it was
worded  very  strongly  –  in  the  face  of  Ammann’s  refusal  to  release  his
verification statistics and because he had ignored the invitation to write a
joint paper, McIntyre was not inclined to play nicely any longer.

[Ammann  and  Wahl]  not  only  repeat  results  that  we  had  previously  published,  but
claim them as their own and then accuse us of having failed to report them. In their
abstract  and  summary,  [they]  make  claims  that  are  unsupported  in  their  text,  then
assert our results are ‘unfounded’, despite the fact that results from their own code
yields  validation  statistics  (unreported  by  [them])  that  strikingly  confirm  claims  in
[our GRL paper] concerning spurious significance in [MBH98].129

Two  pages  of  the  six  that  comprised  McIntyre’s  letter  were  a  listing  of
‘Misrepresentations  and  unsupported  points’  in  Wahl  and  Ammann’s
comment.  If  Famiglietti  intended  to  publish,  then  the  Climatic  Change
readers were also going to hear exactly what McIntyre’s objections were.
Meanwhile,  McIntyre  quietly  filed  a  complaint  of  academic  misconduct
against  Ammann  with  his  employers  UCAR  on  the  grounds  that  Ammann
had withheld the adverse verification statistics in his submission to Climatic
Change. The pressure on Ammann was being steadily ramped up.

The CC paper and the verification statistics
It wasn’t until March 2006 that there was any further progress on the two
Ammann papers. Then, without warning, the status of the revised CC paper
was changed to ‘In press’, meaning that the peer review was complete and
the paper was ready to go to print. In the scientific publishing process this
means that the game is over and the paper is finalised. At this point it is
common practice in scientific circles for authors to make an online preprint
available and McIntyre was pleased to see that this was just what Ammann
had done.
TABLE 8.1: Verification statistics for Ammann’s MBH98 emulation

NH mean R2

NH mean R2

Calibration-period Verification-period

Proxy network
MBH periods
1400–1449
1450–1499
1500–1599
1600–1699
1700–1729
1730–1749
1750–1759
1760–1779
1780–1799
1800–1819
1820–1980
NH: Northern Hemisphere. Reproduced from Wahl and

0.018
0.010
0.006
0.004
0.00003
0.013
0.156
0.050
0.122
0.154
0.189

0.414
0.483
0.487
0.643
0.688
0.691
0.714
0.734
0.750
0.752
0.759

Ammann’s Climatic Change paper.122

The  resubmitted  version  turned  out  to  be  almost  identical  to  the  old  one,
except  that  a  new  section  on  the  statistical  treatments  had  been  added,
presumably as a condition of acceptance.122 Buried deep down at the back
of  the  paper  was  a  startling  revelation.  Wahl  and  Ammann  had  backed
down and done the decent thing. They had presented a table of verification
statistics and, as expected, these completely vindicated everything McIntyre
had  been  saying  over  the  previous  two  years.  The  figures  are  shown  in
Table  8.1.  The  important  section  is  the  right  hand  column  where  the
verification R2 is close to zero for most periods, including particularly the
critical  AD  1400  step.  In  the  AD  1700  step  it  even  proved  necessary  for
Ammann to increase the number of decimal places used in order to prevent

the  R2  from  appearing  as  zero.  These  figures  demonstrated  finally  and
conclusively that the MBH98 reconstructions were not reliable.
Publication chronology of the CC paper
The  CC  paper’s  provisional  acceptance  date  at  Climatic  Change  was  12
December 2005, just a few days before the IPCC deadline, which stated that
any papers cited had to be in press by the time of the lead author meeting on
13–15 December. However, after its provisional acceptance, the paper had
been  rewritten,  with  the  new  sections  containing  the  revelations  on
verification statistics added. This new version was the one that was finally
accepted by the journal and was dated 24 February 2006. This was before
the IPCC’s cut-off point for final journal acceptance, but the revelation of its
failure of the verification statistics making it a very different paper, it really
represented a new submission. In reality then, it had missed the IPCC’s cut-
off date for submission. It is also worth considering what peer review of the
new sections of the paper took place between submission on 24 February
and final acceptance four days later. It seems very likely that there was in
fact no peer review at all. But all this activity around the cut-off date had
another more remarkable side effect. As McIntyre put it:

So  under  its  own  rules,  is  IPCC  allowed  to  refer  to  Ammann  and  Wahl  [2006]?  Of
course not. Will they? We all know the answer to that. When they refer to Ammann
and  Wahl  [2006],  will  they  also  refer  to  its  confirmation  of  our  claims  about  MBH
verification [R2] statistics? Of course not. That information was not available to them
in  December.  But  wait  a  minute,  if  Ammann  and  Wahl  was  in  press  in  December,
wouldn’t that information have been available to them? Silly me.130

In other words, the version of the paper that had gone forward to the IPCC
didn’t include the adverse verification statistics, but the version accepted by
the journal did. The paleoclimatologists got their rebuttal of McIntyre and
the journal got a fig leaf of respectability to hide behind.
The comment is rejected again
Now that the adverse R2 statistic was out in the open, Ammann would have
been struggling for a way to save the paper from ridicule. The only way he
could do this was by arguing that the correct measure of significance was in
fact the RE statistic. This was a pretty feeble case because, as we’ve seen, it
was  pretty  clear  from  the  statistical  and  even  the  climatological  literature
that  a  suite  of  verification  statistics  was  preferred  to  any  one  measure.

Ammann’s problem was that in order to show that his reconstruction passed
even the RE test, he needed to establish a low enough benchmark – as we
have seen the correct benchmark appeared to be higher than the RE score of
the  Hockey  Stick.  So  what  was  Ammann’s  RE  benchmark  in  the  new
version of the CC paper? The paper stated:

We  consider  the  issue  of  appropriate  thresholds  for  the  RE  statistic  in  Appendix  2,
based on analysis and results reported [in the comment, in review with Geophysical
Research Letters].

However, when the reader referred to Appendix 2 he would read essentially
a restatement of this same position:

In implementing this procedure, we found a technical problem that we reported in [the
GRL comment].

Meanwhile,  when  Wahl  and  Ammann  had  discussed  the  actual  level  of
benchmarking they had applied, they had once again referred to Appendix 2
and to the GRL comment:

Numerically,  we  consider  successful  validation  to  have  occurred  if  RE  scores  are
positive, and failed validation to have occurred if RE scores are negative ([in the GRL
comment]; Appendix 2).

So, the GRL comment was necessary to justify a benchmark level of zero
for  the  RE  statistic.  Without  it,  the  Hockey  Stick  was  effectively  broken.
And unfortunately for Ammann, just a couple of weeks after the acceptance
of  the  paper,  there  was  to  be  another  hiccup  that  would  threaten  his
findings.

After all the shenanigans at GRL, with the replacement of the editor and
the  resubmission  of  letters,  the  journal  now  decided  once  again  to  reject
Wahl and Ammann’s comment. Without it, Ammann could not claim that
his results were statistically significant and, since he had purported to have
exactly replicated the Hockey Stick, this was potentially the end of Mann’s
creation too. Ostensibly the journal’s decision was made on the grounds that
the  arguments  were  ‘already  out  there’,  but  it  was  more  likely  that  there
were so many holes in the statistical arguments as to make publishing them
an embarrassment to the journal. McIntyre was extremely unimpressed:

What  a  total  waste  of  time.  Famiglietti  mouthed  off  to  Environmental  Science  and
Technology last August and replaced Saiers as editor in charge of our file. He then
took the comments by Ritson and by [Ammann and Wahl] (already rejected by Saiers)
out of the garbage can, told us that the Ritson comment was accepted, then he rejected
the Ritson comment after he saw our reply. Likewise with Ammann and Wahl . . .
[McIntyre appends a collective dismissal].131

a  See page 47.
b  The subject of cherrypicking is considered in more detail on page 236.
c  See page 153.
d  McIntyre’s side of the story of Huyber’s comment and his response to it is told over a series of

Climate Audit postings.114–116
e  Singular value decomposition.
f    Relevant  quotations  from  Preisendorfer  and  Rencher  were  provided  by  McIntyre  on  one  of  his

g  The story of how Dr Famiglietti came to be handling their paper is revealing and will be covered a

postings on Huybers’comment.116

little later in this chapter.

h  Strictly speaking, Ammann works for the National Center for Atmospheric Research (NCAR), one

of the laboratories run by UCAR. For ease of narrative, I refer throughout to UCAR.

i  See page 152.
j  See page 160.
k  The redaction is in the online copy of the letter cited here, but presumably not in the original sent

to Schneider.
l  See page 204.

9     The Hockey Stick in Washington

In politics, what begins in fear usually ends in folly.

Samuel Taylor Coleridge

Senator Barton takes an interest
When Michael Mann told Antonio Regalado of the Wall Street Journal that
he  would  not  release  his  code  because  this  would  be  ‘giving  into  the
intimidation  tactics’  of  his  opponents,  he  can  little  have  imagined  how
much trouble he was getting himself into. It cannot have crossed his mind
that his comments would catch the attention of Representative Joe Barton,
the Republican chairman of the House Committee on Energy & Commerce
in the US Congress. Barton was a Texan with close connections to the oil
business  and  a  determined  global  warming  sceptic,  two  characteristics
which  are  enough  to  condemn  him  as  irredeemably  biased  in  the  eyes  of
many environmentalists. He was brash, confident and outspoken, and quite
unafraid to put noses out of joint as he fought to get his way. The Hill News,
a newspaper dedicated to the goings-on in Congress, gives a flavour of the
man:

Barton has emerged rapidly as one of the toughest chairmen in the House, unafraid of
rolling his shoulders and using his elbows when he thinks it is necessary to expand or
protect his domain. He is helped in this by an apparent indifference to getting good
press and by having seemingly absorbed a version of Machiavelli’s dictum that it is
more important for political leaders to be feared than to be liked.132

Soon after the Wall Street Journal article was published, Barton contacted
McIntyre to enquire if he had spoken to the paper and if the article was true.
When McIntyre confirmed the story, the congressman swung into action. In
June 2005, Barton wrote letters to Mann, Bradley and Hughes, as well as to
Rajendra Pachauri, the head of the IPCC, and Arden Bement, the head of the
National  Science  Foundation  (NSF),  the  body  which  funds  much  of  the
scientific  research  in  the  USA.133  The  letters  were  co-signed  by  Ed
Whitfield, 
the  Subcommittee  on  Oversight  and
Investigations.  This  was  the  committee  that  had  investigated  earlier
scandals  like  the  downfall  of  Enron,  and  it  had  the  power  to  demand

the  chairman  of 

evidence  under  oath.  Barton  was  therefore  upping  the  pressure  in  a
considerable  way  and  it  is  unlikely  that  he  was  unaware  of  what  he  was
doing.
The letters
Barton’s  letters  to  Mann,  Bradley  and  Hughes  explained  that  the  House
Energy  &  Commerce  Committee  was  concerned  about  the  reports  in  the
Wall Street Journal that the Hockey Stick could not be replicated. He also
pointed to concerns over the independence of the IPCC reports (Mann having
been the lead author of the chapter which assessed his own work), and also
to  the  issue  of  sharing  of  data  and  code.  There  followed  a  long  list  of
demands  for  information:  everything  from  CVs  to  details  of  financial
support and copies of grant agreements. Much of this was directly relevant
to McIntyre’s researches, for example:

6.  Regarding  study  data  and  related  information  that  is  not  publicly
archived,  what  requests  have  you  or  your  coauthors  received  for
data relating to the climate change studies, what was your response,
and why?

7. The authors McIntyre and McKitrick (Energy & Environment, Vol.
16, No. 1, 2005) report a number of errors and omissions in Mann
et.  al.,  1998.  Provide  a  detailed  narrative  explanation  of  these
alleged errors and how these may affect the underlying conclusions
of the work, including, but not limited to answers to the following
questions:

a)      Did  you  run  calculations  without  the  bristlecone  pine  series

referenced in the article and, if so, what was the result?

b)   Did you or your co-authors calculate temperature reconstructions
using the referenced ‘archived Gaspé tree ring data,’ and what were
the results?

c)        Did  you  calculate  the  R2  [sic]  statistic  for  the  temperature
reconstruction,  particularly  for  the  15th  Century  proxy  record
calculations and what were the results?

d)   What validation statistics did you calculate for the reconstruction

prior to 1820, and what were the results?

e)   How did you choose particular proxies and proxy series?

The reaction
Within  days,  the  great  and  the  good  of  the  climate  science  fraternity  had
been stirred into action. Barton was deluged with letters of protest; outrage
and  disgust  were  pronounced  on  all  sides.  The  American  Association  for
the  Advancement  of  Science  wrote  to  the  congressman  saying  that  his
letters read as if he was seeking some way of discrediting Mann, Bradley
and Hughes and encouraged him to rely instead on the ‘multiple layers of
peer  review’  through  which  the  Hockey  Stick  had  passed,  a  position  that
must have brought a smile to McIntyre’s lips.

The BBC quoted paleoclimatologist Tom Crowley making the somewhat
absurd speculation that biologists could be asked for the data and code that
proved  the  theory  of  evolution.134  In  another  interview,  McIntyre  was
accused of sending threatening demands for data to Crowley, an allegation
which  was  demonstrably  untrue.a135  Crowley  also  rather  oddly  accused
Barton of being a mouthpiece for McIntyre.

Meanwhile the Washington Post  called  it  a  witch-hunt  and  a  few  days
later  Nature  thundered  furiously  in  an  editorial  roundly  condemning  the
letters.  In  fact,  with  such  an  avalanche  of  outrage  from  all  sides,  some
observers  can  surely  not  have  helped  but  think  that  maybe  the  scientific
community was protesting just a little too much.

As  the  wave  of  fury  grew,  the  situation  was  further  inflamed  when  it
started getting tangled up with Washington politics. The senior Democrat
on Barton’s Energy and Commerce Committee, Henry A. Waxman, wrote
to Barton complaining about what he described as ‘a transparent effort to
bully  and  harass  climate  change  experts’.136  Meanwhile  the  head  of  the
House  Science  Committee,  a  Republican  called  Sherwood  L.  Boehlert,
demanded that Barton call off his global warming investigation, describing
it as ‘misguided and illegitimate’.136 Boehlert, in contrast to Barton, was an
advocate of restrictions on carbon dioxide emissions, so on the one hand the
argument  can  be  seen  as  a  dispute  between  the  two  sides  of  the  global
warming debate, one trying to force the scientists to come clean about the
reliability  of  the  paleoclimate  reconstructions,  the  other  trying  to  keep
things safely under lock and key. But on the other hand it can also be seen
as a turf war: a dispute over which of these two powerful committees was
going to ‘own’ the global warming issue. Either way, it was going to get
complicated. Barton was not, however, going to back down in the face of a

little criticism from his fellow congressmen, as the committee’s spokesman
made clear:137

Chairman  Barton  appreciates  heated  lectures  from  Representatives  Boehlert  and
Waxman, two men who share a passion for global warming. We regret that our little
request  for  data  has  given  them  a  chill.  Seeking  scientific  truth  is,  indeed,  too
important to be imperiled by politics, and so we’ll just continue to ask fair questions
of honest people and see what they tell us. That’s our job.132

One of the many letters Barton received at this time, and one of the less
outraged  ones,  was  from  Ralph  Ciccerone,  the  head  of  the  National
Academy  of  Sciences  (NAS),  the  leading  learned  society  in  the  USA.
Ciccerone’s position was, on the face of it, rather more helpful, pointing out
that  perhaps  the  House  Energy  and  Commerce  Committee  was  not  best
equipped  to  resolve  a  scientific  issue,  and  offering  the  NAS’s  services  to
investigate  the  current  state  of  paleoclimate  on  the  Committee’s  behalf.
Barton, however, was unimpressed. His spokesman explained:

We can’t evaluate the idea without having seen it, and maybe it’s a darned fine one,
but an offer that says, ‘Please just go away and leave the science to us, ahem, very
intelligent  professionals,’  is  likely  to  get  the  reception  it  deserves.  We  get  a  lot  of
offers to butt out from folks who would rather avoid public scrutiny, and reputable
scientists wouldn’t feel comfortable in the company of most of them.137

Mann’s reply: code
A  few  weeks  later,  Mann’s  reply,  together  with  those  of  Bradley  and
Hughes,  was  delivered  to  Barton.  Mann’s  response  must  have  been
something of a surprise, with the Hockey Stick’s author insisting that he had
already made public all his data and methods, a claim that would have been
very surprising to McIntyre and McKitrick. Meanwhile, Mann was utterly
unrepentant about his refusal to release the code:

My  computer  program  is  a  piece  of  private,  intellectual  property,  as  the  National
Science Foundation and its lawyers recognise. It is a bedrock principle of American
law that the government may not take private property ‘without [a] public use,’ and
‘without just compensation’.

That  notwithstanding,  the  program  used  to  generate  the  original  Mann  et  al.  1998
temperature reconstructions is posted at [my FTP site].138

Readers  by  now  will  recognise  the  standard  Mann  pattern  of  response  –
bluster followed by a partial tactical retreat. Whether Barton was taken-in
by this is not clear.

Bradley and Hughes took a slightly different tack, with Bradley stating
that  he  ‘normally’  archived  his  data  and  Hughes  saying  that  he  had
complied with NSF policies. Neither he nor Bradley directly addressed the
question  of  the  availability  of  their  code,  and  McIntyre  meanwhile  had
evidence that showed plenty of gaps in their data archiving records.139,140

The  citation  of  NSF  policies  by  Mann  and  Hughes  bears  closer
examination. We saw in Chapter 6 that McIntyre had been trying to get hold
of the residual series for MBH98 and also Mann’s computer code since right
back  in  2003.  His  correspondence  with  NSF  had  continued  intermittently
over the years, but his attempts to obtain the data of other climatologists,
including  Hughes,  had  largely  been  rejected  by  the  science  bureaucracy.
However, this experience did at least mean that McIntyre had developed a
good understanding of the guidelines that were used. It was fairly clear that
NSF was indeed advising that scientists did not need to release their code to
third  parties.  However,  whether  it  was  in  their  power  to  do  this  was  not
entirely  clear.  The  universities  (including  Mann’s  own  bases  in  Virginia
and,  before  that,  Massachusetts)  included  clauses  in  their  employment
contracts that reserved the title to intellectual property developed by their
staff to the university itself, directly contradicting the claims of the NSF.

When McIntyre started to dig further into the question of NSF policy, he

came across the following statement:

Appropriate  commercialization  of  the  results  of  research  will  continue  to  receive
encouragement  by  permitting  [universities]  to  keep  principal  rights  to  intellectual
property conceived under NSF sponsorship. The Foundation emphasises, however, that
retention  of  such  rights  does  not  reduce  the  responsibility  of  researchers  and
institutions to make research results and supporting materials openly accessible.141

This was remarkable when set against the way one of the senior staffers at
NSF  had  described  the  policy  to  McIntyre,  namely  that  computer  codes
belonged to individual scientists:

On the question of computer source codes, investigators retain principal legal rights to
intellectual  property  developed  under  NSF  award.  This  policy  provides  for  the
development  and  dissemination  of  inventions,  software  and  publications  that  can

enhance their usefulness, accessibility and upkeep. Dissemination of such products is
at the discretion of the investigator.142

And on the question of data, there was virtual unanimity among the policies
of the various funding programs that data should be archived. In fact one of
these,  the  Paleoenvironmental  Arctic  Sciences  Program,  had  among  its
steering committee members none other than Hughes, while its policy on
data had been coauthored by Bradley. Both of these men seemed to have
adopted  rather  different  approaches  in  their  own  work  to  the  ones  they
advocated for others.
Mann’s reply: verification statistics
Mann’s position on the availability of his code may have been weak, but he
had some other remarkable claims to make in his reply to Barton. After the
by  now  traditional  potshots  at  the  scientific  credentials  of  McIntyre  and
McKitrick  and  Energy  and  Environment’s  status  in  the  firmament  of
scientific 
journals,  Mann  answered  Barton’s  specific  questions  on
McIntyre’s critiques. He was keen to claim that his work was supported by
that of Wahl and Ammann. In fact, he was extremely keen, mentioning their
alleged  refutation  no  less  than  eleven  times  in  his  response  to  the
committee.  He  didn’t,  of  course,  mention  the  fact  that  neither  of  the
Ammann papers were published and that one of them had been rejected.

There  was  another  surprise  when  Mann  came  to  explain  to  the
congressman  whether  he  had  calculated  the  R2.  As  we  have  seen,  the
original MBH98 paper suggested that the figure had been calculated, and in
fact, R2 results had been presented for the AD 1820 step. The results for the
other  steps,  however,  were  nowhere  to  be  seen,  although  Mann  had  later
told  Marcel  Crok  that  the  Hockey  Stick  passed  the  R2  tests.  However,
Mann’s response to Barton was more nuanced, involving a paraphrasing of
the  question  which  gave  it  a  slightly  different  meaning.  While  the
committee  had  asked  whether  he  had  calculated  the  R2,  in  his  answer  he
only claimed that he had not relied upon it.

The  Committee  inquires  about  the  calculation  of  the  R2  statistic  for  temperature
reconstruction, especially for the 15th Century proxy calculations. In order to answer
this question it is important to clarify that I assume that what is meant by the ‘R2’
statistic is the squared Pearson dot-moment correlation, or [R2]b (i.e., the square of the
simple  linear  correlation  coefficient  between  two  time  series)  over  the  1856–1901

‘verification’ interval for our reconstruction. My colleagues and I did not rely on this
statistic in our assessments of ‘skill’ (i.e., the reliability of a statistical model, based
on the ability of a statistical model to match data not used in constructing the model)
because, in our view, and in the view of other reputable scientists in the field, it is not
an adequate measure of ‘skill’.138

This narrative would have left the committee none the wiser as to whether
Mann had actually calculated the R2 statistic for the earlier steps, although it
seemed fairly likely that he had, in view of his remarks in the original paper
and also to Crok. One might also wonder why the Hockey Stick authors had
done this if they didn’t consider it to be ‘an adequate measure of skill’.

Mann’s letter to Barton now appeared to be throwing into doubt whether
the R2 number had actually been calculated at all. However, as we have seen
previously, Mann had been forced by the attention of Congress to release
his  code  and  while  the  Barton  Committee  was  considering  his  response,
McIntyre was busy working through pages of Fortran. It wasn’t long until
he found what he was looking for. There on pages 28–29 of his print out
was the section of the program that demonstrated conclusively that Mann
had calculated the R2. Why would he have published the R2 figure only for
the AD 1820 step? Why withhold the others? The most likely explanation
was that for the other steps, the  R2 was so low as to demonstrate that the
temperature  reconstruction  was  meaningless,  and  the  IPCC’s  assertion  that
MBH98  had  ‘significant  skill  in  independent  cross-validation  tests’  was  a
fiction.
The NAS panel
While  everyone  was  digesting  the  responses  from  Mann,  Bradley  and
Hughes,  the  political  complications  continued  to  multiply.  Having  both
been told to take a running jump by Barton, the NAS and Boehlert’s Science
Committee decided to hook up together: the NAS was going to perform the
investigation  that  Ciccerone  had  proposed,  but  under  the  auspices  of
Boehlert’s  committee  rather  than  Barton’s.  An  announcement  was  issued,
stating that the committee had asked the NAS to put together an expert panel
to  investigate  the  whole  subject  of  paleoclimate.  The  panel  was  to  be
headed  by  Gerald  North,  an  eminent  atmospheric  scientist  from  Texas
A&M  University.  Boehlert  had  seized  back  the  initiative  on  the  global
warming issue.

Boehlert had set out three specific areas for the committee to cover:

• What is the current scientific consensus on the temperature record of
the last 1,000 or 2,000 years? What are the main areas of uncertainty
and how significant are they?

• What is the current scientific consensus on the conclusions reached
by  Drs.  Mann,  Bradley  and  Hughes?  What  are  the  principal
scientific criticisms of their work and how significant are they? Has
the information needed to replicate their work been available? Have
other scientists been able to replicate their work?

• How central is the debate over the paleoclimate temperature record
to the overall consensus on global climate change? How central is
the work of Drs. Mann, Bradley and Hughes to the consensus on the
temperature record?

The  first  McIntyre  and  McKitrick  knew  of  the  committee’s  appointment  was  when  a  letter  from
North arrived asking if they could attend and give evidence. It certainly seemed welcoming enough,
the NAS board describing the two men’s participation as ‘critical’. However, concerns soon mounted
as the identities of the panel members started to leak out. One name that stood out was Doug Nychka,
a colleague and collaborator of Ammann’s at UCAR and a former co-author of Mann’s. This hardly
suggested someone who was likely to be neutral, and as he was the only statistician on the panel at
that  time,  his  appointment  was  a  potentially  serious  issue.  There  was  nobody  on  the  panel  who
appeared to have expertise in critical areas like spurious regression. And in fact, Nychka wasn’t the
only UCAR employee on the panel. Bette Otto-Bliesner turned out to be a superior of Ammann’s, who
worked in the office next door to him at UCAR and who had also published alongside Bradley. Three
other panel members, Karl Turekian, Robert Dickinson and North himself, were ex-UCAR men too.
Alarm bells were also set off by the published comments of another panelist, Kurt Cuffey, who had
said that serious scientific debate on whether global warming was occurring was at an end.143 Given
that this was one of the issues the panel was supposed to be considering, it did rather suggest that his
opinions were already set in stone.
There  were  clear  rules  laid  down  for  the  composition  of  NAS  expert
panels, which mandated that panellists should represent a mix of different
views  and  also  that  they  should  have  relevant  expertise.  It  was  rapidly
becoming  clear  that  this  wasn’t  the  case  for  the  paleoclimate  panel.
However,  the  rules  also  allowed  for  interested  parties  to  make  formal
objections to the appointments, and given the overwhelming preponderance
of Hockey Team associates, it was certainly worthwhile giving this a try.
McIntyre had little expectation that this would produce any changes, but if
there was a whitewash, critics would be able to point to the composition of
the panel, and the NAS couldn’t plead ignorance. With only a month to go
before the panel’s hearings in Washington, there was little time to lose – in
fact  it  appeared  unlikely  that  the  panel  would  have  time  to  consider  any
proposed change before the scheduled start of the hearings in March, but it

was worth the attempt and the letter was duly delivered to the  NAS. In it,
McIntyre  and  McKitrick  protested  at  the  appointments  of  Otto-Bliesner,
Nychka and Cuffey, and also requested panellists with more relevant skills
–  someone  who  could  understand  issues  of  statistical  significance  in  the
peculiar circumstances of multiproxy reconstructions,c and someone with a
background  covering  the  areas  of  journals,  software  evaluation  and
statistical methods who could contribute to the panel’s understanding of the
replication issues. And, adding a little spice to the request, McIntyre also
asked the  NAS  to  include  someone  with  expertise  in  the  area  of  scientific
misconduct.

When the final make-up of the panel was announced towards the end of
February,  it  became  clear  that  McIntyre’s  letter  had  been  largely  ignored.
The only concession that had been made by the academy was to appoint a
second  statistician,  Peter  Bloomfield.  But  even  this  modest  step  was  less
favourable than it might at first have seemed. Bloomfield turned out to have
assisted Keith Briffa on some of his papers, providing statistical guidance
for the confidence interval calculations.d So now, both statisticians on the
panel were to be associates of the Hockey Team. As McIntyre wryly asked
his readers,

Out of all the statisticians in the world, why would they pick one who consulted on
confidence intervals for one of the Hockey Team studies?144

Why indeed?

Until that time, McIntyre had known only that he and Mann would be
speaking before the panel, but at the same time as the panel announcement,
the full list of speakers was also released to the public. The hearings were to
spread over two days, each speaker having just 45 minutes to make their
case. Day one would open with a pair of geophysicists, followed by some
ice core experts (both closely associated with Mann), before they got on to
the meatier matters, with presentations from the tree ring experts, Gabriele
Hegerl  and  Rosanne  D’Arrigo.e  Hegerl  and  D’Arrigo  were  both  Hockey
Team  members  but  they  were  second  team  rather  than  the  first.  The  day
would close with Hans von Storch, who, as we have seen, was a neutral,
and then finally McIntyre and McKitrick. Day two had just two sessions –
the first was Malcolm Hughes, while the honour of closing the event went
to Michael Mann himself. The hearings were now less than ten days away.

Barton strikes back
Before the NAS panel could actually start its hearings, and just days after the
announcement  of  the  speakers,  Barton  made  a  determined  effort  to  wrest
back control of the global warming issue from Boehlert. He announced that
he  had  asked  an  eminent  statistician  called  Edward  Wegman  to  form  a
second expert panel, which was to be tasked with examining the specific
question of the statistics of MBH98.

Wegman had no connection to climate science and no stated position on
global  warming,  so  there  was  no  easy  way  for  anyone  to  criticise  his
appointment. His credentials as a statistician were unimpeachable. With two
panels now due to report, McIntyre could at least get some reassurance that
even  if  the  NAS  panel  decided  to  whitewash  the  whole  question  of  the
Hockey  Stick,  something  that  appeared  increasingly  likely  in  the  light  of
their flouting of their own rules on panel balance, Wegman might at least be
expected  to  understand  the  statistical  problems  with  MBH98.  His  views
would  therefore  provide  a  valuable  counterweight  to  anything  the  NAS
might report.
The academy
At  the  start  of  March  2006,  McIntyre  and  McKitrick  travelled  to
Washington for the panel presentations. The panel was to meet at the  NAS
headquarters on the Mall, a classically inspired building set amid extensive
wooded grounds, close by the Lincoln Memorial.

When the hearings opened on Thursday morning, Mann was nowhere to
be  seen.  In  fact  he  didn’t  appear  at  all  until  Friday,  missing  all  of  the
presentations  on  the  first  day.  It  was  almost  as  if  the  schedule  for  the
hearings had been specifically designed to allow Mann to avoid McIntyre as
much as possible.
Proxy studies
The presentations got underway in the Academy’s lecture room, with talks
on  boreholes  and  corals  as  ways  of  estimating  temperatures  of  the  past.
While  some  of  the  speakers  were  known  for  their  strong  support  for  the
global warming hypothesis, it was remarkable just how cautious they were
about  what  could  be  concluded  from  their  own  area  of  expertise.
Geochemist  Daniel  Schrag  said  that  it  was  very  difficult  to  make  an
estimate of average temperature from instrumental data, let alone proxies,

and that policymakers were demanding more than the scientific community
could  actually  provide  in  practice.  Richard  Alley,  an  expert  in  glaciers,
pointed out that there was no concerted effort to update paleoclimate data
(see Chapter 13) and that what data there was had rarely been collected for
the purposes of climate reconstructions. As he put it, the whole system was
not set up to answer the type of question that was being asked of it. The
scientific community, he said, ‘had not really integrated [polar cores] in a
coherent  way’  because  this  was  ‘not  the  highest  priority  of  the  scientific
community’,145 a remarkable statement given the importance of the global
warming question and all the billions of dollars the subject had received in
research funding over the previous decade. Like Schrag, he too pointed to
the squeeze from policymakers.
Data availability and the statement of task
While  data  availability  had  been  one  of  the  questions  that  Boehlert  had
asked the NAS to investigate, as the first day’s presentations were drawing to
a close, the subject had still not been aired. It wasn’t until von Storch took
his  turn  that  the  panel  was  forced  to  consider  the  way  in  which  the
climatology community appeared to be resisting independent verification of
their work. Their attention was drawn to the matter in a way that was very
hard  for  them  to  ignore,  when  von  Storch  said  that  data  should  be  made
available  to  everyone,  including  ‘adversaries’.  To  reinforce  his  point  he
quoted a notorious statement by the Hockey Team player Phil Jones,f who
had rejected a request for data by an Australian researcher by saying:

We have 25 or so years invested in the work. Why should I make the data available to
you, when your aim is to try and find something wrong with it?g

Von Storch had some more surprises for the committee too. As he drew his
presentation  to  a  close,  he  showed  a  last  summary  slide,  setting  out  his
answers  to  the  questions  with  which  Boehlert  had  requisitioned  the  NAS
panel and report. This was a useful way of tying the various strands of his
talk  together,  leaving  the  panel  with  straightforward  answers  to  the
questions they had been asked to investigate.

To  everyone’s  astonishment  the  arrival  of  the  slide,  entitled  ‘Rep
Boehlert’s  Questions’,  appeared  to  create  confusion  among  the  panel
members,  who  broke  into  an  animated  discussion.  It  eventually  emerged

The panel members were now forced into the uncomfortable position of
having  to  decide,  in  full  view  of  the  witnesses  and  the  other  onlookers,
whether the Boehlert questions were actually within the scope of their task
or not. The rewording was not insignificant. For example, where Boehlert
had asked about MBH98 – what the criticisms of the paper were, whether the
information required to replicate it had been available and whether others
had  actually  managed  to  replicate  it  –  the  NAS  statement  of  task  avoided
mentioning  MBH98  at  all.  Likewise  the  whole  subject  of  replication  was
surreptitiously dropped.

Were  the  changes  made  deliberately  or  by  accident?  We  can  never  be
certain, but our views on this question might be coloured by the observation
that  the  bureaucrats  at  NAS  also  forgot  to  attach  Boehlert’s  original
requisition to the statement of task sent out to the panellists. It looked very
much as if Boehlert had been outmanoeuvred by the scientific bureaucracy.
Why would they do this?

that  the  panel  members  had  not  actually  been  told  anything  about  the
Boehlert questions. Between receiving the Boehlert questions and briefing
the  panel,  Ciccerone  had  apparently  rewritten  the  statement  of  task,
redirecting the panel to somewhat less controversial ground.

McIntyre  was  fascinated  by  the  political  manoeuvring  he  was  seeing
unfold before him. Now that their omission was out in the open, the  NAS
panel was in a quandary. If they failed to answer the Boehlert questions, the
House Science Committee would look very foolish – people would assume
that they had simply been outwitted by the wily mandarins in the NAS. But
what  was  worse  for  the  Science  Committee,  Barton’s  Energy  and
Commerce  Committee  could  now  start  holding  their  own  hearings.  They
could merely point out that although the NAS had asked to address the issues
raised in Barton’s original letters, having been given the opportunity to do
so, the academy had failed to answer either these questions or indeed those
in the Boehlert requisition.

Quite  what  the  panel  would  decide  to  do  was  unclear.  However,
McIntyre  noted  that  at  Mann’s  presentation  the  following  day,  with  the
revelations  over  the  Boehlert  questions  fresh  in  their  minds,  none  of  the
panel  members  saw  fit  to  question  Mann  on  the  Boehlert  question  most
pertinent to his work – ‘has the information needed to replicate your work
been available?’ It didn’t bode well.

Cherrypicking
When  Rosanne  D’Arrigo  stepped  up  to  the  microphone  to  give  her
presentation on tree ring studies, the panel may well have been unprepared
for  the  bombshells  she  was  about  to  drop.  The  earlier  presentations  by
Schrag and Alley had already surprised some observers with their lack any
of the alarmist language that is so common in climatology, but these two
were to be nothing compared to D’Arrigo.

D’Arrigo worked at the Lamont-Doherty Earth Observatory, part of New
York’s Columbia University. She had a long and distinguished publication
record, with many of the big names in paleoclimatology having been her
co-authors at one time or another, Mann, Cook, and Jacoby among them.
There was no inkling that she might be about to make fools of the whole of
the paleoclimate community.

Her  first  bombshell  was  a  slide  in  which  she  discussed  the  issue  of
‘cherrypicking’  –  a  term  used  to  describe  scientists  examining  the  data
records  before  processing  and  removing  those  which  might  give  the
‘wrong’ answer. In other words, a researcher bent on producing a hockey
stick  shaped  temperature  reconstruction  could  simply  introduce  only
hockey  stick  shaped  series  (like  the  bristlecones  and  Gaspé)  into  the
algorithm. Of course, nobody would fall for such an obvious travesty of the
scientific method, at least if it was reported in those terms. Still, to some
extent,  Mann’s  short-centred  PC  algorithm  could  be  seen  as  simply  an
automated way of achieving exactly the same effect. There was no longer a
need to examine every data series individually; now it was enough to let the
short centring process extract hockey sticks from the data.

D’Arrigo was startlingly straightforward on the subject. Cherrypicking,
she  said,  was  necessary  if  you  wanted  to  make  cherry  pie.146  In  other
words, she appeared to be suggesting that you needed to peek at the data to
get  the  result  you  wanted.  The  panel  must  have  been  stunned  by  this
admission, but it appears that nobody took her up on it.

In  fact,  D’Arrigo  was  not  alone  in  her  apparent  belief  that  it  is
scientifically acceptable to cherrypick data. She and her close collaborator,
Gordon Jacoby, had published a widely cited paper in which they selected
ten sites from a total of 36 studied, justifying the omission of the other 26
on  the  grounds  that  they  had  selected  only  the  ‘most  temperature-
influenced’.h  What  made  it  worse  was  that  Jacoby  and  D’Arrigo  had
refused  to  archive  the  data  from  the  26  eliminated  series,  arguing  that

because they didn’t have a temperature signal, they were better left out of
the  archive.  When  McIntyre  had  written  to  the  journal  concerned,  asking
that they obtain the missing data on his behalf, Jacoby promptly refused the
request.

If we get a good climatic story from a chronology, we write a paper using it. That is
our funded mission. It does not make sense to expend efforts on marginal or poor data
and  it  is  a  waste  of  funding  agency  and  taxpayer  dollars.  The  rejected  data  are  set
aside and not archived.83

These  extraordinary  admissions,  which  were  not  available  to  the  panel,
show  that  the  practice  of  cherrypicking  is  not  uncommon  among
paleoclimatologists. An important issue for the panel to report upon, or so
you might think.
Divergence
D’Arrigo  also  spent  part  of  her  talk  discussing  a  new  paper  that  she  had
recently  published.  Her  presentation  included  a  graph  of  her  temperature
reconstruction  and  this  attracted  the  attentions  of  Kurt  Cuffey.  Although
McIntyre  had  complained  about  his  presence  on  the  panel,  Cuffey  had
approached him before the hearings to explain that he was quite capable of
separating his overall views of global warming from his duties as a panel
member. True to his word, he turned out to be one of the most inquisitive
panellists. What had attracted Cuffey’s attention was the fact that the figures
D’Arrigo  had  forecast  for  the  late  twentieth  century  and  the  actual
temperatures  that  had  been  observed  were  wildly  different.  This  was  of
course the so-called ‘divergence problem’, which we touched on in Chapter
3.i Let us recap.

You will remember that estimates of temperature reconstructions can be
made both from tree ring widths and from the wood density. The divergence
problem referred to the simple fact that while the instrumental records all
showed a sharp late-twentieth century warming, tree rings mostly resolutely
refused to respond. Despite the higher temperatures, neither ring widths or
densities  seemed  to  have  been  affected  and  in  fact,  if  anything,  there
appeared to have been a widespread decline in ring widths and density over
recent decades. Why didn’t the warming show up in the tree ring data? This
was  an  enormous  issue  for  temperature  reconstructions  and  could
conceivably undermine the whole approach. If tree rings didn’t pick up the

warming  now,  how  could  anyone  be  sure  that  they  had  picked  up  earlier
warmings like the disputed Medieval Warm Period?

The issue had been recognised for some years. In fact, right at the very
start  of  McIntyre’s  researches  into  MBH98,  he  had  come  across  the
extraordinary  pair  of  papers  by  Briffa  that  demonstrated  how  the
paleoclimate  community  had  dealt  with  the  problem.  The  first  of  these,
Briffa  2000,  was  a  study  of  tree  rings  across  the  world  and  presented  a
picture  of  how  their  growth  patterns  had  changed  across  the  centuries.147
His  results  clearly  showed  a  marked  decline  in  ring  width  density  in  the
twentieth century.

In the second paper, Briffa created a Northern Hemisphere temperature
reconstruction from similar data, which again suggested a recent decline in
temperatures, on the basis of declining tree ring densities.148 However, this
time, Briffa had gone a little further. He created a ‘spaghetti chart’, a graph
of several temperature reconstructions overlaid on top of each other in an
attempt to show how well they matched up against each other (or not). Here
though, he stepped out of line and did something that was highly dubious in
scientific terms: he truncated the chart for his own series at 1960 so that the
‘inconvenient  divergence’  disappeared.  This  is  not  to  suggest  that  he  did
this in a secretive way. In fact he explained his reasoning as follows:

[I]n  the  absence  of  a  substantiated  explanation  for  the  decline,  we  make  the
assumption  that  it  is  likely  to  be  a  response  to  some  kind  of  recent  anthropogenic
forcing.  On  the  basis  of  this  assumption,  the  pre-twentieth  century  part  of  the
reconstructions can be considered to be free from similar events and thus accurately
represent past temperature variability.

This is an explanation that would appear wholly inadequate in most other
areas of science. The hard fact is that tree rings and temperature records are
diverging  in  the  modern  era,  the  one  period  when  both  can  be  directly
observed. The only reasonable conclusion that can be taken away from this
observation  is  that  these  tree  rings  are  not  capable  of  detecting  warming
trends.  Instead,  Briffa  had  simply  assumed  that  the  divergence  didn’t
happen in earlier periods and that the lack of a trend in tree rings in the past
meant that there were no warm periods either. What is more, despite the fact
that  this  hypothesis  cannot  even  be  tested,  Briffa’s  thinking  is  widely
accepted among paleoclimatologists.

The truncation of the divergence in Briffa’s paper wasn’t the end of the
story  either.  In  its  Third  Assessment  Report  in  2001,  the  IPCC  presented
another spaghetti chart, which included Briffa’s reconstruction. Like Briffa,
they  chose  to  truncate  his  series  at  1960  to  eliminate  the  divergence,  but
shockingly they did so without discussing what had been done. Whether by
chance or design, the truncation point of 1960 had one huge advantage for
the  IPCC:  at  that  point,  the  lines  on  the  spaghetti  chart  were  all  bunched
together making it hard to see that one of the lines which had gone into the
bunch had failed to emerge at the other side. The truncation ended up neatly
disguised.

The  citation  given  by  the  IPCC  report  was,  interestingly,  to  Briffa  et  al
2000,  the  original  paper,  which  hadn’t  been  truncated.  We  can  only  hope
that this mis-citation was not deliberate. However, if the full data series had
been included the chart would certainly have told an entirely different story,
and the whole case that the current warming was unprecedented would have
been undermined. It is perhaps worth remembering that the author of this
section of the report was Mann.

To return to the NAS panel, D’Arrigo passed up the opportunity to explain
the  divergence  problem  in  any  great  detail,  noting  only  that  it  had  been
discussed by Briffa and arguing that it only applied to a few sites. In the
light of this omission, McIntyre and McKitrick made some rapid changes to
their  presentation  adding  some  new  slides  to  explain  the  inadequacy  of
Briffa’s  explanations  and  demonstrating  that  the  divergence  problem  was
extremely  widespread.  In  fact  divergence  was  probably  the  norm,  rather
than the exception, as Briffa had noted in his original studies when he had
described it as a ‘widespread phenomenon’.

Hughes  also  addressed  the  subject  of  the  divergence  problem  and  put
forward two possible explanations for it, neither of which can have seemed
terribly  persuasive.  One  idea  was  that  increased  snow  pack  had  caused  a
delay in growth each year, while the second was a speculation by Briffa that
trees  were  being  damaged  by  atmospheric  ozone.  One  of  the  panellists
enquired if a similar divergence might have happened in the past, to which
Hughes made the surprising response that this was a third explanation that
he had hoped to avoid discussing.149 This appeared then to be an admission
that  there  was  a  real  possibility  that  temperature  signals  could  not  be
reliably extracted from tree ring records at all.

Bristlecones
The  question  of  the  reliability  of  the  bristlecones  was  a  clean  sweep  for
McIntyre and McKitrick. Their presentation laid out the full gory detail of
the problem, avoiding none of the controversial angles to the issue. Among
these  was  the  existence  of  the  CENSORED  directory,  which,  you  may
remember,  showed  that  Mann  was  fully  aware  that  a  reconstruction
prepared without the bristlecones had a prominent Medieval Warm Period.
The  panel  cannot  have  failed  to  have  noticed  the  contrast  between  this
evidence and MBH98’s claim that their findings were ‘relatively robust to the
inclusion  of  dendroclimatic  indicators’,  in  other  words  that  you  could
remove any tree ring data you liked and still get no Medieval Warm Period.
It  is  perhaps  not  surprising  therefore  that  none  of  the  speakers  made  any
serious attempt to defend the use of bristlecones. Only Mann broached the
subject,  and  then  only  in  a  rather  oblique  fashion,  referring  to  the
southwestern  USA  as  a  ‘sweet  spot’  for  creating  Northern  Hemisphere
temperature 
reconstructions.  This  slightly  bizarre  explanation  was
essentially a reference to his claims that it remained valid to include a PC4
representing  the  bristlecones,  and  have  these  drive  the  shape  of  the  final
reconstruction,  even  though  the  pattern  only  represented  a  pair  of  tree
species in a pair of small mountain ranges in the western USA. Here then
was  an  issue  where  the  panel  should  have  been  able  to  draw  clear
conclusions.
Verification statistics and confidence intervals
McIntyre was just as pointed in his remarks about Mann’s withholding of
the adverse verification statistics. He explained to the panel how Mann had
reported in MBH98 that he had calculated the  R2 for the Hockey Stick, but
had withheld the fact that the results had indicated that his reconstruction
was  unreliable.  McIntyre  went  on  to  demonstrate  how  the  IPCC  had  later
misrepresented  the  Hockey  Stick  as  having  significant  ‘skill’.  Having
dramatically failed the verification R2 test, the confidence intervals for the
Hockey  Stick  were,  in  the  words  of  Hegerl,  ‘from  floor  to  ceiling’.150  In
other words, you could have no confidence in the result at all.

This  was  a  very  damning  set  of  accusations  and  one  which  promised
some fireworks when Mann came to speak the following day. In the event
though,  absolutely  nothing  happened.  John  Christy,  who  was  seen  as  the
lone  sceptic  on  the  panel,  asked  Mann  about  his  R2  score.  Mann  tried  to

evade the question by denouncing its usage in general, but Christy pressed
him  further,  asking  whether  he  had  in  fact  calculated  the  figure.  Mann’s
reply was sharp and to McIntyre, at least, breathtaking:

We didn’t calculate it. That would be silly and incorrect reasoning.151

This was an extraordinary statement. Mann’s paper clearly stated that ‘For
comparison,  correlation  (r)  and  squared-correlation  (R2)  statistics  are  also
determined’. He had presented R2 information in the paper. The commands
to calculate the R2 were in the code he had submitted to Congress. He had
told Crok that the Hockey Stick passed the R2 test, something he could only
have determined if he had calculated its value in the first place. Not only
did Mann’s statement fly in the face of everything he had said previously,
but it also contradicted the evidence McIntyre had given the day before. It
was  also  laughably  wrong  from  a  statistical  perspective.  There  were  two
qualified statisticians on the panel: Nychka and Bloomfield. Here then was
the moment for them to step up to the mark and prove their independence
and worth. . .

Nychka and Bloomfield, however, said absolutely nothing and indicated
that they had no interest in questioning Mann on the issue. The conversation
moved on.

Later on during Mann’s time before the panel, the questions returned to
the  subject  of  the  verification  statistics  ,  and  he  again  made  some
remarkable  statements  that  should  have  been  followed  up  by  the  two
statisticians. As before though, Nychka and Bloomfield failed to follow up
in even the most rudimentary fashion. For example, Mann said that he had
calculated  another  verification  statistic  (the  CE)  but  neither  Nychka  or
Bloomfield  thought  to  ask  whether  the  Hockey  Stick  had  actually  passed
the  test  (it  hadn’t).  Mann  also  said  that  the  R2  was  ‘not  good’  and  ‘not
sensible’  and  even  that  statisticians  didn’t  use  it  –  an  idea  that  was
outlandish to say the least – and each time he was left unchallenged. Even a
statement by Mann that he was ‘not a statistician’ seems to have left them
unmoved.

Mann,  of  course,  was  vigorous  in  his  own  defence.  He  asserted  that
McIntyre’s  work  was  without  ‘statistical  or  climatological  merit’,  citing
Wahl  and  Ammann  in  his  support.  At  this  point  it  was  incumbent  upon
Nychka to state an interest – he was credited in Ammann and Wahl’s paper

to 

repeat  his  allegation 

for  giving  advice  on  statistical  matters  –  but  again,  he  chose  to  remain
silent.  Mann  went  on 
that  McIntyre’s
‘reconstruction’ failed verification tests. This again directly conflicted with
evidence given by McIntyre and McKitrick the previous day. McIntyre had
stated  plainly  that  he  and  McKitrick  had  not  presented  their  own
reconstruction, but had only demonstrated that MBH98 was not robust. Once
again, however, the panel failed to follow through and question a speaker
on the apparent contradictions in the evidence being presented.
Credibility of MBH
With the statistical arguments largely won by McIntyre and there being no
plausible  defence  of  the  bristlecones,  the  best  possible  line  for  Mann’s
defenders to take was to argue the Hockey Stick’s consistency with other
studies, among them the new papers from Hegerl and D’Arrigo. The panel
was treated to presentations from Hegerl and D’Arrigo showing these new
reconstructions. The implicit argument was that, whatever the outcome of
the debate over the data and methods used in MBH98, its hockey stick shape
was still broadly in line with other studies and that it was therefore probably
correct.

McIntyre was not unaware of this line of reasoning and over the previous
three years he had attempted to replicate some of these other studies too.
This  had  been  a  long  and  difficult  process.  At  every  step  of  the  way  his
attempts to obtain data and code had been blocked by the climate scientists,
and,  as  with  MBH98,  journals  and  funding  agencies  had  refused  to  help.
However,  he  had  developed  a  good  understanding  of  how  most  of  these
secondary  studies  had  achieved  their  results,  and  what  he  told  the  panel
should have given them considerable cause for alarm.

The subject of these confirmations of the Hockey Stick will be examined
in  detail  in  Chapter  10,  but  in  essence  it  was  clear  was  that  the
‘independent’ confirmations were actually nothing of the sort, their alleged
independence  being  nonexistent.  Millennial  temperature  reconstructions
were all produced by a small group of people closely linked to Mann. There
were few people in the field who had not been co-authors either with Mann,
Bradley  or  Hughes,  so  it  was  false  to  argue  that  these  researchers  were
independent in the normal sense of the word. The data used in these studies
was likewise problematic. Nearly all of the papers were based on the same
small set of flawed proxies, most of which were also used in MBH98. It was

therefore  no  surprise  that  these  other  papers  reached  similar  conclusions,
despite the fact that the exact methodology varied from study to study. Nor
was  it  clear  that  they  were  any  more  statistically  robust  than  MBH98.  As
McIntyre  pointed  out  in  his  presentation,  nearly  all  the  reconstructions
relied on a procedure called variance rescaling. In order for this method to
yield  correct  results,  it  is  necessary  first  to  show  that  the  reconstruction
passes a statistical test called the Durbin–Watson statistic. McIntyre pointed
out  that  nearly  all  of  the  other  reconstructions  had  not  performed  the
Durbin–Watson  test,  and  that  if  they  had  they  would  have  failed  it,
indicating that their uncertainties were much higher than reported. He also
pointed out that nearly all of them spectacularly failed the verification R2 as
well.

Here  then  was  an  interesting  dilemma  for  the  panel.  On  the  one  hand
they had Hegerl and D’Arrigo telling them that Mann’s work was supported
by other studies, while on the other hand they had McIntyre showing them
that the other studies were potentially just as flawed. How would the panel
report this difference of opinion? Would they jump one way or the other, or
would they hedge their bets, reporting that opinion was divided?
Follow-up
In view of some of the surprising replies given by Mann and the failure of
the  panellists  to  probe  his  answers,  McIntyre  and  McKitrick  decided  to
make some follow-up comments in a written submission that they sent to
the panel a month after the hearings.152 Their language was stark and to the
point. For example, they addressed Mann’s claim that he had not calculated
the R2 statistic:

One of the panellists asked Mann for the value of his verification [R2] statistic for the
15th century step. Mann said that they did not calculate this statistic. This was untrue,
based on the text of MBH98, as reported in McIntyre and McKitrick [NAS Panel 2006].

There were plenty of other issues too. The latest incarnation of Ammann’s
CC  paper  had  appeared  shortly  after  the  hearings,  and  McIntyre  took  the
opportunity  to  bring  its  catastrophic  failure  of  the  R2  test  to  the  panel’s
attention.  They  also  pointed  out  the  recent  rejection  of  Ammann’s  GRL
comment  and  the  effect  this  had  on  the  CC  paper’s  claims  of  statistical
significance.

One  of  their  most  interesting  points,  however,  was  the  way  they
addressed  the  cherrypicking  issue.  Having  expressed  their  concern  over
how  the  panel  had  been  presented  with  a  series  of  proxies  and
reconstructions with twentieth century upticks, they argued that it was quite
possible to come to different conclusions simply by selecting a different set
of  proxies.  By  selecting  only  proxies  with  warm  medieval  sections  and
calculating an average using a standard multiproxy approach, they showed
that they could produce a ‘reconstruction’ that showed both the twentieth
century  uptick  and  a  medieval  warm  period.  This  ‘applepicking’
reconstruction was tongue-in-cheek, of course, but demonstrated powerfully
that it was possible to get the answer you wanted, simply by picking the
right  proxies.  Whether  this  would  have  any  effect  on  the  outcome  was
another question.
The report
Back  home  in  Canada,  rumours  started  to  reach  McIntyre  and  McKitrick
that the NAS report was going to be ‘two-handed’: the panel would accept
some  of  their  arguments  but  not  others.  This  was  consistent  with  the
impression  McIntyre  had  gained  during  the  hearings,  with  all  the  most
searching questions being studiously avoided by the panel.

thrust  of 

that 

the  report  was 

The  report  was  finally  published  in  June,  with  North,  Cuffey  and
Bloomfield delivering a press conference in Washington to the assembled
media.153  The  overall 
temperature
reconstructions  for  the  last  400  years  were  reliable,  but  there  was  ‘less
confidence’  in  earlier  periods.  How  much  less  confidence  was  not  made
clear.  The  good  news  was  that  the  panel  had  accepted  McIntyre  and
McKitrick’s  statistical  criticisms  of  MBH98.  They  also  accepted  the
arguments against using bristlecone pines, going as far as to say that these
should  be  avoided  in  temperature  reconstructions.  But  their  support
appeared  very  grudging:  their  acceptance  of  McIntyre  and  McKitrick’s
critique  was  outlined  in  the  main  text  but  was  entirely  absent  from  the
summary at the start of the report. Here instead they gave prominence to an
argument  that,  despite  all  its  flaws,  the  Hockey  Stick  was  ‘plausible’
because of its similarity to other temperature reconstructions.

The  panel  appeared  to  get  completely  muddled  up  in  their  attempts  to
support this position. They referred to a study by Osborn and Briffa, which
looked at a number of proxy series and showed that more of them had warm

deviations in the twentieth century than had cool ones.154j The argument in
the paper was that by looking at the proxies in this simple way there was no
need  to  engage  in  the  statistical  brouhaha  that  had  plagued  MBH98.  The
problem with the panel’s citation was that, of the 14 series cited in Osborn
and  Briffa,  one  was  Mann’s  NOAMER  PC1  (coyly  referred  to  as  ‘W.  USA,
regional’) and another was a foxtail series (foxtails, you will remember, are
very similar to bristlecones). In fact, there were problems with most of the
series used in Osborn and Briffa, and McIntyre had described the article as
being little more than a compendium of every dubious proxy series in the
archive with a few others added in to make some ‘noise’.

The report also included a spaghetti chart, showing how MBH98 compared
to  other  major  multiproxy  studies.  These  all  had  broadly  similar  hockey
stick shapes but incredibly, all except one included bristlecone pines in their
proxy  rosters.  It  is  hard,  if  not  impossible,  to  reconcile  the  panel’s
acceptance  of  a  group  of  reconstructions  based  on  bristlecones  with  their
concurrent  statement  that  bristlecones  should  not  be  used  in  temperature
reconstructions. They were certainly aware of the issue, since McIntyre had
raised the subject in his written submissions to the panel.155

Elsewhere, the panel reported that they rejected Mann’s idea that a single
validation statistic was acceptable and said that a reconstruction with a low
CE score was ‘unreliable’, while failing to point out that MBH98 fell into this
category. They also recommended the use of a Durbin–Watson statistic as a
test  of  validity.  This  statement  directly  contradicted  their  position  on  the
‘independent’ reconstructions, because all of those other studies, which the
panel alleged gave broadly the same answer as the Hockey Stick, failed the
Durbin–Watson  statistic  (as  well  as  the  R2).  The  panel  cannot  have  been
unaware  of  this  fact  because,  as  we  have  seen,  McIntyre  had  already
pointed it out to them in his presentation.

Although  McIntyre  said  he  accepted  the  integrity  of  the  panel,  and
indeed wrote to thank Ralph Ciccerone for giving him and McKitrick a fair
hearing,  the  internal  contradictions  in  the  panel’s  report  were  very
frustrating, and led McIntyre to describe the report as ‘schizophrenic’. Just
as  annoying  was  the  failure  of  the  panel  to  address  many  of  the
controversies at all: the difficult questions that had been asked by Barton
appeared  to  have  been  been  quietly  shelved.  Had  Mann  withheld  or
misrepresented  adverse  results?  The  panel  wasn’t  saying,  although  in  the
press conference they all agreed that they had ‘seen nothing that spoke . . .

of any manipulation’, a surprising position when McIntyre had pointed out
to them that Mann had calculated the R2 but had not reported it. Had Mann
withheld his data and code? They would only opine that researchers should
make  these  materials  available,  a  position  that  would  pass  muster  as  a
statement of the patently obvious, but hardly addressed Boehlert’s question
of whether it had been available in practice. At the press conference, Gerald
North explained that they had found the whole subject too large to deal with
and announced that a new panel would be formed specifically to look at the
issue  of  scientists  making  data  and  materials  available,  an  approach  that
looked remarkably like ducking the question and hoping that everyone lost
interest  in  the  meantime.  The  new  panel  was  supposed  to  report  in  mid-
2007, but was strangely delayed, finally appearing only in 2009.k

At the press conference for the launch of the report, McIntyre sensed a
certain  amount  of  unease  among  the  assembled  journalists.  The  Boehlert
questions  had  clearly  not  been  answered,  the  flaws  in  the  Hockey  Stick
seemed  to  have  been  acknowledged  and  there  was  a  growing  recognition
that Mann’s iconic paper had been oversold both to the press and the public.
Some of the questions were therefore quite searching and there was plenty
for the representatives of the panel to deal with. Some of their replies were
very controversial. For example, Peter Bloomfield, the statistician who had
so signally failed to address any of the contradictions in Mann’s statistical
arguments,  explained  to  the  assembled  media  the  panel’s  findings  on
Mann’s  use  of  PC  analysis.  The  usage 
in  MBH98,  he  said,  was
‘unconventional’ and ‘problematic’ and ‘introduced certain distortions’, this
presumably  being  a  coded  way  of  saying  ‘wrong’.  Like  so  many  before
him,  he  managed  to  discuss  the  whole  subject  without  once  mentioning
McIntyre  and  McKitrick.  As  he  continued,  Bloomfield  made  a  curious
claim:  if  you  averaged  the  proxies  in  MBH98,  he  said,  you  got  the  same
answer – a hockey stick. This directly contradicted a slide in McIntyre and
McKitrick’s presentation, which demonstrated just the opposite: the average
of the proxies looked nothing like the Hockey Stick at all, and if anything
had  a  downtick  in  the  twentieth  century  (see  Figure 9.1).  However,  what
was particularly odd about Bloomfield’s claim is that the panel had made it
quite plain that they had not performed any verification work of their own.
So how Bloomfield had arrived at the conclusion he did is unclear, but it is
a claim that has been repeated since.

Schizophrenic as it was, the report had something for everyone. Sceptics
could point to its acceptance that Mannian short centring was wrong while
greens  could  revel  in  its  conclusion  that  the  secondary  studies  gave  the
same answer. Most observers thought that the Hockey Team had done best
out of it.l They could potentially have ended up with their whole field of
study in tatters and their most prominent scientific result held up to ridicule.
As it was, the Hockey Stick was still in play, even if the panel had been
forced  to  rely  on  the  somewhat  dubious  argument  that  Mann  had  used
incorrect methods and inappropriate data but had somehow still managed to
reach the right answer.

The  press  reported  the  panel’s  findings  very  much  according  to  their
prejudices.  Nature  published  an  editorial,  its  headline  declaring  ‘Panel
affirms hockey-stick graph’ while the BBC said that the panel had given its
‘backing  for  [the]  hockey  stick  graph’.158,159  On  the  other  hand  the
Washington Times said that ‘the NAS report has re-established the [Little Ice
Age  and  the  Medieval  Warm  Period],  and  broken  the  Hockey  Stick’.160
Perhaps only the Wall Street Journal really understood what the panel had
done, reporting to its readers ‘Panel study fails to settle debate’.161
The Wegman report
The  dust  was  still  settling  on  the  NAS  report  when  the  excitement  was
ramped up again with the release of the Wegman report. On the day of its
publication, 14 July 2006, Barton also issued a press release to the effect
that the Oversight and Investigations Subcommittee, sometimes referred to
as the Whitfield Subcommittee, would shortly be holding hearings on the
subject  of  Barton’s  original  questions.  Clearly,  the  failure  of  the  NAS  to
address  the  Boehlert  questions  had  left  Barton  with  no  choice  but  to
investigate the Hockey Stick himself.

FIGURE 9.1: The average of Mann’s proxies compared to the Hockey Stick
Top: Simple average of Mann’s proxies. Bottom: The Hockey Stick.
Reproduced from McIntyre and McKitrick’s presentation to the NAS panel.

Rumours circulated around the climate blogs as to who had been invited
to the hearings. It was said that some climatologists were refusing to attend,
which,  if  true,  was  a  dangerous  course  to  take,  since  the  Whitfield
Subcommittee  could  enforce  attendance  through  a  subpoena.  A  few  days
later it was announced that the witnesses were to be the heads of the two
panels,  Gerry  North  and  Ed  Wegman,  together  with  four  researchers:
Thomas  Karl  and  Tom  Crowley,  who  could  be  expected  to  speak  for  the
Team,  with  McIntyre  opposing  and  von  Storch  occupying  his  customary
position  of  honest  broker.  Mann  was  apparently  unavailable,  but  the
Committee had invited him to attend the following week, thus obliging all
the other witnesses to come to Washington again. The first hearings were
just a week away.

Before the hearings could be held, the newspapers got hold of the details
of the Wegman report: an article appeared in the Wall Street Journal which
showed that Wegman was going to come out almost entirely on McIntyre
and McKitrick’s side of the argument.

The report commissioned by the House Energy Committee, due to be released today,
backs up and reinforces that conclusion. The three researchers – Edward J. Wegman of
George Mason University, David W. Scott of Rice University and Yasmin H. Said of
Johns Hopkins University – are not climatologists; they’re statisticians. Their task was
to look at Mr. Mann’s methods from a statistical perspective and assess their validity.
Their conclusion is that Mr. Mann’s papers are plagued by basic statistical errors that
call  his  conclusions  into  doubt.  Further,  Professor  Wegman’s  report  upholds  the

finding of Messrs. McIntyre and McKitrick that Mr. Mann’s methodology is biased
toward producing ‘hockey stick’ shaped graphs.162

McIntyre had been supplied with a copy of the Wegman report ahead of the
hearings,  and  it  was  just  as  supportive  as  the  Wall  Street  Journal  was
suggesting.15  Not  only  did  the  authors  share  the  NAS  panel’s  view  that
Mann’s short-centred PC calculation was biased, but they had also said so in
language that eschewed the bureaucratic double-speak adopted by North’s
panel.

In general, we found MBH98 and MBH99 to be somewhat obscure and incomplete and
the criticisms of [McIntyre and McKitrick] to be valid and compelling.15

Wegman  had  been  able  to  replicate  McIntyre’s  work  completely  and  had
accepted  all  of  his  arguments.  Wegman  and  his  colleagues  had  also  gone
beyond a straightforward analysis of the MBH98 data and methods and had
tried to analyse the reasons why a paper as flawed as Mann’s had managed
to slip through the peer review process and then had managed to reach a
position of such huge importance for the policy-making community. Why
was it that it had fallen to a retired minerals consultant and an economist to
expose  its  errors?  Why  had  the  paleoclimate  community  defended  it  so
vociferously?

In order to get to the bottom of these issues, Wegman and his coauthors
had  performed  an  analysis  of  the  links  between  paleoclimate  researchers.
This showed that most of them were indeed joined to each other by ties of
co-authorship and, moreover, that Mann had links to almost everyone else –
he was like a spider at the centre of a web of collaboration, in a position to
influence  everyone  around  him.  Wegman’s  conclusion  from  this  ‘social
network’ study was that the paleoclimate community was too insular, too
self-contained, too close-knit. Their insularity and the fact that Mann was at
the  very  centre  of  the  community  meant,  said  Wegman,  that  no  effective
independent review of Mann’s work was likely (see Figure 9.2) and where
mistakes were made, it was difficult for climatologists to correct their work:

[O]ur  perception  is  that  this  group  has  a  self-reinforcing  feedback  mechanism  and,
moreover, the work has been sufficiently politicised that they can hardly reassess their
public positions without losing credibility.15

But  his  criticisms  went  further:  paleoclimatologists,  he  said,  believed  too
passionately in the anthropogenic global warming hypothesis and they had
failed to enlist the help of statisticians in their work:

It is important to note the isolation of the paleoclimate community; even though they
rely  heavily  on  statistical  methods  they  do  not  seem  to  be  interacting  with  the
statistical community.

Here  at  last,  after  three  years  of  being  accused  of  incompetence  and
dishonesty,  was  complete  vindication  for  McIntyre.  It  was,  he  said,
immensely gratifying. What would the politicians now make of Wegman’s
findings when they were discussed at the hearings, only a matter of days
away?
Barton Committee hearings
The hearings were held in in Room 2123 of Rayburn House, the House of
Representatives’  huge  office  building  on  Washington’s  Capitol  Hill,
overlooking  Independence  Avenue.  Barton’s  Energy  and  Commerce
Committee  had  delegated 
its  Subcommittee  on
Investigations and Oversight, whose chairman, Ed Whitfield, would preside
over the proceedings.m

responsibility 

to 

The hearings opened with statements from the politicians. From the start
it was clear that they were more interested in fighting over political territory
than in understanding the lessons that should be learned from the Hockey
Stick affair. Republicans were keen to protect the findings of the  NAS and
Wegman panels from attack, while Democrats tried to limit the impact of
the reports on the wider global warming argument. Each and every member
of the subcommittee needed to have their say, most opting simply to state
their  position  on  global  warming.  The  witnesses  all  had  to  sit  patiently
while the members of the committee held forth.

For the attentive, these opening statements did contain hints of some of
the  lines  of  questioning  the  Democrat  members  were  going  to  take.  Bart
Stupak, the representative from Michigan and the senior Democrat on the
committee,  called  the  reports  ‘irrelevant’  and  said  that  it  was  difficult  to
assess Wegman’s work because it wasn’t peer reviewed. Stupak’s colleague
Jay Inslee referred to the hearings as a snipe hunt. There were attempts to
defend  Mann  by  arguing  that  MBH98  was  the  first  study  of  its  kind  –
although  it  wasn’tn  –  and  that  Mann  should  therefore  be  given  a  certain

amount  of  leeway.  There  was  much  talk  of  melting  glaciers  and  record-
breaking  temperatures.  It  was  therefore  a  relief  when  Whitfield  finally
wound up the political venting and called the first two witnesses.
FIGURE 9.2: Mann at the the centre of a paleoclimate web

Lines  indicate  links  of  co-authorship.  While  each  clique  is  largely  self-
contained, Mann has worked with all of the authors shown. Adapted from
The Wegman Report

The committee was to hear opening statements from Wegman and North,
each outlining the contents of their reports, before cross-examining them on
their findings. The Democrat members had very few cards to play. The NAS
panel  seemed  to  have  backed  the  criticisms  of  Mann’s  work,  and  now
Wegman  was  saying  the  same  thing,  but  louder  and  more  clearly.  Their
situation was made worse when North made his agreement with Wegman
on the statistical findings absolutely clear, although he maintained his line
that, despite Mann’s mathematics being wrong, there was still a possibility
that his conclusions were correct.

CHAIRMAN BARTON: Dr. North, do you dispute the conclusions or the methodology of
Dr. Wegman’s report?

DR NORTH: No, we don’t. We don’t disagree with their criticism. In fact, pretty much
the  same  thing  is  said  in  our  report.  But  again,  just  because  the  claims  are  made,

doesn’t mean they are false.

Barton  tried  to  see  off  some  of  the  potential  lines  of  attack  before  they
happened.  He  told  the  committee  that  he  had  ‘heard’  that  Wegman  had
voted  for  the  Democratic  candidate,  Al  Gore,  in  the  last  presidential
election,  and  Wegman  indicated  that  this  was  indeed  the  case,  neatly
preventing  any  suggestion  of  political  bias.  Barton  also  asked  Wegman
about his previous contacts with the committee and with McIntyre, again
preventing the Democrats from insinuating that Wegman might suffer from
a lack of independence. The questioning of Wegman was at times somewhat
aggressive but all very peripheral to Wegman’s core argument that Mann’s
work  had  not  correctly  implemented  a  PC  calculation.  There  was  close
questioning of his social network study and much discussion of the IPCC’s
1990  chart  showing  the  Medieval  Warm  Period,o  which  Wegman  had
reproduced in his report, although it is not clear what the Democrat team
hoped  to  achieve  by  doing  this.  Wegman  was  also  asked  some  entirely
irrelevant  questions  about  his  views  on  the  larger  question  of  global
warming.  Representative  Jan  Schakowsky  of  Illinois,  for  example,  asked
him  if  he  wasn’t  concerned  that  his  results  would  be  used  by  sceptics  to
discredit  the  global  warming  hypothesis.  Von  Storch,  in  evidence  given
later,  declared  himself  ‘shocked’  by  Schakowsky’s  implication  that
Wegman  should  have  written  something  other  than  the  truth  if  that  was
useful for the policy process.

North, meanwhile, had a much easier time of it. Having conceded that
Mann’s  statistics  were  wrong,  he  was  still  maintaining  his  line  that  the
independent  confirmations  suggested  that  Mann’s  findings  were  correct
regardless. During his time at the microphone, North was questioned on the
subject by the Democrat Henry Waxman, and these exchanges should have
thrown some light on how the panel dealt with McIntyre’s evidence that the
other  studies  were  just  as  contaminated  by  use  of  bristlecones  as  was
Mann’s.

DR NORTH: But as I have said, it is only one of several lines of evidence that are used
in drawing those conclusions.

MR WAXMAN: And so therefore you have further studies that seem to come to similar
conclusions?

DR NORTH: There are other studies, and they were shown on the graphic that I showed
you.

MR WAXMAN: And they weren’t based on the Mann studies, were they?

DR NORTH: They were not based on the Mann studies. Now, there are cases where
they  use  the  same  data  so  there  is  some  correlation  and  that  is  what  I  think  Dr.
Wegman referred to and that is correct. See, there is only a limited amount of data, so.
. .

North’s  final  statement  shows  that  he  knew  at  least  something  about  the
proxies  used  in  the  spaghetti  graph  studies  –  clearly  McIntyre’s  evidence
had been considered. Some further light was shone on this question some
weeks later, when North took part in an online colloquy about the Hockey
Stick and McIntyre took the opportunity to question him further.

MCINTYRE:  The  [NAS]  Panel  stated  that  strip-bark  tree  forms,  such  as  found  in
bristlecones and foxtails, should be avoided in temperature reconstructions and that
these proxies were used by Mann et al. Did the Panel carry out any due diligence to
determine whether these proxies were used in any of the other studies illustrated in the
NRC spaghetti graph?

DR NORTH: There was much discussion of this matter during our deliberations. We did
not dissect each and every study in the report to see which trees were used. The tree
ring people are well aware of the problem you bring up. I feel certain that the most
recent studies by Cook, D’Arrigo and others do take this into account. The strip-bark
forms in the bristlecones do seem to be influenced by the recent rise in CO2 and are
therefore not suitable for use in the reconstructions over the last 150 years. One reason
we place much more reliance on our conclusions about the last 400 years is that we
have several other proxies besides tree rings in this period.164

If the NAS panel didn’t look at ‘each and every’ study in terms of the proxies
used, then the implication is that they looked at least at some of them. This
is  the  only  explanation  that  would  also  be  consistent  with  the  evidence
North gave to the Whitfield Committee – that there was some commonality
of  data  between  the  studies.  The  inescapable  conclusion  is  that  the  panel
must have been aware that bristlecones were used in at least some of the
‘independent’ confirmations. It remains a mystery why the NAS panel didn’t
at  least  raise  this  as  a  question  mark  over  the  integrity  of  the  purported
confirmation – something that would require further assessment so that the
impact  of  the  flawed  proxies  could  be  properly  assessed.  Either  way,
McIntyre was not impressed, commenting to Climate Audit readers:

I’ve said over and over how frustrated I am that the due diligence of the  NAS panel
was so negligible and slight and that they relied on mere literature review for so much
of their study. It’s ludicrous for them to say that bristlecones should be ‘avoided’ in
temperature reconstructions and then to ‘bring in other evidence’ – a ‘half-dozen other
reconstructions’ that use bristlecones – without testing for the impact of bristlecones
on these reconstructions. I’ll do the testing of the impact of bristlecones on the other
reconstructions, but the NAS panel should have done it themselves.165

With  the  interrogation  of  the  heads  of  the  panels  complete,  Whitfield’s
committee  moved  on  to  the  scientists,  but  in  fact  there  was  very  little  of
substance discussed that hadn’t already been said at the NAS panel. McIntyre
reiterated  his  criticisms  of  Mann,  and  set  out  the  problems  with  the  peer
review process and the failure of scientists to archive data and code, but it
was  clear  by  this  time  that  the  panel  was  rapidly  losing  interest  in  the
subject.  Even  when  Mann  himself  took  the  microphone  a  week  later,  the
contradictions between his claims and those that McIntyre had given were
brushed aside by the assembled politicians. The committee had moved on:
the  Republican  side  had  its  condemnation  of  Mann’s  paper  and  had
concluded  that  they  had  won  this  particular  battle.  The  Democrats,
meanwhile, had got North and Wegman to state that nothing in their reports
affected the overall case for global warming. The tide of the wider war was
still flowing their way.
An interlude: Dr Thompson’s Thermometer
During  the  interrogation  of  Wegman  and  North  there  was  an  interesting
exchange  which,  while  not  directly  relevant  to  the  story,  does  tell  us
something about the importance of the Hockey Stick to politicians. During
the questions to North and Wegman, one of the Republican congressmen,
Cliff  Stearns  of  Florida,  started  to  discuss  the  importance  of  the  Hockey
Stick  and  how  it  seemed  to  crop  up  everywhere  after  the  IPCC  report  of
2001.  He  pointed  particularly  to  its  use  in  Al  Gore’s  movie,  An
Inconvenient  Truth,  and  the  book  of  the  same  title.  There  is  indeed  a
mention of the Hockey Stick in An Inconvenient Truth, when Gore makes
the case that the Hockey Stick is supported by ice core records:

[S]o-called global warming skeptics often say that global warming is really an illusion
reflecting nature’s cyclical fluctuations. To support their view, they frequently refer to
the Medieval Warm Period. But as Dr Thompson’s Thermometer shows, the vaunted
Medieval  Warm  Period  (the  third  little  red  blip  from  the  left  below)  was  tiny  in
comparison to the enormous increases in temperature in the last half-century – the red
peaks  at  the  far  right  of  the  graph.  These  global-warming  skeptics  –  a  group

diminishing  almost  as  rapidly  as  the  mountain  glaciers  –  launched  a  fierce  attack
against  another  measurement  of  the  1000  year  correlation  between  CO2  and
temperature known as the ‘Hockey Stick’, a graphic image representing the research
of  climate  scientist  Michael  Mann  and  his  colleagues.  But  in  fact  scientists  have
confirmed  the  same  basic  conclusions  in  multiple  ways  with  Thompson’s  ice  core
record as one of the most definitive.166

‘Dr  Thompson’  was  a  reference  to  Lonnie  Thompson,  a  distinguished
paleoclimatologist  who  recreated  temperatures  from  ice  core  records;  his
‘thermometer’ was simply a reference to these temperature reconstructions.
So  according  to  Gore,  Thompson’s  ice  core  reconstruction  confirmed
Mann’s work – another independent confirmation to add to those shown by
North in the NAS report.

When Stearns raised the subject of Gore’s citation of the Hockey Stick,
he was interrupted by the Democrat Schakowsky, who asked him to yield
the floor so that she could make a point. Stearns was reluctant to do so, but
Schakowsky was extremely insistent, and she was joined by her colleague
from the Democratic side, Bart Stupak. Even with two people asking him to
give way, Stearns still refused, insisting that Wegman should comment on
the inclusion of the Hockey Stick in the movie, apparently trying to score a
political point by linking the flaws in Mann’s paper to the work of Al Gore.
Wegman started to reply, saying that there was some ambiguity in Gore’s
citations  and  for  a  moment  confusion  reigned.  However,  after  a  few
moments, calm was restored when Wegman concluded that Gore had in fact
referred  to  the  ice  core  studies.  Stearns  then  finally  gave  way  to
Schakowsky,  who  had  by  then  tried  no  less  than  five  times  to  get  the
microphone.

MS SCHAKOWSKY: Thank you. I just want to read to you from that same – it says ‘But
as Dr. Thompson’s Thermometer shows,’ and so it is not based on Dr Mann. This is a
different source which our staff had confirmed with Al Gore. I just want to make . . .

MR STEARNS: I respect that.

MS SCHAKOWSKY: . . . that point. I know, but your question wanted to reinforce the
notion that this was based on this false or inaccurate Dr Mann study . . .

MR STEARNS: Well, I think . . .

MS SCHAKOWSKY: . . . and it is not.

MR STEARNS: Okay.

And that was how the subject was left – with all parties concluding that the
hockey stick shaped chart in Gore’s book and movie were based on ice core
records,  thus  demonstrating  support  for  the  conclusions  of  the  Hockey
Stick.  They  knew  that  it  was  an  ice  core  study  because  Schakowsky  had
checked with Gore’s office.

More than a year later, McIntyre started to ponder the subject of Gore’s
hockey stick. As he studied the graphic used, a number of things came to
his  attention.  For  a  start  the  resolutionp  appeared  to  get  higher  in  the
nineteenth  and  twentieth  centuries.  This  was  peculiar  because  ice  core
studies should give the same resolution for all years. How had Thompson
managed to get greater detail in the modern era? Moreover, in the twentieth
century,  the  chart  appeared  to  show  positive  and  negative  values
simultaneously. Even the style of the chart didn’t seem quite like anything
else that Thompson had produced before. McIntyre was intimately familiar
with  everything  Thompson  had  published  and  there  was  nothing  in  his
papers  that  looked  quite  like  the  chart  in  Gore’s  movie.  Where,  he
wondered,  could  it  have  come  from?  Bemused,  he  did  what  he  often  did
when completely stuck for an answer and asked his Climate Audit readers if
they had any ideas.

It was perhaps a surprise that the first comment was by one of his chief
internet  opponents,  an  Australian  computer  scientist  and  scourge  of  the
sceptic  community  called  Tim  Lambert.  It  was  even  more  surprising  that
Lambert knew exactly  where  the  graph  had  come  from.  Unfortunately  he
wasn’t  letting  on,  at  least  not  immediately.  As  the  Climate  Audit  readers
studied the chart, more strange things were noticed. The temperature axis
was upside down, with negative values at the top and positive ones at the
bottom,  implying  that  the  world  was  actually  cooling.  The  x-axis  didn’t
meet  the  y-axis  at  zero.  Finally,  Lambert  asked  if  McIntyre  was  only
kidding that he didn’t know the source study for the graph, and this gave
some of the readers enough of a clue to work the answer out. What Gore
called  ‘Dr  Thompson’s  Thermometer’  was  in  fact  Mann’s  Hockey  Stick
itself, recoloured, placed on different axes and given a new name. It was no
wonder  that  he  could  claim  that  his  graph  looked  remarkably  like  the
Hockey  Stick:  it  was  the  Hockey  Stick.  Its  appearance  in  place  of
Thompson’s ice core graph turned out to be a copying error, which neither

Gore, nor his staff, nor Thompson, their scientific adviser, had noticed. The
splice of reconstructed and instrumental data was not acknowledged in An
Inconvenient  Truth,  but  if  you  referred  to  Lonnie  Thompson’s  original
papers,  it  was  possible  to  see  the  version  of  the  Hockey  Stick  Gore  had
used, with the splice clearly shown.

Gore  was  famously  awarded  a  Nobel  Prize  for  his  work  on  global
warming. It was, as McIntyre said, too funny. Thompson was later asked in
a public meeting what he had done to correct this mistake. He replied that
he had no responsibility to make the error known to the public.167
Ritson tries again
As  the  dust  settled  on  the  NAS  report  and  the  brouhaha  over  the  Barton
hearings  faded  away,  the  press  and  the  participants,  and  some  of  the
observers, started to set down where they thought the arguments had got to.
McIntyre and McKitrick had won at least a partial victory with both the NAS
and Wegman panels agreeing that Mannian short centring was biased. There
was still the NAS’s mystifying use of bristlecone-infected studies in support
of the Hockey Stick, but there was still much cause for satisfaction.

Meanwhile the Hockey Team was going to make one last attack on the
findings  of  the  two  panels.  At  the  end  of  August,  a  posting  appeared  at
RealClimate  called  ‘Followup  to  the  “Hockey  Stick”  hearings’.168  The
posting pointed to some of Mann’s answers to follow-up questions from the
House  Committee,  one  of  which  proved  rather  surprising.  As  the
RealClimate posting put it:

Among  the  more  interesting  of  these  documents  are  a  letter  and  a  series  of  email
requests from emeritus Stanford Physics Professor David Ritson who has identified
significant apparent problems with the calculations contained in the Wegman report,
but curiously has been unable to obtain any clarification from Dr. Wegman or his co-
authors in response to his inquiries. We hope that Dr. Wegman and his co-authors will
soon  display  a  willingness  to  practice  the  principle  of  ‘openness’  that  they  so
recommend in their report . . .168

We have, of course, met David Ritson already, as the author of the ‘goofy’
comment  on  McIntyre’s  GRL  paper,  which  was  rejected  twice  by  the
journal.q Ritson, it seemed, had been trying to obtain the data Wegman had
used to perform the calculations in his report, and had emailed three times
without  getting  a  response,  a  result  which  he  believed  amounted  to  a
blanket  refusal.  Noting  that  some  of  his  own  data  requests  were  still

outstanding after three years,  McIntyre  suggested  on  his  blog  that  Ritson
might be somewhat premature to jump to this kind of conclusion after three
weeks.

The significant problem that Ritson claimed to have found was an error
in  the  way  Wegman  had  modelled  the  statistical  properties  (‘the
autocorrelation  structure’)  of  proxy  series  in  his  simulations.  The  errors
were,  said  Mann,  the  same  as  those  that  McIntyre  had  made  in  his  own
Hockey Stick studies and were ‘so basic that they would almost certainly
have  been  detected  in  a  standard  peer  review’.169  It  was  surprising  for  a
self-confessed  non-statistician  to  accuse  one  of  the  world’s  leading
exponents of that subject of making a mistake in his statistical workings,
particularly in those terms. What was even more amazing though was that
to  carry  this  claim  off,  Mann  was  going  to  have  to  show  not  only  that
McIntyre and Wegman were wrong, but also von Storch, Huybers and the
NAS  panel,  all  of  whom  had  concluded  that  Mannian  short  centring  was
biased.  It  was  also  extremely  odd  that  Ritson’s  original  comments  on
McIntyre’s  GRL  paper  contained  no  mention  of  these  allegedly  ‘basic
errors’. A few weeks later, the Hockey Team’s clutching at straws became
downright  embarrassing  when  McIntyre  pointed  out  that  the  way  he  and
Wegman had determined the statistical structure of the simulated data was
identical to that used by Mann. It was, he pointed out, an extraordinarily
weak  point  for  the  Hockey  Team  to  make  a  stand  on,  and  sure  enough,
Ritson promptly made a diplomatic retreat and dropped the subject.
Taking wing
Before we move on to the next chapter there is one final aspect to the story
of the NAS panel that bears repeating. In the aftermath of the report Gerry
North was in much demand, and one of the lectures he gave on the panel’s
work gave some interesting insights into the nature of their review. These
will be important when we reach this book’s final chapter, when we look at
the implications of the Hockey Stick affair for the global warming debate
and for science in general.

In a talk he gave at his own Texas A&M University, North explained to

his audience the way the panel had worked.

We didn’t do any research in this project, we just took a look at the papers that were
existing and we tried to draw some kinds of conclusions from them. So here we had
twelve people around the table, all with very different backgrounds from one another

and we just kind of winged it to see . . . so that’s what you do in that kind of expert
panel . . .170

North said these words, not with any sense of dissatisfaction or of concern.
His  tone  was  matter-of-fact;  this  was  just  the  way  things  were  in  expert
panels. It was just one more dismaying revelation from the Hockey Stick
affair  –  faced  with  the  most  important  scientific  questions  for  decades,
asked to study and report on a subject of incalculable economic, political
and social importance, a group of distinguished scientists got round a table,
talked about some papers and just ‘kind of winged it’.

a  See page 280.
b  Mann actually used the alternative notation r2. See note on page 16.
c  Calculating statistical significance in multivariate models using highly autocorrelated time series is

highly complex, with a strong risk of spurious significance.

d  This was a problem that was presumably somewhat challenging for Bloomfield given that the post-

1960 figures had gone off at a tangent to the rest of the record. That is another story though.

e  Hegerl is, strictly speaking, a statistician.
f  See page 61.
g  Jones’comments were made in an email to the Australian researcher, Warwick Hughes. Hans von
Storch apparently confirmed with Jones that this was a true representation of his position before
quoting him to the NAS panel.

h  If you can’t see why this is so egregious, consider the trials of a new drug in which only the results
of the ‘ten best-responding patients’ are reported – it would be illegal in most places in the world.
However, in climatological circles, this kind of behaviour appears to be readily accepted.

i  See page 63.
j  The paper is examined in more detail on page 298.
k  When the report finally appeared, it turned out that the panel had largely ducked the question of
whether  Mann’s  data  had  been  available.  While  mentioning  briefly  that  Mann  had  ‘resisted’
requests for his data, the panel preferred to discuss what it saw as Barton’s ‘intimidation’ of the
Hockey  Stick  authors.  Discussion  of  the  Hockey  Stick  was  limited  to  just  two  pages  of  the
report.156

l  Climate policy academic Roger Pielke Jnr described it as ‘a near-complete vindication for the work

of Mann et al’.157

m  The story of the hearings and the quotations below are based largely on the official transcript.163
n  MBH98 referred to Bradley and Jones 1993 and several other multiproxy studies.
o  See page 25.
p    The  resolution  is  simply  the  smallest  time  period  that  can  be  distinguished  in  a  proxy  or  a
reconstruction.  Tree  rings  give  annual  resolution,  while  other  proxies  might  only  show  much
longer periods.
q  See page 200.

10     Zone Defence

Great spirits have always found violent opposition from mediocrities. The latter cannot understand it when a man
does not thoughtlessly submit to hereditary prejudices but honestly and courageously uses his intelligence.

Albert Einstein

The NAS panel had concluded that short centring was biased and that the bristlecones were flawed,
but they also said that the similarity of the MBH98 reconstruction to subsequent papers showed that
Mann had managed to get the right answer anyway. This was not a new idea. From the time of the
Third Assessment Report (TAR) back in 2001, the IPCC had attempted to bolster the position of the
Hockey  Stick  with  spaghetti  charts  –  graphics  showing  all  of  the  temperature  reconstructions
together. In TAR the spaghetti chart had shown MBH99 alongside a couple of other studies – Briffa’s
infamous truncated reconstruction and another by Phil Jones. By the time of the Fourth Assessment
report,  other  reconstructions  had  appeared  as  well  –  Moberg  et  al,  Jones  and  Mann,  Esper  et  al,
Crowley and Lowery, and others too.

McIntyre had filled many hours with the detailed analysis of each of these allegedly independent
verifications of Mann’s work and, before we go on to look at the IPCC’s Fourth Assessment Report
itself, we need to understand what he had found so that we can consider just how independent they
really are. We have already seen that many of these studies include bristlecone proxy data, strongly
suggesting that they are just as unreliable as MBH98. We have also seen that many of them fail key
statistical tests like the R2, CE and the Durbin–Watson statistic. However, over the following years,
McIntyre’s  researches  had  unearthed  many  more  surprising  details  that  cast  further  doubt  on  the
wisdom of using these ‘independent’ studies to support important government policy decisions. First
though, we need to ask ourselves just what we mean by a ‘confirmation’. What do we look for when
another paper is said to confirm a study like MBH98? What factors make it a good confirmation and
what would an inadequate one look like?

A temperature reconstruction can be questioned firstly on the basis of its data: were the proxies
appropriate,  were  they  distorted  by  factors  other  than  temperature,  was  the  measurement  data
processed correctly and so on? The other area that needs to be considered is the methodology: was it
appropriate to answering the question asked? Was it correctly applied?

As far as the data is concerned, there are a host of issues that could reduce our confidence in the
validity  of  the  temperature  reconstructions  that  appeared  in  the  years  after  MBH98.  We  have  seen
throughout  this  book  that  there  are  enormous  question  marks  over  the  validity  of  tree  rings  as  a
proxy for temperature, so we would presumably consider a confirmation of Mann’s work that relied
on  tree  rings  less  convincing  than  one  that  was  based  on  other  proxies,  such  as  ice  cores  or
speleothems.a We would presumably be still less impressed by a study that was not only based on
tree rings but also used inappropriate trees such as bristlecones. We would likewise not wish to rely
on studies whose data was not publicly archived, was outdated or whose inclusion was not clearly
justified – in other words there should be no question of cherrypicking.

When it came to the methodology, we would expect that a valid confirmation would avoid the
incorrect short-centring methodology that Mann used in his PC calculations, and it should also avoid
using ad-hoc statistical methods that had not been thoroughly tested by the statistical community.
Beyond that, we would have to consider each methodological choice on its merits.

However,  a  study  that  reached  the  same  conclusions  as  Mann  while  passing  all  of  these  tests
would represent strong support for the NAS panel’s case that Mann had reached the correct answer
regardless of his incorrect method and inappropriate proxies. There is, however, another criterion
that  we  should  consider  –  one  that  might  colour  our  view  of  the  reliability  of  the  paper  as  a

confirmation. Mann and the NAS had been at pains to point out that the confirmations of MBH98 were
‘independent’, and we would therefore need to consider just how independent they truly were. What
was  the  relationship  between  each  of  the  authors  and  Mann,  Bradley  and  Hughes?  While  it  is
important to say that coauthorship wouldn’t make a study incorrect – this would be the same logical
fallacy that assumes that any study funded by oil companies is incorrect – we would surely find a
confirmation by a close colleague of Mann’s, such as Ammann or Crowley, less convincing than one
by an opponent.

While this failure to supply all of the data didn’t prevent progress, it was unhelpful and raised the
possibility  that  McIntyre’s  time  would  be  wasted  in  identifying  differences  between  the  archived
versions and the versions used in the paper. However, undeterred, McIntyre set about his task.

The proxy roster in Jones et al consisted of just 17 series, a strikingly smaller number than the
more than 400 used in MBH98. Much of Mann’s claim to scientific rigour was based on the huge size
of the proxy roster, and it is fair to say that the much smaller number used by Jones was more the

The purpose of this chapter, therefore, is to survey some of the studies that appeared in the IPCC’s
Third Assessment Report of 2001 and in the years thereafter, so that we can assess their reliability.
While this might seem a rather dry subject after the excitement of the earlier parts of Mann’s story,
don’t skip on, because there are tales here that are at least as amazing as those that have been told
earlier in this book.
Jones et al 1998
Jones  et  al  1998  was  and  remains  one  of  the  most  important  multiproxy  studies,  published  by  a
British  team  at  around  the  same  time  as  the  original  Hockey  Stick  paper.171  McIntyre  had  been
looking at it for almost as long as the Hockey Stick itself. In some ways he saw Jones and his co-
authors as being in competition with Mann to be the first team to ‘get rid of the Medieval Warm
Period’, but the British team’s attempt to achieve this feat had been far less ambitious than Mann’s.
Their final reconstruction was much less dramatic, with the Medieval Warm Period still clearly in
evidence, although it peaked at levels lower than those reached in the twentieth century portion of
the reconstruction. Jones et al could therefore be used to support the idea that the Medieval Warm
Period was a weak and maybe localised phenomenon, but it didn’t really help Mann’s case, which
was that the Medieval Warm Period never happened at all.

We have come across Phil Jones a couple of times already during the course of this story, and his
co-authors on the 1998 paper included another familiar name, that of Keith Briffa. Both men had
worked with other members of Mann’s team and are seen as core members of the Hockey Team, so
without even looking at the detail of the paper, it is clear that Jones et al 1998 was not strictly an
independent confirmation. However, this is a minor point; the details of the paper are considerably
more damning.

McIntyre’s first step in trying to replicate a paper was to collate the data. While data might be
cited correctly and accurately in the papers, it was always possible that what had actually been used
was different in some way to the official versions, whether due to an error in the archive or one
made by the authors. Because of this possibility, McIntyre’s approach was to try to obtain the data,
as used, direct from the authors. This could then be checked by him against the archived data in
exactly the way he had done for MBH98.

However,  having  approached  Jones  for  the  figures,  he  was  disappointed  to  discover  that  the
Englishman  was  even  more  reluctant  than  Mann  to  supply  the  full  details  of  his  research,  only
agreeing to supply two ‘grey’ data series. (Grey series are versions of data that are different to the
official  versions  in  some  way  and  are  passed  between  authors  without  ever  being  archived.  This
makes a study that uses grey series virtually impossible to replicate and indeed the use of grey data
is either forbidden or at least frowned upon in many other disciplines.) The remaining series, Jones
said, McIntyre should take from the archives.

norm for paleoclimate reconstructions. It is of course questionable just how reliable any temperature
reconstruction can be when it is based on such a small quantity of data, especially as we know how
tree rings vary from site to site and even within sites. One of the issues that was constantly raised
about  paleoclimate  reconstructions  was  their  lack  of  confidence  intervals,  which  would  allow
readers to assess this issue of reliability in a scientific way. Jones et al was no exception.

In terms of the actual series behind the paper, no less than 13 of the 17 series used by Jones were
also used in MBH98 and MBH99, which again throws doubt on the independence of the study, but this
time in relation to its data rather than its authorship. On the positive side, however, none of these
series had been extracted from bristlecone pines. That was the good news. Of the 17 series though,
how many had a hockey stick shape? Was this shape coming from the majority of the data, or was it
coming  from  just  a  few  of  the  series,  as  was  the  case  in  Mann’s  papers?  With  so  few  series  to
examine, it was simple to check the graphs, and it was quickly apparent that there were very few
hockey stick shaped series at all: in fact the relatively low temperatures in Jones’ medieval sections
could be ascribed to just a single series: the one called Polar Urals.
Briffa and Polar Urals
Polar Urals was one of only three series that covered the medieval period in Jones et al. Because of
the length of the record it was a very popular series among paleoclimatologists, appearing in most of
the reconstructions published up to that time. It had first come to prominence in a series of papers
that Briffa had published earlier in the 1990s, in which he made the startling claim that 1032 was the
coldest year of the millennium, at least in the area of Polar Urals. If true, this would have completely
overturned climate history, implying that the Medieval Warm Period was non-existent or at best a
local phenomenon.

The  problems  McIntyre  discovered  related  to  the  quality  control  procedures  used  when
processing the cores. Dendrochronologists usually have carefully defined criteria for assessing the
reliability of their measurements. Briffa was no exception and in earlier studies he had calculated a
measure which he called the ‘subsample signal strength’. This figure was simply an assessment of
how many cores had been extracted and also how well the graphs of the growth in individual cores
matched up against each other. What was peculiar was that in the Polar Urals study, Briffa had been
silent  on  what  quality  control  measures  he  had  used.  Intrigued,  McIntyre  calculated  the  numbers
himself  and  he  was  able  to  show  that,  in  periods  prior  to  1100  at  least,  Polar  Urals  failed  the

In the published scientific literature, however, and unacknowledged in the Briffa papers, there
was actually strong evidence that this claim was mistaken. A number of authors had noted telltale
changes  in  certain  environmental  indicators,  such  as  the  regeneration  of  larch  and  a  move  of
treelines to higher altitudes, which were strongly suggestive of Polar Urals experiencing a warmer
climate in the eleventh century.172,173

With this contradictory evidence in mind, it was important to assess the impact of Polar Urals on
the Jones reconstruction. McIntyre was able to show that by removing this one series from the proxy
roster it was possible to make the medieval period appear warmer than the twentieth century. This
was an important result. If these other indicators were suggesting that the Polar Urals region was
actually warm in the eleventh century, did that mean that Briffa’s Polar Urals proxy series was not
actually representative of the region? It was going to be necessary to dig further into Briffa’s work.

Like  so  many  other  studies  of  its  kind,  the  early  years  of  the  Polar  Urals  chronology  were
distinguished by sparsity of data. In essence there just weren’t enough trees on the site that were of
suitable antiquity and in an adequate state of preservation to allow reliable cores to be taken. In fact,
it turned out that the claim about the relative coldness in the year 1032 was based on cores taken
from just four trees. This lack of data was troubling enough, but when McIntyre started to look at
the measurement data – the core samples taken from these four trees – he was shocked by what he
found.

subsample signal strength test – a finding that was hardly surprising in view of the small number of
samples.

In fact, the quality control procedures adopted for the Polar Urals series seem to have been beset
with difficulties. The tree cores used were in a very poor state, with some of them having gaps as
long  as  59  years  and  others  having  as  many  as  seven  gaps.  In  the  93  cores  that  made  up  the
chronology, there were 41 gaps, suggesting a real problem with the quality of the data. Jones, who
had  worked  with  Briffa  on  the  Polar  Urals  study,  explained  to  McIntyre  that  the  reason  for  the
problem  was  that  it  had  proved  necessary  to  cut  the  cores  into  pieces  to  get  them  into  the  x-ray
machine  which  measured  the  wood  density.  This  had  apparently  caused  some  of  the  rings  to  be
unmeasurable. This seemed implausible to McIntyre, who had noticed that there were some sections
of the core which only consisted of three rings – far too small for Jones’ explanation to hold. He
therefore decided to get a second opinion from Douglas Larson, an experienced dendrochronologist
and a colleague of McKitrick at the University of Guelph. Larson had taken an early interest in the
work McKitrick was doing with McIntyre, and was therefore happy to help. He didn’t mince his
words:

When one breaks a core, it fractures easily along a spring wood boundary because that wood is weaker than summer
wood, with small cells. No wood actually falls away when a core is broken unless you use your teeth to break it. Or a
hammer. If [Briffa and his team] have more than one missing ring at each end of a break, the series should not be used
at all. If there are ‘lots’ of breaks to allow for the reorientation of the series in the radiograph, then that means that they
were  sloppy  when  they  took  the  core  and  they  were  nowhere  near  the  pith,  so  the  core  is  a  tangent  instead  of  a
radius.174

FIGURE 10.1: A well-dated tree
Top:  Correlation  based  on  ring  widths.  Middle:  Correlation  based  on  density.  Bottom:  Average
correlation.
Not only was there a problem with the quality of the data, but the cross-dating appeared to be highly
dubious too. Cross-dating is the way dendrochronologists work out which year to assign to each ring
of the tree. The principle is relatively simple. Imagine a graph of ring widths for a well-dated tree;
say one that’s still alive. You can assign rings to years simply by counting back from the outside of
the tree towards the inside. Now say you also have a graph of ring widths from a dead tree which
you want to date. You can’t count back rings as you did for the live tree, because you don’t know
when it died. However, in order to get an accurate dating, all you have to do is to slide the graph of
the dead tree rings against those from the live one, one ring at a time, and measure the correlation
after each shift of one year. The idea is that when the rings match up – peak matching peak and
trough matching trough – there will be a sudden spike in the measured correlation. When you shift
one step further the spike will just as quickly disappear again. In this way it’s possible to get very
precise datings.

FIGURE 10.2: Tree with uncertain dating

Figure  10.1  shows  McIntyre’s  simulation  of  this  effect.  The  three  charts  show  three  different
ways of calculating the correlation between a tree with an unknown start year (No. 862030) and one
where the start year is known, based on this approach of sliding the graphs along each other until the
peaks and troughs match up. The spike at the year 1015 indicates that the correlation is best when
this start date is assigned to the unknown tree. The fact that the spike in the correlation appears at
the same date in all three charts gives the researcher confidence that year assigned is correct.

The  problem  with  the  cross-dating  on  the  Polar  Urals  trees,  or  at  least  those  four  trees  which
supported Briffa’s claim about how cold it was in 1032, was that the spikes, such as they were, did
not provide the necessary certainty in the dating. One of the trees did have a clear correlation spike –
in fact this was the tree we saw in Figure 10.1 – but the other three gave no clear indication of how
the dates should be assigned. One of these, Tree 862470, is shown in Figure 10.2.

This, then, presented McIntyre with a mystery to solve. How had Briffa managed to assign a date
to these trees without a correlation spike? There was no way of knowing for sure, but it did look
very much as if the lowest density ring had simply been assigned to the required date of 1032 in
order to back up the claim that this was the coldest year of the millennium.

Fortunately, an opportunity presented itself to show that these four trees were indeed misdated. In
2005, McIntyre discovered that an update to the Polar Urals chronology had been collected in 1999.
By a stroke of good fortune he was able to obtain the details, and was gratified to see that the new
figures showed an entirely different story to the old ones. Temperatures in the eleventh century now
appeared to be higher than those in the modern era. This, together with the revelation of the poor
cross-dating, appeared to be conclusive evidence that the cold eleventh century was an artefact of
poor data quality rather than a genuine climatic effect. It was now likely that the Polar Urals series
was entirely unreliable and this meant that doubt was cast on all of the multiproxy reconstructions of
which it had formed a component.

This  must  have  represented  something  of  a  problem  for  the  Hockey  Team,  but  in  the  end  the
solution was simple enough: the issue was bypassed by the simple expedient of not publishing the
update. There was another potential problem though: any new studies by the same authors would
now have to avoid using the update, making hockey sticks much harder to manufacture: there just
weren’t  that  many  hockey  stick  shaped  series  to  choose  from.  Fortunately  for  the  Hockey  Team,
there turned out to be one that was suitable.

Presumably  not  wanting  to  draw  attention  to  the  substitution,  Briffa  didn’t  discuss  the  use  of
Yamal rather than Polar Urals in the text of the paper, but the change was significant. The two series
– Yamal and the Polar Urals update – are reproduced in Figure 10.3 and considering they are from
sites  just  100  km  or  so  apart,  the  difference  in  their  shapes  is  remarkable.  Even  a  cursory
examination  of  the  two  charts  makes  the  reasons  for  the  Yamal  substitution  clear.  Yamal  is  pure
hockey stick, with only a hint of a Medieval Warm Period, while the Polar Urals update is just the
opposite, with strong growth in the eleventh century and no twentieth century growth spurt. Polar
Urals rarely saw the light of day again in a paleoclimate study.
Briffa’s Tornetrask series
Briffa was also responsible for one of the other series used in Jones et al 1998. This was a tree ring
study based on samples taken at Tornetrask in northern Sweden. Like Polar Urals, the Tornetrask
series  was  much  used  in  paleoclimate  temperature  reconstructions,  appearing  in  nearly  every
multiproxy temperature reconstruction at that time.b

Although Briffa had not archived any numbers, McIntyre was eventually able to get hold of the
underlying data since a later researcher had done so. McIntyre found that he was able to emulate the
chronology quite closely by processing the raw ring density measurement data through the various
steps that are used to standardise tree rings (see Chapter 2).

FIGURE 10.3: Yamal and the Polar Urals update
The Yamal substitution
With Polar Urals now unusable, there was a pressing need for a hockey stick shaped replacement.
The solution came in the shape of a series from the nearby location of Yamal, which replaced Polar
Urals as the representative of this region in Briffa’s next paper147 and indeed in pretty much every
paleoclimate reconstruction thereafter.

Briffa had published his Tornetrask findings in a series of papers in the 1990s.175–177 However,
these papers turned out to have a serious problem: the tree ring density was falling in the twentieth
century,  suggestive  of  lower  temperatures.  The  ring  widths  meanwhile  were  getting  wider,
suggesting higher temperatures. This would obviously have been a bit of a headache for Briffa and
his approach to dealing with the issue was remarkable. Noting that prior to the twentieth century
divergence the ring widths and ring densities had tracked each other fairly well, he simply asserted
that in the absence of any better explanation it was reasonable to conclude that the divergence was
not  climate-related.  Then,  and  without  providing  any  further  justification,  he  simply  adjusted  the
ring density figures to bring them into line with the ring widths. Essentially, he bent the diverging
line  back  up  to  where  it  ‘should’  have  been.  When  this  adjustment  was  carried  through  to  the
temperature  reconstructions,  it  had  the  effect  of  lowering  the  temperatures  in  all  periods  prior  to

1750, introducing an artificial warming trend into every one of the multiproxy studies in which it
was later used. Briffa’s only defence of his actions was to point out that it slightly improved his
verification statistics.c

Like  Polar  Urals,  the  story  of  Tornetrask  also  features  the  impact  of  an  update  to  the  original
study. While McIntyre had been aware of this update for several years, it wasn’t until the middle of
2007 that he was finally able to get hold of the data. Prior to that, his every attempt to see the figures
had been obstructed by Briffa. Eventually the details of the update emerged in the PhD thesis of
Håkan Grudd, a scientist working at the University of Stockholm.178

Grudd had updated the proxy record for Tornetrask up to the year 2004 (it previously stopped in
1980)  and  he  had  found  that  with  the  updated  data,  the  divergence  of  ring  widths  and  densities
disappeared.  That  was  the  good  news.  The  bad  news  was  that  the  chronology  now  showed,  in
Grudd’s words, ‘a long medieval warm period, centred on  AD 1000’ and also several other warm
periods  with  temperatures  in  excess  of  those  in  the  twentieth  century.  In  fact  Grudd’s  updated
version  looked  almost  identical  to  Briffa’s  version  before  the  adjustments  were  made.  So  it  was
pretty clear that Briffa’s original ad-hoc solution was not only unjustified, but unnecessary too.

So much for Tornetrask. So much for Jones et al 1998. We will now move on and look at another

important millennial temperature reconstruction in the shape of the work of Tom Crowley.
Crowley
Tom Crowley and his wife Gabriele Hegerl (who we met at the NAS panel hearings) were based at
Duke University in North Carolina. Crowley was one of the very senior figures in paleoclimatology,
with a list of publications that would pass muster among the most prolific scientists in the world. He
was also an important member of the Hockey Team, having published papers with Mann, Bradley,
Hughes, Ammann, Osborn and Briffa. His contribution to the spaghetti charts was encapsulated in
two  papers  published  in  2000.  The  first  was  Crowley  and  Lowery;179  the  second,  published  six
months later, was known simply as Crowley 2000.180

The reconstructions differed somewhat between the two papers. Crowley and Lowery showed a
long  decline  in  temperatures  from  1000  to  the  mid-nineteenth  century  followed  by  a  twentieth
century uptick, which had declined to sub-medieval levels by the end of the record. By the time of
the second paper, however, the reconstruction was rather different, showing a long gentle decline
from the year 1000 to the end of the nineteenth century and then a twentieth century uptick, which
was remarkably similar to MBH98, and which was still heading upwards at the end of the record in
the late 1980s.

As  ever,  McIntyre’s  first  step  was  to  try  to  get  the  data.  From  the  first  email,  right  back  in
December 2003, this quickly turned into a long and rather unpleasant saga: months of requests for
data stonewalled and evaded until everyone was roundly sick of it. The length of the correspondence
clearly annoyed everyone involved, with Crowley, as we saw in the last chapter, accusing McIntyre
of using ‘threatening language’ soon after the publication of MM05(GRL).d At the same time, Crowley
wrote an article in the journal Eos in which he described what he saw as McIntyre’s unacceptable
way of dealing with him:

I can attest that his initial message was of a somewhat peremptory character, requesting all my files, programs, and
documentation, and that a quick followup by him had a more threatening tone, implying that the director of the US
National Science Foundation (NSF) would be contacted if I did not comply.181

McIntyre responded to these accusations by posting up all of his correspondence with Crowley. The
initial message with the ‘somewhat peremptory character’ was as follows:

Dear Dr. Crowley,
I am interested in examining the actual proxy data used in Crowley–Lowery 2000, which was referenced by IPCC. I

have been unable to locate the data, as used, at the World Data Center for Paleoclimatology. Can you direct me to an
FTP location where you have archived this data or otherwise make the data available. Thank you for your attention.
Yours truly, Stephen McIntyre182

There was clearly no mention of code or documentation. There is also no mention of project funding
in  any  of  the  correspondence,  and  in  particular  no  mention  of  the  NSF.  In  fact,  it  was  Crowley
himself who first raised the subject of funding. In his first reply to McIntyre, some six months after
the initial data request and two months after McIntyre had made a formal complaint to Ambio, the
journal in which Crowley and Lowery 2000 was published, Crowley explained that the project was
not  federally  funded,  the  implication  presumably  being  that  he  did  not  have  to  release  his  data.
McIntyre responded to Crowley as follows:

Both  Lonnie  Thompson  and  Phil  Jones  –  the  stated  sources  for  your  [article’s]  data  –  have  been  supported  by  US
federal funding and obligations might well ensue from obtaining the data from these authors.182

It’s hard to see this as anything other than a reasonable response. It is also striking that this email,
which  Crowley  had  implied  followed  straight  on  behind  the  initial  request  for  data  –  ‘a  quick
followup’ in Crowley’s own words – was actually sent more than seven months after the first one.

Even with a formal complaint to the journal in place, there were continuing delays, with Crowley
first demanding McIntyre’s own data and code (it was already public), then saying that he was in
Europe and unable to post the data, then that he had been sick. It was October 2004 before some
data was finally dispatched to McIntyre. Even then it was not actually what was requested. Instead
of the original data series, Crowley sent a smoothed and transformed version. The original data, he
explained, had been mislaid when he moved his place of work from Texas A&M to Duke. It was, as
one  Climate  Audit  reader  memorably  put  it,  ‘the  scientific  equivalent  of  “the  dog  ate  my
homework”’.

Even without the original data sources, a certain amount of analysis of Crowley’s results could
still  be  performed.  The  proxies  –  just  15  of  them  –  included,  as  expected,  many  of  the  usual
suspects: bristlecones, Polar Urals and Tornetrask. There was also a Chinese ice core series called
Dunde, prepared by Thompson (of ‘Dr Thompson’s Thermometer’ fame – see page 259). A little
further digging quickly revealed that these four components accounted for all of the hockey stick
shape in the Crowley and Lowery 2000 reconstruction.

Apart from purporting to demonstrate that the modern warming was in excess of the Medieval
Warm  Period,  the  principal  finding  of  Crowley  and  Lowery  2000  was  that  the  timing  of  the
Medieval Warm Period was inconsistent, appearing in different places at different times. As we saw
in  Chapter  1,  this  observation  would  imply  that  the  different  warmings  were  likely  to  have  had
different  causes  and  that  the  Medieval  Warm  Period  was  therefore  insignificant  compared  to
twentieth century warming. As Crowley put it:

None of the records between Germany and western China about 100 degrees of longitude contribute significantly to
peak [Medieval Warm Period] warming from about 1070–1105.179

The evidence to support this claim was restricted to just four proxy series: Polar Urals, the Dunde
ice cores, a study of snowfall dates in China by Zhu, and finally QiLian Shan, a Chinese tree ring
series. We have already seen that Polar Urals was entirely unreliable, but there turned out to be huge
question marks over the other series too.
The snow in China
The Zhu snowfall study was published in the early 1970s. The authors had examined the dates of the
last  snows  each  winter  during  part  of  the  Song  dynasty  (960–1279),  and  had  concluded  that  the
temperatures around the year 1200 were rather low.183 However, more than twenty years later Zhang

De’er,  a  researcher  from  the  Chinese  Academy  of  Meteorological  Sciences  and  a  sometime  co-
author  of  Crowley,  attempted  to  replicate  Zhu’s  paper.  To  his  surprise,  Zhang  discovered  that  it
contained a major flaw: Zhu had made a mistake in converting the dates in the source records from
the  Song  dynasty  lunar  calendar  into  a  modern,  solar  calendar  format.  When  this  error  was
corrected, all of the dates of the final snowfalls each year shifted back towards the start of the year,
implying that the climate was relatively warmer. This had the effect of reintroducing the Medieval
Warm  Period  in  the  record.  Zhang  also  confirmed  this  finding  by  means  of  a  survey  of  taxation
records  from  the  same  era.  From  these  ancient  documents  he  found  that  he  could  determine  the
distribution of citrus trees in Song dynasty China and he concluded that these had been growing at
much higher latitudes than at present. The records suggested, Zhang wrote, that ‘the annual mean
temperature in the mid-thirteenth century was 0.9°C higher . . . than at present’.184

These findings raised some uncomfortable questions about Crowley’s own research. How was it
that  he  had  ended  up  using  the  incorrect  Zhu  series  from  1973  rather  than  Zhang’s  more  recent
corrected  version?  After  all,  he  knew  Zhang  well  –  they  had  written  a  paper  together.  McIntyre
decided  to  probe  the  issue  and  wrote  to  Crowley  once  again  to  ask  about  his  reasoning.
Unfortunately, in his reply Crowley failed to answer the question and no explanation has ever been
forthcoming.
QiLian Shan
The  QiLian  Shan  proxy  series  was  also  extremely  suspect.  As  we  saw  in  Chapter  2,  one  of  the
important criteria for using a tree in a temperature reconstruction is that its growth should be limited
by  temperature  rather  than  any  other  factor.  QiLian  Shan  is,  however,  located  in  semi-desert  in
Western China and a number of authors had reported that the growth of trees in the area was in fact
limited by rainfall, as would be expected from a tree in this kind of terrain.

When  McIntyre  posted  this  finding  on  his  website,  he  was  strongly  criticised  by  a  number  of
paleoclimatologists. Professional scientists were generally given a pretty hard time by Climate Audit
readers, so it was very difficult for them to interact meaningfully when anything they wrote would
be pounced on by a mass of more or less irate sceptics. McIntyre therefore set up a post especially
for these professionals, indicating that they could leave their comments there and that any responses
by the readership would be removed. The results were quite revealing. One scientist, who didn’t
want to be identified, wrote this of the QiLian Shan series:e

Those  in  the  know,  who  really  know  the  science,  know  not  to  use  that  chronology  and  know  who  still  use  that
chronology. The work that uses that chronology for a temperature reconstruction is less-respected than others. Please,
do not cast the whole field as deceitful or ignorant of this.185

This  was  a  remarkable  thing  to  say,  given  that  Crowley  and  Lowery  2000  was  widely  cited  in
paleoclimate studies and at that time was being given a prominent role in IPCC reports, apparently
without objections from the paleoclimate community. One can only wonder why this anonymous
scientist did not make his feelings known to the IPCC during the review process.
Ice cores
Lonnie Thompson’s work was different in many ways to the studies we have looked at so far. His
temperature reconstructions were based on ice core records rather than tree rings, an area of study
which put a whole new set of issues and uncertainties on the table. Recreating temperatures from ice
cores is almost as fraught with difficulties as the tree ring studies. The principle is to take air trapped
in the ice cores and to measure the amount of the 18O isotope in it. This isotope should be a proxy
for temperature. That, at least, was the theory. However, it was a theory whose basis in physics was
rather suspect. For example, the relationship between 18O and temperature from tropical ice cores
like Dunde, which was taken from Chinese mountain glaciers, was the reverse of the relationship

used in the polar ice core studies. In McIntyre’s words, the basis for the relationship was entirely
statistical  –  the  idea  that  18O  was  a  proxy  for  temperature  appeared  to  have  no  grounding  in
physics.186  This  is  the  kind  of  thing  that  sounds  alarm  bells  for  statisticians  on  the  lookout  for
spurious correlations.

Thompson was closely connected with the Hockey Team and he seemed to have taken a leaf out
of the books of his colleagues: his data was almost impossible to obtain. McIntyre’s long-running
correspondence with Thompson and Science, the journal that had published some of Thompson’s
most  important  findings,  had  drawn  as  near  to  a  blank  as  makes  no  difference.  This  was  not  an
insignificant issue as there were several different versions of Thompson’s data doing the rounds of
the paleoclimate community, making replication extremely hard, if not outright impossible.

One area that could be probed, however, was that of verification statistics – just how well did
Thompson’s processed data match up against actual temperatures? Thompson had this to say about
his reconstruction’s statistical performance:

For the period from 1895 to 1985, the correlation coefficient rf is 0.5 (significant at the 99.9% level). This correlation
suggests  that  [changes  in]  the  Dunde  Ice  Cap  18O  should  serve  as  a  good  proxy  for  larger-scale  temperature
variations.187

McIntyre, however, was not going to take his word for it and set about a replication, his calculations
coming up with a figure for r of 0.48, just a whisker away from Thompson’s 0.5. But, and there is
usually a but with Hockey Team studies, McIntyre didn’t stop there:

Since Thompson is on the Hockey Team, you have to ask yourself why he only did the correlation from 1895 on. Any
bets on what the correlation was for 1851–1895? Minus 0.36.186

It  is  therefore  safe  to  conclude  that  Dunde  is  not  a  reliable  proxy  for  temperature:  it  failed  its
verification statistics.
Methods
With all of the proxy series supporting Crowley’s claims about the Medieval Warm Period at best
highly dubious and at worse completely refuted, it is clear that Crowley and Lowery 2000 cannot be
much of a confirmation of the Hockey Stick. But data issues aside, there were further surprises from
Crowley when it came to his methodological decisions. When regressing the proxy records against
the temperature records to establish the mathematical relationship between the two, Crowley had
only performed the calculation for 1861–80 and 1920–65; there was a 40-year gap in the middle.
This, said Crowley, was because there was a breakdown in the relationship between the two in this
period, caused perhaps by carbon dioxide fertilisation. In order to deal with the problem, Crowley
had simply spliced in the instrumental temperature record. What is remarkable is that even with the
splice  in  place,  the  reconstruction  still  failed  its  verification  statistics.  Crowley  hadn’t  actually
reported them, but in McIntyre’s emulation, which appeared to closely match Crowley’s results, the
R2 was 0.005 and the Durbin–Watson statisticg was 1.3, well short of the 1.5 required to give any
confidence in the reconstruction.

In  a  follow-up  paper,  Crowley  went  even  further  and  replaced  all  of  the  proxies  from  1870
onwards with instrumental data.180 The result was rather like Mann’s original hockey stick, with a
huge,  and  in  Crowley’s  case,  undifferentiated  uptick  in  temperatures  in  the  twentieth  century.
Crowley had his hockey stick, and from there on there was no doubt that it would be seen again and
again and again – as indeed it was.
Esper 2002

Jan  Esper,  whom  we  have  already  met  in  Chapter  9,  was  a  young  Swiss  paleoclimatologist
employed by the Swiss Federal Research Institute. His most important contribution to paleoclimate
was his 2002 paper in Science.188 Although not usually considered to be a part of the Hockey Team,
his co-authors, Cook and Schweingruber, were pretty much core members.

Although  Esper  and  his  colleagues  had  concluded  that  there  had  been  a  large-scale  Medieval
Warm Period, at least in the Northern Hemisphere, it is far from clear that their conclusions were
any more robust than those of Mann or Jones or any of the others who had reached the opposite
conclusion.  Replicating  Esper’s  paper  proved  to  be  fraught  with  difficulty  for  McIntyre.  While
Esper had given the names of the sites used, this was as far as he went in citing data. Esper, like so
many paleoclimatologists didn’t archive his data. Without either a copy of the data as used or an
exact citation of a dataset in one of the tree ring databases, it was nearly impossible for an outsider
to work out what had been done. McIntyre had therefore been trying to get hold of Esper’s original
data directly, but it had been a long hard struggle. His first email request was sent as far back as May
2004, and 18 months later he was still knocking at Esper’s door. In the face of this intransigence, in
September 2005, McIntyre felt he had reached the end of his tether and that he had no choice but to
take the matter up with Science.

The  response  from  Science  was  quick  in  coming  but  was  rather  odd,  in  that  the  editors  told
McIntyre  that  correspondence  between  them  should  be  treated  as  confidential  and  could  not  be
posted publicly. One wonders whether they felt a little embarrassment at their inability to enforce
their  own  data  archiving  policies.  However,  they  did  attach  13  of  the  14  chronologies  used  in
Esper’s paper, but for some reason the Mongolia series was not included. Esper had also omitted
some  methodological  details  which  McIntyre  had  requested.  Another  letter  went  out  to  Science,
itemising the missing information and at the same time, Benny Peiser, a British social scientist who
published  an  influential  email  newsletter  on  climate  change,  wrote  a  public  letter  to  Science  in
support of McIntyre’s request and calling on his readers to petition the journal in support.

Six months later, the correspondence was still dragging on; the methodological details were still
missing, the Mongolia chronology was still withheld, as was measurement data for four of the sites.
These four included Mongolia and Polar Urals, although the latter was delivered shortly afterwards.
The  other  two  were  a  couple  of  foxtail  sites,  and  McIntyre  had  heard  on  the  grapevine  that  the
original author, Lisa Graumlich, had lost the data in an office move – a situation that was eerily
reminiscent  of  Tom  Crowley’s  move  from  Texas  A&M  to  Duke,  and  which  only  reinforced  the
wisdom of McIntyre’s calls for the archiving of all data at the time of publication.

McIntyre  was  unfailingly  polite,  which  suggests  considerable  powers  of  self  control,  and  it  is

rather remarkable to observe how he worded his 39th email to Science.

Perhaps it’s simply that [Esper] hasn’t considered your requests important enough to respond to. If that is the case,
perhaps you could write to him in firmer tones than you have done so far.189

Cherrypicking again
One of the methodological problems which was intriguing McIntyre was the possibility that Esper
had cherrypicked his data. Esper referred in the paper to not using all of the data from certain series,
but without explaining how he had decided which data to retain, or why certain data was deemed
unsuitable. This was a crucial methodological decision and it was therefore necessary to understand
it in order to replicate Esper’s findings. In his response to McIntyre, Esper had referred to some
remarks he had made in a later paper, saying that these explained the data removals.190 However
when McIntyre examined this purported explanation, what Esper said seemed actually to confirm
his worst fears – that Esper was merely cherrypicking hockey stick shaped series:

Before venturing into the subject of sample depth and chronology quality, we state from the beginning, ‘more is always
better’. However . . . this does not mean that one could not improve a chronology by reducing the number of series

used if the purpose of removing samples is to enhance a desired signal. The ability to pick and choose which samples
to use is an advantage unique to dendroclimatology.190

. . . which is a statement to send a shudder down the back of any reputable scientist.

In the same paper, Esper had also shown that paleoclimatologists didn’t only cherrypick those
sites they felt best met their purposes, but also, when they collected the raw data in the field, they
were cherrypicking the ‘best’ trees too.

It  is  important  to  know  that  at  least  in  distinct  periods  subsets  of  trees  deviate  from  common  trends  recorded  in  a
particular  site.  Such  biased  series  represent  a  characteristic  feature  in  the  process  of  chronology  building.  Leaving
these trees in the pool of series to calculate a mean site curve would result in a biased chronology as well. However if
the variance between the majorities of trees in a site is common, the biased individual series can be excluded from the
further investigation steps. This is generally done even if the reasons for uncommon growth reactions are unknown.190

Esper argued that he had taken these steps to avoid getting a biased chronology. To some readers,
however,  they  might  sound  much  more  like  a  way  of  obtaining  one.  After  all,  the  object  of  the
exercise was to discover what signal was in the tree rings, not to choose a subsection of the rings
that gave a ‘desired signal’.

Shortly  after  this  correspondence,  Science  stopped  responding  to  McIntyre’s  emails,  but,
undeterred, he set about doing what analysis he could. He had 13 of the 14 chronologies, and he
could at least see what they looked like.

Of  the  13,  two  were  bristlecones,  which  obviously  over-represented  this  type  of  tree  in  the
reconstruction. Of course, they both had markedly hockey stick shaped curves. There were also all
sorts of oddities among the other series. For example, there were a couple that were extremely short,
only lasting a few hundred years. Even more surprising was the inclusion of the Polar Urals update,
the  first  sighting  of  this  record  in  a  major  climate  reconstruction.  How  Esper  had  arrived  at  his
chosen set of proxies is not clear, but the impact of using the Polar Urals, rather than substituting
Yamal as so many of his paleoclimate peers had done, would have been to leave the reconstruction
with a pronounced Medieval Warm Period. It is possible that he had then had to introduce some
bristlecone series into the datebase in order to drag medieval temperatures back down again.

Some months later, McIntyre was able to attempt a replication of Esper’s paper, when he got hold
of a version of the Mongolia data that had been published as part of an entirely different study. Esper
had still not yielded up any new information on his methodology, so McIntyre decided to take the
simple approach by averaging the data series and rescaling them to match Esper’s original result.h

The  results  were  intriguing.  For  most  of  the  series,  Esper’s  reconstruction  and  McIntyre’s
emulation of it matched fairly well, until it came to the twentieth century. At this point, McIntyre’s
reconstructed temperatures headed downwards, while Esper’s kept on rising. This was presumably
due  to  some  methodological  decision  Esper  had  made,  but  if  this  was  the  case,  then  it  strongly
suggested  that  his  results  were  not  robust.  The  conclusions  could  not  reasonably  be  based  on  an
undisclosed methodological decision that made the results so different to the series mean.
Rutherford et al
We have touched upon Scott Rutherford’s paper earlier in the book, insofar as it was used to try to
rebut McIntyre’s own papers.i93 We noted also that its authors were all core Hockey Team members
– apart from Rutherford, there was Mann, Bradley, Hughes, Briffa, Osborn and Jones, so this could
not be seen as an independent confirmation of the Hockey Stick. The arguments in Rutherford et al
were the basis of Mann’s ripostes to McIntyre’s Nature submission, and some of what follows will
therefore be familiar from earlier in the story.

Rutherford’s paper attempted to demonstrate the reliability of the Hockey Stick by means of two
new  reconstructions.  The  first  of  these  took  a  roster  of  proxies  and  used  a  completely  different

methodology  to  Mann,  called  RegEM.  As  we  have  seen,  Mann  had  argued  that  the  similarity  of
Rutherford’s results to his own demonstrated that the Hockey Stick was correct. The problem with
this  approach  was  that  McIntyre  was  able  to  show  that  the  proxy  roster  used  by  Rutherford  was
identical to the one used in MBH98, including Mann’s faulty PC1. Merely processing the same biased
dataset through a different algorithm hardly demonstrated Mann’s point. The second reconstruction
took a different approach, eliminating the PC analysis step from the reconstruction, with the proxy
series going straight into the RegEM calculation. You may remember that the rationale for using PC
analysis  was  to  stop  certain  types  of  proxies  being  overrepresented  in  the  reconstruction.
Eliminating  PC  analysis  therefore  left  the  proxy  database  with  a  huge  preponderance  of  North
American bristlecone pines, so again, Rutherford was not demonstrating anything significant, other
than that there were many different ways in which a biased reconstruction could be created.

A little later, a reply came through from Weaver, indicating that he had received an assurance to
this effect from Rutherford. However, Weaver had also said that the paper would go ahead in its
current  form,  without  discussion  of  the  McIntyre  papers.  Whether  failing  to  address  McIntyre’s
criticisms of the dataset constituted ‘full, true and plain’ disclosure is, of course, debatable.

There  was  another  disclosure  issue  with  Rutherford’s  paper  too.  One  of  the  problems  with
creating  a  reconstruction  that  closely  tracked  the  results  of  MBH98  was  that  Rutherford’s
reconstruction was necessarily going to fail its R2, just as Mann’s had done. Like Mann, Rutherford
had chosen to discuss only the RE statistic. However, Rutherford did make a better fist of trying to

Rutherford’s paper had originally been submitted to the Journal of Climate in 2003, but there was
an unusually long delay until it finally appeared in 2005. While there was no firm evidence, it is
likely  that  this  delay  was  caused  by  the  publication  of  McIntyre’s  2003  paper  in  Energy  and
Environment with its description of the litany of errors in Mann’s dataset. This impacted directly
upon Rutherford’s paper, which used the MBH98 PC1 unchanged – short centring and all. In fact, as
McIntyre looked into Rutherford’s work, he was able to explain a great deal of the mystery that had
surrounded the pcproxy.txt file of proxy data that he had originally received from Rutherford back in
2003  when  he  started  his  investigations  into  MBH98.j  It  looked  very  much  as  if  the  version  of
pcproxy.txt that Rutherford had sent him had been originally prepared for Rutherford’s own paper.
In preparing these figures, he seemed to have introduced errors into the database – the same errors
that had alerted McIntyre to the possibility that there were serious problems with MBH98. Amusingly,
McIntyre discovered similar mistakes in Rutherford’s collation of instrumental temperature records
for the same paper.

So,  when  McIntyre’s  Energy  and  Environment  paper  had  hit  the  presses,  it  looked  as  if
Rutherford been forced to clean up the database and rewrite his paper, hence the delays in getting it
published. McIntyre had since come to the conclusion that the version of the data he was originally
sent was actually not the one Mann used in MBH98, although it was hard to be certain because of the
number of different versions of the dataset that had been issued by the Hockey Team.

While  the  datasets  in  Rutherford  et  al  were  therefore  just  as  flawed  as  MBH98, Rutherford had
managed  to  avoid  any  discussion  of  the  criticisms  McIntyre  had  made  in  MM03.  In  normal
circumstances it would have been the job of the journal editor and the peer reviewers to require the
authors to address these issues, but unfortunately the editor of Journal of Climate, Andrew Weaver,
was  a  fierce  opponent  of  McIntyre  and  McKitrick.  He  had  gone  on  the  record  as  saying  that
McIntyre  and  McKitrick’s  first  paper  should  never  have  been  published  and  stating  that  it  was
‘dangerous’ to give equal space to both sides in a scientific dispute.191 It was therefore unlikely that
he would to be responsive. Nevertheless, the attempt had to be made, and McIntyre wrote an email
to the journal pointing out the failure to address the findings set out in MM03, and going on to suggest
to Weaver that he was probably going to have to ask Rutherford and his colleagues to certify that
they had made ‘full, true and plain disclosure’.

explain why, going to the slightly absurd lengths of presenting some theoretical cases where the R2
would have given the wrong answer. While it was true that in the scenarios Rutherford discussed R2
would not have given a reliable answer, these kinds of situation were well known, and statistical
measures  had  been  developed  to  deal  with  them.  That  is  why  the  statistical  authorities  all
recommend the use of a suite of verification statistics.

There were other issues too: splices of instrumental data, truncations, adhoc changes to standard
methodologies. When all of these issues are put together it becomes clear that Rutherford et al was
neither an independent confirmation of the Hockey Stick nor a study on which great reliance could
be placed.
Moberg
Another  paper  that  made  a  big  impact  on  publication  came  from  a  group  of  researchers  led  by
Anders  Moberg  of  the  University  of  Stockholm.  The  paper,  Moberg  et  al  2005,  was  an  exciting
development  for  McIntyre  because  Moberg’s  group  was  genuinely  independent  of  the  Hockey
Team192 – a first for multiproxy studies – although it should be said that Moberg had worked with
Hughes in the past. The paper was announced to the world by Nature, who, for their own reasons,
illustrated it with a picture of the Hockey Stick rather than Moberg’s own work. The reasons for this
error  became  clear  when  the  actual  paper  was  examined.  Moberg’s  reconstruction  found  a  clear
Medieval Warm Period and Little Ice Age. However the reconstruction still suggested that current
temperatures  were  unprecedented,  a  position  that  was  achieved  by  truncating  the  proxy  records,
which often showed declining temperatures, and, just as Mann had done with the original Hockey
Stick, by overlaying the end of the reconstruction with the upward-trending instrumental record.k
Data
Moberg’s  approach  to  temperature  reconstruction  was  rather  different  to  previous  researchers.
Noting one of the problems with tree ring reconstructions, namely that they were thought to miss
longer term trends in temperature, Moberg decided to use mainly non-tree ring records such as lake
sediments  and  speleothems  instead.  While  these  ‘low-resolution’  proxies  could  not  distinguish
between  the  temperatures  of  individual  years  in  the  way  that  tree  rings  could,  they  did  have  the
advantage of extending much further back into the past, and the Moberg reconstruction was to be a
2000-year history. It was hoped that ‘low resolution’ proxies would pick up longer-term climatic
changes that were being missed by the tree ring proxies. However, his reconstruction would also use
tree ring data in the modern period, combining the two datasets to give the best of both worlds.

The limited involvement of the Hockey Team was immediately apparent in the paper, in that there
were clear data citations given for most (but not all) of the proxy series. That is to say that Moberg
had provided hypertext links to the actual version of each series used. This was a huge step forward
for  temperature  reconstructions,  but  nevertheless  was  still  less  than  adequate  because  of  the
exceptions:  of  the  eleven  low  resolution  series,  two  were  not  archived,  and  in  fact  were  ‘grey’l
versions.

Because he was trying to create such a long reconstruction, the problem of a lack of suitable data
was just as pertinent for Moberg and his team as it had been for earlier researchers. There were a
few  surprises  though.  Firstly  Moberg  didn’t  directly  use  Thompson’s  ice  core  records,  whose
problems  we  have  already  discussed,  although  he  did  use  a  series  known  as  the  Yang  composite
which  included  some  Thompson  data.  The  other  low  frequency  proxies  included  series  based  on
speleothems, ice melt records and forminafera, which are fossilised shells of plankton. By now you
will probably be able to have a reasonable guess at the tree ring proxies used. There were no fewer
than three bristlecone series, including one which was not even a current version, and of the other
tree ring series, our old friends Yamal and Tornetrask were there as well as a couple of unfamiliar
Siberian series, Taimyr and Indigirka.

Series  8,  an  analysis  of  the  18O  isotope  of  oxygen  in  a  Norwegian  stalagmite  proved  to  be  a
bugbear  for  McIntyre.  When  he  wrote  to  Moberg  to  ask  about  the  data  series,  which  had  been
derived from grey data, he was told that it had been obtained from the original author, Lauritzen.
However,  Moberg’s  version  of  the  data  ended  in  1938,  whereas  Lauritzen’s  original  article  only
extended to the end of the nineteenth century. McIntyre’s analysis of the two articles suggested that
Moberg may have made an error somewhere in his data processing and applied the wrong values to
the wrong dates by some 80-odd years. In answer to his enquiry, Moberg suggested that McIntyre
approach  Lauritzen  for  the  original  data.  Lauritzen,  however,  refused  to  comply,  saying  that  the
figures were unpublished. Given that they appeared to have been used in Lauritzen’s paper as well
as Moberg’s this seemed implausible.

Indigirka soon turned into another paleoclimate farce too, with Moberg explaining that he’d got
the data from someone else, and they’d had it from someone else again. Perhaps, he suggested to
McIntyre, the original Russian authors might supply the figures. When the request was passed on to
the Russians, the reply soon came back that ‘the series developers do not want to disseminate it.
They say this series will be re-calculated soon to reject some errors in it’. Another dead-end. The
irritation started to show in McIntyre’s postings:

If the series developers did not want to disseminate it, you’d think that allowing it to be used in a multiproxy study in
Nature is pretty strange way of not disseminating it. Secondly, if the series developers now change the series, what
good is the new version in understanding Moberg et al 2005? So if I want to actually look at the data, I now need to get
into the same old war: write a materials complaint to Nature and fight with [them] for 12 months. And I’m sure that
this  stuff  is  ear-marked  for  [the  IPCC’s  Fourth  Assessment  Report].  What  a  goofy  way  of  running  a  scientific
community. Then people get mad at me for being hard to get along with.

There was nothing for it but to start the whole tedious process of a materials complaint. Unlike the
earlier complaint about Mann’s data and code, and contrary to McIntyre’s expectations, this time
there was actually a relatively speedy outcome. It turned out that Moberg had been using Lauritzen’s
data without permission, and Nature now required him to produce a corrigendum and provide the
data.
Glob. bulloides
As soon as Moberg et al was published, McIntyre had set to work, as he always did, to find which
series  were  driving  the  shape  of  the  reconstruction.  Experience  had  shown  him  that  in  most
temperature reconstructions, the ‘hockey stick-ness’ was driven by just a few of the proxies, with
the  others  all  representing  noise  and  cancelling  each  other  out  in  the  final  reckoning.  Moberg’s
reconstruction was slightly different in that most of the series were actually tending to produce a
pronounced Medieval Warm Period, with just a few others pulling its peak down to the level of the
modern warming.

The Moberg proxy series had been calibrated against local instrumental records, in order to check
that the proxy was in fact responding to temperature. . . except for two of them. One of these, the
Arabian Sea, Glob. bulloidesm series, turned out to be the only proxy that had modern values higher
than those of the medieval warming. In other words it was one of the ones that was suggesting that
modern  temperatures  were  unprecedented.  However,  when  McIntyre  started  to  examine  the
scientific literature on Glob. bulloides, its inclusion in the reconstruction started to look very strange
indeed. Glob. bulloides turned out to be a subpolar species of forminafera, which proliferated in the
Arabian Sea only when there was an upwelling of cold water from the bottom of the ocean. So at
first sight, proliferation of Glob. bulloides would suggest lower temperatures and not higher ones, as
was  implied  in  Moberg’s  paper.  Moberg  had  explained  that  he  had  included  the  Glob.  bulloides
series in order to give a better geographical spread to the proxy data, and that the series was only
indirectly related to temperature. What he seemed to be saying was that the cold water upwelling
was  evidence  of  warming  elsewhere.  But  for  increased  prevalence  of  a  subpolar  species  to  be

evidence  of  higher  temperatures  was  rather  bizarre  even  for  the  eccentric  world  of  paleoclimate.
And as Soon and Baliunas knew to their cost,n the paleoclimate community in general and Mann in
particular were highly critical of scientists who did not demonstrate a direct relationship between
each proxy and temperature. Fortunately for Moberg, there was no similar outcry over his use of
Glob. bulloides. Why should Mann complain so vociferously about Soon and Baliunas but remain
silent about Moberg?

Apart  from  Glob  bulloides,  the  rest  of  the  twentieth  century  warming  pattern  in  Moberg’s
reconstruction seemed to be driven by just two other series: Yang’s composite, which, as we have
seen, owed its shape to Thompson’s undisclosed ice core data, and the Agassiz ice melt record, this
latter being the other series which hadn’t been calibrated against temperature.

Although  McIntyre  was  able  to  understand  the  main  factors  behind  the  shape  of  the
reconstruction, without the correct data it was not going to be possible to emulate it properly and the
process was slowed up considerably because, as always, the source code was unavailable. However,
with the information he had, it appeared that Moberg’s statistical handling of the reconstruction had
also  been  inadequate,  failing  many  of  the  key  tests  and  lacking  convincing  confidence  interval
calculations.
Osborn and Briffa 2006
After the publication of McIntyre’s papers in Energy and Environment and Geophysical Research
Letters, the Hockey Team responded with a new series of papers to back their argument that even if
McIntyre was right about short-centred PCs (and of course they weren’t accepting that he was) the
independent confirmations still suggested that Mann’s conclusions were right. Osborn and Briffa’s
2006  paper  published  in  Science  was  one  of  the  most  prominent  of  these,  a  contribution  to  the
debate by the European wing of the Hockey Team.

When  the  paper  was  published  towards  the  start  of  2006  it  immediately  attracted  significant
media  attention,  with  a  BBC  article  entitled  ‘Climate  warmest  for  millennium’  trumpeting  its
findings.193 McIntyre, meanwhile, published his own review on Climate Audit. He was less than
impressed with what he found.
Proxies
Of the 14 proxies used in Osborn and Briffa, two were cores from bristlecones or foxtails. Among
the  other  members  of  the  roster  were  Yamal  and  Tornetrask,  which  we  have  just  seen  are  also
unreliable,  together  with  some  of  Thompson’s  unarchived  ice  core  studies.  But  even  the
inadequacies  of  Osborn  and  Briffa’s  proxy  roster  was  as  nothing  compared  to  the  next  surprise.
McIntyre discovered that one of their proxies was a PC series, and it used Mann’s discredited short
centring algorithm.
Still picking cherries
So,  apart  from  poor  quality  proxies  and  biased  PC  analysis,  what  were  the  main  problems  with
Osborn  and  Briffa?  The  question  that  hangs  over  the  paper,  like  so  much  of  the  rest  of  the
paleoclimate field is that of cherrypicking. While they had eventually used only 14 proxies, they had
actually started out with a much larger database, whittling this down to the final 14 by eliminating
any that were not genuinely responding to temperature. To do this, they had matched up the tree ring
widths to the instrumental temperatures in the local area (the gridcell), and had then withheld from
the calibration all those series where there was not a reasonable correlation between the two. The
problem was that while they said that they had applied this test to some of the proxies, there was no
mention  of  whether  they  had  applied  it  to  the  PC  series.  They  merely  asserted  that  there  was  a
correlation. McIntyre ran a check on the data to make sure, and found that of the six series in the PC
calculation only one had a positive correlation with temperature.

McIntyre  wasn’t  the  only  one  who  noticed  problems  with  the  Osborn  and  Briffa  paper.  Gerd
Bürger (see below), of the Free University of Berlin, submitted a comment to Science, picking up on
the cherrypicking issue and expanding on it.194 Bürger explained that since Osborn and Briffa had
screened a large number of proxies for those with a positive correlation to temperature, they risked
ending up with some whose correlation was only a matter of chance – in other words, just because
the ring widths went up in line with twentieth century temperatures, it didn’t mean that they always
tracked each other. It was quite possible that the hockey stick shape Osborn and Briffa achieved had
only come about because they had effectively selected only series with a twentieth century uptick.
To explain this a little, if you have several series of random data and you take an average, they will
all cancel out to nothing – the peaks in one series will tend to offset the troughs in another. If you
take enough random series the average will be a straight line centred on zero. On the other hand, if
you take a set of random series and select only those with twentieth century upticks, you will get a
hockey stick. This is because in the twentieth century, the upticks are all in synch and add together,
giving  a  strong  uptick  in  the  average.  The  earlier  periods  are  all  still  random  numbers,  so  they
continue to cancel out, giving the long flat shaft. This effect was just as pertinent to Mann’s papers
as to Osborn and Briffa.

One  of  McIntyre’s  readers,  statistician  David  Stockwell,  had  published  a  short  article
demonstrating the effect.195 Stockwell simply took several red noise series, calibrated them against
the  actual  temperature  records,  and  then  averaged  the  result.  The  answer:  a  hockey  stick.  As
Stockwell concluded:

All  the  salient  aspects  of  past  climate  usually  associated  with  millennial  reconstructions  are  essentially  already
encoded into the methodology, so that a ‘hockey-stick’ shape is inevitable on any data resembling natural . . . series.195

Stockwell’s observations were quite distinct from McIntyre’s earlier experiments on getting hockey
sticks from red noise.o McIntyre had demonstrated that the PC algorithm had a tendency to produce
hockey sticks from red noise. What Stockwell was demonstrating on the other hand was that the
calibration process could produce hockey sticks from red noise in its own right. So now it was clear
that  not  one  but  two  separate  steps  of  Mann’s  paper  had  an  inbuilt  tendency  to  produce  hockey
sticks from nothing.

Bürger was quite explicit about what this meant for Osborn and Briffa’s results:

[T]he reported anomalous warmth of the 20th century is at least partly based on a circularity of the method, and similar
results could be obtained for any proxies, even random-based proxies.194

Bürger emphasised that it was necessary to adopt the most stringent statistical testing to deal with
this issue. He was able to show that when the effect of the proxy selection procedure was factored in
to the significance levels for the reconstruction, Osborn and Briffa’s hockey stick suddenly lost its
statistical significance. Osborn and Briffa seemed to recognise the validity of this argument, saying
‘we  agree  with  Bürger  that  the  selection  process  should  be  simulated  as  part  of  the  significance
testing process in this and related work’.196 However, they tried to minimise the impact of Bürger’s
findings  by  claiming  that  this  screening  had  only  a  small  effect  on  their  results  since  they  had
eventually used ‘almost all’ of a small number of proxy series available. There were, they claimed,
only 16 suitable ones available and they claimed that this meant that the significance levels they had
demonstrated were highly unlikely to have been achieved by chance. McIntyre found this statement
extremely surprising, since he was personally aware of a multitude of other suitable series that they
could have used, but hadn’t. Osborn and Briffa had even listed Rutherford et al 2005 as one of the
papers whose proxies were covered by the statement of ‘almost all’, but this latter paper included all
of Briffa’s network of 300 or more tree ring series. Why didn’t they use all those?

McIntyre  charitably  wondered  if  perhaps  Osborn  and  Briffa  actually  meant  ‘proxy  series  that
went back to the year 1000’? But even then, there were still far more than 14 or 16 series – at least
double, he thought. And it wasn’t just the tree ring records. What about the ice cores? Why didn’t
they use Mount Logan? Perhaps because it had a divergence problem – twentieth century values
were  falling  rather  than  rising.  Or  what  about  Rein’s  offshore  Peru  (strong  medieval  warmth)  or
Mangini’s speleothems (medieval warmth again)?
McIntyre summarised his thought to his readers:

[W]hen Osborn and Briffa say that the universe from which they’ve selected can be represented by selecting 14 of 16
[proxy series], this is completely absurd. There has probably been cherrypicking from at least 3 times that population.
But aside from all that, the active ingredients in the 20th century anomaly remain the same old whores: bristlecones,
foxtails, Yamal. They keep trotting them out in new costumes, but really it’s time to get them off the street.197

. . . and all the others
We have looked at several of the independent confirmations of the Hockey Stick in detail and have
seen that they leave much to be desired. Little is added by making the same criticisms against the
others.  However,  for  the  record,  these  studies  (and  the  issues  affecting  them)  are  Hegerl  et  al
(cherrypicking,  data  not  archived),  D’Arrigo  et  al  (divergence  problems,  data  not  archived)  and
Mann and Jones 2003 (bristlecones, short-centred PCs).

It’s  also  worth  summarising  what  we  have  learned  about  the  commonality  of  proxies  in  the
various studies used by the IPCC (see Table 10.1). Is it possible to maintain that these studies are in
any way independent of the Hockey Stick when the same proxies, most of which are known to be
flawed, turn up again and again and again?

Before we move on to the next chapter, there are a couple of other studies that are worth looking
at  briefly  because  they  come  from  researchers  who  were  genuinely  independent  of  the  Hockey
Team. Their findings are important because they throw some more light on the lack of robustness of
the IPCC’s temperature reconstructions.
TABLE 10.1: Commonality of proxies in temperature reconstructions

 

Proxy series
Polar Urals
Tornetrask
Jacoby
Mongolia
Jacoby
treeline
Bristlecones
Dunde/Yang
Greenland
δ18O
Jasper
Taimyr

MBH98

&

MBH99

Rutherford Jones
98

Crowley

00

Briffa

00

Esper
02

Moberg Osborn,
Briffa

D’Arrigo

Mann,
Jones
03

x
x
x

x

x
x
x

x
x
x

x

x
x
x

x
x

x

x

x

 
 

x
x

x

x
x
x

x

x
x
x

x

x
x

 
 

 

x
x
x

x
x

x
x

 

x
x
x

x
x
x

x

 

x
x
x

x
x

x

 

 

x
x
x

x
x
x

x
x

x
x
x

x

x

x
x

 
 

European
docs
CETR

x

x

x

x

x

x

x

x

 

Adapted from the Wegman report.15

Loehle
Just because paleoclimate researchers select proxies from a short menu of hockey stick shaped tree
ring series, doesn’t mean that this is the only way to do a temperature reconstruction. We have seen
how  tree  ring  studies  are  fraught  with  problems  –  the  apparent  inability  to  pick  up  longer  term
trends, the apparently inverted U-shape response to temperature, the problems with calibration and
carbon dioxide fertilisation, and so on. In the light of all these issues, one of McIntyre’s readers, an
ecologist called Craig Loehle, decided to see what would happen if you didn’t use tree rings at all.
His study was not so much an attempt to create a new temperature reconstruction as a somewhat
simplistic attempt to see what the patterns in non-tree ring data looked like. His idea was that by
keeping everything very simple, much rancorous debate would be avoided. On this score at least, he
was to be sorely disappointed.

We have seen how paleoclimate studies were criticised, by sceptics at least, for failing to describe
their selection criteria for proxies, the suspicion being that this permitted cherrypicking of hockey
stick  shaped  series.  Loehle  was  keen  to  avoid  criticism  on  this  score  and  therefore  adopted  the
innovative,  for  paleoclimate  research,  approach  of  laying  out  his  selection  criteria  in  black  and
white:  he  chose  all  available  non-tree  ring  series  that  were  at  least  2000  years  long,  had  been
calibrated  to  temperature  and  had  at  least  one  measurement  per  century  over  the  length  of  the
record. This gave him only 18 series, although as we have seen this is still a larger number than in
some paleoclimate reconstructions. Demanding that the series be calibrated to temperature would
deal, he hoped, with the criticisms that had been directed at Soon and Baliunas, and which should
also have been directed at Mann.

Loehle’s  methods  were  rather  simple  when  compared  to  the  intricacies  of  Mann’s  papers  –
smooth the series to remove noise, subtract the mean to put them all on the same baseline and take
the average.

Although his index ended in 1980, missing much of the modern warming, his results showed that
non-tree ring proxies suggested a Medieval Warm Period that had been a real, global phenomenon,
with temperatures as much as 0.3°C above those prevalent in the modern period. He was also able to
demonstrate a certain amount of rigour in the results he had calculated by showing that the story was
largely the same if you removed a series from the database or if you took a random subsample of the
proxy database. While Loehle was careful not to read too much into his results, his study did seem
to suggest that the hockey stick shape of the other paleoclimate studies might be, in part, an artefact
of the use of tree rings rather than a genuine reflection of climatic history.

The paper was initially submitted to Geophysical Research Letters, where it was rejected out of
hand  on  the  grounds  that  the  journal  was  no  longer  interested  in  publishing  temperature
reconstructions.  This  was  slightly  surprising  given  the  political  and  scientific  importance  of  the
subject.  However,  taking  the  hint,  Loehle  next  sent  the  manuscript  to  Energy  and  Environment,
where the reception was much more favourable.

The publication of Loehle’s paper was met with a storm of criticism and not only from scientists
working  in  the  paleoclimate  mainstream.198  Despite  the  fact  that  Loehle’s  conclusions  reinforced
their preconceptions, many Climate Audit readers also joined in with the pulling apart of the paper,
happy  to  point  out  any  flaws  they  could  find.  Both  sides  pointed  out  that  Loehle’s  data  was  not
available at the point of publication, an oversight which was careless, given the criticisms that had

been regularly launched at the paleoclimate mainstream on this score. While this was remedied in
fairly short order, a moral victory was probably given away unnecessarily on this point.

The more substantive criticisms of the paper related to the absence of any estimates of errors.
Loehle said that he couldn’t see how these could be calculated, but the Climate Audit readership,
which  included  some  fairly  high-powered  academics,  was  able  to  demonstrate  that  it  was  in  fact
possible, if less than obvious.

There  were  other  errors  too.  For  example,  it  was  pointed  out  that  some  of  the  data  series
presented figures on a regular basis while others had irregular gaps, and it was suggested that this
could lead to a distortion of the results. A dating error was also revealed.

In the face of all of these issues, Loehle issued a correction.199 His main findings still stood, but
he now had a much firmer basis on which to stand his conclusions. While some commenters had
been quite dismissive of his work, in many ways, the story of his paper is a model of how science
should work in the twenty-first century. The review was open and vigorous; even exciting. The data
was  available  for  anyone  interested  to  look  at  (shortly  after  publication  at  least).  The  author
responded  publicly  to  questions  and  criticisms.  The  paper  was  not  perfect,  but  what  was
extraordinary about the affair was the speed with which the community, both professional scientists
and  interested  amateurs,  were  able  to  identify  the  errors,  determine  how  to  resolve  the  difficult
issues, and force a correction. The original paper was published in November 2007, the corrigendum
in  January  2008.p  It  is  remarkable  to  compare  this  rapid  turnaround  to  the  other  paleoclimate
studies, where on occasion there would be a delay of years even before the study data was made
available.
Bürger and Cubasch
As Hans von Storch once pointed out, McIntyre and McKitrick’s papers on the Hockey Stick made
one very important change to the culture of climatology. Before MM03, criticism of Mann’s work was
not  possible.  Anyone  who  stuck  their  heads  above  the  parapet  would  have  been  shot  down  by  a
barrage  of  criticism,  as  Soon  and  Baliunas  had  discovered  to  their  cost.  In  the  years  after  the
appearance  of  MM03  and  particularly  MM05(GRL),  it  became  considerably  easier  to  publish  studies
critical of MBH98, although it is fair to say that it was sometimes still a struggle.

In Chapter 8,  we  looked  at  von  Storch  and  Zorita,  one  of  the  papers  critical  of  Mann’s  work.
Another  set  of  authors  adopted  a  rather  different  approach  in  a  series  of  papers  published  in  the
years following MM05(GRL). Gerd Bürger, whom we have just met as the author of a comment on
Osborn and Briffa, together with a colleague called Ulrich Cubasch, fired their first shots at MBH98 in
a  2005  paper  called  ‘Are  multiproxy  climate  reconstructions  robust?’200  The  title  was  somewhat
misleading,  because  it  focused  very  much  on  MBH98,  and  in  particular  the  effect  of  key
methodological  decisions  on  the  shape  of  the  reconstruction.  Bürger  and  Cubasch  identified  six
decisions  that  they  felt  were  key  to  Mann’s  methodology.  The  different  combinations  of  these
decisions meant that in the final reckoning there were 64 different ways in which the reconstruction
could have been put together, none of which could be intrinsically preferred over the others. In fact
McIntyre  was  able  to  point  out  several  other  important  choices  that  Bürger  and  Cubasch  had
overlooked, so there were actually more than 64 possible combinations, but the point remained the
same: if you couldn’t choose one combination over the others, how could you possibly decide which
of the mish-mash of different reconstructions was the correct one (see Figure 10.4).q

FIGURE 10.4: Some of the 64 flavours of temperature reconstruction
The  figure  shows  just  32  of  the  different  possible  temperature  reconstructions.  Reproduced  from
Bürger and Cubasch.200

Bürger and Cubasch made other criticisms as well. For example, they pointed out that it was not
legitimate to extrapolate temperatures in the reconstruction beyond the range of the calibration. In
other words, you couldn’t reconstruct temperatures that were outside the range of the temperatures
during calibration. You just didn’t know if your model still worked outside that range. This proviso
was, as Bürger and Cubasch put it, the basic condition of statistical regression. Unfortunately it was
exactly what Mann had done, with some proxies being used far outside their calibration range.

There were by now a lot of nails in the coffin of Mann’s reconstruction. Would they be enough to
bury it once and for all? The IPCC was going to be the final arbiter, and the answer would not be long
in coming.

a  See page 43.
b  For example, Bradley and Jones 1993, Hughes and Diaz 1994, Jones et al 1998, MBH98, MBH99, Crowley and Lowery 2000, Briffa

et al 2000, Bradley, Hughes and Diaz 2003, Mann and Jones 2003, Jones and Mann 2004.

c    Justifying  adjustments  by  reference  to  verification  statistics  is  considered  incorrect  methodology  by  statisticians  because  it
essentially makes the verification period an extension of the calibration period. This, though, is a story that is beyond the scope of
this book.

d  See page 222.
e  The comment was posted to the Climate Audit comment thread by Rob Wilson, a paleoclimatologist now working at St Andrews

University. He described the author as being ‘a friend and colleague’.

f  See note on verification statistics on page 16.
g  See page 244.
h  Rescaling means adjusting the mean and standard deviation of one series to make them the same as another.
i  See page 179.
j  See page 74.
k  See page 33
l  See page 269.
m  Globivalvulina bulloides.
n  See page 56.
o  See page 114.
p  Readers should note that Loehle’s paper was published too late to be considered in the IPCC report, the story of which is told in the

next chapter. I have included it here for the sake of tidiness of the narrative.

q    Remarkably,  during  the  Barton  hearings,  Gerald  North  said  that  he  thought  all  of  these  64  reconstructions  looked  like  hockey

sticks, although he added that they were ‘a bit curved’.163

11     The Hockey Stick and the IPCC

IPCC  reports  are  being  produced  in  a  very  open  process  under  the  discipline  of
science, where honesty and balance are hallmarks of that discipline.

Sir John Houghton

The point is that every single man who was there knows that the story is nonsense, and
yet it has never been contradicted. It will never be overtaken now. It is a completely
untrue story grown to legend while the men who knew it to be untrue looked on and
said nothing.

Josephine Tey
The Daughter of Time

Introduction
In August 2005, McIntyre received an email from the IPCC informing him
that  he  had  been  nominated  to  be  an  expert  reviewer  of  its  Fourth
Assessment  Report  (4AR),  which  was  due  to  be  published  at  the  start  of
2007.  While  in  theory  this  was  a  valuable  opportunity  to  influence  the
content of the report, McIntyre had no illusions that the IPCC was going to
take his criticisms on board wholesale and report that Mann’s Hockey Stick
was indeed flawed. However, much would be revealed about the attitudes of
the IPCC by the way that they reacted to dissenting views, and when the truth
finally came out, politicians and IPCC bureaucrats would be hard pushed to
carry off a claim that nobody knew about the flaws in Mann’s work. Their
own  policies  required  IPCC  lead  authors  to  collate  all  of  the  different
opinions on any particular issue and to report any which were technically
valid, even where these opinions conflicted with or could not be reconciled
to the positions held by the majority. In theory, at least, they couldn’t avoid
reporting on McIntyre’s findings on the Hockey Stick.
IPCC protocols
The IPCC review process was promoted as a model of its kind, but in reality
it had serious flaws. In fact, so serious were these shortcomings that they
would not have passed muster in any other professional setting. Not least of
these was the issue of conflict of interest. We saw in Chapter 1 how, for the
IPCC’s Third Assessment Report, Mann had been appointed the lead author
on  the  paleoclimate  chapter.a  From  this  powerful  position,  he  had  been

responsible for reviewing his own work, forming an opinion on the rest of
the scientific corpus and writing the final text. What is more, as a result of
his dominant position within the IPCC report-writing process, he reached a
position of leadership in the paleoclimate world, a position that was only
enhanced by his status as the author of the Hockey Stick studies, which he
had himself highlighted in the IPCC Third Assessment Report. As McIntyre
explained to the Climate Audit readership, this situation would have been
entirely  unacceptable  in  a  commercial  situation,  and  in  fact  would  have
been entirely illegal outside of a banana republic.

For  someone  used  to  processes  where  prospectuses  require  qualifying  reports  from
independent geologists, the lack of independence [in the IPCC report-writing process]
is simply breathtaking and a recipe for problems, regardless of the reasons initially
prompting this strange arrangement.

Businesses  developed  checks  and  balances  because  other  peoples’  money  was
involved,  not  because  businessmen  are  more  virtuous  than  academics.  Back  when
paleoclimate research had little implication outside academic seminar rooms, the lack
of any adequate control procedures probably didn’t matter much. However, now that
huge public policy decisions are based, at least in part, on such studies, sophisticated
procedural controls need to be developed and imposed.201

This conflict of interest had not gone unnoticed, and several commentators
in the science policy community had picked up on the subject after Barton
had raised it in his letters to the Hockey Team (see Chapter 9).202,203 One
comment  had  come  from  von  Storch,  who  sagely  observed  that  to  have
scientists who already dominated a debate also authoring the key review of
that  debate  was  a  sure  road  to  trouble;  the  situation  demanded  the
involvement of scientists who really were independent.204 However, when
the authors for 4AR were announced, it soon became clear that none of these
objections had been taken on board: Mann had been replaced as lead author
by none other than his British teammate, Briffa. The IPCC had no intention
of allowing an independent review of the field.
Nature of the review
To some extent, IPCC reports are much like a listing prospectus for a large
company.  Appearing  every  five  years  or  so,  they  set  out  our  complete
understanding of climatology: what is known, what is still not understood
and  what  uncertainties  there  are.  The  reports  cover  paleoclimate  studies
which  look  back  in  time,  instrumental  records  describing  the  present  and

climate models which peer into the future. They should be a warts-and-all
summary of the state of the science, enabling politicians and the public to
reach policy decisions on a rational basis.

With  the  importance  of  the  subject  in  mind,  readers  might  therefore
expect the IPCC review process to involve a very detailed examination of the
scientific  papers  that  inform  our  understanding  of  the  climate.  However,
IPCC due diligence turns out to be entirely different to the kind of work that
an auditor working on a listing prospectus would peform. Business auditors
examine  all  of  the  major  assumptions  made  in  preparing  accounts  and
prospectuses,  often  tracing  key  values  back  to  source  documents  and
reperforming  key  calculations  and  estimates.  All  information  is  available,
everything  is  open  to  question.  IPCC  reviews,  on  the  other  hand,  consist
merely  of  the  assembling  of  a  panel  of  experts  who  survey  the
climatological literature and form an opinion of where the truth lies. Like
the NAS panel, they are content to ‘wing it’. And, as we will see, the  IPCC
also takes a different approach to data availability.

In its defence, the IPCC review process is at least rather more open than
that of the NAS, in that anyone can contribute comments on the draft reports,
and the protocols in place require the author teams to respond to every one.
McIntyre may therefore have felt that he could at least try to bring a little
more scepticism and rigour to the process, but it soon became apparent that
IPCC bureaucrats had other ideas.

Soon after starting work on his review, McIntyre noticed that the draft
paleoclimate  chapter  referred  to  two  unpublished  manuscripts  –  one  by
Rosanne D’Arrigo, the other by Gabriele Hegerl. The  IPCC’s own policies
referred  to  reviewers  being  supplied  with  ‘specific  material  referenced  in
the  document  being  reviewed,  which  is  not  available  in  the  international
published literature’ and therefore, almost as a matter of course, McIntyre
decided to get hold of the authors’ data, assuming, not unreasonably, that
this  was  an  entirely  appropriate  step  for  an  IPCC  reviewer  to  take.  In
September, therefore, McIntyre wrote to the  IPCC Technical Services Unit
(TSU) based at UCAR in Boulder, Colorado, to ask them to obtain for him the
download locations for the datasets used by Hegerl and D’Arrigo.

By now of course, you will know that data is rarely made available in
paleoclimate  circles  and  that  official  bodies  tend  to  be  quite  happy  to
connive in this secrecy. The IPCC turned out to be no exception and a few
days after his email was sent McIntyre received a reply from TSU explaining

that  they  would  not  be  able  to  help  him.  According  to  TSU’s  Martin
Manning,  the  IPCC  only  assessed  published  literature  and  did  not  want  to
‘act  as  a  global  clearing  house’  for  scientific  data.  Undeterred,  McIntyre
wrote back explaining that his request merely required a short email to the
authors  from  TSU  staff,  and  pointed  out  that  if  D’Arrigo  and  Hegerl
subsequently refused to release the data then this would (and should) colour
the IPCC’s opinions on whether the papers should be cited in the final drafts.
Unfortunately, Manning rebuffed this idea in an ill-tempered email, which
he presumably thought would end the matter.

McIntyre,  however,  was  not  yet  willing  to  play  along  and  decided  to
appeal  Manning’s  decision  to  Susan  Solomon,  the  chairman  of  IPCC
Working  Group  I,  the  subcommittee  responsible  for  the  scientific  report.
His letter reiterated the request and pointed out that it appeared to be well
within the stated terms of reference for reviewers. Meanwhile, he also sent
a direct request for the data to D’Arrigo and Hegerl.

The almost inevitable refusals were received shortly afterwards. For her
part, Hegerl refused to release any data until after her paper was published
and, while D’Arrigo declined to correspond with McIntyre directly, one of
her  co-authors  referred  him  to  the  editors  of  the  Journal  of  Geophysical
Research (JGR), the journal which was to publish her paper.

Being referred to the journal should have been the end of the matter: JGR
was a publication of the American Geophysical Union (AGU) and therefore
had  clear  data  policies,  which  stated  unequivocally  that  all  datasets  used
must  be  archived  in  a  public  database.  On  the  face  of  it  then,  D’Arrigo
should not be able to resist a data request for very long. McIntyre therefore
penned a long letter to the journal’s editors, explaining the inadequacies of
the data citations in D’Arrigo’s paper and noting the apparent conflict with
the  journal’s  policies.  He  asked  politely  that  the  information  be  made
available  in  a  format  that  would  allow  him  to  reproduce  the  results.
Needless to say, the data was never sent.

While this correspondence was ongoing, McIntyre also reverted to Susan
Solomon, explaining the refusal from Hegerl and D’Arrigo and asking once
more  for  the  IPCC  to  intervene.  Again,  this  was  to  no  avail,  and  it  now
looked as if Solomon was running short of excuses. Her reply was startling:
she  first  declared  that  the  attempt  to  obtain  the  data  from  the  AGU  was  a
breach  of  McIntyre’s  terms  of  reference  as  a  reviewer,  this  document
apparently requiring that any material provided as part of the review should

not be ‘distributed, quoted or cited’ without permission. Readers can judge
for themselves whether McIntyre had actually done any of these things in
asking for the data.

A surprising number of the comments were trivial – votes of thanks and
congratulations  and  so  on,  but  there  were  plenty  of  more  substantive

More  bizarrely  still,  Solomon  then  accused  McIntyre  of  trying  to
influence editorial decisions at the journal, saying that it was ‘inappropriate’
for  him  to  use  his  IPCC  status  to  obtain  information  from  the  authors.  It
seems  fairly  unlikely  that  data  requests  really  were  ‘inappropriate’  under
IPCC rules, since Manning had told McIntyre to make such a request just a
few days earlier. However, with Solomon closing her letter by threatening
to remove McIntyre’s reviewer status if the line was not toed, it was pretty
clear  that  this  enquiry  was  unlikely  to  bear  any  fruit,  and  McIntyre
reluctantly  decided  to  call  a  halt.  Nobody,  it  seemed,  should  look  too
closely at any paleoclimate studies under the IPCC’s auspices – like the NAS,
they expected reviewers just to ‘wing it’.
First Order Draft
And  so  to  the  First  Order  Draft.205,206  When  McIntyre  saw  the  proposed
text, he realised that the tone of the paleoclimate chapter was going to be
less  promotional  than  it  had  been  when  Mann  had  been  lead  author  –  at
least as far as the Hockey Stick was concerned. However, as then drafted it
was  not  going  to  be  any  kind  of  a  triumph  for  him  and  McKitrick.  The
Hockey Stick was still there and the spaghetti graph was still there, with the
Briffa’s temperature reconstruction still truncated. On the other hand there
was  not  a  mention  of  the  divergence  problem  and  as  we  will  see,  the
coverage  of  the  Hockey  Stick  affair  was  controversial,  if  not  downright
scandalous.

However, there was no alternative but to enter a response, and McIntyre
and McKitrick started to collect their thoughts. Reviewers’ comments had
to be entered into a spreadsheet, which would be emailed to TSU in Boulder
once it was completed. Manning and his team would then compile all of the
contributions from the different reviewers into a consolidated pack, which
they would issue to the author team, who would in turn compose a response
to  each  comment.  In  this  way,  each  reviewer  would  be  unaware  of  the
contributions of the others until after the event.

contributions.  McKitrick  had  begun  his  comments  by  firing  a  warning
across the bows of the chapter authors:

Bear in mind that a great many observers, especially the most motivated critics of the
AR4, will start their reading by turning to the paleoclimate chapter and seeing how the
IPCC  deals  with  the  hockey  stick.  I  will  present  my  comments  on  this  chapter  as
helpfully  and  objectively  as  I  can.  But  I  begin  with  some  exasperation  at  this  first
draft. You may not want any advice from me, but for what it’s worth, do consider.
Chapter  6  is  obstinate  in  its  rejection  of  criticisms  of  the  hockey  stick,  yet  is
surprisingly weak on the technical issues at stake. If you truly want to proceed with
the chapter in its current form then you will not only be handing the IPCC’s traditional
critics a large club to beat the  AR4 with, but you will alienate those many scientists
who  have  hitherto  given  the  IPCC  the  benefit  of  the  doubt,  but  who  have  followed
these issues and are looking for a serious treatment of them, not a brittle, dogmatic
dismissal.205

There was plenty for McIntyre and McKitrick to do just to try to tone
down the promotional tone of the first draft. The authors seemed very keen
to  sell  the  paleoclimate  chapter  to  their  readers:  they  had  expounded  at
some length on the allegedly high quality of the proxy records, which were
said  to  reflect  environmental  change  ‘in  a  highly  quantitative  and  well-
understood  manner’  and  to  have  undergone  ‘comprehensive  calibration
with . . . instrumental data’. As readers will be aware, these two statements
are  wrong  and  debatable  respectively:  as  we  have  seen,  proxy-based
temperature reconstructions are fraught with difficulty and there was severe
doubt  that  many,  or  even  any,  of  the  proxies  were  capturing  temperature
information  at  all.  The  authors  had  gone  on  to  claim  that  multiproxy
reconstructions provided ‘more rigorous estimates than single proxy’ ones,
a position that was again debatable.

Fortunately, McIntyre’s comments on this issue were all acknowledged
by  the  chapter  authors  and  were  earmarked  for  redrafting.  On  the  other
hand, McIntyre’s attempts to get a fuller explanation of the uncertainties in
the proxy studies were rejected. While the uncertainties were mentioned in
the summary, it appeared that it was considered inappropriate to spell them
out in too much detail.
Briffa on the Hockey Stick
When it came to the meat of the chapter, the authors first reviewed the main
studies  from  the  Third  Assessment  Report:  the  Hockey  Stick,  Jones  et  al
1998 and Briffa’s truncated tree ring study from 2001, before going on to
look at developments since that time. McIntyre’s work on the Hockey Stick

papers was considered worthy of a paragraph of its own, but as a summary
of the state of the debate, it was shocking:

McIntyre and McKitrick (2003) produced a Northern Hemisphere reconstruction that
differs radically from that of Mann et al (1999), in indicating a period of significant
warmth in the 15th century, even though they attempted to employ the same method
and  [proxies].  However  they  omitted  several  important  proxy  series  used  in  the
original  reconstruction  [and  their  reconstruction  failed  verification  statistics].  The
Mann et al (1999) series was subsequently successfully reproduced by by Wahl and
Ammann (2004).

Readers  will  have  noted  that  there  is  almost  nothing  correct  about  this
paragraph. As McIntyre pointed out in his comments, he and McKitrick had
not  produced  a  Northern  Hemisphere  reconstruction  (or  indeed  any
reconstruction at all). They had simply shown that Mann’s reconstruction
was not robust and failed its verification statistics. It is hard to believe that
Briffa  and  his  colleagues  can  actually  have  been  unaware  of  McIntyre’s
repeated statements to this effect, or that they can have missed reference to
it in McIntyre’s original papers. McKitrick added that it sounded almost as
if the authors had got their material directly from the pages of RealClimate
rather than from the scientific literature, and their wording did rather seem
to  echo  Mann’s  accusation  that  the  two  Canadians  had  ‘censored’  key
indicators from the proxy network. Clearly somewhat riled, he went on to
point  out  that  the  missing  data  was,  in  large  part,  the  bristlecone  pines,
which were not valid proxies.

So  it  is  not  that  we  ‘omit’  some  important  proxies  and  end  up  with  a  lousy  result,
instead  we  remove  some  lousy  proxies  and  end  up  with  an  important  result:  the
conclusions  fall  to  pieces.  The  issue,  as  we  have  said  over  and  over,  is  robustness.
Mann’s  conclusions  are  not  robust.  They  are  not  statistically  robust,  nor  are  they
robust to removal of a small network of bristlecone proxies that are widely viewed
among dendrochronologists (including Hughes himself in another paper) to be invalid
as temperature proxies. What we have shown is not that the 15th century was ‘warm’,
but  that  Mann’s  results  do  not  provide  evidence  that  the  late  20th  century  was
climatologically exceptional.

The silence was almost deafening, although Briffa and his colleagues did
refer to the next draft (notably, however, without saying that they accepted
McKitrick’s  arguments).  In  fact  this  was  a  consistent  response  from  the
author  team:  Noted  –  see  edited  text,  Noted  –  see  edited  text.  The
implication of course was that the edited text would cover McIntyre’s work

more fairly, but McKitrick must have wondered if there was anything he or
McIntyre  could  say  that  would  persuade  a  core  member  of  the  Hockey
Team to accept their case. His low expectations were to be entirely justified.
The Hockey Team fights back
While  McKitrick  was  attacking  the  author  team’s  interpretations  of  the
Hockey Stick controversy, Mann was vigorously defending his version of
events. His comments forcefully promoted the claims of Wahl and Ammann
(‘showed the original Mann et al reconstruction to be robust . . .’) and those
of  Rutherford  et  al  (‘demonstrate[s]  that  each  of  the  criticisms  raised  by
McIntyre and McKitrick (2003) are without merit’). At the same time he
was also launching direct attacks, saying that McIntyre’s papers had been
refuted five timesb and also doing his best to malign the separate criticisms
of von Storch, whose paper, he alleged, included a ‘fundamental error’. Jan
Esper  and  David  Ritson  were  also  fighting  the  Team’s  corner.  Esper  was
trying  to  downgrade  discussion  of  the  Hockey  Stick  affair,  which  he  felt
was  dominating  proceedings,  while  Ritson  wondered  innocently  if,  since
the  authors  didn’t  refer  to  McIntyre’s  2005  papers,  they  should  consider
removing discussion of his work entirely.

McIntyre had plenty of ammunition to counter Mann’s arguments, which
were fairly brazen since Mann must have known that the author team would
have read the replies to Huybers and von Storch. McIntyre had also pointed
out  that  the  author  team  were  being  highly  misleading  over  Ammann’s
alleged replication of the Hockey Stick. They had referred to the CC paper
as ‘Wahl and Ammann 2004’, the date suggesting that it had been published
already.  In  fact,  as  we  have  seen,  at  that  point  it  was  stuck  in  a  kind  of
publishing  limbo,  held  up  by  the  problems  with  the  GRL  comment.c
McIntyre  also  pointed  out  that  Wahl  and  Amman  had  not  reproduced
Mann’s claims of statistical skill and had not reproduced the Hockey Stick.
McKitrick  was  even  more  forthright,  saying  of  the  supposition  that  the
Hockey  Stick  had  been  reproduced,  ‘The  last  claim  is  false’.  He  also
pointed  out  that  if  the  authors  felt  that  failing  verification  statistics  was
sufficient grounds for condemning a paper (as suggested by their spurious
point about the McIntyre ‘reconstruction’), then they should be condemning
MBH98  as  well,  since  it  had  failed  its  verification  statistics  with  some
panache. This presented Briffa and his colleagues with a dilemma. One one
hand  they  had  Mann  telling  them  that  Ammann’s  work  replicated  the

Hockey  Stick  and  on  the  other  was  McIntyre  saying  point  blank  that  it
didn’t.  How  would  they  deal  with  this  difference  of  opinion?  As  we  saw
above, they were duty bound to report differences of opinion.

Quite apart from getting pretty much every fact in their paragraph on the
Hockey  Stick  wrong  (even  though  the  paragraph  ran  to  only  three
sentences), the authors also managed to omit some key indicators of their
own:  such  as  the  existence  of  two  McIntyre  and  McKitrick  papers  from
2005, such as the PC analysis arguments and the bristlecones. As an attempt
to  represent  a  complex  scientific  dispute,  it  was  either  incompetent  or
biased.  The  authors’  vague  responses  to  McIntyre  and  McKitrick’s
comments did not bode well for a happy resolution.
The NAS defence
It was clear from the rest of the paleoclimate chapter that even if the authors
could  not  save  Mann  directly,  they  would  still  attempt  to  kill  off  the
Medieval Warm Period by adopting the ‘NAS defence’: they would declare
that  ‘independent’  confirmations  of  Mann’s  reconstruction  meant  that  his
findings were correct, even if his data and methods were invalid.

The independent confirmations that we looked at in Chapter 10 were all
advanced in Mann’s support, although with a caveat about the commonality
of  proxies  between  them.  All  were  on  the  receiving  end  of  a  vigorous
McIntyre attempt to shoot them down.

In his review, McIntyre first flagged up Rutherford et al, which had used
the same flawed PC methodology and the same flawed proxies as Mann had
used  in  MBH98.  Quite  illogically,  Briffa  and  his  colleagues  on  the  author
team  declared  that  this  didn’t  matter  because  the  rest  of  Rutherford’s
methodology was different, as if this somehow excused the use of a biased
dataset.  Briffa  either  ignored  or  missed  McIntyre’s  comment  on  the
divergence problem in D’Arrigo et al since no response was forthcoming.
When it came to Hegerl et al, which didn’t cite data sources properly, Briffa
merely replied that it did.d Briffa’s own paper, Briffa 2005, was not even
available for review and here at least, the author team appeared to accept
the point. They also appeared to accept McIntyre’s criticisms of Jones et al
1998, but how these comments would be incorporated in the next draft was
anyone’s  guess.  Far  too  many  of  them  were  ‘noted’  although  not,
apparently, ‘accepted’. An answer to this question would have to wait.
Second Order Draft

With his review comments all submitted, McIntyre returned to his research
and his blogging. The second draft of the IPCC report was due to be issued in
April 2006, at which point a whole new set of review comments had to be
submitted  and  responded  to.  McIntyre  was  keen  to  understand  the  author
team’s  thinking  as  they  moved  from  the  first  to  the  second  draft  and,
fortunately,  according  to  IPCC  policies  the  first  draft  review  comments
should  be  made  available  at  the  same  time  as  the  second  draft  was
published.  McIntyre  therefore  wrote  to  the  IPCC  to  ask  how  he  could  get
access to the paleoclimate chapter comments. To his surprise, a short time
later a large package was delivered to his home, which turned out to contain
the full set of review comments – rather than email over a file or post a disc,
the IPCC had printed out the whole document and posted it to him. This was
incredibly  inconvenient  as  it  meant  that  it  was  impossible  to  search  for
relevant comments in a document of over 200 pages. Quite why TSU should
put themselves to so much trouble when they could simply have emailed an
electronic  file  is  unclear.  They  had  also  rather  oddly  marked  the  pages,
‘Confidential,  Do  Not  Cite,  Quote  or  Distribute’,  so  there  was  to  be  no
discussion of the contents either. So much for an open review process.
Late breakers
Shortly  before  the  deadline  for  the  IPCC  review,  McIntyre  had  noticed  a
surge of submissions of new papers to the review. Apart from Ammann’s
Climatic  Change  papere  there  were  new  findings,  apparently  hot  off  the
press,  from  familiar  names  such  as  Hegerl  et  al  and  D’Arrigo  et  al  (see
above);  Wahl,  Ritson  and  Ammann  was  another,  and  Osborn  and  Briffa
another. Some months later, in early 2006, having seen how the  IPCC  had
ignored their own deadlines in order to include Ammann’s Climatic Change
paper, McIntyre took the trouble to compare the actual publication dates of
these ‘late breakers’ with the IPCC’s timetable requirements.

The IPCC guidelines stated that a paper had to be available in draft form
by 12 August 2005, published or in press by the end of December and in
final preprint form by the end of February 2006. A cursory glance at the
publication  timetables  for  the  late  breakers  showed  that,  as  McIntyre  had
suspected, most of them had actually failed to meet the deadlines but had
been carried forward into the review regardless.
TABLE 11.1: IPCC papers and their publication progress

Paper

Submitted Accepted Published

12 Aug 05 16 Dec 05 27 Feb 06
IPCC deadline
Wahl, Ritson and Ammann 3 Oct 05 27 Feb 06 28 Apr 06
8 Jul 05 28 Feb 06 20 Apr 06
Hegerl et al
Wahl and Ammann
10 May 05 28 Feb 06  
23 Sep 05 17 Jan 06 10 Feb 06
Osborn and Briffa

Clearly, at the end of February the paleoclimate community was in some
turmoil, as papers were rushed through the peer review process in time to
make the IPCC deadline. Of course, according to the letter of IPCC rules, they
were already too late, having missed the deadline for journal acceptance by
some margin. It may have been thought that if it could be demonstrated that
the papers had received journal acceptance by the later publication deadline
there was at least a small fig leaf to hide behind.

While this might seem like something of a technicality, it is in fact of
great importance to the quality of the review. The lead author meeting at
which the second draft was written had taken place in New Zealand on 13–
15 December 2005. The authors were supposed to have taken into account
comments submitted on the first draft, but of course the reviewers of the
first draft could not have read these late-breaking papers, none of which had
completed peer review by the time comments had to be submitted. In fact,
with the acceptance dates being in February, even the authors preparing the
second draft would not have had access to the papers in their final form. So
in practice, the review failed to meet even the rather low standards set by
the IPCC.
The Hegerl substitution
When McIntyre started his second draft review, he was astonished to find
that  one  of  the  papers  cited  had  changed.207,208  When  he  had  looked  at
Hegerl et al during the first draft review, he had read a text from Nature
which,  as  we  saw  above,  had  only  been  accepted  for  publication  on  28
February 2006, thus making it ineligible for the report. Had it been cleared
for  inclusion  on  a  nudge  and  a  wink,  like  all  the  other  late  breakers?
However, some time between the first and second drafts, the paper had been
switched  for  an  entirely  different  Hegerl  et  al  article,  this  time  from  the
Journal  of  Climate.  The  latter  paper  had  not  even  been  accepted  for
publication  in  April  2006  and  therefore,  on  any  measure,  could  not  be
considered. The reasons for the swap became clear when McIntyre realised
temperature
that 

the  Journal  of  Climate  paper 

included  a  new 

reconstruction, which had not been mentioned in the Nature article at all.
McIntyre  pointed  out  this  illegitimate  switch  in  his  second  draft  review
comments,  but  was  rebuffed  with  the  bald  statement  that  the  Hegerl
submission  met  the  IPCC  guidelines.  This  was  apparently  not  true,  as  the
IPCC guidelines stated clearly that final preprints must be available by late
February.209
The new spaghetti chart
McIntyre also gave Briffa strong censure for trying to do a ‘Michael Mann’
and use his status as an IPCC lead author to promote his own work. Between
the first and second drafts, Briffa had managed to insert references to his
new paper co-authored with Tim Osborn. This paper, as we saw above, had
actually failed to meet the IPCC publication deadlines, but here it was, bold
as brass, highlighted in a second spaghetti diagram in the new draft. When
McIntyre pointed out this inconvenient fact, his comment was rejected, with
another  bold  assertion  that  its  inclusion  did  not  contravene  current  IPCC
policies.

The new spaghetti diagram was designed to demonstrate how the various
proxy  series  that  went  into  temperature  reconstructions  all  told  different
stories about the Medieval Warm period, and many of McIntyre’s ‘same old
whores’ were there – Mann’s PC1 (bashfully labelled ‘W USA’), Yamal and
so on. The only clear message that came from the diagram was that there
seemed to be a general uplifting of proxy values in the twentieth century;
the rest of the graph looked rather like random data, which in McIntyre’s
opinion was exactly what it was: red noise. The twentieth century uplift was
simply  an  artefact  of  having  cherrypicked  series  which  showed  such  an
uplift. Briffa didn’t see it that way, of course, and rejected all attempts by
McIntyre to point out what was wrong. Why did Briffa include Mann’s PC1,
which was essentially just the bristlecones, trees which were known to be
flawed proxies? Briffa merely replied that the purpose of the figure was ‘to
illustrate in a simple fashion, the variability of numerous records that have
been used in published reconstructions of large-scale temperature changes’
and ‘not to give a very detailed account of the specific limitations in data or
interpretation  for  each’.  So  according  to  Briffa,  the  findings  of  the  NAS
panel and the Wegman report – that the statistical treatments used in Mann’s
papers were wrong – were to be recast as mere ‘limitations’. Readers can no
doubt  assess  for  themselves  whether  using  incorrect  mathematics  is  a

‘limitation’ or whether it is in fact just plain incorrect. Briffa said that the
carbon dioxide fertilisation issue was complex and that it would be covered
in the final draft.

Meanwhile, we might wonder again why Briffa preferred Yamal over the
Polar Urals update, when the latter had a better correlation to temperature
records, suggesting it was a more reliable proxy? Of course, Yamal had no
Medieval  Warm  Period,  and  many  will  assume  that  this  was  the  reason.
Again,  Briffa  referred  to  wanting  to  demonstrate  the  variability  in  the
records, although this didn’t seem to actually address the point at issue.
The Hockey Stick in the second draft
The  authors  had  suggested  that  they  would  take  on  board  McIntyre  and
McKitrick’s  criticisms  of  IPCC’s  treatment  of  their  papers  when  they  put
together the second draft. However, when this document became available
in April 2006,f it was clear that they had done no such thing. The best that
could be said about their new summary of the Hockey Stick affair was that
it was somewhat different. It remained a travesty of the truth:

McIntyre and McKitrick (2003) reported that they were unable to replicate the results
of Mann et al (1998). Wahl and Ammann (accepted) demonstrated that this was due to
the omission by McIntyre and McKitrick of several proxy series used by Mann et al
(1998). Wahl and Ammann were able to reproduce the original reconstruction closely
when  all  records  were  included.  McIntyre  and  McKitrick  (2005)  raised  further
concerns  about  the  details  of  the  Mann  et  al  (1998)  method,  principally  relating  to
[verification statistics and PC methodology]. The latter may have some foundation, but
it is unclear if it has a marked impact upon the final reconstruction.

So the argument that McIntyre had ‘censored’ key data still stood, despite
the nature of these alleged omissions having been made abundantly clear.
Invalid  proxies  (like  the  bristlecones),  obsolete  data  (like  Twisted  Tree,
Heartrot  Hill)  and  extrapolations  (like  Gaspé)  should  have  been  omitted
from  the  dataset  by  any  diligent  researcher.  By  failing  to  point  out  why
McIntyre had omitted this data and by refusing to discuss its validity, the
authors were handing the victory to their colleagues in the Hockey Team.
McKitrick was forthright once again:

The opening sentence . . . misrepresents the situation by failing to point out that the
‘results’  of  Mann  et  al  were,  principally,  the  supposed  findings  of  unprecedented
robustness  and  statistical  significance.  Not  only  have  these  results  NOT  been
replicated by others, but they have been amply disproven, by teams on both sides.208

McKitrick  pointed  out,  yet  again,  that  Wahl  and  Ammann’s  purported
replication failed its R2 statistic, but the argument appears to have been lost
on Briffa, who failed to address it in his response, which concentrated on
the  benchmarking  for  the  RE.  On  the  latter  point,  Briffa  launched  into  an
explication of his own views on the issue, without making any supporting
citations, and on this basis rejected McKitrick’s points. The text, he said,
gave a balanced view and he said that McIntyre’s 0.51 benchmark level was
‘somewhat  overstated’,  although  he  failed  to  explain  how,  if  McIntyre’s
published  benchmark  was  only  ‘somewhat’  overstated,  he  could  justify
rejecting it entirely in favour of Ammann’s still unpublished benchmark of
zero.

Broadening their defence, Briffa and his colleagues on the author team
tried  to  rationalise  their  summary,  explaining  that  they  had  received
diametrically opposite opinions on the Hockey Stick issue and had tried to
strike  a  balance.  This  once  again  highlighted  a  breathtaking  lack  of  due
diligence  (or  perhaps  of  intellectual  honesty).  If  the  authors  had  received
opposite  opinions  on  Wahl  and  Amman’s  paper,  should  they  not  have
investigated the difference themselves? They had only to ask Ammann for
his verification statistics and the answer would have become crystal clear –
that the Hockey Stick was not a credible reconstruction. As it was, they had
been silent on the dispute, and Briffa was to find himself relying on a draft
paper that was subsequently shown to be not just wrong, but cynically so, a
finding that cast him in a very bad light.

McIntyre also pointed out that the version of Wahl and Amman which
had been accepted by Climatic Change differed markedly from the version
which was considered by the IPCC – as we have seen, the paper eventually
included an admission that its verification statistics were a failure, making it
useless for assessing historic climate. Once again, Briffa and his colleagues
were  not  interested,  failing  to  address  the  specific  point  and  brushing
McIntyre aside with a statement that his comment was ‘rejected’ and that
the inclusion of each paper was ‘allowed under current rules’. In fact the
declaration about conformity with current rules was something of a feature
of the second draft, with each of McIntyre’s objections to the late-breaking
papers being rebuffed with this statement or a variation on it. This slightly
awkward turn of phrase turned out to be important, although its significance
was not discovered until some time later.

Briffa also made a lengthy response to the question of the robustness of
the  reconstructions  to  the  PC  algorithm.  Unsurprisingly,  he  sided  with  the
rest of the Hockey Team, echoing the claims of Mann and Ammann that,
provided you retained enough PCs, you still got a hockey stick. It is worth
reminding  ourselves  what  this  means  in  terms  of  the  flow  of  data  from
proxies  to  final  reconstruction.  The  bristlecone  pines  appear,  under  a
standard centring regime, in the PC4, representing just 8% of the variance of
the total dataset. What this means is that their hockey stick shape is a rather
unimportant pattern in the dataset, as would be expected since bristlecones
are  a  couple  of  closely  related  species  from  a  small  area  of  the  western
USA. However, because they correlate well to temperature in the twentieth
century, they dominate the calibration results and hence the reconstruction
too.  In  this  way  the  temperature  reconstruction  for  the  whole  Northern
Hemisphere is made to look like the growth pattern of a few trees in the
White Mountains of the USA. This is apparently accepted as reasonable by
the IPCC’s author team. It had certainly not been viewed as reasonable by
Wegman,  who  had  told  one  of  the  senators  on  the  House  Energy  and
Commerce  Committee 
that  adjusting  your
methodology  after  the  event  was  not  an  appropriate  way  of  conducting  a
statistical experiment:

in  no  uncertain 

terms 

Wahl and Ammann [argue that] if one adds enough principal components back into
the  proxy,  one  obtains  the  hockey  stick  shape  again.  This  is  precisely  the  point  of
contention. . . . A cardinal rule of statistical inference is that the method of analysis
must be decided before looking at the data. The rules and strategy of analysis cannot
be changed in order to obtain the desired result. Such a strategy carries no statistical
integrity and cannot be used as a basis for drawing sound inferential conclusions.210

What of the report’s assertion that McIntyre’s criticisms of the biased PC
methodology ‘might have some foundation’? It is hard to see this as a fair
assessment of the state of the scientific debate. Two expert panels – the NAS
and Wegman – had considered the issue in detail and both had concluded
that Mann’s methodology was wrong. Von Storch had also said that short
centring  was  wrong.  Huybers  had  agreed.  How  could  Briffa  possibly
overturn this with a vague statement that ‘it might have some foundation’;
how could he possibly claim that he had ‘struck a balance’?

The  rest  of  the  chapter  was  little  better.  The  spaghetti  graph  of
multiproxy reconstructions still included the Hockey Stick, it still included
the  Briffa  truncation  and  it  still  included  bristlecones  and  foxtails  in

abundance.  Briffa’s  conclusions  appeared  even  more  wayward  when  the
subject of the divergence problem was raised. Richard Alleyg raised Briffa’s
failure  to  mention  this  question  in  the  second  draft,  and  McIntyre  had
pointed out the truncation of the results in Briffa’s own notorious papers:

Show  the  Briffa  et  al  reconstruction  through  to  its  end;  don’t  stop  in  1960.  Then
comment and deal with the ‘divergence problem’ if you need to. Don’t cover up the
divergence by truncating this graphic. This was done in IPCC TAR; this was misleading.

The reply from Briffa and his co-authors must have amazed everyone:

Rejected  –  though  note  ‘divergence’  issue  will  be  discussed,  still  considered
inappropriate to show recent section of Briffa et al. series.

This  was  an  extraordinary  answer.  It  was  apparently  considered
‘inappropriate’ for the IPCC to set out the truth, warts and all. McIntyre was
outraged  at  what  appeared  to  be  a  disgraceful  example  of  political
expediency standing in the way of the truth, and when he posted Briffa’s
words up at Climate Audit, the IPCC added insult to injury, demanding that
he remove them immediately. If their authors were going to behave in this
way, then it was perhaps better if the general public knew nothing about it.
The lead author meeting
The comments on the second draft were returned to IPCC at the start of June,
ready for the lead author meeting on the 25th–30th in Bergen, Norway at
which  the  final  draft  would  be  prepared.  Shortly  after  the  end  of  this
meeting, there was another extraordinary development, which revealed the
reason for the author team’s repeated declarations that the inclusion of the
late breakers didn’t breach ‘current rules’. In the days after the closing of
the meeting, an email was issued to all IPCC reviewers, including McIntyre,
which  contained  an  entirely  new  set  of  guidelines  for  the  inclusion  of
literature in the review.211 These are reproduced below:

GUIDELINES FOR INCLUSION OF RECENT SCIENTIFIC
LITERATURE IN THE WORKING GROUP I FOURTH

ASSESSMENT REPORT

We are very grateful to the many reviewers of the second draft of the Working Group I
contribution to the IPCC Fourth Assessment Report for suggestions received on issues

of  balance  and  citation  of  additional  scientific  literature.  To  ensure  clarity  and
transparency in determining how such material might be included in the final Working
Group I report, the following guidelines will be used by Lead Authors in considering
such suggestions.

In preparing the final draft of the  IPCC  Working  Group  I  report,  Lead  Authors
may include scientific papers published in 2006 where, in their judgment, doing so
would  advance  the  goal  of  achieving  a  balance  of  scientific  views  in  addressing
reviewer comments.

However, new issues beyond those covered in the second order draft will not be
introduced  at  this  stage  in  the  preparation  of  the  report.  Reviewers  are  invited  to
submit copies of additional papers that are either in-press or published in 2006, along
with the chapter and section number to which this material could pertain, via email to
ipcc-wg1@al.noaa.gov, not later than July 24, 2006.

In  the  case  of  in-press  papers,  a  copy  of  the  final  acceptance  letter  from  the
journal is requested for our records. All submissions must be received by the TSU not
later than July 24, 2006 and incomplete submissions cannot be accepted.

It appeared then that the IPCC was going to allow three extra weeks for new papers to be included,
provided that they didn’t open any new topics and provided the papers were accepted before the 24
July. Suddenly the significance of Briffa’s statements about the late breakers being allowed ‘under
current  rules’  become  clear.  Presented  with  overwhelming  evidence  that  Wahl  and  Amman’s  CC
paper, Hegerl et al and Osborn and Briffa should all have been excluded from the review, the IPCC,
and perhaps Briffa himself, had simply rewritten the rules to permit their inclusion.
Even  then,  the  IPCC  wasn’t  entirely  off  the  hook,  because  Wahl  and
Amman’s  CC  paper  relied  upon  their  GRL  comment  for  its  statistical
arguments, and the GRL comment had been rejected by the journal. In order
to  maintain  the  fiction  of  a  March  acceptance  date  they  needed  the  GRL
comment to find its way through peer review at some point. The story of
how this came about will be told in the next chapter.
The SPM
The  Summary  for  Policymakers  (SPM),  a  short  distillation  of  the  whole
IPCC report, was due to be released in February 2007. However, even the
simple  act  of  scheduling  the  publication  of  this  document  opened  up
another set of questions over the propriety of the IPCC’s procedures.

When the publication date was announced, the  IPCC also stated that the
main body of the report, on which the SPM was based, was not itself to go
into print until three months later. This was a little odd, but when seen in the
light of the procedures for making final revisions to the report, it became
rather sinister. The procedures read as follows:

Changes (other than grammatical or minor editorial changes) made after acceptance
by the Working Group or the Panel shall be those necessary to ensure consistency with

the Summary for Policymakers or the Overview Chapter.212

In other words, the IPCC was intending to ensure that the policy document
was consistent with the scientific one by making changes to the scientific
document! As McIntyre commented to his readers:

Unbelievable.  Can  you  imagine  what  securities  commissions  would  say  if  business
promoters  issued  a  big  promotion  and  then  the  promoters  made  the  ‘necessary’
adjustments to the qualifying reports and financial statements so that they matched the
promotion? Words fail me.213

As  the  publication  date  for  the  SPM  approached,  IPCC  insiders  started  to
leak  the  contents  to  the  media,  and  it  looked  very  much  as  if  there  were
going to be doom-laden headlines once again. In a widely distributed report
from  the  Associated  Press,  Andrew  Weaverh  declared  that  ‘This  isn’t  a
smoking gun; climate is a battalion of intergalactic smoking missiles . . .’214
Meanwhile climatologist Kevin Trenberth advised readers to ‘Look for an
‘iconic  statement’  –  a  simple  but  strong  and  unequivocal  summary  –  on
how  global  warming  is  now  occurring’.214  While  these  statements  didn’t
address  the  Hockey  Stick  question  directly,  neither  did  they  suggest  that
Briffa was going to rebalance his summary of the Hockey Stick.

A couple of days later Rajendra Pachauri, the head of the IPCC, continued
the process of whipping up media attention by telling Reuters that ‘There
are a lot of signs and evidence in this report which clearly establish not only
the fact that climate change is taking place, but also that it really is human
activity that is influencing that change’.215 Unnamed sources told the same
reporter that the Fourth Assessment Report would declare that the IPCC ‘is at
least  90  per  cent  sure  than  human  activities,  led  by  the  burning  of  fossil
fuels, are to blame for global warming over the past 50 years’.

Then, on the last day of January, the news was leaked that the sceptics
had  all  been  expecting:  the  Hockey  Stick  would  survive  to  fight  again
another  day.  This  was  the  unavoidable  conclusion  of  an  article  in  the
Toronto Globe and Mail, based on yet another IPCC leak of the SPM:

It  concludes  the  higher  temperatures  observed  during  the  past  50  years  are  so
dramatically  different  from  anything  in  the  climate  record  that  the  last  half-century
period was likely the hottest in at least the past 1,300 years.216

This quotation was strongly suggestive that the Hockey Stick had been
given  a  reprieve,  and  sure  enough  when  the  SPM  made  its  official
appearance the following day, Mann’s conclusions were right there at the
start of the paleoclimate section:

Average Northern Hemisphere temperatures during the second half of the 20th century
were very likely higher than during any other 50-year period in the last 500 years and
likely the highest in at least the past 1,300 years.217

Recent studies, it reported, had shown rather more variability than had been
concluded  from  the  Third  Assessment  Report  (in  other  words,  there  was
more of a Medieval Warm Period apparent than Mann had said six years
earlier)  but  drew  ‘increased  confidence  from  additional  data  showing
coherent  behaviour  across  multiple  indicators  in  different  parts  of  the
world’  (emphasis  added)  –  in  other  words  the  independent  confirmations
were alleged to have confirmed the Hockey Stick position on the Medieval
Warm Period. This was a surprising statement, firstly because as we have
seen, the proxies were all behaving in a very incoherent fashion, but also
because, as McIntyre noted, the word ‘coherent’ was not used (at least in
this  context)  in  either  of  the  drafts  of  the  report.  So  either  the  idea  of
coherence had been inserted in the final draft by Briffa (and had therefore
not formed part of the IPCC’s much-vaunted review process) or it appeared
in  the  Summary  for  Policymakers  but  not  in  the  still-unpublished  report
itself (in which case it would presumably be one of those issues where the
scientific  report  would  have  to  be  changed  to  bring  it  into  line  with  the
SPM.)

Another striking feature of the  SPM was that, while the  IPCC  seemed  to
like the conclusions of the Hockey Stick papers, they omitted any mention
of  the  Hockey  Stick  itself.  This  shift  in  emphasis  from  the  Third
Assessment  was  picked  up  by  some  watching  journalists  who  wondered
whether the change was significant. One of the coordinating lead authors of
the paleoclimate chapter, Eystein Jansen, told a Norwegian newspaper that
the omission was necessary for reasons of space and also because it was a
bit  difficult  to  explain  to  politicians!218  He  went  on  to  explain,  however,
that  the  IPCC  had  performed  no  specific  evaluations  of  McIntyre’s
arguments, another claim which seemed bizarre. If McIntyre’s claims about
the validity (or lack of it) of bristlecone pines as proxy thermometers were
correct,  then  many  of  the  studies  which  the  IPCC  was  relying  on  for  its

position on the Medieval Warm Period were thrown into severe doubt. How
then could they explain a failure to specifically assess McIntyre’s findings?
The full report
‘The  Physical  Science  Basis:  Contribution  of  Working  Group  I  to  the
Fourth  Assessment  Report  of  the  Intergovernmental  Panel  on  Climate
Change’ was released very quietly on 29 April 2007. It was almost as bad
as it could have been. Most of McIntyre and McKitrick’s comments on the
second  draft  had  been  ignored,  including,  incredibly,  those  in  which  they
had corrected Briffa and his team on what their claims actually were – he
was still claiming that the Canadians had created a reconstruction of their
own, in the face of their vehement statements to the contrary.

With Briffa having introduced this new argument only in the final report,
it  was  not  possible  for  McIntyre  or  McKitrick  to  point  out  the  holes  in
Briffa’s  case,  and  there  was  no  doubt  that  more  than  one  aspect  of  his
position  could  be  disputed.  Firstly,  arguing  that  Mann  had  adjusted  for

McIntyre’s  comments  that  the  Hockey  Stick  and  Ammann’s  purported
replication of it had both failed their verification statistics seemed to have
had  some  effect,  because  Briffa  had  decided  to  drop  the  subject  entirely
from the text. However, hiding the lack of robustness of the Hockey Stick
behind a veil of silence could hardly be said to give a fair reflection of the
scientific literature, if indeed that had been the intention of the report. The
general  thrust  of  the  final  text  was  that  McIntyre’s  criticisms  might  have
some validity but that they were probably not of material importance.

There were some other significant changes too. Briffa had clearly felt he
could  not  credibly  pass  over  the  divergence  problem  and  the  bristlecones
without mentioning them at all, and a lengthy paragraph had been added. Of
course, having ignored McIntyre and McKitrick’s comments on these issues
in the first draft and then, in the second round having referred them to the
final draft, this new text was all Briffa’s own and was entirely unreviewed.
The carbon dioxide adjustment that wasn’t
While the new paragraph did address the subject of possible carbon dioxide
fertilisation,  Briffa’s  treatment  of  it  was  brief  and  insubstantial.  His  new
position  was  that  Mann  had  adjusted  for  possible  carbon  dioxide
fertilisation  in  MBH99  and,  since  this  latter  paper  still  had  a  hockey  stick
shape, the possibility that the bristlecones contained a non-climatic signal
could be discounted.

carbon  dioxide  fertilisation  didn’t  address  the  effect  of  carbon  dioxide
fertilisation on all the other reconstructions in the spaghetti graphs, which
used bristlecones without any adjustments. These other studies must have
been tainted just as much as the Hockey Stick.

The  second  flaw,  however,  was  more  intriguing.  The  claim  that  Mann
had adjusted for carbon dioxide fertilisation had been around for some time,
and McIntyre had looked into it previously. He had prepared a graph of the
two  Hockey  Stick  papers  side  by  side  and  had  calculated  the  difference
between them in the period of overlap from 1400 to 1980. It turned out that
in the overlap period the results presented in the two papers were identical
(see Figure 11.1).155 So either Mann had not adjusted the final result, or his
adjustment  calculation  amounted  to  a  convoluted  way  of  getting  back  to
where he started from.
Dealing with divergence
To return to the IPCC report, the divergence problem was addressed in rather
clearer  terms  than  were  the  bristlecones,  although  Briffa  had  claimed,
without  a  supporting  citation,  that  the  problem  was  limited  to  ‘some
northern, high latitude regions’. This was a surprising position for him to
take  because  it  appeared  to  contradict  his  statement  in  1998  that  the
divergence problem was an issue which affected the whole of the northern
hemisphere.i However, he had at least done something about his infamous
truncation of the problem. Noting the excision of the data, he said that it
had  been  done  while  ‘implicitly  assuming  that  the  ‘divergence’  was  a
uniquely  recent  phenomenon’.  He  went  on,  however,  to  note  that  certain
‘others’ (Hockey Team members don’t like to mention McIntyre by name)
had  argued  for  a  breakdown  in  the  relationship  between  tree  rings  and
temperature. If these ‘others’ were correct, he went on, ‘this would imply a
similar limit on the potential to reconstruct possible warm periods in earlier
times’.  In  other  words  the  IPCC’s  claim  that  modern  temperatures  were
unprecedented might be resting on a scientific method that was incapable of
detecting warmings in the climate.

FIGURE 11.1: Was a correction made to MBH99?

This was an amazing admission: it meant that most of the paleoclimate
chapter was in danger of having to be thrown out as worthless conjecture. It
was clearly hugely important, with major implications for politicians, and
yet this vital fact had still not found its way into the SPM. How different
would  the  latter  document  have  looked  if  its  key  finding  had  read  as
follows?

Average Northern Hemisphere temperatures during the second half of the twentieth
century may have been higher than during any other 50-year period in the last 500
years.  However,  there  is  evidence  that  the  data  and  methods  used  reconstruct  past
climates may be incapable of detecting warming episodes in earlier centuries and this
position is therefore subject to considerable uncertainty.

a  See page 39
b  Mann referred to Ammann’s two whoppers in Climatic Change and GRL, Rutherford et al and the
comments  on  McIntyre’s  GRL  paper  by  Huybers  and  von  Storch.  These  are  all  considered
elsewhere in this book and, as we have seen, are all far from constituting refutations.

c  See Chapter 8.
d  McIntyre’s point was presumably that the citation was inadequate to allow the actual dataset to be

identified, but this subtlety seems to have been missed by the authors.

e  See page 217.
f    The  official  archive  of  the  drafts  is  at  the  Harvard  University  Library,  whose  online  retrieval
system  seems  to  be  carefully  designed  to  make  accessing  the  information  as  hard  as  possible.
Readers  wishing  to  access  the  material  in  a  more  user-friendly  manner  may  want  to  use  an
alternative.207
g  See page 233.
h  See page 292.
i  See page 64.

12     The IPCC Aftermath

At last the secret is out, as it always must come in the end; The delicious story is ripe
to tell to the intimate friend; Over the tea-cups and in the square the tongue has its
desire; Still waters run deep, my dear, there’s never smoke without fire.

W.H. Auden
At last the secret is out

A little is revealed
In Chapter 8 we saw how Ammann and Wahl’s CC paper went forward into
the IPCC review process in rather peculiar circumstances, and also how the
paper relied on the GRL comment in order to establish its RE benchmark of
zero.  But  the  GRL  comment  had  then  been  rejected  twice  by  the  journal,
leaving  the  CC  paper  with  no  justification  of  its  claims  of  statistical
significance.

By early 2007, and a year after it had been accepted for publication, the
CC paper was nowhere to be seen. As the publication date for the final IPCC
report  loomed,  this  left  the  IPCC  and  Climatic  Change  with  a  problem.
McIntyre observed:

I’m intrigued as to what the final [CC paper] will look like. They have an intriguing
choice:  the  inclusion  of  a  reference  to  this  article  in  [the  IPCC’s Fourth Assessment
Report]  was  premised  on  [it]  being  ‘in  press’,  which  would  prohibit  them  from  re-
working [it] to deal with the [rejection of the GRL comment]. But the article needs to
be reworked since it will look pretty silly to describe [the  GRL comment] as ‘under
review’ over 18 months after it has been rejected.219

The new comment
In the background, however, much had been happening. In September 2007,
and  with  the  IPCC  report  published,  the  CC  paper  suddenly  appeared,
preceded  in  the  same  journal  by  another  paper  by  the  same  authors.
Remarkably,  in  view  of  his  apparently  having  been  misled  by  Ammann
over the rejection of the GRL comment, it nevertheless looked as if Stephen
Schneider, the editor of Climatic Change, had decided to allow Wahl and
Ammann to rewrite their rejected GRL comment and to submit it to Climatic
Change instead. All reference to the rejected GRL comment in the CC paper
could be replaced by reference to the new paper.

One advantage of this approach was that it allowed the CC paper to retain
its  original  acceptance  date,  and  hence  justify  its  inclusion  in  the  IPCC
review. It did leave the IPCC with the embarrassing problem that a paper that
was allegedly accepted in March 2006 relied upon another paper that even
the journal itself said was only received in August that year, (and in reality,
it was even later than that). And this mattered because unless the CC paper
had  been  accepted  by  the  journal  before  the  IPCC  deadline,  it  should  not
have been accepted for inclusion in the Fourth Assessment Report. But, as
we’ve  seen,  the  IPCC  needed  the  CC  paper  so  that  they  could  claim  that
McIntyre’s  work  had  been  rebutted.  So  despite  the  inconsistency  being
pointed out to them (see Chapter 11), they had waved the objections aside
as irrelevant.

With identical authorship and a maze of cross-references between them,
it was extremely difficult to discern how the arguments in the two Climatic
Change  papers  relied  on  each  other.  Ammann  and  Wahl’s  claim  of
statistical  significance  for  the  CC  paper  was  based  on  their  having
established  an  RE  benchmark  of  zero.  This  claim  was  repeated  at  several
places  in  the  main  text  of  the  paper  and  each  time  they  referred  to  an
appendix and to the new paper. In the appendix, however, there was further
discussion  of  the  issue  and  a  further  citation  of  the  new  paper,  but  no
justification of it. Mystifyingly, however, the new paper referred back to the
CC paper, citing its use of a benchmark of zero. There were some details of
what  they  had  done  to  establish  the  benchmark,  but  the  calculations
themselves  were  apparently  to  be  found  in  the  online  Supplementary
Information (SI). This presented a problem for McIntyre, because there was
no trace to be seen of the SI. In fact, as far as McIntyre could tell, even the
peer reviewers had not been given access to it. McIntyre wrote to Ammann
one more time to request the benchmarking data and code, and once again
Ammann refused to hand it over, this time in terms which left no room for
doubt. His reply was remarkable:

Under such circumstances, why would I even bother answering your questions, isn’t
that just lost time?220

The appearance of the benchmark
Again, everything fell silent. For the next year nothing more was heard of
the  two  papers.  McIntyre  continued  to  press  from  his  blog  for  release  of
Ammann’s SI and with the IPCC report now published, politicians were able

Then  in  August  2008,  and  entirely  unannounced,  the  Supplementary
Information suddenly appeared on Caspar Ammann’s website, some three
years  after  that  first  press  release  announcing  the  alleged  refutation  of
McIntyre’s work. With it, and a godsend to McIntyre, was the code used to
establish the benchmark for the RE statistic. With no more than a few days
work, McIntyre was able to establish exactly what had been done. What he
found was stupefying.

It turned out that Ammann had calculated almost exactly the same figure
as McIntyre. The number he had arrived at was 0.52, just a whisker away
from McIntyre’s own 0.54. As we saw, however, Ammann had reported in
the paper that it was sufficient only to score above zero.

to take advantage of the political space it had created. With Ammann and
Wahl’s  alleged  refutation  of  McIntyre  in  print  and  in  the  IPCC  report,
naysayers could safely be ignored and the policy agenda advanced.

So how had Ammann reconciled the two numbers – 0.52 and zero? A
benchmark of 0.52 must have represented a big problem for him and Wahl.
It was entirely inadequate for their purposes because Mann’s Hockey Stick
had  a  verification  RE  of  0.48,  leaving  it  tantalisingly  just  below  the
calculated benchmark. Unless they could somehow work things so that the
benchmark  came  down  a  little,  the  Hockey  Stick  would  be  declared
‘statistically insignificant’, with all the disastrous ramifications that would
have  on  political  and  climatological  careers  around  the  world.  However,
achieving this feat and getting the Hockey Stick up above the benchmark
level would have been tricky. Remember, a thousand runs of random data
were  pushed  through  the  statistical  sausage  machine.  In  other  words,  a
thousand  attempts  were  made  to  reconstruct  the  temperatures  of  the  past
using random data instead of proxies. The RE number – the correlation with
the actual temperature records – was recorded for each. Then all the runs
were sorted in order of RE value, the best runs having the highest RE and the
worst the lowest. Ammann needed to show that the Hockey Stick  RE was
right up there with the best simulations – in the top one percent. It was no
good simply removing runs which had a higher score than the Hockey Stick
because  this  would  be  seen  as  statistical  malpractice.  It  would  be  like
crossing  out  all  the  questions  you  got  wrong  in  a  test,  then  claiming  a
perfect score.

The  method  Ammann  and  Wahl  had  chosen  to  achieve  their  objective
turned out to be only a little less obvious, but no more legitimate. They said

that each of the 1000 runs should be examined to compare the RE score it
achieved in the verification period to the one it got in the calibration. Their
argument  was  that  if  it  did  too  well  in  the  verification  compared  to
calibration, it should be rejected. They proposed therefore that the ratio of
the RE scores in the calibration and verification periods – which I will refer
to as the CV ratio – should be used as a test. This wasn’t a test of whether
the Hockey Stick methodology was correct but of whether the random data
they were going to compare the Hockey Stick to was ‘suitable’. They were
essentially  going  to  filter  out  some  of  the  random  data.  This  meant,  of
course, that it was no longer actually random, although Wahl and Ammann
didn’t say so.

Wahl and Ammann next argued that if one of the runs had a CV score that
indicated  it  was  unsuitable,  rather  than  being  thrown  out  entirely,  its  RE
score should be changed arbitrarily to −9999. This meant that it remained in
the  list  of  runs,  but  was  pushed  to  the  bottom,  making  the  Hockey  Stick
appear relatively more significant.

This extraordinary set of methodological steps was entirely unknown to
statistics  or  to  any  other  branch  of  science  –  it  appeared  to  have  been
contrived entirely for the purposes of saving the Hockey Stick. But in order
to do this, they needed to set the correct level for the CV ratio. If it were set
too  low,  say  around  0.5,  not  enough  of  the  high  scoring  runs  would  be
knocked off the top of the list and the Hockey Stick would not appear in the
top 1%. Set it too high and the Hockey Stick itself would fail the CV ratio
test and would have its RE score of 0.813 reset to −9999, indicating a total
failure. In the event, Wahl and Ammann chose, apparently arbitrarily, to set
the CV test level to be 0.75 and noted with satisfaction that under the terms
of their new methodology, Mann’s results were significant.

The new ratio was a bizarre contrivance, but it provided at least some
kind of a fig leaf for the Hockey Team and Climatic Change to hide behind.
With this new, and pretty much arbitrary, step in place, Wahl and Ammann
were  able  to  reject  several  of  the  runs  which  stood  between  the  Hockey
Stick and what they saw as its rightful place as the gold standard for climate
reconstructions. That the statistical foundations on which they had built this
paleoclimate castle were a swamp of ad hoc and arbitrary methodological
steps was, to the Team, apparently an irrelevance. For political and public
consumption,  the  Hockey  Stick  still  lived,  ready  to  guide  political  and
economic decision-making for years to come.

Review comments again
The IPCC’s sleight of hand over the eligibility of Ammann’s paper for the
Fourth Assessment Report was now out in the open, but there were to be
some  more  strange  revelations  about  the  conduct  of  the  review.  Shortly
before  the  publication  of  the  final  report,  McIntyre  had  requested  from
IPCC’s Technical Support Unit (TSU) the full set of review comments for the
second draft. Having experienced the farce of being sent a paper copy by
the IPCC for the first draft comments, he did wonder what Martin Manning
and his TSU colleagues would get up to this time round and he was certainly
not disappointed.

According  to  IPCC  policies,  once  the  final  report  was  published,  the
second  draft  review  comments  would  be  placed  in  a  public  archive.
Manning’s initial response to McIntyre’s request for the review comments
was  that  TSU  had  been  ‘setting  up  the  arrangements’,  but  that  the  review
comments  were  now  available  from  George  Clark,  the  curator  of  the
Environmental  Science  and  Public  Policy  Archives  at  Harvard
University.221 This seemed simple enough, but when McIntyre submitted a
request to Clark for a copy, he received an extraordinary reply.

[P]lease let me know your desired time to visit (no later than one week prior) so that I
can make sure the materials will be ready for you. I will be away from the office June
21–July 5, so the materials will not be available during that date range.221

In  other  words  it  appeared  that  Clark  was  expecting  him  to  travel  from
Toronto to Boston if he wanted to look at the comments. Wearily, McIntyre
set about the trying to grind down the bureaucrats before they ground him
down. He duly composed a letter to the secretary of the IPCC, Renate Christ,
complaining about the suggestion that he would have to travel to Boston to
see the comments. Eventually, a reply was sent by Manning denying that
TSU had said that this was necessary – it was of course Clark at  Harvard
who said that – and that Clark had confirmed that he was ‘very willing’ to
copy and send what McIntyre required.

Clark  had  in  the  meantime  indicated  the  terms  on  which  he  would

perform the photocopying:

I  can  provide  a  photocopy  of  up  to  100  pages  for  research  purposes  only  (not
republication) for our interlibrary loan fee of $34 plus 40 cents per page [there were a
total of 1834 pages of comments]. Copyright of the material resides with its authors. It
may be possible for you to hire a research assistant locally to look over the materials if

that  would  be  helpful  in  selecting  materials  of  most  interest.  I  can  recommend
someone if you like.221

McIntyre’s  complaint  dragged  on  for  months,  with  the  IPCC  seemingly
moving  heaven  and  earth  to  avoid  giving  him  a  convenient  searchable
digital  copy.  It  wasn’t  until  the  end  of  June  that  they  relented  a  little,
Manning sending an email indicating that he was now willing to make the
comments available, provided that they were not redistributed to anyone. It
was not at all clear whether Manning meant that McIntyre could not quote
from the text or merely that he was not to distribute the document to anyone
else. Neither could McIntyre understand from where Manning thought he
derived the authority to add this restriction – review comments were, after
all, meant to be kept in an open archive, so they could hardly be said to be
confidential.

The next day, McIntyre received a large package from the IPCC, but given
the restrictions that Manning had placed on its use, McIntyre chose to leave
it  unopened:  opening  it  might  have  implied  acceptance  of  Manning’s
conditions. The correspondence continued to flow both ways, with Manning
resorting to some fairly bizarre justifications for restricting the use of the
comments, but eventually he backed down further and agreed to make an
electronic copy available. Even so, he wanted to retain the restrictions on
their  use.  An  open  review  process  could  only  be  achieved  he  said,  if  the
review comments could not be selectively quoted.

We would not be promoting a transparent and open process, nor would we be acting
responsibly to our authors and many expert reviewers, if there were no restraint on
others selectively editing and redistributing review materials.221

With those watching the process now doubled up in laughter at Manning’s
intellectual contortions, and a barrage of freedom of information requests
threatening to force disclosure in the near future, the end was clearly not far
off.  A  few  days  later  the  IPCC  capitulated  and  the  comments  were  finally
posted up at UCAR’s website.
Some things remain secret
The  contents  of  the  second  draft  comments  document  have  mostly  been
discussed in Chapter 11, but there were other intriguing aspects to this part
of  the  story  that  need  to  be  recounted.  When  he  had  looked  over  the
contributions of his fellow reviewers, McIntyre had realised that, despite all

the submissions and rejections and resubmissions of Ammann and Wahl’s
two  articles,  neither  of  his  two  opponents  appeared  to  have  made  a
comment on the IPCC paleoclimate chapter. This had seemed so unlikely as
to  be  simply  unbelievable,  but  at  the  time,  there  was  little  McIntyre  was
able do about it and he had set the issue aside.

Then in 2008, fully two years later, when McIntyre was going over the
second  draft  review  comments,  he  found  himself  considering  one  of  the
answers Briffa had given to a comment left by McKitrick. McKitrick had
been  discussing  the  benchmarking  of  the  RE  statistic  and  the  shenanigans
over  the  submission  dates  of  the  CC  paper,  and  had  received  a  lengthy
response  from  Briffa  rejecting  his  points.  However,  to  his  surprise,
McIntyre noticed that the arguments used by Briffa were identical to those
used by Ammann in his new CC comment.220 This was rather extraordinary,
because these arguments had not appeared anywhere in the literature at the
time  of  the  IPCC  review;  Ammann’s  comment  on  MM05(GRL)  hadn’t  been
resurrected  until  August  2006,  months  after  the  deadline  for 
IPCC
submissions. This could mean only one thing: Ammann and Wahl had been
allowed  to  make  contributions  to  the  IPCC  report  in  secret  and  entirely
outwith the normal review process.
A new member of the audit team
As McIntyre’s examination of the IPCC review process continued, more and
more oddities came to light. For example, when the review comments were
posted  up  on  the  UCAR  website,  it  was  discovered  that,  although  the
contributions of the expert reviewers and the official government reviewers
had  been  made  available,  there  was  no  sign  of  the  review  editors’
comments. Review editors were a kind of umpire for the review process –
their  job  was  to  ensure  that  where  there  were  disputes,  the  lead  authors
didn’t simply enforce their own point of view, but fairly represented both
sides of the argument.

Review  Editors  will  assist  the  Working  Group/Task  Force  Bureaux  in  identifying
reviewers  for  the  expert  review  process,  ensure  that  all  substantive  expert  and
government  review  comments  are  afforded  appropriate  consideration,  advise  lead
authors  on  how  to  handle  contentious/  controversial  issues  and  ensure  genuine
controversies are reflected adequately in the text of the Report.

Review editors will need to ensure that where significant differences of opinion on
scientific  issues  remain,  such  differences  are  described  in  an  annex  to  the  Report.

Review  editors  must  submit  a  written  report  to  the  Working  Group  Sessions  or  the
Panel . . .222

Clearly, the treatment of the Hockey Stick affair was hugely controversial,
and  yet  the  final  draft  had  sided  pretty  clearly  with  Mann  and  his  team.
What then had been the contribution of the review editors? Had they tried to
have McIntyre’s objections incorporated and yet been ignored, or had the
checks and balances for which they were responsible been worthless?

David Holland, a British Climate Audit reader, took it upon himself to
investigate.  The  review  editors  for  the  paleoclimate  chapter  were  John
Mitchell,  the  chief  scientist  of  the  UK’s  Meteorological  Office,  and  Jean
Jouzel of France. Holland decided to approach Mitchell for the information
and sent a request off to the Met Office. This went unanswered for several
weeks, and Holland eventually decided to put in a request under the UK’s
Freedom  of  Information  Act.  At  this  point,  a  reply  was  received  from
Mitchell,  but  it  was  evasive:  the  signoff  by  review  editors,  Mitchell
explained,  was  only  available  from  Manning’s  TSU,  which  was  in  the
process of being closed down, its task complete. Fortunately this appeared
to  be  Mitchell  excusing  himself  for  not  replying  to  Holland  earlier:  the
Freedom of Information request did its job and at the end of January 2007,
Holland took possession of the full set of review editor comments.

The review editor comments were based around a standard form letter,
although  this  letter  was  slightly  different  for  each  of  the  IPCC  working
groups.  These  letters  were  remarkably  brief,  amounting  to  just  a  single
paragraph. Working Group II, which looks at the impacts of climate change,
required  review  editors  to  confirm  that  review  comments  had  been
appropriately  considered  by 
IPCC
procedures’.  Working  Group  I,  which  oversaw  the  scientific  papers,  and
therefore the paleoclimate chapter, had a slightly but significantly different
sign-off, omitting the last few words used by Working Group II. In other
words,  they  seemed  to  be  neglecting  to  get  confirmation  that  the  authors
had  dealt  with  the  review  in  accordance  with  IPCC  procedures.  And  there
was  another  oddity  in  Working  Group  I.  One  of  the  review  editors  had
chosen not to use the form letter but had submitted his own report: none
other than Mitchell, the review editor of the paleoclimate chapter.

the  authors  ‘in  accordance  with 

So, how had Mitchell dealt with the Hockey Stick controversy? How had
he ensured that scientifically valid comments were properly covered in the

report. His comments were as follows:

. . . I can confirm that the authors have in my view dealt with reviewers’ comments to
the  extent  that  can  reasonably  be  expected.  There  will  inevitably  remain  some
disagreement on how they have dealt with reconstructions of the last 1000 years and
there is further work to be done here in the future, but in my judgment, the authors
have made a reasonable assessment of the evidence they have to hand. . . . This has
gone some way towards reconciliation but I sense not everyone is entirely happy.
      With these caveats I am happy to sign off the chapter. . .223

And as far a substantive comment went, this was the totality of Mitchell’s
output as a review editor. He clearly recognised that there was a dispute, but
he seemed to have taken it upon himself to consider whether the authors’
assessment  was  reasonable,  rather  than  whether  they  had  reported  both
sides of the argument as their mandate required them to do. He certainly did
not  offer  an  opinion  as  to  whether  the  authors  had  complied  with  IPCC
procedures.

Jouzel’s report was even more scant:

. . . I can confirm that all substantive expert and government review comments have
been afforded appropriate consideration by the writing team in accordance with IPCC
procedures.223

McIntyre and Holland were both amazed at how perfunctory these reviews
seemed  to  have  been.  The  level  of  attention  that  appeared  to  have  been
given  to  the  process  made  a  mockery  of  the  IPCC’s  claims  about  the
thoroughness of their procedures. Holland therefore decided to make sure
that there was really nothing else to uncover, sending off another request for
supplementary and working papers related to the review.

The reply from Mitchell was again rather surprising. Firstly, it seemed
that there had been no supplementary papers submitted with the review, but
the real surprise was Mitchell’s revelation that he had destroyed his working
papers. There was, he said, no requirement to retain them. This appeared to
conflict  directly  with  IPCC  policies,  which  stated  that  review  comments
would be kept in an open archive for a minimum of five years.

Holland persisted, submitting a series of email requests covering all the
possible  ways  in  which  IPCC  might  be  trying  to  evade  handing  over  the
information.  He  first  asked  for  all  of  the  emails  Mitchell  had  sent  in  his
capacity as an IPCC review editor, but received the extraordinary reply that
there had been none, apart from a few emails to IPCC colleagues prompted

by  Holland’s  initial  request.  So  despite  having  been  in  the  position  of
review editor for several years and despite there being a major difference in
scientific  opinion  in  Mitchell’s  chapter,  he  had  not  actually  managed  to
exchange any emails with the authors.

Holland  made  a  further  request,  which  was  again  stonewalled,  and  he
therefore decided to take a slightly different tack, asking when Mitchell had
destroyed his emails and working papers and requesting that the Met Office
retrieve them from their backups and archives. This new approach seemed
to have the desired effect, since there was a sudden change in tune from the
scientists. Met Office officials were mistaken, it seems, in advising Holland
that  Mitchell’s  emails  had  been  destroyed.  Mitchell’s  work  had  allegedly
been performed in a personal capacity and as such, all of the emails relating
to  his  work  were  therefore  not  disclosable.  As  Holland  persevered,
requesting  details  of  expense  claims  and  information  about  Mitchell’s
holidays,  the  Met  Office  changed  tack  once  again,  invoking  a  derogation
from  the  Freedom  of  Information  Act  that  permitted  information  to  be
withheld  where  its  release  would  affect  British  relations  with  an
international organisation, by which they presumably meant the IPCC.

Meanwhile, Holland had also been pursuing Briffa for information about
the IPCC process and was meeting with a similar wall of obfuscation, not the
least  of  which  was  Briffa’s  failure  to  reply  to  Holland’s  emails.  Holland
had, however, managed to get hold of some of Mitchell’s correspondence,
among which was an email from Briffa in which he had told Mitchell that
he would make a brief reply to Holland ‘when he got round to it’. Tiring of
the  delays,  Holland  took  the  legal  route  once  more,  requesting  the  data
under  the  stronger  terms  of  the  Environmental  Information  Regulations
(EIR), and this time also asking for copies of any correspondence between
Ammann  and  Briffa  on  the  subject  of  the  Fourth  Assessment  Report,  a
request designed to probe the issue of whether Ammann had been providing
Briffa  with 
an
acknowledgement from Briffa’s employers at the University of East Anglia,
which confirmed that his request was being considered under the terms of
EIR.

findings.  He 

unpublished 

promptly 

received 

Then  a  month  later,  he  received  a  refusal  under  the  Freedom  of
Information Act. The grounds given were partly cost and partly because it
was  alleged  that  the  correspondence  was  confidential.  This  was  another
astonishing  claim,  given  that  the  IPCC  review  process  was  allegedly  open

and  transparent.  The  rejection  letter  explained  that  ‘the  persons  and
organisations [i.e. Ammann] giving this information to us . . . believe it to
be confidential and would expect [it] to be treated as such’.224 As the IPCC
was the only organisation that can conceivably have been in a position to
ask for confidentiality, this claim strongly suggested that  IPCC  bureaucrats
had sought to undermine the openness that governments had built into the
operating  procedures  for  the  review.  Either  way  there  was  to  be  no  light
shone  on  Mitchell’s  work  or  on  Ammann’s  review  comments  from  this
corner.a

a  At the time of writing, David Holland continues to press Briffa and Mitchell to release the papers.

The two scientists are still steadfastly refusing.

13     Update the Proxies!

Nature is often hidden, sometimes overcome, seldom extinguished.

Francis Bacon

Mann’s comments
The IPCC’s conclusion that the divergence problem was restricted to a few
proxies in a few geographical areas did not appear to have a firm grounding
in  the  scientific  literature.  Part  of  the  difficulty  in  assessing  the  situation
more rigorously was that, incredibly and despite all the billions of dollars
poured  into  global  warming  research,  virtually  none  of  the  proxy  records
had been updated since 1980. We have seen a couple of exceptions, albeit
problematic ones, in the shape of the Polar Urals and Gaspé updates. Even
the original MBH98 paper only included proxy records up to 1980, lagging
nearly twenty years behind the publication date. By the time of the Fourth
Assessment Report in 2007, the proxies had seemingly still made no further
progress,  with  the  record  apparently  remaining  stuck  at  1980.  Even  the
scale on the IPCC’s spaghetti graph only ran up to the year 2000. If they had
shown the scale right up to date, the proxy records would have ended nearly
30 years short of the date of the report. But by truncating the scale at the
end of the millennium this was reduced to 20 years, helping to obscure the
failure to update the record and at the same time avoiding any discussion of
the lack of a rise in temperature since the end of the millennium.

Early  on  in  the  blog  war  between  Climate  Audit  and  RealClimate,
Michael Mann had posted up a partial explanation of why there had been no
updating of proxy records for nearly 25 years. Responding to a question on
the divergence problem he said:

Most  reconstructions  only  extend  through  about  1980  because  the  vast  majority  of
tree-ring, coral, and ice core records currently available in the public domain do not
extend  into  the  most  recent  decades.  While  paleoclimatologists  are  attempting  to
update  many  important  proxy  records  to  the  present,  this  is  a  costly,  and  labor-
intensive  activity,  often  requiring  expensive  field  campaigns  that  involve  traveling
with heavy equipment to difficult-to-reach locations (such as high-elevation or remote
polar sites). For historical reasons, many of the important records were obtained in the
1970s and 1980s and have yet to be updated.225

Mann’s response clearly covered all the different proxy types, but it is fair
to say that the vast majority of these are tree rings. While McIntyre had not
collected any tree ring samples himself, his career in the mining industry
had  given  him  an  easy  familiarity  with  outof-the-way  places,  and  his  gut
feel  was  that  Mann  was  making  much  more  of  the  difficulties  than  was
justified. Indeed there were hints in the literature that it was considerably
easier to collect tree ring samples than Mann would have his RealClimate
readers  believe.  In  one  of  the  classic  bristlecone  studies,  Lamarche  had
written:

D.A.G. [Graybill] and M.R.R. [Rose] collected tree ring samples at 3325 m on Mount
Jefferson, Toquima Range, Nevada on 11 August 1981. D.A.G. and M.R.R. collected
samples  from  13  trees  at  Campito  Mountain  (3400  m)  and  from  15  trees  at  Sheep
Mountain (3500 m) on 31 October 1983.226

So  if  it  was  possible  to  collect  samples  from  28  trees  on  two  different
mountains  on  a  single  day  in  the  1980s,  it  can  hardly  have  been  very
difficult  to  do  the  same  in  the  twenty-first  century,  especially  now  that
funding  was  pouring  into  climatology.  For  sure  there  were  some  proxies
such as the Himalayan ice cores that would require more work and cost to
update, but the majority should be entirely routine.

McIntyre  had  amused  himself  by  ridiculing  Mann’s  claims  that  these
sites were difficult to reach, quoting extensively from a guidebook to one of
the  bristlecone  sites,  Almagre  Mountain,  an  area  which  was  much
frequented by hikers and cyclists. As he explained:

To get to these sites from UCAR headquarters in Boulder, a scientist would not merely
have to go 15 miles [south-west] of Colorado Springs and go at least several miles
along a road where they would have to be on guard for hikers and beware of scenic
views,  they  would,  in  addition,  have  to  go  all  the  way  from  Boulder  to  Colorado
Springs.  While  lattes  would  doubtless  be  available  to  UCAR  scientists  in  Colorado
Springs, special arrangements would be required for latte service at [Almagre], though
perhaps a local outfitting company would be equal to the challenge. Clearly updating
these proxies is only for the brave of heart and would require a massive expansion of
present paleoclimate budgets. No wonder paleoclimate scientists have been unable to
update these records since Graybill’s heroic expedition in 1983.227

Testing the Starbucks hypothesis
In  2007  McIntyre  started  to  look  anew  at  the  issue  of  the  divergence
problem. Over the previous three years, several of his critics had accused

him  of  attacking  other  people’s  results  rather  than  actually  doing  original
work  of  his  own.  Now,  with  the  NAS  and  IPCC  reports  behind  him,  he
thought  he  saw  an  opportunity  to  disprove  Mann’s  assertions  about  the
difficulties of collecting tree ring samples, shoot down the arguments of his
critics, and at the same time make an important contribution to the climate
debate: he would update the proxies himself. With nearly twenty-five years
of new growth on many of the trees, there was a wonderful opportunity to
answer the question that had plagued dendroclimatology for years – were
the  tree  rings  actually  capturing  any  temperature  information  at  all?  A
whole new verification – an out-of-sample test – could be performed on the
last 25 years simply by collecting a few tree ring samples.

By happy coincidence, McIntyre had a sister living in Colorado Springs,
close to many of the key bristlecone sites, and so, in the summer of 2007,
he arranged to pay her a visit. McIntyre’s idea was to try to find some of the
trees  that  had  been  sampled  by  Graybill  back  in  the  1980s.  Finding  the
exact trees was going to be no easy task because, while their positions had
been mapped and the trees would normally also be marked in some way,
finding these markers was going to take a lot of doing. In order to share the
load and to make more of a party of the trip, he therefore arranged to link
up with a Climate Audit reader, called Pete Holzmann, who lived locally
and had indicated that he was willing to help out. Having obtained a permit
from  the  US  Forest  Service,  the  two  men  hired  an  off-road  vehicle  and
armed themselves with supplies and sampling equipment. Their tongue-in-
cheek idea was to test the hypothesis that it was possible to have coffee at
Starbucks in the morning, sample some tree rings during the day and still be
home  in  time  for  supper.  Much  amusement  was  had  posing  for  photos
outside the local Starbucks before McIntyre and Holzmann, together with
their wives, headed for their chosen site: Almagre mountain.

Getting to Almagre was just as straightforward as they had expected, and
they were able to extract several cores on day one. Finding the original trees
that  Graybill  had  sampled  back  in  the  1980s  was  slightly  more  difficult
though. Modern dendrochronologists use GPS to record the exact position
of  each  tree  they  sample,  but  Graybill’s  work  predated  this  technology.
There was no alternative but to examine each tree in turn, trying to find the
small plaques with which Graybill had marked the trees. Fortunately, after
three days of hunting high and low, Holzmann finally located a group of
marked trees, their identities confirmed in several places by checking their

appearance  to  photos  that  Graybill  had  taken  during  his  research.  By  the
time they returned to Colorado Springs, the Climate Auditors had managed
to take 64 cores from 36 trees at five different locations. They had found 17
Graybill trees and had resampled 8 of these. Mission accomplished.

FIGURE 13.1: Analysis of rings of Almagre tree 84–55

Safely  back  in  Canada,  McIntyre  posted  up  a  report  of  his  trip  for  his
Climate  Audit  readers.  The  samples  were  immediately  sent  off  to  a
professional dendrochronology lab for analysis, and the results trickled out
over the next weeks and months. By October, McIntyre was able to report
the  first  analysis  from  one  of  the  original  Graybill  trees,  and  the  figures
were just as he had expected: tree ring widths, in this tree at least, had been
declining for all of the 1980s and 1990s and into the twenty-first century.
They  had  completely  failed 
twentieth-century
temperature  spurt  that  was  shown  in  the  instrumental  record  (see  Figure
13.1).

to  capture 

the 

late 

FIGURE 13.2: Two cores from tree 84–56

Of  course,  this  was  just  a  single  tree  and  nothing  could  be  concluded
from a sample of one, but it certainly whetted the appetites of the Climate
Audit  readers.  As  further  results  came  in  though,  it  became  rather  more
intriguing. On one particular tree, number 84–56, Holzmann had decided to
extract two separate cores – one from the west and one from the southwest
– and to McIntyre’s astonishment, it turned out that the ring measurements
from these two samples were entirely different. Figure 13.2 shows the ring
analysis of the two cores.

The  explanation  for  this  discrepancy  turned  out  to  be  surprisingly
obvious. When sizing up the tree in question, Holzmann had noticed that
the tree wasn’t in fact circular in cross-section – it was a distinct oval, and
this distortion appeared to have been driven by the fact that the tree was
stripbarked. Stripbarking, you will recall,a is a form of die-back of the bark
on one side of the tree, which can cause a growth spurt on the opposite side
in future years, as the tree compensates for the loss of part of its bark.

What  this  suggested,  then,  was  that  the  growth  spurt  could  have  been
caused, not by temperature changes as proposed by Mann, not by carbon
dioxide fertilisation, as Graybill had mooted all those years ago, but by the
process of the tree compensating for stripbarking. Graybill had reported that
he had actually sought out stripbarked trees, thinking these would show the
effects  of  carbon  dioxide  fertilisation  better.  Was  it  possible  that  he  had
inadvertently documented an entirely different effect? The implications of
McIntyre’s finding would be immense, if repeated across the other trees in
the sample. Remember, the hockey stick shape of Mann’s PC1 was derived
almost  entirely  from  Graybill  bristlecone  pines.  If  the  growth  spurts  that
Graybill had found were due to stripbarking rather than temperature rises,
the Hockey Stick would no longer be credible. Another, perhaps decisive,
nail would be hammered into the coffin of Mann’s paper.b
The Ababneh thesis
As if this were not enough, at around the same time an entirely new source
of  evidence  about  the  origins  of  the  growth  spurt  in  bristlecone  pines
unexpectedly came to light. McIntyre had been aware that one of Malcolm
Hughes’ students had been studying bristlecone pines and he had heard that
this  student  had  updated  the  Sheep  Mountain  chronology  in  2002.

Unfortunately, like so many other proxy updates, it had never appeared in
print, an omission that was surprising in view of the importance of the site
and the paucity of updated proxy records. Once again, McIntyre’s mining
background  coloured  his  judgement  of  why  this  should  be  –  in  his
experience, delayed results usually indicated that the results didn’t give the
‘required’ answer. He had followed this hunch up several times, attempting
to  contact  the  student  in  question,  whose  name  was  Linah  Ababneh.
Unfortunately she failed  to  reply  to  his  emails  and  McIntyre  had  set  this
inquiry aside for another day.

What  was  still  more  amazing  was  that,  while  one  of  Ababneh’s
supervisors  was  Hughes,  her  thesis  contained  not  a  mention  of  the
divergence  problem.  Hughes  did  not  even  seem  to  have  required  her  to
attempt to reconcile her findings to Graybill’s, an amazing omission given
his intimate knowledge both of Graybill’s work and its importance in the
Hockey Stick papers. And all the while, Hughes was still publishing papers
that used the old Graybill version of the data (with the hockey stick shape).d

Then  one  day,  while  trawling  through  Hughes’  website  to  see  if  there
was anything new to read, he chanced upon a web page dedicated to the
PhD theses of students in Hughes’ department. And there, at the top of a list
he  noticed  the  newly  published  thesis  of  Linah  Ababneh.228  It  was
breathtaking.

Ababneh’s  project  had  involved  an  investigation  of  the  differences
between  the  stripbark  and  wholebark  trees,  looking  specifically  at  sites
visited by Graybill and Idso. She had visited two sites, Sheep Mountain and
Patriarch Strip, sampling both types of tree in both sites. What her thesis
showed was that during the 1980s and 1990s there was no sign of the surge
in  ring  widths  that  should  have  accompanied  the  rise  in  instrumental
temperatures,  a  clear  confirmation  of  the  divergence  problem.  But  the
implications  were  even  more  serious  than  that.  When  you  compared  her
Sheep  Mountain  results  to  Graybill’s  figures  for  the  same  site  and  to  the
earlier  Lamarche  study,  it  was  immediately  apparent  that  her  new  results
didn’t even confirm the existence of the growth spurt that the two earlier
researchers  had  reported.  Figure  13.3  shows  Ababneh’s  Sheep  Mountain
update  alongside  Graybill’s  results  for  the  same  location.  There  is  no
hockey stick shape shown in new figures, and remember, Mann’s PC1 was
dominated by Sheep Mountain.c The implications were of huge importance
to climate science and to the political world.

To McIntyre and his readers, this was simply unbelievable. Why would he
not use his own student’s more-up-to date data, or at the very least, discuss
it in his publications?

FIGURE 13.3: The Ababneh update to Sheep Mountain
Black: Ababneh update; grey: Graybill

The significance of these findings is hard to overestimate: not only had
Mann used incorrect statistics to force his temperature reconstruction into
the  hockey  stick  shape  of  the  bristlecone  pines,  but  it  now  appeared  as
though that shape was merely an artefact of stripbarking. Nor was MBH98 the
only paper where Sheep Mountain was a key proxy. It was used in MBH99
and Mann and Jones 2003 as well. From these papers it had also found its
way into Rutherford et al, which used Mann’s PC1, and to Hegerl et al 2006
and Osborn and Briffa 2006, both of which used the Mann and Jones PC1.

Ababneh had indicated in her thesis that her underlying data would be
archived in due course. However, by the time McIntyre noticed her thesis,
there was still no sign that she had done this and so he decided to contact
her  directly.  Since  the  completion  of  her  doctorate,  Ababneh  had  left  the
University of Arizona and had moved to a new base at William and Mary
College in Virginia. Fortunately McIntyre was able to find an email address
for her and sent off a brief message enquiring after the data and asking a
few questions about her work. There was no reply, and further enquiries at
the  University  of  Arizona  determined  that  Ababneh’s  data  was  no  longer
held there. So, in an eerie echo of the farce of the Gaspé series, the data for
an updated tree series, which was found to have lost its former hockey stick
shape, had not been archived and had then promptly been lost.

Shortly  afterwards,  a  Climate  Audit  reader  managed  to  make  contact
with Ababneh by telephone. Obliged to respond, Ababneh said, somewhat
implausibly, that her attorney had advised her that McIntyre’s approach had

been ‘improper’ and that she should not supply the data.230 When McIntyre
tried again to make email contact, his messages were returned undelivered,
presumably due to his IP address having been blocked.e

Without  Ababneh’s  data,  it  was  tricky,  but  not  impossible,  to  make
further progress. Hans Erren,f who was still involved in the sceptic efforts,
digitised the graphical representations of the data in Ababneh’s thesis, and
while  this  wasn’t  quite  as  good  as  having  the  actual  data,  it  did  allow
McIntyre  to  make  a  powerful  case.  For  example,  when  he  recreated  the
Mann  and  Jones  PC1  using  the  updated  Sheep  Mountain  chronology,  he
found  that  the  Hockey  Stick  disappeared  entirely.  The  effect  is  shown  in
Figure  13.4.  The  top  chart  includes  the  original  Graybill  data,  processed
through a Mannian short-centred algorithm, which produces a pronounced
hockey stick shape. The lower chart shows what happens when the Graybill
numbers  are  swapped  for  figures  derived  from  the  Ababneh  thesis  and  a
standard centring algorithm is used: the hockey stick shape disappears.

FIGURE 13.4: Mann & Jones PC1 with different versions of Sheep Mountain

Top: Using Graybill version and short centring.

Bottom: Using Ababneh update and standard centring.

The results now seemed unarguable. Evidence from multiple sources was
now demonstrating unequivocally that the bristlecone pines, which gave the
Hockey  Stick  its  attention-grabbing  shape,  were  not  in  fact  capturing
temperature  information  at  all.  Surely  now,  after  so  many  years  of  work,
after  so  many  flaws  had  been  uncovered  and  demonstrated  to  the
satisfaction of all but its most dogmatic supporters, surely now the Hockey
Stick was broken once and for all.

a  See page 172.
b  McIntyre presented the results of his trip at the AGU Fall Meeting in 2007, but has yet to publish

the findings in a journal.

f  See page 55.

c  See page 121.
d  See, for example, Hughes and Salzer 2007, which includes the Graybill version of Sheep Mountain

rather than the Ababneh update.229

e    McIntyre  suggested  he  thought  that  Ababneh  was  a  pawn  whose  actions  should  be  viewed
sympathetically: ‘I really don’t want to discuss Linah Ababneh’s decisions or motives any more;
she’s young and trying to make her way in the climate community.’230

14     A New Hockey Stick

When you’re in a hole, stop digging.

Denis Healey

By  2007,  there  was  little  more  for  McIntyre  and  McKitrick  to  discover
about the Hockey Stick. Certain areas remained a mystery – the confidence
interval  calculations  and  the  basis  on  which  Mann  had  retained  PCs,  for
example – but without the rest of Mann’s code, these were unlikely ever to
be understood. Climatology is a big subject, however, and there were other
areas  into  which  McIntyre  would  delve  from  time  to  time,  often  with
amazing results.

One  particularly  fruitful  subject  was  the  quality  of  the  instrumental
temperature records and the code used to process the weather station data
into headline figures. With the help of a ragtag group of occasional climate
auditors, McIntyre started asking probing questions about every aspect of
the temperature records. As far back as 2005, he had unearthed a significant
and embarrassing error in the adjustments the UK’s Climatic Research Unit
had been making to its sea surface temperatures. In 2007, another error was
found, this time in James Hansen’s NASA surface temperature records. This
discovery led to a reassessment of how hot the 1990s were in relation to the
rest of the instrumental record for the USA. Hansen had previously claimed
that  the  1990s  were  hotter  than  anything  seen  previously,  but  with  the
correction in place, it emerged that it had in fact been hotter in the 1930s.
The findings were seen by sceptics as something of a coup for McIntyre, as
they led to another round of newspaper headlines around the world.231–233
Shortly  afterwards,  all  the  media  attention  forced  Hansen  to  issue  a
correction  and  to  finally  release  his  long-withheld  computer  code.  This
quickly caused another embarrassment when observers noted that the code
was  both  extraordinarily  labyrinthine  and  also  very  badly  documented.
Eyebrows were raised by participants on both sides of the global warming
debate.

Meanwhile, another close associate of the Climate Audit website, former
TV  weatherman  Anthony  Watts,  began  a  volunteer  effort  to  survey  the
hundreds of weather stations that were the basis of the land record for US

temperatures.  The  poor  quality  of  the  siting  and  maintenance  of  the
majority  of  the  stations  again  raised  questions  over  how  reliable  the
instrumental record really was.

By 2007, Climate Audit had become a hub for global warming sceptics,
where news and opinion were exchanged alongside discussion of the results
of the many research projects undertaken by McIntyre and the readers. But
it was more than just that. Guest writers, among them several professional
scientists, were posting articles on subjects as diverse as hurricanes, polar
ice records and climate models. Even some prominent climatologists started
to risk the opprobium of the research community by posting comments and
engaging  in  the  debate  at  Climate  Audit.  Meanwhile,  the  recognition  of
McIntyre’s work by many professional scientists continued to develop and
he now received regular invitations to lecture at seminars around the world.
With so much expertise now assembled around him, McIntyre’s humble
website had itself been transformed into something resembling a series of
scientific  seminars.  Despite  its  unstructured  nature,  which  critics  might
characterise 
community  was
demonstrating a remarkable ability to get to the bottom of difficult scientific
questions. In 2008, the Climate Auditors had a chance to show just what
they could do.

‘haphazard’,  McIntyre’s  online 

as 

On  the  morning  of  2  September  2008,  McIntyre  posted  a  terse  notice,
informing  his  readers  that  there  were  media  reports  of  a  new  Mann
temperature  reconstruction,  which  was  shortly  to  be  published  in  the
Proceedings of the National Academy of Sciences (PNAS).234 Over the next
few hours some of the details of what was in the study started to creep out.
It was soon obvious that this was going to be a serious attempt to breathe
life back into the Hockey Stick. Despite all the evidence that proxy records
were not reaching new levels in the twentieth century, Mann was apparently
still  maintaining  that  modern  temperatures  were  unprecedented:  the  new
study  was  another  hockey  stick,  this  time  suggesting  that  modern
temperatures  were  the  highest  they  had  been  in  1300  years.  Mann  was
certainly  not  one  to  throw  in  the  towel.  However,  knowing  that  most
proxies were not hockey stick shaped, McIntyre and his readers could be
fairly  certain  that  the  shape  of  the  new  study  was  either  erroneous  or
contrived. They just had to uncover how it had been done.

From a public relations point of view, the big selling point of the new
paper  was  to  be  a  claim  that  Mann  had  been  able  to  get  a  hockey  stick

without  using  tree  rings.  This  was  obviously  important  in  view  of  the
controversy  over  the  reliability  of  tree  ring  proxies  in  general  and  the
bristlecones in particular. As Mann explained in the news release:

Ten years ago, we could not simply eliminate all the tree-ring data from our network
because  we  did  not  have  enough  other  proxy  climate  records  to  piece  together  a
reliable global record.235

This  was  a  curious  statement.  For  the  previous  ten  years,  Mann  had
maintained  that  the  Hockey  Stick  was  reliable,  and  moreover  that  it  was
‘robust  to  the  presence/absence  of  dendroclimatic  indicators’.14  He  had
even maintained this stance in the face of strong evidence, in the form of
the CENSORED directory, that this was not so and that he knew it was not so.
The media were ready and waiting for the new paper. The BBC was first
out  of  the  blocks,  reporting  that  the  ‘Climate  “hockey  stick”  is  revived’,
closely followed by Canada.com, who reported that ‘Past decade warmest
in 1,300 years’ and the Christian Science Monitor whose headline read ‘A
gnarlier “hockey stick”, the same message’.236–238 Nature was of course in
the  forefront  of  the  media  barrage,  predicting  that  the  new  paper  would
silence Mann’s critics.239

While  the  media  had  received  their  press  releases  in  advance,  the
Climate  Auditors  were  caught  unawares,  but  within  minutes  of  McIntyre
posting up his report, sceptical readers were swinging into action. At first
there were only the news reports to look at, but by mid-morning the paper
itself had been posted online and, by eleven a.m., McIntyre was linking to
some early comments on the article by one of his regular correspondents, a
Czech  theoretical  physicist.  Just  minutes  later,  Demetris  Koutsoyiannis,
professor  of  hydrology  at  the  University  of  Athens,  also  posted  some
observations on the shape of the reconstruction.

One hopeful sign in the paper was a statement indicating that Mann had

taken on board the findings of the NAS panel:

We were guided in this work by the suggestions of a recent National Research Council
report  [i.e.  the  NAS  report]  concerning  an  expanded  dataset,  updated  data,
complementary  strategies  for  analysis,  and  the  use  of  thoroughly  tested  statistical
methods.234

This was encouraging as far as it went, although it rather alarmingly didn’t
mention the panel’s warnings about the unsuitability of bristlecone pines for
use in temperature reconstructions. Would Mann dare to use bristlecones in
the new paper? Only time would tell.

To  everyone’s  surprise  the  data  and  code  appeared  to  have  been
published alongside the new paper, in an apparent triumph for McIntyre’s
ceaseless  efforts  to  improve  standards  in  this  area.  Climate  Audit  readers
busied  themselves  with  downloading  everything  they  could.  Interesting
details were picked up and posted with bewildering speed. Some of Mann’s
new reconstructions seemed to have much larger Medieval Warm Periods
than  MBH98.  The  R2  was  still  not  being  used.  McIntyre  pointed  out  that
Mann was still using a method that fished for proxies with correlations to
temperature,  with  all  the  attendant  risks  of  spurious  correlation  and  the
creation of a hockey stick entirely from noise.

Craig Loehle, the ecologist we met in Chapter 10, noticed something odd
in the smoothing of the instrumental records. His observation was picked up
within  the  hour  by  a  retired  professor  of  statistics.  Somehow,  Mann  had
managed to get a forty-year smooth of the instrumental record to extend up
to the present day. By rights it should not have been possible to get past the
1960s,  because  the  smoothing  formula  replaces  the  raw  value  for  a  data
point with a weighted average of values prior to and after it. By evening
time, another statistician had worked out the extraordinary story of how the
instrumental record had been artificially extended by padding the data for
many years into the future, the future values created by repeating the most
recent real values in reverse order.

Several readers raised the subject of the new paper’s peer review. At the

end of the paper, was the following acknowledgement:

We are indebted to G. North and G. Hegerl for their valuable insight, suggestions, and
comments and to L. Thompson for presiding over the review process for this paper.234

This suggested strongly that Gerry North and Gabriele Hegerl had been the
peer  reviewers  of  the  paper,  something  that  might  have  been  considered
rather  unfortunate  in  view  of  Wegman’s  criticisms  of  the  paleoclimate
community being too insular. The absence of a statistician among the peer
reviewers also seemed inappropriate. However, with North having headed
the  NAS  panel,  there  was  at  least  an  expectation  that  he  would  bring  his

Unfortunately,  within  days  of  the  paper’s  publication,  McIntyre  was
reporting  that  most  of  the  ‘usual  suspects’  were  included  in  the  proxy
database:  Briffa’s  Tornetrask  record  had  been  used  instead  of  Grudd’s
update, Yamal replaced the Polar Urals update once again. And as everyone
feared, the bristlecones were there too in the shape of Sheep Mountain, the
obsolete hockey stick version used instead of Ababneh’s update. This was
remarkable when one realises that Hughes, who had supervised Ababneh’s
thesis,  was  a  co-author  on  the  new  paper.  He  must  have  known  that
Ababneh’s  work  failed  to  replicate  Graybill’s.  The  inclusion  of  the  series
was even more extraordinary when one realises that it flew in the face of
the NAS panel’s recommendation against the use of bristlecones, and the fact
that North failed to spot this fatal flaw during his peer review.

There were some new data series as well, and McIntyre started to post
graphs of these for his readers to view. They were devoured, several people
noticing  that  few  if  any  of  them  showed  any  modern  warming.  The  sole
exception among the first batch was a group of four lake sediment series
from  Finland,  known  as  the  Tiljander  proxies.  Readers  delved  and  dug,
locating the original paper in which the series had been described and also
the PhD thesis of its author, Mia Tiljander. It turned out that the twentieth
century uptick in Tiljander’s proxies was caused by artificial disturbance of
the sediment caused by ditch digging rather than anything climatic. Mann
had acknowledged this fact, but then, extraordinarily, rather than reject the
series, he had purported to demonstrate that the disturbance didn’t matter.
The way he had done this was to perform a sensitivity analysis, showing
that you still got a hockey stick without the Tiljander proxies.

experience  to  bear  and  that  many  of  the  problems  with  MBH98  would  be
avoided in the new paper.

Great care is needed when reading scientific papers, particularly in the
field  of  paleoclimate,  and  this  was  one  of  the  occasions  when  one  could
have come away with an entirely wrong impression if the closest attention
had not been paid. The big selling point of Mann’s new paper was that you
could  get  a  hockey  stick  shape  without  tree  rings.  However,  this  claim
turned  out  to  rest  on  a  circular  argument.  Mann  had  shown  that  the
Tiljander  proxies  were  valid  by  removing  them  from  the  database  and
showing that you still got a hockey stick. However, when he did this test,
the  hockey  stick  shape  of  the  final  reconstruction  came  from  the
bristlecones.  Then  he  argued  that  he  could  remove  the  tree  ring  proxies

(including the bristlecones) and still get a hockey stick – and of course he
could, because in this case the hockey stick shape came from the Tiljander
proxies.  His  arguments  therefore  rested  on  having  two  sets  of  flawed
proxies  in  the  database,  but  only  removing  one  at  a  time.  He  could  then
argue that he still got a hockey stick either way. As McIntyre said, you had
to watch the pea under the thimble.

As  the  readers  dug  on,  another  of  the  sceptics’  favourite  proxies
reappeared. Briffa’s tree ring density series turned up like a bent penny in
Mann’s  new  paper.  Once  again  the  inconvenient  divergence  had  been
truncated at 1960. This time though there was a remarkable new twist to the
story. The inclusion of the Briffa series would have presented Mann with a
problem  because  he  needed  data  that  ran  right  up  to  the  present  day.
However, Mann had a trick up his sleeve. This involved a new approach to
the problem of filling in gaps in the proxy series. We have seen how, where
there was a gap in a series in MBH98, Mann had extended the final value of
the series up to the year 1980. In the case of Gaspé he had copied the value
from the year 1404 into earlier years, extending the start date of the series
backward to the year 1400. In the new paper, however, he adopted a more
sophisticated methodology.

The RegEM algorithm of [Tapio Schneider] was used to estimate missing values for
proxy series terminating before the 1995 calibration interval endpoint, based on their
mutual  covariance  with  the  other  available  proxy  data  over  the  full  1850–1995
calibration interval.234

In  other  words  the  missing  data  had  been  infilled  using  a  mathematical
algorithm, which looked at the other series and calculated a likely value for
the missing data. While the question of infilling data in this way is fraught
with difficulty at the best of times, the effect in the case of the Briffa series
was remarkable. Here, there was no missing data anyway, or at least there
wouldn’t  have  been  if  the  inconvenient  downward  trending  twentieth
century hadn’t have been deleted. However, with the truncation in place, the
RegEM algorithm infilled the gap it had created with a new, upward trending
set of data points. The downtick had become an uptick. This procedure had
passed peer review. Climate Audit readers were speechless.

Another bombshell the same day was the discovery that any comments
on the paper had to be submitted within three months of publication. After
that, no contributions would be accepted. If McIntyre and McKitrick were

going  to  submit  a  critique  of  the  new  Mann  paper,  they  and  the  Climate
Audit readers were going to have to pull out all the stops.

Over the next month information poured in. A breakthrough was made
when one reader, who had been studying Tiljander’s Finnish lake sediment
study,  noticed  that  Mann  had  made  an  extraordinary  error.  He  had
misinterpreted  the  way  the  sediment  responded  to  temperature  changes.
Tiljander and her colleagues had explained in their original study that a high
x-ray  density  measurement  in  the  sediment  was  indicative  of  lower
temperatures,  as  higher  snowfall  led  to  a  bigger  spring  melt,  increased
erosion and so to a higher mineral content in the water.240 But as we saw
above,  twentieth  century  ditch  digging  had  also  caused  higher  mineral
content  in  the  water.  Mann’s  new  algorithm,  however,  had  picked  up  the
high  x-ray  density  this  had  caused  and  matched  it  up  against  global
temperatures. Finding a good correlation, it had therefore interpreted high
x-ray  density  as  evidence  of  higher  global  temperature.  So  not  only  did
ditch digging cause the uptick rather than climate, but Mann’s interpretation
of the series was also upside-down.

In fact, Tiljander had noted evidence of a strong Medieval Warm Period
in the record, something that Mann had presumably missed because he was
looking at it upside-down. Finnish TV had even broadcast a documentary
on  Finnish  climate  history,  including  the  Tiljander  series  as  evidence  of
medieval warmth. The point was somewhat moot because, as we have seen,
the twentieth century uptick in the series (or from Tiljander’s perspective,
downtick)  was  due  to  ditch  digging  rather  than  climatic  factors,  but
nevertheless it was an embarrassing error for Mann and one that threatened
to  undermine  his  non-tree  ring  reconstructions,  which  appeared  to  be
strongly influenced by the Tiljander proxies.

Some strange screening of the proxy series was also uncovered. Mann
had reported that he had rejected proxy series that didn’t have a significant
correlation to temperatures in their local gridcell – there were thresholds set
for each type of proxy. However, it was actually slightly more complicated
than  that.  If  the  series  failed  the  test  of  correlation  to  the  local  gridcell,
Mann  gave  them  another  chance  by  testing  the  correlation  to  the  next
nearest  gridcell.  Then,  one  of  McIntyre’s  professional  statistician  readers,
who was trawling through Mann’s code, stumbled across yet another oddity.
On some of the reconstructions it turned out that Mann had used a different
screening  procedure,  simply  removing  any  series  that  had  a  negative

It  went  on  .  .  .  and  on.  Among  the  new  proxy  series,  readers  were
astonished  to  see  one  that  was  apparently  a  documentary  record  of
temperatures  in  East  Africa  dating  back  to  1400.  If  true  this  would  have
overturned everything known about the history of the continent, but it was
quickly discovered that Mann had inadvertently swapped the latitude and
longitude,  and  the  series  should  have  been  located  in  Spain.  So  in  an
amusing echo of the ‘rain in Maine falling in the Seine’, it looked as though
‘the  rain  in  Spain  was  falling  mainly  in  the  plains’  of  Kenya.  It  became
positively farcical when it was discovered a short time later that the proxy
wasn’t a documentary record at all – it really was a rainfall record.241 It was
doubly surprising when one notes that this series had apparently passed the
test of correlation with its gridcell temperature record. This test seemed less
than plausible if one could test it against the wrong gridcell and still have it
pass.

In  the  event,  someone  on  Mann’s  team  was  clearly  reading  Climate
Audit  because  the  error  was  corrected  and  a  revised  reconstruction  was
posted to Mann’s Supplementary Information site. The correction led to a
change of 0.5°C in the eighteenth century. As McIntyre noted, Mann had
claimed  at  the  NAS  hearings  that  he  knew  temperatures  right  back  to  the
fifteenth  century  to  an  accuracy  of  0.2°C,  a  position  that  now  looked
increasingly untenable.

correlation to temperature. However, whichever way you looked at it, these
screenings  would  essentially  only  give  series  with  upward  trending
twentieth century records. As we saw on page 300, if you add such series
together, you will get a hockey stick, simply because the twentieth century
portions  add  together  to  give  the  blade,  while  the  ups  and  downs  in  the
earlier sections cancel out to give the long flat handle. Mann should have
taken  this  into  account  in  calculating  his  new  benchmark  significance
levels, but it appeared that he had not done so.

There  was  so  much  ammunition  that  it  was  hard  for  McIntyre  and
McKitrick to know where to start when they sat down to write their formal
comment on Mann’s new paper.242 This excess was a problem because as
well as demanding that comments be received within three months of the
publication  of  the  original  article,  PNAS  also  imposed  a  maximum  of  250
words.  However,  the  restrictions  did  at  least  force  the  two  men  to
concentrate only on the most significant issues. It was eventually decided
that they would discuss confidence intervals briefly, pointing out that using

conventional  statistical  methods  they  could  show  that  Mann’s  uncertainty
bounds  were  infinitely  large  prior  to  1800  –  in  other  words  that  his  new
reconstruction was of no use prior to that date. Apart from this, they would
restrict themselves to listing the most important of the remaining issues –
the calibration process producing hockey sticks from red noise, the strange
screening  process,  proxies  incorporating  thermometer  data,  the  upside-
down Tiljander proxies, and the use of proxies which were not responding
to  temperature,  including  the  bristlecones.  A  short  comment  was  duly
composed and submitted to the journal. At the end of 2008, the comment
was accepted for publication, the journal indicating that Mann and his co-
authors would be invited to submit a reply.

Mann’s reply appeared at the start of February 2009. McIntyre described
it as ‘amusing’. With as little space to develop his arguments as McIntyre,
Mann  appeared  to  be  content  to  brush  most  of  the  Canadians’  critique
aside.243 He did, however, manage to include his usual robust rejections of
any criticism:

McIntyre and McKitrick raise no valid issues regarding our paper. . .
McIntyre and McKitrick’s claim . . . is unsupported in peer-reviewed literature and
reflects an unfamiliarity with the concept of screening regression/validation . . .
The claim that upside down data were used is bizarre . . . McIntyre and McKitrick
misrepresent [the NAS] report . . . In summary their criticisms have no merit . . .

Mann’s claim that McIntyre had misrepresented the NAS panel was also very
surprising,  since  the  report  had  been  unequivocal:  ‘strip-bark  samples
should be avoided for temperature reconstructions’.153 Mann appeared to be
relying  on  Wahl  and  Ammann’s  Climatic  Change  paper  to  support  this
position.  Ammann  had  made  a  fairly  weak  attempt  to  defend  the  use  of
bristlecones,  arguing  that  Mann’s  approach  did  not  require  proxies  to
correlate with their local temperature at all, the idea being that trees could
capture  temperature  signals  from  far  away  by  means  of  teleconnections.a
How Mann thought that this argument was tenable in a paper where proxies
were specifically screened for correlation to local temperatures is unclear.
With so few words to put his case though, any meaningful explanation was
impossible.

Mann’s approach to the upside-down Tiljander proxy took a similar line.
He  appeared  to  be  arguing  that  it  didn’t  matter  because  the  upside-down
proxy correlated to temperature and that was all his algorithm cared about.

If this was his case then it was strange indeed, because without any physical
mechanism to justify the proxy shape, all Mann had was a correlation. As
McIntyre had observed right back at the start of the story, just because you
could get a correlation between interest rate futures and temperature didn’t
mean that you could reconstruct historic temperatures from data extracted
from a Bloomberg terminal.b

It  was  the  same  for  Mann’s  claim  about  McIntyre’s  comments  on
screening. Despite Mann’s assertion that McIntyre’s claim was unsupported
in the literature, this was the essence of Bürger and Cubasch’s paper.c Those
of a suspicious frame of mind could also note that Mann didn’t actually say
that McIntyre’s claims were wrong, although again, it is difficult to be sure,
since the word count was so short.d

a  See page 47.
b    The  use  of  the  Tiljander  proxies  in  an  inverted  orientation  was  repeated  in  a  later  paper  by
Kaufman et al.,244 its author team including many familiar names from this story, such as Bradley,
Briffa,  Overpeck,  Ammann  and  Otto-Bleisner  (but  not  Mann).  Speaking  of  this  paper,  one
prominent expert in the field, in no way a sceptic, made the following observation: ‘Proxies have
been  included  selectively,  they  have  been  digested,  manipulated,  filtered,  and  combined,  for
example,  data  collected  from  Finland  in  the  past  by  my  own  colleagues  has  even  been  turned
upside down such that the warm periods become cold and vice versa’.245 Subsequent to this error
being pointed out, Kaufman agreed that he had made a mistake and issued a corrigendum. Mann
has yet to follow suit.

c  See page 299.
d  At time of writing, McIntyre continues his research into Mann’s new hockey stick, in particular the

methodologies used.

15     The Meaning of the Hockey Stick

When later generations learn about climate science, they will classify the beginning
of the twenty-first century as an embarrassing chapter in the history of science. They
will  wonder  about  our  time  and  use  it  as  a  warning  of  how  the  core  values  and
criteria of science were allowed little by little to be forgotten, as the actual research
topic of climate change turned into a political and social playground.245

Atte Korhola, Professor of Environmental Change
University of Helsinki

The  Hockey  Stick  and  the  independent  confirmations  of  it  appear  to  be
fatally  flawed.  We  have  seen  two  expert  panels  agree  that  Mann’s  short
centring methodology was wrong. We have learned that bristlecones are in
near-universal  use  in  millennial  temperature  reconstructions  even  though
there  is  widespread  agreement  that  they  are  not  reliable  temperature
proxies.

Does it matter that the Hockey Stick was wrong? Does it matter if all the
millennial temperature reconstructions are wrong? Or even if we can never
know what the temperature of the past was? What does the Hockey Stick
affair  tell  us  about  the  IPCC  and  the  way  that  professional  climatologists
operate? Does any of what you have just read matter?
Relying on peer review
The  Hockey  Stick  was  a  peer-reviewed  paper,  published  in  one  of  the
world’s  most  prestigious  scientific  journals.  It  passed  another  allegedly
much  more  detailed  review  on  its  way  to  the  position  of  prominence  it
attained in the  IPCC’s Third Assessment Report. How was it that so many
leading  climatologists  failed  to  notice  its  many  flaws?  What  were  these
panels  of  experts  thinking  of?  Before  we  can  answer  these  questions,  we
need to understand a little about the peer review system: how it evolved and
how it actually works in practice.
History of peer review
Peer review is as old as science. There are traces of scientists’ work being
reviewed by their fellows in sources as far back as the writings of the great
Arab physicians of the Middle Ages; European examples are known from
the  earliest  days  of  the  Enlightenment.  It  will  surprise  many  readers,
however,  to  learn  that  peer  review  was  only  rarely  used  in  mainstream

Western  scientific  publication  before  the  middle  of  the  twentieth  century,
and that some of the greatest works of Western science made their way into
the  literature  without  a  peer  reviewer’s  imprimatur.  Notable  examples
abound and they include the great works of Albert Einstein from 1905 and
Watson and Crick’s paper on the structure of DNA from 1953. Einstein’s
Annalen  der  Physik  papers  were  reviewed  by  the  journal’s  editors,  Max
Planck and Wilhelm Wien, but were given the nod on their say-so alone.
John Maddox, the editor of Nature stated that no review of the paper was
necessary  because  it  was  self-evidently  correct.a  This  was  very  much  the
way that journals operated at the time, with reviewers called in to provide
input  on  the  suitability  of  a  paper  for  publication  only  as  and  when  the
editors felt it necessary.

Since  that  time,  procedures  have  tightened  up  considerably,  so  that
nowadays, nearly all papers are reviewed, but the fact that peer review used
to  happen  only  when  required  tells  us  something  rather  profound  about
what it was designed to do. Before the growth in its use in the second half
of the twentieth century, it was assumed that scientific papers were suitable
for publication, based on the review of the journal editor. Peer review was a
process  that  dealt  with  the  exceptions  –  those  papers  where  the  editor
required specialist input or a second opinion in order to decide if the paper
should  go  forward.  Sign-off  by  peer  reviewers  did  not  and  does  not
automatically make a scientific paper correct, a point made eloquently by
Mann in his RealClimate posting ahead of McIntyre’s two papers in 2005:
‘a necessary but not sufficient condition’ was the way he put it.97 In fact, it
is fairly clear that even this is going too far: if Einstein, Watson and Crick,
and nearly everyone else before 1950 could all get by without peer review,
then it seems fairly clear that in terms of discovering the truth, it is not even
a necessary condition.
So what is peer review for then?
What then does peer review do for society? The answer seems to be that it
achieves very little for society. In fact, most of the benefits of the process
seem to attain to the journals rather than to society at large. Journals are all
seeking the best, the most significant scientific papers. They want articles
that are important and perhaps newsworthy too. This kind of paper will sell
journals  and  will  bring  in  the  subscriptions,  so  one  of  the  principal
objectives of peer review is to gauge how important a paper is. As far as

this  end  is  concerned,  peer  review  probably  functions  quite  satisfactorily.
However, journals also want to avoid publishing papers containing errors
and  so  a  second  objective  for  a  peer  reviewer  is  to  identify  errors.  Here,
there can be little doubt that peer review is not up to the job.

Nobody  really  knows  how  many  scientists  perform  their  reviews
carefully, how many merely skim the papers and how many just give them
the nod, but attempts have been made to provide an answer to this question.
Medical journals have been at the forefront of research into the efficacy of
peer  review  as  a  way  of  identifying  scientific  error  and  their  conclusions
have  been  largely  unfavourable.  Richard  Smith,  a  former  editor  of  the
British Medical Journal (BMJ) has been one of the most prominent critics
of peer review. As he puts it:

We have little evidence on the effectiveness of peer review, but we have considerable
evidence of its defects. In addition to being poor at detecting gross defects and almost
useless for detecting fraud, it is slow, expensive, profligate of academic time, highly
subjective, something of a lottery, prone to bias and easily abused.246

Smith’s successor at the BMJ, Fiona Godlee, seems to share his less than
fulsome  opinions  of  the  efficacy  of  peer  review.  She  and  her  colleagues
performed  a  trial  in  which  eight  errors  were  inserted  into  a  genuine
manuscript,  which  was  then  sent  out  to  420  reviewers.  Of  the  221  who
responded,  nobody  spotted  more  than  five  of  the  mistakes,  the  typical
reviewer spotted only two and a sixth of the respondents missed all eight.247
The reasons for these failures become clear when we consider the nature
of  a  peer  review.  A  peer  review  normally  consists  only  of  reading  a
scientific  manuscript  through.  It  does  not  involve  obtaining  the  data,
reviewing  the  code  or  reperforming  calculations.  Peer  review  is  not  due
diligence  in  the  way  a  business  auditor  would  understand  the  term,  and
there is no pretence by journals that it is. It works as if an auditor read the
company’s annual report but did not actually examine any of the underlying
transactions  and  estimates.  He  might  be  able  to  offer  an  opinion  on  the
appropriateness  of  the  company’s  stated  policy  on  providing  for  obsolete
inventory but he would be unable to comment on whether that policy had
been applied in practice or, if it had, whether it had been applied correctly.
In  fact,  even  a  traditional  business  audit  only  claims  to  give  ‘reasonable
assurance’ that a set of accounts are not materially misstated, so it is hard to

see how the considerably more cursory checks of a peer review provide any
comfort to the reader at all.

Yet despite this, politicians and the public seem somehow to believe that
the fact that a paper has passed peer review means that it is correct. There
appears to be a striking disconnect between the scientific community and
the  politicians  who  rely  on  their  findings  to  inform  important  policy
decisions.  As  McKitrick  put  it  in  a  paper  he  co-authored  with  Bruce
McCullough  of  Philadelphia’s  Drexel  University,  ‘some  government  staff
are  surprised  to  find  out  that  peer  review  does  not  involve  checking  data
and calculations, while some academics are surprised that anyone thought it
did’.248

The supposedly more rigorous process of expert panel review of single
issues is little better. As we have seen, expert panels are easily packed with
scientists  of  the  ‘correct’  opinions,  dissenters’  views  can  be  ignored  or
suppressed, and reports can be biased to give the answer that is required.
We  saw  in  Chapter  9  how  McIntyre’s  attempts  to  get  someone  with
statistical expertise installed on the NAS panel was sidestepped by means of
appointing  people  closely  associated  with  the  Hockey  Team,  who  then
signally failed to ask the pertinent questions. Statistician Jim Zidek, writing
in  the  Journal  of  the  Royal  Statistical  Society  (JRSS),  has  noted  with
bewilderment  the  absence  of  statistical  expertise  among  IPCC  experts,  an
oversight  that  is  particularly  damning  in  the  area  of  paleoclimate,  where
signal  processing  and  statistics  are  the  main  areas  of  contention.249  How
can it be right that Mann, who has freely admitted to not being a statistician,
was  felt  to  be  an  appropriate  overseer  of  the  panel’s  paleoclimate
deliberations?

Assembling even the most illustrious panel around a table and ‘winging
it’ through the relevant scientific literature is surely unequal to the task of
discovering  the  truth,  particularly  through  the  fog  of  a  scientific  debate:
after one set of peer reviewers have reviewed a paper during the course of
its publication at the journal, very little is added by having another set of
experts  read  it  through  again.  The  findings  of  expert  panels  are  often
themselves peer reviewed, adding yet more spurious authority to the results,
but with no more likelihood of establishing the truth of a scientific question.
In  the  case  of  the  IPCC  reports,  national  science  academies  have  also
weighed  in,  issuing  statements  of  support  for  the  findings  and  further

protecting the ‘consensus’ from any challenge. But on what basis do they
do this? At best, yet another layer of peer review.

The inadequacies of peer review as the basis for finding fraud and error
in  scientific  papers  appear  clear  and  well  established  in  the  scientific
literature.  Both  sides  of  the  global  warming  debate  apparently  agree  that
peer review is ‘a necessary but not sufficient condition’. Yet peer review is
the  only  oversight  there  is  of  the  validity  of  the  scientific  case  for
catastrophic  manmade  global  warming  and  on 
this  flimsy  basis
governments  make  far-reaching  policy  decisions  that  affect  everyone  and
will continue to affect our children for decades into the future.
Sound science
The Hockey Stick affair is not the first scandal in which important scientific
papers  underpinning  government  policy  positions  have  been  found  to  be
non-replicable – McCullough and McKitrick review a litany of sorry cases
from  several  different  fields  –  but  it  does  underline  the  need  for  a  more
solid basis on which political decision-making should be based. That basis
is  replication.  Centuries  of  scientific  endeavour  have  shown  that  truth
emerges only from repeated experimentation and falsification of theories, a
process that only begins after publication and can continue for months or
years or decades thereafter. Only through actually reproducing the findings
of a scientific paper can other researchers be certain that those findings are
correct.

In  the  early  history  of  European  science,  publication  of  scientific
findings  in  a  journal  was  usually  adequate  to  allow  other  researchers  to
replicate them. However, as science has advanced, the techniques used have
become  steadily  more  complicated  and  consequently  more  difficult  to
explain.  The  advent  of  computers  has  allowed  scientists  to  add  further
layers of complexity to their work and to handle much larger datasets, to the
extent that a journal article can now, in most cases, no longer be considered
a definitive record of a scientific result. There is simply insufficient space in
the pages of a print journal to explain what exactly has been done. This has
produced a rather profound change in the purpose of a scientific paper. As
geophysicist  Jon  Claerbout  puts  it,  in  a  world  where  powerful  computers
and  vast  datasets  dominate  scientific  research,  the  paper  ‘is  not  the
scholarship itself, it is merely advertising of the scholarship’.b The actual

scholarship is the data and code used to generate the figures presented in the
paper and which underpin its claims to uniqueness.

In passing we should note the implications of Claerbout’s observations
for the assessment for our conclusions in the last section: by using only peer
review to assess the climate science literature, the policymaking community
is  implicitly  expecting  that  a  read-through  of  a  partial  account  of  the
research  performed  will  be  sufficient  to  identify  any  errors  or  other
problems with the paper. This is simply not credible.

With a full explanation of methodology now often not possible from the
text of a paper, replication can usually only be performed if the data and
code are available. This is a major change from a hundred years ago, but in
the twenty-first century it should be a trivial problem to address. In some
specialisms it is just that. We have seen, however, how almost every attempt
to  obtain  data  from  climatologists  is  met  by  a  wall  of  evasion  and
obfuscation, with journals and funding bodies either unable or unwilling to
assist.  This  is,  of  course,  unethical  and  unacceptable,  particularly  for
publicly funded scientists. The public has paid for nearly all of this data to
be collated and has a right to see it distributed and reused.

As  the  treatment  of  the  Loehle  paper  shows,c  for  scientists  to  open
themselves up to criticism by allowing open review and full data access is a
profoundly uncomfortable process, but the public is not paying scientists to
have  comfortable  lives;  they  are  paying  for  rapid  advances  in  science.  If
data is available, doubts over exactly where the researcher has started from
fall away. If computer code is made public too, then the task of replication
becomes simpler still and all doubts about the methodology are removed.
The debate moves on from foolish and long-winded arguments about what
was  done  (we  still  have  no  idea  exactly  how  Mann  calculated  his
confidence intervals) onto the real scientific meat of whether what was done
was correct. As we look back over McIntyre’s work on the Hockey Stick,
we  see  that  much  of  his  time  was  wasted  on  trying  to  uncover  from  the
obscure wording of Mann’s papers exactly what procedures had been used.
Again,  we  can  only  state  that  this  is  entirely  unacceptable  for  publicly
funded  science  and  is  unforgiveable  in  an  area  of  such  enormous  policy
importance.

As well as helping scientists to find errors more quickly, replication has
other benefits that are not insignificant. David Goodstein of the California
Insitute of Technology has commented that the possibility that someone will

try to replicate a piece of work is a powerful disincentive to cheating – in
other words, it can help to prevent scientific fraud.251 Goodstein also notes
that, in reality, very few scientific papers are ever subject to an attempt to
replicate them. It is clear from Stephen Schneider’s surprise when asked to
obtain the data behind one of Mann’s papers that this criticism extends into
the field of climatology.d In a world where pressure from funding agencies
and the demands of university careers mean that academics have to publish
or perish, precious few resources are free to replicate the work of others. In
years gone by, some of the time of PhD students might have been devoted
to replicating the work of rival labs, but few students would accept such a
menial task in the modern world: they have their own publication records to
worry about. It is unforgiveable, therefore, that in paleoclimate circles, the
few attempts that have been made at replication have been blocked by all of
the parties in a position to do something about it.

Medical  science  is  far  ahead  of  the  physical  sciences  in  the  area  of
replication.  Doug  Altman,  of  Cancer  Research  UK’s  Medical  Statistics
group, has commented that archiving of data should be mandatory and that
a  failure  to  retain  data  should  be  treated  as  research  misconduct.252  The
introduction of this kind of regime to climatology could have nothing but a
salutary  effect  on  its  rather  tarnished  reputation.  Other  subject  areas,
however, have found simpler and less confrontational ways to deal with the
problem.  In  areas  such  as  econometrics,  which  have  long  suffered  from
politicisation  and  fraud,  several  journals  have  adopted  clear  and  rigorous
policies  on  archiving  of  data.  At  publications  such  as  the  American
Economic  Review,  Econometrica  and  the  Journal  of  Money,  Credit  and
Banking, a manuscript that is submitted for publication will simply not be
accepted unless data and fully functional code are available. In other words,
if the data and code are not public then the journals will not even consider
the article for publication, except in very rare circumstances. This is simple,
fair  and  transparent  and  works  without  any  dissent.  It  also  avoids  any
rancorous disagreements between journal and author after the event.

Physical  science 

journals  are,  by  and 

the
econometricians on this score. While most have adopted one pious policy or
another,  giving  the  appearance  of  transparency  on  data  and  code,  as  we
have  seen  in  the  unfolding  of  this  story,  there  has  been  a  near-complete
failure  to  enforce  these  rules.  This  failure  simply  stores  up  potential
problems for the editors: if an author refuses to release his data, the journal

large, 

far  behind 

is  left  with  an  enforcement  problem  from  which  it  is  very  difficult  to
extricate themselves. Their sole potential sanction is to withdraw the paper,
but this then merely opens them up to the possibility of expensive lawsuits.
It is hardly surprising that in practice such drastic steps are never taken.

The  failure  of  climatology  journals  to  enact  strict  policies  or  enforce
weaker  ones  represents  a  serious  failure  in  the  system  of  assurance  that
taxpayer-funded science is rigorous and reliable. Funding bodies claim that
they rely on journals to ensure data availability. Journals want a quiet life
and will not face down the academics who are their lifeblood. Will Nature
now  go  back  to  Mann  and  threaten  to  withdraw  his  paper  if  he  doesn’t
produce the code for his confidence interval calculations? It is unlikely in
the extreme. Until politicians and journals enforce the sharing of data, the
public can gain little assurance that there is any real need for the financial
sacrifices they are being asked to accept.

Taking steps to assist the process of replication will do much to improve
the conduct of climatology and to ensure that its findings are solidly based,
but  in  the  case  of  papers  of  pivotal  importance  politicians  must  also  go
further. Where a paper like the Hockey Stick appears to be central to a set
of policy demands or to the shaping of public opinion, it is not credible for
policymakers to stand back and wait for the scientific community to test the
veracity  of  the  findings  over  the  years  following  publication.  Replication
and falsification are of little use if they happen after policy decisions have
been  made.  The  next  lesson  of  the  Hockey  Stick  affair  is  that  if
governments  are  truly  to  have  assurance  that  climate  science  is  a  sound
basis  for  decision-making,  they  will  have  to  set  up  a  formal  process  for
replicating  key  papers,  one  in  which  the  oversight  role  is  peformed  by
scientists who are genuinely independent and who have no financial interest
in the outcome.
The Hockey Stick and the global warming hypothesis
The  case  that  global  warming  is  happening,  is  manmade  and  will  be
catastrophic  does  not  rely  on  the  paleoclimate  studies  alone  and  we
therefore need to understand the other strands of the argument. Once we are
clear on how the whole global warming hypothesis stacks up, we can assess
the effect of eliminating the Hockey Stick. Before we do that, we first need
to be clear on what is meant by a hypothesis.

The classic explanation of the scientific method that was outlined by the
Austrian  philosopher,  Karl  Popper,  in  the  years  before  the  Second  World
War, describes the formulation of a hypothesis – an idea to be tested – and
the performance of experiments that seek to falsify it. As each attempt at
falsification  is  rejected,  confidence  in  the  hypothesis  grows  until  it  is
accepted, for the moment, as the truth.

The global warming hypothesis is, in very simple terms, that man-made
emissions of carbon dioxide into the atmosphere are causing the Earth to
heat  up  –  the  case  made  by  Arrhenius  more  than  100  years  ago.  Since
Arrhenius’s  time,  this  simple  idea  has  become  vastly  more  sophisticated,
particularly over the last couple of decades, as scientists have learned more
and  more  about  the  factors  that  affect  the  climate  system  and  how  they
interact. Vast computer models now incorporate an array of different inputs
to  the  climate  system  (known  as  ‘forcings’)  –  sunlight,  carbon  dioxide,
emissions  from  volcanos  and  so  on,  together  with  the  feedbacks  such  as
clouds  and  rainfall.  As  the  models  grow  and  grow,  they  become  steadily
more complex but, it is hoped, in the process these vast artificial worlds will
become more realistic representations of the world’s climate than the simple
models that have gone before.

It is important to recognise one important fact about the climate models:
they  are  hypotheses.  Newer  and  more  sophisticated  and  perhaps  better
hypotheses,  but  hypotheses  all  the  same.  It  is  extraordinarily  common  to
hear qualified scientists to talk about ‘the evidence from the models’, as if
evidence  could  be  derived  from  anything  other  than  the  real  world.  The
distinction  is  not  a  trivial  one.  It  is  terribly  easy  for  scientists  to  fool
themselves  that  what  emerges  from  their  models  is  a  reflection  of  reality
rather than of the assumptions they have fed into them. Garbage in, garbage
out is a constant concern for climate modellers.

The  question  of  whether  the  models  are  reliable  is  not  particularly
relevant to deciding if the Hockey Stick affair is important or not, so let us
leave  that  question  aside  for  the  moment  without  getting  into  the  details.
For  the  purposes  of  the  hypothesis  that  man’s  activities  are  causing  the
world to warm and will make it warm still further in future, we can assume
that  the  models  represent  the  best  current  understanding  of  the  climate
system.  We  must  then  test  that  hypothesis  against  evidence  from  the  real
world.

So  what  evidence  can  we  look  at  that  will  affirm  or  contradict  the
hypothesis? We can, of course, observe that surface temperatures rose at the
end of the twentieth century: good evidence in favour of the hypothesis. We
could  of  course  observe  the  stalling  of  the  rise  since  the  millennium,
apparently contradicting it. We could also look to see if clouds are behaving
in the way that the models suggest they should, or if changes to the major
weather patterns are consistent.

Whether you feel that either of these arguments represents a compelling
case  for  drastic  political  action  is  largely  a  matter  of  opinion,  but  it  is
probably  fair  to  say  that  fewer  people  will  be  convinced  by  the  latter
argument than by the former. Those who favour the so-called precautionary

Where do the Hockey Stick and the other temperature reconstructions fit
in  here?  Reconstructions  of  past  temperatures  say  nothing  about  whether
current  temperatures  are  rising.  We  have  seen  that  most  of  the  proxies
terminate  before  1980,  shortly  after  the  models  suggest  that  the  largest
effects of global warming should kick in; inexplicably there have been few
attempts  to  update  them  since.  The  reconstructions  tell  us  nothing  about
whether  or  by  how  much  temperatures  might  continue  to  rise.  In  fact,  a
reliable  temperature  reconstruction  (if  such  a  thing  were  possible)  would
only tell us if the current temperatures were unprecedented or not and, when
you  consider  the  implications  of  this,  it  is  clear  that  hockey  sticks  are
peripheral to the case for manmade global warming. It is entirely possible
that,  if  it  were  not  for  carbon  dioxide  heating  the  climate  system  up,
temperatures would actually be rather low in historic terms. In other words,
temperatures  could  be  entirely  precedented  and  the  global  warming
hypothesis could still be correct. Similarly, in a world that is still emerging
from the last ice age, temperatures should be rising and we should expect
them  to  be  higher  than  they  were  before.  So  temperatures  can  be
unprecedented  and  the  global  warming  hypothesis  could  still  be  wrong.
Unprecedented temperatures are persuasive but far from conclusive.

If,  having  seen  the  evidence  presented  here,  we  believe  that  the
temperature  reconstructions  are  not  reliable,  where  does  that  leave  us?  In
terms of the case for drastic action, the argument has changed just slightly
from  ‘temperatures  are  rising  in  line  with  the  models  and  are  now
unprecedented’ to ‘temperatures are rising in line with the models’. Again,
sceptics will want to note the twenty-first century stall of the warming, but
let’s leave that aside for now.

principle  will  always  want  to  avoid  future  costs  and  will  choose  drastic
action  regardless.  My  own  view  is  that  this  is  unreasonable;  I  prefer  to
consider  both  costs  and  benefits  of  any  possible  actions,  but  people  will
differ on these issues.

To this extent then, the Hockey Stick affair is not the beginning and end
of  the  global  warming  story:  it  can  cause  the  arguments  to  be  framed
differently  but  it  will  not  decide  the  outcome.  In  fact,  this  position  was
agreed very early on in the debate, when McIntyre and two Hockey Team
members, Rasmus Benestad and William Connolley, all posted comments
on the blog of a climate policy writer, Roger Pielke Jnr, coming to similar
conclusions  on  the  Hockey  Stick’s  scientific  importance  –  namely  that  it
wasn’t  very  important  at  all.253  In  fact,  even  the  IPCC  agreed  with  the
position that paleoclimate reconstructions were not particularly important to
the  scientific  case  for  manmade  global  warming.  This  being  the  case,
McIntyre  had  suggested  that  they  delete  the  whole  paleoclimate  chapter
from  the  draft  Fourth  Assessment  Report,  but  the  powers  within  the
bureaucracy did not take him up on the idea.

So  why  then  have  you  just  read  a  whole  book  about  this  particular
scientific paper? Why has the debate over the Hockey Stick been so drawn
out and so heated? Why does the Hockey Stick matter so much to the IPCC?
As we will see in the next section, the chief importance of the Hockey
Stick lies not in that it is central to the case for manmade global warming,
but in the fact that the IPCC promoted it as if it were.
The Hockey Stick and the IPCC
In  that  same  Pielke  comment  thread,  McIntyre  had  put  forward  the  view
that the Hockey Stick’s centrality was not so much to the scientific debate,
as  to  the  political  one.  In  other  words,  its  importance  lay  in  its  use  as  a
promotional  tool:  a  single  compelling  graphic  that  could  be  used  to
persuade  the  public  and  policymakers  of  the  strength  of  the  case  for
dramatic  action.  We  saw  right  back  at  the  start  of  the  story  just  how  the
Hockey  Stick  has  been  used  to  promote  the  idea  of  manmade  global
warming.e It was the Hockey Stick that was behind Sir John Houghton at
the  launch  of  TAR,  it  was  the  Hockey  Stick  that  was  behind  the  constant
claims that modern temperatures are unprecedented. Even now, years after
it  has  been  shown  to  be  flawed,  it  still  appears  in  school  textbooks  and
government and environmentalist literature. This being the case, it is hard to

disagree  with  the  BBC’s  opinion  that  ‘it  is  hard  to  overestimate  how
influential this study has been’. There can be little doubt that it would have
been much harder to sell the idea of manmade global warming if the Third
Assessment  Report  had  been  illustrated  with,  for  example,  Briffa’s
reconstruction. Even with the divergence effect erased from the record, as
shown in Figure 15, the rhetorical effect is considerably weaker than that of
Mann’s reconstruction. Of course, this effect could have been reversed to
some  extent  by  overlaying  the  end  of  the  record  with  the  instrumental
record, as Mann did in the original Hockey Stick paper, but whether this
would  have  been  a  fair  representation  of  the  proxy  records  is  another
question.

The IPCC’s sale of a dud to governments, which governments then sold
on to their citizens, has a much wider significance than its narrow relevance
to the scientific debate. Hans von Storch explained some of these issues in
comments he made shortly after the publication of the IPCC report.

The debate about the hockey stick is most significant when it comes to the culture of
our  science.  Posting  the  hockey  stick  as  key  evidence  in  the  [Summary  for
Policymakers] and Synthesis Report of the IPCC was simply stupid and evidence for
what  [biologist  Dennis]  Bray  calls  post-sensible  science  –  as  science  which  is
encroached [upon] by moral entrepreneurship. Or post-normal science. We have more
cases  of  this  type  of  claim-making,  which  is  usually  a  mix  of  ‘good’  political
intentions  and  personal  drive  for  the  limelight.  Have  we,  as  a  community,  become
better in rejecting such claims? I am afraid we have not.254

FIGURE 15.1: Briffa’s reconstruction from 2001
So  in  addition  to  what  it  tells  us  about  the  efficacy  of  peer  review,  the
Hockey Stick tells us about the culture of climate science and in particular

As  a  discipline,  climatologists  seem  to  have  got  themselves  into  a  rut.
Procedures that would be seen as shocking in other areas of scientific study
appear to be routine when climate is being studied – use of inappropriate
data, cherrypicking, truncations and extrapolations and reliance on ad-hoc
and untested methodologies are just the start of it. Climatologists appear to
have become fixated on the idea of catastrophic manmade global warming
and react in horror to any questioning of their findings, with frantic appeals
to  a  spurious  consensus  that  is  worthless  in  establishing  the  truth.  They
appear unable to break out of the mindset that ‘the science is settled’, an
odd position for a discipline that is still very much in its infancy. It has yet
to  reach  the  state  of  maturity  in  which  it  can  welcome  questioning  from
outsiders and take it in its stride.

A  surprising  number  of  climatologists  seem  to  believe  that  their  work
should not be subject to scrutiny by others and particularly not by outsiders.
The reaction of the profession to criticism of the Hockey Stick was, almost
universally,  to  circle  the  wagons  and  to  attack  the  mere  suggestion  that
Mann’s work was less than perfect. Requests for data and code appear to be
routinely  rejected.  This  is  not  the  behaviour  of  the  scientist  but  of  the
political  activist.  When  data  was  withheld  by  one  of  their  number,
climatologists sat silent or jeered from the sidelines. Likewise, not one of
the great, learned societies took a stand on the issue. Yet when Joe Barton
wrote to Mann, Bradley and Hughes asking for the  MBH98 code there was
uproar, with the American Association for the Advancement of Science, the
National  Academy  of  Sciences  and  the  European  Geophysical  Union  all
fulminating  against  Barton’s  political  ‘interference’.  The  idea  that  they
should have condemned Mann for withholding his code does not seem to
have occurred to them. Nor do they seem to have pondered the idea that
Barton  was  only  getting  involved  in  the  first  place  because  the  scientific
community – academics, journals, funding agencies and learned societies –
had all signally failed to police the problem themselves.

the IPCC, and it tells us about the culture of science in general. Its effect in
these areas should give us great cause for concern.

The  Hockey  Stick  was,  from  the  very  beginning,  a  test  of  the  IPCC’s
impartiality.  It  was  clear  before  the  publication  of  Mann’s  paper  that  the
Medieval  Warm  Period  was  a  major  problem  for  those  who  argued  that
man’s activities were having an adverse effect on the climate. The public
would simply not be convinced of the case for drastic action if temperatures

appeared to have been warmer a few hundred years ago. The arrival of the
Hockey Stick and its startling rise to prominence should have given the IPCC
pause  for  thought  on  these  grounds  alone.  Their  handling  of  the  paper
represented  an  opportunity  for  IPCC  officials  to  demonstrate  that  their
organisation  could  be  an  ‘honest  broker’  between  environmentalists  and
sceptics, a chance for it to show that its procedures were fair and balanced
and that it could be a reliable source of advice to politicians. On each count,
its failure was complete and catastrophic. At each step along the path to the
report, IPCC insiders made certain that any criticisms of the Hockey Stick
were quashed and that doubts about the veracity of the other paleoclimate
reconstructions were swept aside. The panel was stacked against the critics,
rules were bent and broken and criticisms were ignored or brushed off; all
with apparent impunity and without a word of protest from anyone in the
climatological or the wider scientific community.

The fact that the IPCC promoted a Hockey Stick that was not central to
the  scientific  debate  simply  because  it  was  a  good  sales  tool,  and  then
defended it in the face of all criticism shows us that it is not a disinterested
participant in the debate. It has chosen to be a advocate rather than a judge.
It  has  an  agenda.  How  then,  can  those  who  are  undecided  on  the  global
warming  issue  accept  anything  it  says  as  an  unbiased  judgement  on  the
facts rather than a statement of a political position? They can no longer be
sure.

Quite apart from what the Hockey Stick tells us about the positioning of
the IPCC in the global warming debate, the panel’s need for a sales tool also
suggests  something  important  about  the  overall  case  for  manmade  global
warming.  None  of  the  corruption  and  bias  and  flouting  of  rules  we  have
seen in the course of this story would have been necessary if there is, as we
are  led  to  believe,  a  watertight  case  that  mankind  is  having  a  potentially
catastrophic effect on the climate. What the Hockey Stick affair suggests is
that the case for global warming, far from being settled is actually weak and
unconvincing.

The  implications  for  policymakers  are  stark.  They  have  granted  an
effective monopoly on scientific advice to an organisation that has proven
itself to be corrupt, biased and beset by conflicts of interest. Their advisers
on the global warming issue are essentially a law unto themselves, the only
oversight of their actions and findings provided by volunteers like McIntyre
and his ragtag band of sceptic supporters. There is no conceivable way that

politicians can justify this failing to their electorates. They have no choice
but to start again.
Was the Hockey Stick an isolated problem?
Critics of the arguments I have put forward in this final chapter might well
ask whether the promotion of the Hockey Stick was a one-off. Perhaps, they
might  suggest,  these  problems  are  only  only  found  in  the  area  of
paleoclimate.  It  is  unfair,  they  might  say,  to  write  off  the  IPCC  as  wholly
biased based on the problems with just one fairly peripheral area of their
remit. Should we not give them the benefit of the doubt?

So  let  us  finish  then  by  looking  at  another  remarkable  episode  in  the
story of the Fourth Assessment Report, unrelated to the contentious area of
paleoclimate – the way that clouds affect the climate.

Of itself, the direct warming produced by manmade emissions of carbon
dioxide would not be enough to trouble mankind. The potential effects are,
it seems, only catastrophic because of feedbacks – other effects caused by
the  initial  warming.  For  example,  it  is  hypothesised  that  the  small  direct
warming caused by carbon dioxide will cause some melting of the ice caps.
As the white cover to the poles reduces in size, it is said, less and less heat
will  be  reflected  back  into  space,  and  so  there  will  be  a  further  warming
which would cause further melting and further warming and so on. These
so-called ‘positive feedbacks’ are what makes manmade global warming so
dangerous.

However, as well as positive feedbacks there are also negative feedbacks
and the most important of these is the influence of certain types of clouds.
The particular kind of clouds with which we are concerned are low-level or
‘boundary layer’ clouds.

In  2009,  McIntyre  reported  on  a  paper  written  in  2006  by  a  French
researcher,  Sandrine  Bony.255  Bony  et  al  was  a  review  paper,  surveying
recent  developments  in  scientific  understanding  of  climate  feedbacks
including, of course, the critical effects of water vapour and clouds.255 On
the subject of boundary layer clouds, Bony and her co-authors had this to
say:

Boundary layer clouds have a strongly negative [feedback effect] . . . and cover a very
large fraction of the area of the Tropics . . . Understanding how they may change in a
perturbed climate therefore constitutes a vital part of the cloud feedback problem.255

Unequivocally then, boundary layer clouds cool the planet, and strongly so.
How then was this knowledge conveyed to the public and policy-makers in
the  Fourth  Assessment  Report?  Chapter  8  of  the  report  was  authored  by,
amongst  others,  Sandrine  Bony  herself,  and  we  should  therefore  note  in
passing  that  this  represented  another  example  of  IPCC  authors  reviewing
their  own  work.  Certainly,  much  of  the  relevant  section  had  been  lifted
almost word for word from Bony’s paper: in the following extract, parts of
the text of the Fourth Assessment Report are identical. The differences are,
however, significant.

Boundary-layer  clouds  have  a  strong  impact  .  .  .  and  cover  a  large  fraction  of  the
global ocean . . . . Understanding how they may change in a perturbed climate is thus
a vital part of the cloud feedback problem . . .256

So suddenly, the strongly negative feedback noted by Bony in her Journal
of Climate paper became only a ‘strong impact’.

Later  in  the  same  paper,  Bony  had  noted  the  findings  of  two  earlier
researchers, Klein and Hartmann, who had observed a correlation between
cloud  cover  and  temperature  stability  in  the  tropics.  This,  Bony  reported,
‘leads to a substantial increase in low cloud cover in a warmer climate . . .
and  produces  a  strong  negative  feedback’.  So  once  again,  there  was  an
unequivocal case being made that the feedback from boundary layer clouds
is both strong and negative – tending to cool the Earth rather than warm it.
However,  by  the  time  this  statement  found  its  way  through  to  the  Fourth
Assessment  Report  it  was  once  again  much  more  ambiguous.  The
correlation between low-level clouds and temperature stability had ‘led to
the suggestion that a global climate warming might be associated with an
increased  low-level  cloud  cover,  which  would  produce  a  negative  cloud
feedback’. In other words, there was now only a suggestion rather than a
firm conclusion and it appeared to relate to an effect that was negative, but
not apparently strongly so.

Another  instance  was  the  reporting  of  a  disagreement  over  whether
clouds in a warmer world would reflect more heat back into space (a higher
‘albedo’,  in  the  jargon).  Bony  et  al  had  reported  two  papers  finding  in
favour of such an effect, and three against. Yet once again, by the time the
IPCC came to pronounce upon the issue, things had changed radically, with
all mention of a possible cooling effect removed, leaving only those papers
arguing against it.

Who knows what other instances there are of arguments contrary to the
IPCC  consensus  disappearing  into  the  ether,  of  doubts  suppressed  and
questions ignored? That so many strange happenings have been uncovered
by  the  handful  of  sceptics  actively  researching  the  subject  would  suggest
that these problems are just the tip of the iceberg. It is clear that it would be
foolish in the extreme to give the IPCC the benefit of the doubt. Their record
is too poor, the stakes too high.

a  Watson and Crick’s paper was actually handled by Maddox’s Predecessor, Jack Brimble.
b   Although  originally  writing  about  computer  science,  Claerbout’s  point  applies  just  as  much  to
climatology  and  other  fields  of  study.  The  actual  words  quoted  are  a  distillation  of  Claerbout’s
ideas due to Buckheit and Donoho.250

c  See page 303.
d  See page 160.
e  See page 39.

16     The Beginning of the End?

You can’t fool all of the people all of the time.

PT Barnum

Shortly before calling a halt to the seemingly endless series of corrections
and revisions to this book, there were some dramatic developments on one
of the many threads of the paleoclimate story. I will attempt to cover these
briefly since they are likely to prove important.

Back in Chapter 10, we saw how an update to the Polar Urals series had eliminated its hockey stick
shape, jeopardising its use in subsequent paleoclimate studies. Keith Briffa had then come up with a
new chronology from the nearby location of Yamal. The Yamal data had been collected by a pair of
Russian scientists, Hantemirov and Shiyatov, and had been published in 2002.257 In their version of
the series, Yamal had shown little by way of a twentieth century trend. Strangely though, Briffa’s
version, which had made it into print even before that of the Russians, was somewhat different.147
While it tracked the Russians’ version for most of the length of the record, Briffa’s version had a
sharp uptick at the end of the twentieth century – another hockey stick. As we have seen, after its first
appearance  in  Quaternary  Science  Reviews,  Briffa’s  version  of  Yamal  was  seized  upon  by
climatologists, appearing again and again in temperature reconstructions; it was virtually ubiquitous
in the field: it contributed to the reconstructions in Mann and Jones 2003, Jones and Mann 2004,
Moberg  et  al  2005,  D’Arrigo  et  al  2006,  Osborn  and  Briffa  2006  and  Hegerl  et  al  2007,  among
others.
When McIntyre started to look at the Osborn and Briffa paper in 2006,
he  quickly  ran  into  the  problem  of  the  Yamal  chronology:  he  needed  to
understand exactly how the difference between the Briffa and Hantemirov
versions of Yamal had arisen. McIntyre therefore wrote to the Englishman
asking  for  the  original  tree  ring  measurements  involved.  When  Briffa
refused,  McIntyre  wrote  to  Science,  who  had  published  the  new  paper,
pointing  out  that,  since  it  was  now  six  years  since  Briffa  had  originally
published  his  version  of  the  chronology,  there  could  be  no  reason  for
withholding  the  underlying  data.  After  some  deliberation,  the  editors  at
Science  declined  the  request,  deciding  that  Briffa  did  not  have  to  publish
anything  more,  as  he  had  merely  re-used  data  from  an  earlier  study.
McIntyre should, they advised, approach the author of the earlier study, that
author being, of course, Briffa himself. Wearily, McIntyre wrote to Briffa
again, this time in his capacity as author of the original study in Quaternary

Science  Reviews  and  once  again,  as  he  had  expected,  the  request  was
refused.

That  was  how  the  investigation  of  the  Yamal  series  stood  for  the  next
two years until, in July 2008, a new Briffa paper appeared in the pages of
the Philosophical Transactions of the Royal Society B, the Royal Society’s
journal  for  the  biological  sciences.258  The  new  paper  discussed  five
Eurasian tree ring datasets, which, in fairly standard Hockey Team fashion,
were unarchived and therefore not susceptible to detailed analysis. Among
these  five  were  Yamal  and  the  equally  notorious  Tornetrask  chronology.
McIntyre  observed  that  the  only  series  with  a  strikingly  anomalous
twentieth century was Yamal. It was frustrating therefore that he had still
not managed to obtain Briffa’s measurement data. It appeared that he was
going  to  hit  another  dead  end.  However,  in  the  comments  to  his  Climate
Audit article on the new paper, a possible way forward presented itself. A
reader pointed out that the Royal Society had what appeared to be a clear
and robust policy on data availability:a

As a condition of acceptance authors agree to honour any reasonable request by other
researchers for materials, methods, or data necessary to verify the conclusion of the
article . . . Supplementary data up to 10 Mb is placed on the Society’s website free of
charge and is publicly accessible. Large datasets must be deposited in a recognised
public  domain  database  by  the  author  prior  to  submission.  The  accession  number
should be provided for inclusion in the published article.259

Having had his requests rejected by every other journal he had approached,
McIntyre  had  no  great  expectations  that  the  Royal  Society  would  be  any
different, but it seemed worth another attempt and he duly sent off an email
pointing  out  that  Briffa  had  failed  to  meet  the  Society’s  requirement  of
archiving  his  data  prior  to  submission  and  that  the  editors  had  failed  to
check  that  Briffa  had  done  so.260  The  reply,  to  McIntyre’s  surprise,  was
very encouraging:

We take matters like this very seriously and I am sorry that this was not picked up in
the publishing process.261

Was the Royal Society, in a striking contrast to every other journal in the
field, about to enforce its own data availability policy? Had Briffa made a
fatal mistake?

Summer gave way to autumn and as October drew to a close, McIntyre
had heard nothing more from the Royal Society. However, in response to
some further enquiries, the journal sent him some more encouraging news –
Briffa would be producing most of his data, although not immediately. Most
of it would be available by the end of the year, with the remainder to follow
in early 2009.

The first batch of data appeared on schedule in the dying days of 2008
and it was something of a disappointment. The Yamal data, as might have
been expected, was to be archived with the second batch, so there would be
a  further  delay  before  the  real  action  could  start.  Meanwhile,  however,
McIntyre could begin to look at what Briffa had done elsewhere. It was not
to be plain sailing. For a start, Briffa had archived data in an obsolete data
format, last used in the era of punch cards. This was inconvenient but it was
not an insurmountable problem – with a little work, McIntyre was able to
move  ahead  with  his  analysis.  Unfortunately,  Briffa  had  also  thrown  a
rather  larger  spanner  in  the  works:  while  he  had  archived  the  tree  ring
measurements, he had not supplied any metadata to go with it – in other
words there was no information about where the measurements had come
from: there was only a tree number and the measurements that went with it.
However,  McIntyre  was  well  used  to  this  kind  of  behaviour  from
climatologists and he had some techniques at hand for filling in some of the
gaps. Climate Audit postings on the findings followed in fairly short order,
some of which were quite intriguing. There was, however, no smoking gun.
There followed a long hiatus, with no word on the remaining data from
the Royal Society or from Briffa. McIntyre would occasionally visit Briffa’s
web page at the CRU website to see if anything new had appeared, but to no
avail. Eventually, in late September 2009, a reader pointed out to McIntyre
that  the  remaining  data  was  now  available.  It  had  been  quietly  posted  to
Briffa’s webpage, without announcement or indeed the courtesy of an email
to Mcintyre. It was nearly ten years since the initial publication of Yamal
and three years since McIntyre had requested the measurement data from
Briffa. Now at last some of the questions could be answered.

When  McIntyre  started  to  look  at  the  numbers  it  was  clear  that  there
were going to be the usual problems with a lack of metadata, but there was
also much more than this. In typical climate science fashion, just scratching
at the surface of the Briffa archive raised as many questions as it answered.
Why did Briffa only have half the number of cores covering the Medieval

The lack of twentieth century data was still more remarkable when the
Yamal chronology was compared to the Polar Urals series to which it was
now  apparently  preferred.  The  ten  or  twelve  cores  used  in  Yamal  was
around  half  the  number  used  in  Polar  Urals,  which  should  presumably
therefore  have  been  considered  the  more  reliable.  Why  then  had
climatologists almost all preferred to use Yamal? Could it be because it had
a hockey stick shape?

None  of  these  questions  was  likely  to  be  answered  without  knowing
which trees came from which locations. Hantemirov had made it clear in his
paper  that  the  data  had  been  collected  over  a  wide  area  –  Yamal  was  an
expanse  of  river  valleys  rather  than  a  single  location.  Knowing  exactly
which  trees  came  from  where  might  well  throw  some  light  onto  the
question  of  why  Briffa’s  reconstruction  had  a  hockey  stick  shape  but
Hantemirov’s didn’t.

Warm Period that the Russian had reported? And why were there so few
tree  ring  cores  in  Briffa’s  twentieth  century?  There  were  only  12  trees
contributing  to  Briffa’s  estimate  of  the  average  ring  width  for  the  Yamal
area in 1988, an amazingly small number in what should have been the part
of the record when it was easiest to obtain data. By 1990 the count was only
ten,  dropping  still  further  to  just  five  in  1995.  Without  an  explanation  of
how the selection of this sample of the available data had been performed,
the suspicion of ‘cherrypicking’ would linger over the study, although it is
true  to  say  that  Hantemirov  also  had  very  few  cores  in  the  equivalent
period, so it is possible that this selection had been due to the Russians and
not Briffa.

As so often in McIntyre’s work, the clue that unlocked the mystery came
from a rather unexpected source. At the same time as archiving the Yamal
data,  Briffa  had  recorded  the  numbers  for  another  site  discussed  in  his
Royal  Society  paper:  Taimyr.  Like  Yamal,  Taimyr  had  also  emerged  in
Briffa’s Quaternary Science Reviews paper in 2000. However, in the Royal
Society  paper,  Briffa  had  made  major  changes,  merging  Taimyr  with
another site, Bol’shoi Avam, located no less than 400 km away. While the
original  Taimyr  site  had  something  of  a  divergence  problem,  with
narrowing  ring  widths  implying  cooler  temperatures,  the  new  composite
site of Avam–Taimyr had a rather warmer twentieth century and a cooler
Medieval Warm Period. The effect of this curious blending of datasets was
therefore, as so often with paleoclimate adjustments, to produce a warming

trend.  However,  this  was  not  the  only  thing  about  the  series  that  was
interesting McIntyre. What was odd about Avam–Taimyr was that the series
seemed to have more tree cores recorded than had been reported in the two
papers on which it was based. So it looked as if something else had been
merged in as well. But what?

With  no  metadata  archived  for  Avam–Taimyr  either,  McIntyre  had
another puzzle to occupy him, but in fact the results were quick to emerge.
The Avam data was collected in 2003, but Taimyr only had numbers going
up to 1996. Also, the Taimyr trees were older, with dates going back to the
ninth  century.  It  was  therefore  possible  for  McIntyre  to  make  a  tentative
split of the data by dividing the cores into those finishing after 2000 and
those finishing before. This proved to be a good first cut, but the approach
assigned 107 cores to Avam, which was more than reported in the original
paper.  This  seemed  to  confirm  McIntyre’s  impression  that  there  was
something else in the dataset.

that 

At the same time, McIntyre’s rough cut approach assigned 103 cores to
Taimyr,  a  number  which  meant  that  there  were  still  over  100  cores  still
unallocated. The only way to resolve this conundrum was by a brute force
technique of comparing the tree identification numbers in the dataset to tree
ring data in the archives. In this way, McIntyre was finally able to work out
the provenance of at least some of the data.

Forty-two of the cores turned out to be from a location called Balschaya
Kamenka, some 400 km from Taimyr. The data had been collected by the
Swiss  researcher,  Fritz  Schweingruber.  The  fact 
the  use  of
Schweingruber’s data had not been reported by Briffa was odd in itself, but
what intrigued McIntyre was why Briffa had used Balschaya Kamenka and
not any of the other Schweingruber sites in the area. Several of these were
much  closer  to  Taimyr  –  Aykali  River  was  one  example,  and  another,
Novaja Rieja, was almost next door.

FIGURE 16.1: The Yamal sensitivity test
Grey line: Briffa; black line: McIntyre.

By  this  point  McIntyre  knew  that  Briffa’s  version  of  Yamal  was  very
short of twentieth century data, having used just a selection of the available
cores, although the grounds on which this selection had been made were not
clear.  It  was  also  obvious  that  there  was  a  great  deal  of  alternative  data
available from the region, Briffa having been happy to supplement Taimyr
with data from other locations such as Avam and Balschaya Kamenka. Why
then had he not supplemented Yamal in a similar way, in order to bring the
number of cores up to an acceptable level?

The  reasoning  behind  Briffa’s  subsample  selection  may  have  been  a
mystery, but with the other information McIntyre had gleaned, it was still
possible  to  perform  some  tests  on  its  validity.  This  could  be  done  by
performing a simple sensitivity test, replacing the twelve cores that Briffa
had used for the modern sections of Yamal with some of the other available
data.  Sure  enough,  there  was  a  suitable  Schweingruber  series  called
Khadyta River close by to Yamal, and with 34 cores, it represented a much
more reliable basis for reconstructing temperatures.

McIntyre therefore prepared a revised dataset, replacing Briffa’s selected
12  cores  with  the  34  from  Khadyta  River.  The  revised  chronology  was
simply staggering (see Figure 16.1).  The  sharp  uptick  in  the  series  at  the
end  of  the  twentieth  century  had  vanished,  leaving  a  twentieth  century
apparently without a significant trend. The blade of the Yamal hockey stick,
used  in  so  many  of  those  temperature  reconstructions  that  the  IPCC  said
validated Mann’s work, was gone.

At time of writing, the problems with the Yamal series have only been public for a matter of a few
weeks. Briffa has made a public response, showing that he can get similar results with an expanded

dataset, but without making a substantive defence of the Yamal data. It seems that Briffa was working
with the 12 Yamal series in order to compare his standardisation methodology to Shiyatov’s, although
as  others  have  pointed  out,  Briffa’s  preferred  method  normally  requires  much  larger  numbers  of
samples. It is not beyond the realms of possibility that the assessment of the series will change in the
coming weeks and months as people on both sides of the global warming argument study McIntyre’s
case. For now though, it appears that the tree ring approach to temperature reconstructions lies in
tatters.

a  The reader in question was in fact the author of this book.

17     The CRU Hack

Extraordinary claims demand extraordinary evidence.

Carl Sagan

In mid-November 2009 someone accessed the servers of CRU, the Climatic
Research  Unit  at  the  University  of  East  Anglia  (UEA),  and  extracted  vast
quantities of data. A matter of a few days later, an archive of a thousand
emails  together  with  the  long-sought  data  and  code  for  the  HADCRUT
temperature index were posted on a web server located in Russia. Pointers
to the leaked information were posted on the websites of some prominent
sceptics and by the following day the climate blogosphere was in uproar as
a  series  of  embarrassing  revelations  about  the  conduct  of  prominent
scientists was made public. Over the next few days the story was picked up
by  the  mainstream  media  until  there  was  a  avalanche  of  outrage  and
scandal.  The  immediate  reaction  was  that  CRU  had  been  the  victim  of  a
hacker,  although  at  time  of  writing  opinion  appears  to  be  gravitating
towards the culprit being an insider. At time of writing, CRU director, Phil
Jones  has  stepped  down  pending  an  investigation  into  the  emails  and
Mann’s employer, Penn State University, have launched an inquiry into his
conduct.

Many of the emails leaked are extremely illuminating for our story, and I
will attempt to cover here what has been discovered in the few days since
the  leak.  At  the  time  of  writing  Phil  Jones  has  verified  that  the  unit’s
systems were compromised, but the volume of data released means that it
has not been possible to verify that all the leaked information is genuine.
This caveat needs to be borne in mind while reading the rest of this chapter.
Each extract below is headed by an indication of who is writing, the date
and  lastly  the  file  number  in  the  archive  of  leaked  emails.  These  are
currently  circulating  widely  on  the  internet,  although  there  is  no  obvious
permanent repository as yet, so I do not provide a reference. Readers should
be able to locate an electronic copy without difficulty.
The Soon and Baliunas paper

In Chapter 3a we saw how the publication of the Soon and Baliunas paper
led  to  the  resignation  of  several  of  the  editors  of  Climate  Research,  the
journal that had published the paper.The story of the resignations from the
Hockey Team’s side starts in March 2003 with Phil Jones notifying the rest
of the team of the publication of the Soon paper and advising his colleagues
that they would be best to ignore it. Afterwards he turned his attention to
the editorial staff of the journal. The Soon paper had been handled by Chris
de  Freitas  of  the  University  of  Auckland  in  New  Zealand,  overseen  by
editor-in-chief, Hans von Storch.

Jones: 10 March 2003: 1062618881
The responsible [editor] for this [paper is] a well-known skeptic in NZ. He has let a
few papers through by [sceptics Pat] Michaels and [William] Gray in the past. I’ve
had words with Hans von Storch about this, but got nowhere.

Later  though,  Jones  changed  his  mind  about  a  policy  of  ignoring  the
publication.

Jones: 11 March 2003: 1062618881
Writing this I am becoming more convinced we should do something – even if this is
just  to  state  once  and  for  all  what  we  mean  by  the  [Little  Ice  Age]  and
[MedievalWarm Period]. I think the skeptics will use this paper to their own ends and
it will set paleo back a number of years if it goes unchallenged. I will be emailing the
journal to tell them I’m having nothing more to do with it until they rid themselves of
this troublesome editor. A CRU person is on the editorial board, but papers get dealt
with by the editor assigned by Hans von Storch.

Mann clearly felt the same way:

Mann: 11 March 2003: 1047388489
The Soon & Baliunas paper couldn’t have cleared a ‘legitimate’ peer review process
anywhere.  That  leaves  only  one  possibility–that  the  peer-review  process  at  Climate
Research has been hijacked by a few skeptics on the editorial board. And it isn’t just
de  Frietas,  unfortunately  I  think  this  group  also  includes  a  member  of  my  own
department . . .

The  skeptics  appear  to  have  staged  a  ‘coup’  at  Climate  Research  (it  was  a
mediocre  journal  to  begin  with,  but  now  its  a  mediocre  journal  with  a  definite
‘purpose’) . . .

My  guess  is  that  von  Storch  is  actually  with  them  (frankly,  he’s  an  odd

individual, and I’m not sure he isn’t himself somewhat of a skeptic himself) . . .

The  problem,  Mann  said,  was  that  having  criticised  sceptics  for  not
publishing in the scientific literature, they now appeared to have a journal
that  would  publish  their  views.  He  went  on  to  explain  what  he  thought
should be done.

Mann: 11 March 2003: 1047388489
I think we have to stop considering Climate Research as a legitimate peer-reviewed
journal.  Perhaps  we  should  encourage  our  colleagues  in  the  climate  research
community to no longer submit to, or cite papers in, this journal. We would also need
to consider what we tell or request of our more reasonable colleagues who currently
sit on the editorial board . . . [ellipsis in original]

Tom Wigley seemed to share these views, concerned that de Freitas was
giving an easy time to sceptic papers. He said that one sceptic paper that he
and  a  colleague  had  rejected  had  been  accepted  by  de  Freitas,  who  had
rejected  Wigley’s  subsequent  complaint,  saying  that  the  three  other
reviewers had been happy to publish.

The team seem to have concluded that their first step should be to issue a
formal complaint to the journal. Meanwhile though, emails were exchanged
at a furious pace, opinions about what other steps could be taken bouncing
around  a  long  distribution  list  of  interested  scientists,  including  James
Hansen, Stephen Schneider and the new IPCC chairman Rajendra Pachauri.
There  was  clearly  a  strong  feeling  that  the  Soon  and  Baliunas  paper  was
very  poor  and,  in  Mann’s  words,  was  being  used  to  start  a  political
disinformation campaign.

Later that month, conclusions began to be reached. Mann had this to say:

Mann: 24 April 2003: 1051202354
I would emphasize that there are indeed, as Tom notes, some unique aspects of this
latest assault by the skeptics which are cause for special concern. This latest assault
uses  a  compromised  peer-review  process  as  a  vehicle  for  launching  a  scientific
disinformation  campaign  (often  vicious  and  ad  hominem)  under  the  guise  of
apparently legitimately reviewed science . . . Fortunately, the mainstream media never
touched the story (mostly it has appeared in papers owned by Murdoch and his crowd,
and dubious fringe on-line outlets). Much like a server which has been compromised
as a launching point for computer viruses, I fear that Climate Research has become a
hopelessly  compromised  vehicle  in  the  skeptics’  (can  we  find  a  better  word?)
disinformation campaign, and some of the discussion that I’ve seen (e.g. a potential
threat  of  mass  resignation  among  the  legitimate  members  of  the  Climate  Research
editorial board) seems, in my opinion, to have some potential merit. This should be
justified not on the basis of the publication of science we may not like of course, but
based on the evidence (e.g. as provided by Tom [Wigley] and Danny Harvey and I’m

sure there is much more) that a legitimate peer-review process has not been followed
by at least one particular editor.b

Intriguingly  Mann  also  alluded  to  ‘problems’  at  Geophysical  Research
Letters  where,  he  said,  the  sheer  volume  of  papers  meant  that  some  by
sceptics would slip through the net.

Mann: 23 April 2003: 1051202354
While it was easy to make sure that the worst papers . . . didn’t see the light of the day
at  [Journal  of  Climate],  it  was  inevitable  that  such  papers  might  slip  through  the
cracks at GRL.

This remarkable message makes it clear that Climate Research was not the
first journal where normal procedures had been undermined by the Hockey
Team.  We  will  see  later  that  it  was  not  the  last  time  either.  It  is  also
interesting  to  note  Mann’s  comments  in  light  of  the  statements  made  by
Andrew Weaver, the editor of Journal of Climate, about the suitability of
MM03 for publication.c

Other  ideas  for  action  included  the  possibility  of  a  rebuttal  in  Climate
Research  or  perhaps  even  a  more  prominent  journal.  Alternatively,  they
thought, a direct approach could be made to the US Office of Science and
Technology  Policy.  But  while  several  members  of  the  community  voiced
their support, the conversation seemed once again to return to the subject of
the editors. Tom Wigley was among the hawks:

Wigley: 24 April 2003: 1051190249
I do not know the best way to handle the specifics of the editoring. Hans von Storch is
partly to blame – he encourages the publication of crap science ‘in order to stimulate
debate’. One approach is to go direct to the publishers and point out the fact that their
journal is perceived as being a medium for disseminating misinformation under the
guise of refereed work. I use the word ‘perceived’ here, since whether it is true or not
is not what the publishers care about – it is how the journal is seen by the community
that counts.

I think we could get a large group of highly credentialed scientists to sign such a

letter – 50+ people.

Note that I am copying this view only to Mike Hulmed and Phil Jones. Mike’se
idea to get editorial board members to resign will probably not work – must get rid of
von  Storch  too,  otherwise  holes  will  eventually  fill  up  with  people  like  Legates,
Balling, Lindzen, Michaels, Singer, etc.f I have heard that the publishers are not happy
with von Storch, so the above approach might remove that hurdle too.

In  June  2003,  we  catch  sight  of  an  email  from  Chris  de  Freitas  to  Otto
Kinne,  the  publisher  of  Climate  Research.  Kinne  has  apparently  had  a
written  complaint  from  Mike  Hulme  about  the  publication  of  Soon  and
Baliunas.

De Freitas: 18 June 2003: 1057944829
I have spent a considerable amount of my time on this matter and had my integrity
attacked  in  the  process.  I  want  to  emphasize  that  the  people  leading  this  attack  are
hardly impartial observers. Mike himself refers to ‘politics’ and political incitement
involved.  Both  [Mike]  Hulme  and  [Clare]  Goodess  are  from  the  Climatic  Research
Unit of UEA that is not particularly well known for impartial views on the climate
change  debate.  The  CRU  has  a  large  stake  in  climate  change  research  funding  as  I
understand  it  pays  the  salaries  of  most  of  its  staff.  I  understand  too  the  journalist
David Appellg was leaked information to fuel a public attack . . .

Mike  Hulme  refers  to  the  number  of  papers  I  have  processed  for  [Climate
Research]  that  ‘have  been  authored  by  scientists  who  are  well  known  for  their
opposition to the notion that humans are significantly altering global climate’. How
many can he say he has processed? I suspect the answer is nil. Does this mean he is
biased  towards  scientists  ‘who  are  well  known  for  their  support  for  the  notion  that
humans are significantly altering global climate?’ Mike Hulme quite clearly has an
axe or two to grind, and, it seems, a political agenda. But attacks on me of this sort
challenge my professional integrity, not only as a [Climate Research] editor, but also
as an academic and scientist. Mike Hulme should know that I have never accepted any
research money for climate change research, none from any ‘side’ or lobby or interest
group or government or industry. So I have no pipers to pay.

He goes on to defend his conduct in publishing Soon and Baliunas:

De Freitas: 18 June 2003: 1057944829
The criticisms of Soon and Baliunas . . . article raised by Mike Hulme in his 16 June
2003  email  to  you  was  not  raised  by  the  any  of  the  four  referees  I  used  (but  is
curiously similar to points raised by David Appell!). Keep in mind that referees used
were  selected  in  consultation  with  a  paleoclimatologist.  Five  referees  were  selected
based on the guidance I received. All are reputable paleoclimatologists, respected for
their expertise in reconstruction of past climates. None (none at all) were from what
Hans [von Storch] and Clare [Goodess] have referred to as ‘the other side’ or what
Hulme refers to as ‘people well known for their opposition to the notion that humans
are  significantly  altering  global  climate’.  One  of  the  five  referees  turned  down  the
request to review explaining he was busy and would not have the time.

A few days later we see Phil Jones circulating a copy of de Freitas’ email to
some of his colleagues, asking that they keep the contents to themselves.

I  don’t  want  to  start  a  discussion  of  it  and  I  don’t  want  you  sending  it  around  to
anyone else, but it serves as a warning as to where the debate might go . . . I have

learned  one  thing.  This  is  that  the  reviewer  who  said  they  were  too  busy  was  Ray
[presumably Bradley] . . . It is clear . . . [that] a negative review was likely to be partly
ignored, and the article would still have come out.

He goes on to say, however, that de Freitas will not identify the other four
reviewers, but that he thinks that one might be a paleoclimatologist called
Anthony Fowler, who is not a known sceptic. So it appears that the Team
had strong evidence that at least two of the five invited referees were not
sceptics,  and  indeed  one  was  one  of  their  own  members.  However,  they
appeared undeterred.

This  appears  to  be  the  last  of  the  correspondence  relating  to  Climate
Research. However, as we have already seen, in July 2003, von Storch and
two  other  Climate  Research  editors  resigned  from  their  positions  on  the
journal. Did the Hockey Team act on their plans? At the moment we cannot
say for certain, although it certainly appears that they planned to do so.
On McIntyre and McKitrick 2003
Shortly before the release of MM03, Mann was passed details of the paper’s
release by an unidentified source. This source had apparently been provided
the  information  by  a  third  party.  Amusingly,  the  third  party  included  the
following statement:

Anonymous: October 2003: 1067194064
Personally, I’d offer that [McIntyre’s conclusions that there were problems with the
Hockey  Stick’s  robustness]  was  known  by  most  people  who  understand  Mann’s
methodology: it can be quite sensitive to the input data in the early centuries. Anyway,
there’s going to be a lot of noise on this one, and knowing Mann’s very thin skin I am
afraid he will react strongly, unless he has learned (as I hope he has) from the past . . .

Clearly then, as far back as 2003, the knowledge that the Hockey Stick was
flawed  was  not  restricted  to  just  sceptics.  Mann  decided  to  circulate  the
news of the paper’s publication to the rest of the Hockey Team:

Mann: 26 October 2003: 1067194064
This  has  been  passed  along  to  me  by  someone  whose  identity  will  remain  in
confidence . . .

My  suggested  response  is:  1)  to  dismiss  this  as  stunt,  appearing  in  a  so-called
‘journal’ which is already known to have defied standard practices of peer-review. It is
clear,  for  example,  that  nobody  we  know  has  been  asked  to  ‘review’  this  so-called
paper.  2)  to  point  out  the  claim  is  nonsense  since  the  same  basic  result  has  been
obtained by numerous other researchers, using different data, elementary compositing
techniques, etc. Who knows what sleight of hand the authors of this thing have pulled.

Of course, the usual suspects are going to try to peddle this crap. The important thing
is to deny that this has any intellectual credibility whatsoever and, if contacted by any
media, to dismiss this for the stunt that it is . . .

A few days later, we see Mann thanking some colleagues who had attacked
McIntyre’s paper, and Mann repeats his claims about McIntyre’s use of a
spreadsheet:

Mann: 29 October 2003: 1067450707
They didn’t use the proxy data available on our public FTP site, which I had pointed
them too – instead they used a spreadsheet file that my associate Scott Rutherford had
prepared.  In  this  file,  most  of  the  early  series  were  overprinted  at  later  years.  This
resulted in the reconstruction becoming increasingly spurious as one goes further back
in time – the estimates prior to 1700 or so were rendered meaningless. There were also
some  other  methodological  errors  that  will  be  detailed  shortly,  but  this  was  the  big
one.

It  is  interesting  that  Mann  does  not  at  this  point  claim  that  McIntyre
requested a spreadsheet, as he would in his later public pronouncements.h

There  is  also  a  draft  of  Mann’s  formal  rebuttal  of  MM03, a much more
strongly worded document than what was eventually published.i Later still,
we see a request from Bradley to the CRU team – Jones, Osborn and Briffa –
to publish the rebuttal for the Americans.

Bradley: 30 October 2003: 1067532918
If you are willing, a quick and forceful statement from The Distinguished CRU Boys
would help quash further arguments, although here, at least, it is already quite out of
control. . .

Briffa was happy to help:

Briffa: 30 October 2003: 1067542015
I agree with this idea in principle. Whatever scientific differences and fascination with
the nuances of techniques we may/may not share, this whole process represents the
most  despicable  example  of  slander  and  downright  deliberate  perversion  of  the
scientific  process,  and  biased  (unverified)  work  being  used  to  influence  public
perception and due political process.

going on to say that he was minded to ask Nature to write an editorial on
the subject. He did however have make a caveat:

Briffa: 30 October 2003: 1067542015
Much of the detail in Mike’s response though is not sensible (sorry Mike) and is rising
to their bait.

Over the next few weeks, the Team worked on the response. By the end of
October 2003, they had a revised draft. Osborn commented as follows:

Osborn: 31 October 2003: 1067596623
The single worst thing about the whole [McIntyre and McKitrick] saga is not that they
did their study, not that they did things wrong (deliberately or by accident), but that
neither  they  nor  the  journal  took  the  necessary  step  of  investigating  whether  the
difference between their results and yours could be explained simply by some error or
set of errors in their use of the data or in their implementation of your method. If it
turns  out,  as  looks  likely  from  Mike’s  investigation  of  this,  that  their  results  are
erroneous, then they and the journal will have wasted countless person-hours of time
and caused much damage in the climate policy arena.

As we have seen, McIntyre and McKitrick were unable to do this because
Mann had cut off communications with them prior to publication.j Osborn
was also concerned about becoming too closely associated with the dispute,
firstly because he said the CRU team were not fully aware of the details of
MBH98, but also because if there was a subsequent independent assessment
of  the  dispute,  they  would  be  unable  to  be  involved,  having  already  tied
their colours to Mann’s cause. He went on to discuss details of McIntyre’s
findings that were concerning him:

Osborn: 31 October 2003: 1067596623
(a) Mike, you say that many of the trees were eliminated in the data they used. Have
you  concluded  this  because  they  entered  ‘NA’  for  ‘Not  available’  in  their  appendix
table? If so, then are you sure that ‘NA’ means they did not use any data, rather than
simply  that  they  didn’t  replace  your  data  with  an  alternative  (and  hence  in  fact
continued  to  use  what  Scott  had  supplied  to  them)?  Or  perhaps  ‘NA’  means  they
couldn’t find the PC time series published (of course!), but in fact could find the raw
tree-ring  chronologies  and  did  their  own  [PC  analysis]  of  those?  How  would  they
know which raw chronologies to use?

The impossibility of working out which proxy series went into which step
of the reconstruction was, as we have seen, was a real issue for McIntyre
and McKitrick.

Osborn continued to make perspicious observations on what might have

happened.

Osborn: 31 October 2003: 1067596623
Or  did  you  come  to  your  conclusion  by  downloading  their  ‘corrected  and  updated’
data matrix and comparing it with yours – I’ve not had time to do that, but even if I
had and I found some differences, I wouldn’t know which was right seeing as I’ve not
done  any  [PC  analysis]  of  western  US  trees  myself?  My  guess  would  be  that  they
downloaded raw tree-ring chronologies (possibly the same ones you used) but then
applied [PC analysis] only to the period when they all had full data – hence the lack of
PCs in the early period (which you got round by doing [PC analysis] on the subset that
had earlier data). But this is only a guess, and this is the type of thing that should be
checked with them – surely they would respond if asked? . . . And if my guess were
right,  then  your  wording  of  ‘eliminated  this  entire  data  set’  would  come  in  for
criticism, even though in practise it might as well have been.

As we saw in Chapter 3, Mann’s initial response to the publication of MM03
had included the claim that McIntyre had requested the data in spreadsheet
format,  a  claim  that  was  not  substantiated  when  McIntyre  published  his
correspondence with Mann.k This was Osborn’s next concern.

Osborn: 31 October 2003: 1067596623
(b) The mention of FTP sites and Excel files is contradicted by their email record on
their website, which shows no mention of Excel files (they say an ASCII file was sent)
and also no record that they knew the FTP address. This doesn’t matter really, since the
reason for them using a corrupted data file is not relevant – the relevant thing is that it
was corrupt and had you been involved in reviewing the paper then it could have been
found prior to publication. But they will use the email record if the FTP sites and Excel
files are mentioned.

We  also  saw  in  the  footnote  on  page  64  that  there  had  also  been  some
suggestions  that  the  paper  had  not  been  peer  reviewed,  and  this  was
Osborn’s next point.

Osborn: 31 October 2003: 1067596623
(c)  Not  sure  if  you  talk  about  peer-review  in  the  latest  version,  but  note  that  they
acknowledge input from reviewers and Fred Singer’s email says he refereed it – so
any statement implying it wasn’t reviewed will be met with an easy response from
them.*

Osborn  next  addressed  the  RE  statistic.  Mann  had  apparently  suggested
including some kind of emulation of the McIntyre and McKitrick results.
Then by showing that the emulation had a poor  RE  score,  he  could  argue
that it was less reliable than his own paper.

Osborn: 31 October 2003: 1067596623

(d)  Your  quick-look  reconstruction  excluding  many  of  the  tree-ring  data,  and  the
verification RE you obtain, is interesting – but again, don’t rush into using these in any
response. The time series of PC1 you sent is certainly different from your standard one
– but on the other hand I’d hardly say you ‘get a similar result’ to them, the time series
look  very  different  (see  their  Fig.  6d).  So  the  dismal  RE  applies  only  to  your
calculation, not to their reconstruction. It may turn out that their verification RE is also
very negative, but again we cannot assume this in case we’re wrong and they easily
counter the criticism.

Osborn  went  on  to  urge  Mann  to  be  much  more  cautious  in  his  public
pronouncements:

Osborn: 31 October 2003: 1067596623
(e) Claims of their motives for selective censoring or changing of data, or for the study
as a whole, may well be true but are hard to prove. They would claim that theirs is an
honest attempt at producing a key scientific result. If they made errors in what they
did, then maybe they’re just completely out of their depth on this, rather than making
deliberate errors for the purposes of achieving preferred results.

He closed by outlining a list of other issues Mann referred to in the draft.

Osborn: 31 October 2003: 1067596623
It is again a bit of a leap of faith to say that these explain the different results that they
get. Certainly they throw doubt on the validity of their results, but without actually
doing  the  same  as  them  it’s  not  possible  to  say  if  they  would  have  replicated  your
results if they hadn’t made these errors. After all, could the infilling of missing values
have made much difference to the results obtained, something that they made a good
deal of fuss about?

To say they ‘used neither the data nor the procedures of MBH98’ will also be an
easy target for them, since they did use the data that was sent to them and seemed to
have  used  approximately  the  method  too  (with  some  errors  that  you’ve  identified).
This reproduced your results to some extent (certainly not perfectly, but see Fig 6b
and 6c).

Then they went further to redo it with the ‘corrected and updated’ data – but only

after first doing approximately what they claimed they did (i.e. the audit).

Mann  seemed  happy  enough  with  Osborn’s  contribution.  However,  his
reply contains another tantalising turn of phrase:

Mann: 31 October 2003: 1067596623
Let’s let our supporters in higher places use our scientific response to push the broader
case  against  [McIntyre  and  McKitrick].  So  I  look  forward  to  people’s  attempts  to
revise the first [paragraph in] particular. I took the liberty of forwarding the previous
draft  to  a  handful  of  our  closetl  colleagues,  just  so  they  would  have  a  sense  of
approximately  what  we’ll  be  releasing  later  today  –  i.e.,  a  heads  up  as  to  how
[McIntyre and McKitrick] achieved their result . . .

Quite who these ‘supporters in higher places’ are is of course a mystery, but
it is worryingly suggestive of political interference in the scientific process.
Meanwhile,  Osborn  and  Briffa,  like  others  in  the  course  of  this  story
seem to have been less than impressed with Mann’s antics. Osborn wrote to
his colleague a couple of weeks later:

Mann: 12 November 2003: 1068652882
I  do  wish  Mike  had  not  rushed  around  sending  out  preliminary  and  incorrect  early
responses – the waters are really muddied now. He would have done better to have
taken  things  slowly  and  worked  out  a  final  response  before  publicising  this  stuff.
Excel files, other files being created early or now deleted is really confusing things!

On McIntyre and McKitrick 2005
When  McIntyre’s  GRL  paper  was  published  at  the  beginning  of  2005,  it
clearly  presented  the  Hockey  Team  with  a  problem.  While  a  paper  by  a
retired  mining  executive  in  Energy  and  Environment  could  be  brushed
aside,  something  in  a  prominent  journal  like  GRL  was  much  harder  to
ignore. In the email archive we can see that Mann was quick into the fray:
Steve  Mackwell,  the  GRL  editor-in-chief  is  seen  replying  to  a  Mann
complaint, which appears to have concerned the fact that he had not been
allowed to review and respond to the paper before publication. Mackwell
explained that the editor responsible was James Saiers, who, he said, had
been  fully  aware  that  McIntyre’s  paper  challenged  Mann’s  work  and  had
therefore  ordered  a  particularly 
review.  Mackwell  was
sympathetic, but stood firm and suggested that Mann respond by means of a
published comment.

It appears that this was the point at which Mann discovered that Saiers
was the editor of McIntyre’s paper. Having taken on board this snippet of
information  Mann  swung  into  action  and  instigated  an  immediate
investigation  of  Saiers’  background.  Later  the  same  day  he  reported  his
findings to his colleagues on the Hockey Team:

thorough 

Mann: 20 January 2005: 1106322460
Just  a  heads  up.  Apparently,  the  contrarians  now  have  an  ‘in’  with  GRL.  This  guy
Saiers has a prior connection [with] the University of Virginia Dept. of Environmental
Sciencesm  that  causes  me  some  unease.  I  think  we  now  know  how  the  various
Douglass et al papers [with Pat] Michaels and [Fred] Singer, the Soon et al paper, and
now this one have gotten published in GRL.

Tom  Wigley  appears  to  have  been  absolutely  horrified  that  GRL  had
published McIntyre.

Wigley: 20 January 2005: 1106322460
This is truly awful. GRL has gone downhill rapidly in recent years. I think the decline
began before Saiers. I have had some unhelpful dealings with him recently with regard
to a paper [a colleague] and I have on glaciers – it was well received by the referees,
and so is in the publication pipeline. However, I got the impression that Saiers was
trying to keep it from being published.

Proving  bad  behavior  here  is  very  difficult.  If  you  think  that  Saiers  is  in  the
greenhouse  skeptics  camp,  then,  if  we  can  find  documentary  evidence  of  this,  we
could go through official AGU channels to get him ousted. Even this would be difficult.
How different is the GRL paper from the Nature paper? Did the authors counter any of
the criticisms?

Mann was grateful for the support.

Mann: 20 January 2005: 1106322460

Thanks Tom,
Yeah, basically this is just a heads up to people that something might be up here. What
a shame that would be. It’s one thing to lose Climate Research. We can’t afford to lose
GRL. I think it would be useful if people begin to record their experiences [with] both
Saiers and potentially Mackwell (I don’t know him – he would seem to be complicit
[in] what is going on here).

If  there  is  a  clear  body  of  evidence  that  something  is  amiss,  it  could  be  taken
through the proper channels. I don’t [think] that the entire AGU hierarchy has yet been
compromised!

In a later message to Malcolm Hughes, he expands on these concerns:

Mann: 21 January 2005: 1106322460
I’m not sure that GRL can be seen as an honest broker in these debates anymore, and it
is  probably  best  to  do  an  end  run  around  GRL  now  where  possible.  They  have
published far too many deeply flawed contrarian papers in the past year or so. There is
no possible excuse for them publishing all 3 Douglass papers and the Soon et al paper.
These were all pure crap.

There appears to be a more fundamental problem [with] GRL now, unfortunately .

. .

As we have seen, Saiers was ousted as editor in charge of the McIntyre and
McKitrick paper, replaced by Jay Famiglietti.n Later that year, Mann can be
seen discussing possible further sceptic papers criticising the Hockey Team:

Mann: 15 November 2005: 1132094873

The GRL leak may have been plugged up now [with] new editorial leadership there,
but these guys always have Climate Research and Energy and Environment, and will
go there if necessary.

So now there appears to be strong evidence that the Hockey Team sought to
undermine the peer review process at at least three journals. It seems likely
that they were successful on each occasion.
Getting rid of the Medieval Warm Period
The email archive at CRU shines some light on the infamous ‘get rid of the
MedievalWarm  Period’  email.  In  early  2008,  David  Holland,  the  sceptic
who had been seeking Caspar Ammann’s correspondence with Briffa, wrote
to Jonathan Overpeck to inquire if the remarks attributed to him by David
Deming were true. Overpeck seems to have been at a loss to know what to
do, and wrote to several of his colleagues, including Mann, Jones and Susan
Solomon, to ask for advice.

Overpeck: 25 March 2008: 1206628118
I have no memory of emailing [Deming], nor any record of doing so (I need to do an
exhaustive search I guess),o nor any memory of him [from that] period. I assume it is
possible that I emailed . . . him long ago, and that he’s taking the quote out of context,
since [I] know I would never have said what he’s saying I would have, at least in the
context he is implying.

So did Overpeck really make the outrageous statement he is alleged to have
done. Perhaps we will never know. Were the Hockey Team really trying to
‘get  rid  of  the  Medieval  Warm  Period’?  Two  more  emails  from  the  CRU
archive can help colour our views.

In  1999,  prior  to  the  Third  Assessment  Report,  there  is  an  email  from

Briffa in which he says:

Briffa: 22 September 1999: 0938018124
I  know  there  is  pressure  to  present  a  nice  tidy  story  as  regards  ‘apparent
unprecedented warming in a thousand years or more in the proxy data’ but in reality
the situation is not quite so simple.

Who it was that was applying the pressure is unclear, however. Briffa goes
on to discuss the difficulties in justifying such a conclusion.

Briffa: 22 September 1999: 0938018124

We don’t have a lot of proxies that come right up to date and [in] those that do (at least
a significant number of tree proxies) [there are] some unexpected changes in response
that do not match the recent warming. I do not think it wise that this issue be ignored
in the chapter.

There  is  also  a  tantalising  email  from  Mann  to  several  team  members
concerning  an  article  they  were  proposing  to  co-author.  Discussing  the
length of the temperature record they would present he said

Mann: 4 June 2003: 1054736277
I think that trying to adopt a timeframe of [2000 years], rather than the usual [1000],
addresses a good earlier point that [Overpeck] made [with] regard to the memo, that it
would  be  nice  to  try  to  ‘contain’  the  putative  [Medieval  Warm  Period],  even  if  we
don’t yet have a hemispheric mean reconstruction available that far back. . .

It seems clear then that there was outside pressure on the scientists to ‘get
rid of the Medieval Warm Period’, a pressure that in some cases at least,
was not entirely unwelcome. And if future developments turn out to show
that Overpeck did not make the statement attributed to him, it seems clear
that he at least had indicated to his Hockey Team colleagues that he would
be happy to ‘contain’ evidence of past warming.
On the existence of the Medieval Warm Period
In  their  email  correspondence,  several  of  the  scientists  were  much  less
gung-ho  about  the  extent  of  medieval  warmth  than  might  have  been
expected. Back in 1999, Briffa had been saying this:

Briffa: 22 September 1999: 0938031546
For the record, I do believe that the proxy data do show unusually warm conditions in
recent  decades.  I  am  not  sure  that  this  unusual  warming  is  so  clear  in  the  summer
responsive data. I believe that the recent warmth was probably matched about 1000
years ago. I do not believe that global mean annual temperatures have simply cooled
progressively over thousands of years as Mike appears to and I contend that that there
is  strong  evidence  for  major  changes  in  climate  over  the  Holocene  that  require
explanation  and  that  could  represent  part  of  the  current  or  future  background
variability of our climate.

It  is  striking  how  few  of  these  doubts  found  their  way  into  the  Third
Assessment  Report.  By  2003,  the  doubts  were  still  lingering:  Ed  Cook
pointed  out  that  Ray  Bradley  viewed  the  Medieval  Warm  Period  as
‘mysterious and very incoherent’. He went on:

Cook: 29 April 2003: 1051638938
Of course [Bradley] and other members of the MBH camp have a fundamental dislike
for  the  very  concept  of  the  [Medieval  Warm  Period],  so  I  tend  to  view  their
evaluations as starting out from a somewhat biased perspective, i.e. the cup is not only
‘half-empty’; it is demonstrably ‘broken’. I come more from the ‘cup half-full’ camp
when it comes to the [Medieval Warm Period], maybe yes, maybe no, but it is too
early to say what it is. Being a natural skeptic, I guess you might lean more towards
the MBH camp, which is fine as long as one is honest and open about evaluating the
evidence (I have my doubts about the MBH camp). We can always politely(?) disagree
given the same admittedly equivocal evidence.

The  emphasis  in  the  last  quotation  is  added.  This  is  an  extraordinary
statement  for  Cook  to  make  about  one  of  the  most  important  scientists
working in the field of climatology. Briffa’s response indicated that he too
was very cautious about the reality of medieval warmth.

Briffa: 29 April 2003: 1051638938
Can  I  just  say  that  I  am  not  in  the  MBH  camp  –  if  that  be  characterized  by  an
unshakable  ‘belief’  one  way  or  the  other,  regarding  the  absolute  magnitude  of  the
global [Medieval Warm Period].

He did, however, go on to say that he was inclined to believe in the  IPCC
assessment of the time, namely that there was ‘likely unprecedented recent
warmth’.
On Wahl and Ammann
The  questions  over  when  exactly  Wahl  and  Ammann’s  CC  paper  was
submitted and accepted by Climatic Change have already been outlined.p

We see in the emails the struggle to justify the acceptance of Wahl and
Ammann’s  CC  paper  by  the  IPCC  deadline.  Wahl  has  been  discussing
whether the CC paper will meet the IPCC deadline with Jonathan Overpeck
and  they  have  agreed  to  approach  Climatic  Change  editor,  Stephen
Schneider, for advice. Wahl’s email to Schneider is as follows:

Wahl: 11 February: 1139845689
What  I  have  understood  from  our  conversations  before  is  that  if  you  receive  the
[manuscript] and move it from ‘provisionally accepted’ status to ‘accepted’, then this
can be considered in press, in light of [Climatic Change] being a journal of record.

To which Schneider responds:

Schneider: 11 February: 1139845689
Your interpretation is fine – get me the revision soon so I have time to assess your
responses in light of reviews in time! Look forward to receiving it, Steve

In other words, for the purposes of Stephen Schneider, it was sufficient to
interpret  ‘accepted’  as  ‘in  press’.  This  then  enabled  the  CC  paper  to  be
accepted into the IPCC review.

We have also seen that the CC paper relied on statistical arguments in the
other paper, the comment, which was not even submitted until well after the
CC  paper  had  gone  forward  to  the  IPCC  review.  In  September  2007,  just
before the final publication of the CC paper, Jones had clearly noticed that
this  was  likely  to  be  the  cause  of  some  criticism  and  emailed  Wahl  and
Ammann accordingly:

Jones: 12 September 2007: 1189722851

Gene/Caspar,
Good to see these two [papers] out. Wahl/Ammann doesn’t appear to be in Climatic
Change online [at] first, but comes up if you search. You likely know that McIntyre
will check this one to make sure it hasn’t changed since the IPCC close-off date July
2006! . . .

[As for the resurrected comment] – try and change the Received date! Don’t give

those skeptics something to amuse themselves with.

Soon  after  this  startling  statement  from  Jones,  Wahl  made  the  following
statement, in which he appears to admit the problems with the submission
date for the revised comment.

Wahl: 12 September 2007: 1189722851
There were inevitably a few things that needed to be changed in the final version of
[the CC paper] . . . I tried to keep all of this to the barest minimum possible, while still
providing  a  good  reference  structure.  I  imagine  that  [McIntyre  and  McKitrick]  will
make the biggest issue about the very existence of the [revised comment], and then the
referencing of it in [the  CC paper]; but that was simply something we could not do
without,  and  indeed  [the  CC  paper]  does  a  good  job  of  contextualizing  the  whole
matter.

Then  Wahl  seems  to  suggest  that  Stephen  Schneider,  who  you  may
remember  has  been  closely  associated  with  the  growth  of  the  global
warming phenomenon, had been involved in deciding if the Team could get
away with the charade of the revised comment:

Wahl: 12 September 2007: 1189722851
Steve Schneider seemed well satisfied with the entire matter, including its intellectual
defensibility  (sp?)  and  I  think  his  confidence  is  warranted.  That  said,  any  other
thoughts/musings you have are quite welcome.

In Chapter 12,q we saw how, at the start of May 2008, David Holland had
made  his  first  Freedom  of  Information  requests  to  Briffa,  seeking
background information on how certain decisions on the  IPCC chapter had
been taken and, later, requesting all the information the University of East
Anglia  held  on 
including  Briffa’s
correspondence  with  Caspar  Ammann.  The  CRU  emails  reveal  just  how
problematic these requests were for the Hockey Team. In an email to Mann,
Bradley and Ammann on 9 May, Jones said of Holland:

the  Fourth  Assessment  Report 

Jones: 9 May 2008: 1210341221
You can delete this attachment if you want. Keep this quiet also, but this is the person
who is putting in FOI requests for all emails Keith and Tim have written and received
re Ch 6 of AR4. We think we’ve found a way around this.

A way round was certainly needed. In their initial response to Holland, CRU
had advised him that the request must be handled under the Environmental
Information Regulations (EIR). This may have been a mistake on their part.
The  problem  was  that  there  were  far  fewer  exemptions  available  to  them
under EIR than under the Freedom of Information Act (FOI). At some point
over the next few weeks a decision seems to have been made to tell Holland
that the information he requested was not environmental in nature and that
EIR  would  not  apply.  This  would  then  allow  them  to  handle  the  request
under  the  much  weaker  terms  of  FOI  and  so  to  invoke  its  exemptions  for
information  provided  in  confidence  and  requests  that  would  be  too
expensive to process.

Meanwhile, after the issues regarding the publication date of Ammann’s
CC paper became public, Holland had written again, probing the possibility
that there had been a submission of review comments out with the normal
channels. He asked for any additional comments on the drafts that were not
in the online database of review comments, for any additional papers that
had been submitted to the review, and also for any correspondence between
Briffa and Ammann.

The main aspects of this request were dealt with by Phil Jones. Jones’
email,  which  was  addressed  to  Freedom  of  Information  officer,  David
Palmer, but was copied also to Briffa, Osborn and senior faculty manager,
Michael McGarvie, is a truly remarkable document:

. . . Keith (or you Dave) could say that . . .
(1) Keith didn’t get any additional comments in the drafts other than those supplied by
IPCC . . . (2) Keith should say that he didn’t get any papers through the IPCC process,
either. I was doing a different chapter from Keith and I didn’t get any.

Jones: 28 May 2008: 1212009215

What we did get were papers sent to us directly – so not through IPCC asking us
to refer to them in the  IPCC  chapters.  If  only  Holland  knew  how  the  process  really
worked!!  Every  faculty  member  [in  Briffa’s  department]  and  all  the  post-docs  and
most PhDs do, but seemingly not Holland.

So . . . Keith should say that he didn’t get anything extra that wasn’t in the IPCC
comments. As for [Holland’s request for any correspondence with Ammann] Tim has
asked  Caspar,  but  Caspar  is  one  of  the  worse  responders  to  emails  known.  I  doubt
either he emailed Keith or Keith emailed him related to IPCC. I think this will be quite
easy  to  respond  to  once  Keith  is  back.  From  looking  at  these  questions  and  the
Climate Audit web site, this all relates to two papers in the journal Climatic Change. I
know how Keith and Tim got access to these papers and it was nothing to do with
IPCC.

So  clearly,  Ammann  had  not  provided  a  secret  review  but,  shockingly,
Briffa had received a copy of the CC paper and the revised comment directly
from  others.  Unpublished  material,  unavailable  to  the  external  reviewers,
had been used to inform the IPCC review. Even worse, Briffa, Palmer and a
senior member of faculty staff appear to have been sniggering at Holland’s
attempts to get at the truth, all the time ignoring their statutory duty to help
and assist him, thus flouting the spirit of the law.

The  last  part  of  Holland’s  request,  in  which  he  asks  for  copies  of
Ammann’s  correspondence  with  Briffa,  seemed  to  concern  the  scientists
rather  more.  Despite  the  fact  that  Briffa  had  got  hold  of  Ammann’s  new
comment from someone else, the  CRU team still appeared determined that
nothing  would  be 
that  Ammann’s
correspondence  was  not  obviously  confidential.  The  UK  Information
Commissioner’s  guidelines  said  however,  that  in  order  to  determine
confidentiality,  it  was  necessary  to  determine  that  any  release  of  the
information  was  legally  actionable,  if  necessary  by  consulting  the  person
affected.263 To that end, on 27 May 2008, David Palmer, the FOI officer at
the  University  of  East  Anglia,  wrote  to  Tim  Osborn  asking  if  the

released.  The  problem  was 

correspondence between Ammann and CRU was in fact confidential. Osborn
took the hint and wrote the same day to Ammann.

Osborn: 27 May 2008: 1211924186
Our  university  has  received  a  request,  under  the  UK  Freedom  of  Information  law,
from someone called David Holland for emails or other documents that you may have
sent to us that discuss any matters related to the IPCC assessment process. We are not
sure what our university’s response will be, nor have we even checked whether you
sent us emails that relate to the IPCC assessment or that we retained any that you may
have  sent.  However,  it  would  be  useful  to  know  your  opinion  on  this  matter.  In
particular, we would like to know whether you consider any emails that you sent to us
as confidential.

Ammann replied that he would need to look through his correspondence:

Ammann: 27 May 2008: 1211924186
Well, I will have to properly answer in a couple days when I get a chance digging
through emails. I don’t recall from the top of my head any specifics about IPCC.

Ammann  clearly  hadn’t  taken  the  hint  about  the  confidentiality  of  the
correspondence  and  Osborn  therefore  decided  to  make  the  point  slightly
clearer:

Osborn: 30 May 2008: 1212166714
I don’t think it is necessary for you to dig through any emails you may have sent us to
determine  your  answer.  Our  question  is  a  more  general  one,  which  is  whether  you
generally consider emails that you sent us to have been sent in confidence. If you do,
then we will use this as a reason to decline the request.

Ammann replied the same day.

Ammann: 30 May 2008: 1212156886
In  response  to  your  inquiry  about  my  take  on  the  confidentiality  of  my  email
communications with you, Keith or Phil, I have to say that the intent of these emails is
to reply or communicate with the individuals on the distribution list, and they are not
intended  for  general  ‘publication’.  If  I  would  consider  my  texts  to  potentially  get
wider  dissemination  then  I  would  probably  have  written  them  in  a  different  style.
Having said that, as far as I can remember (and I haven’t checked in the records, if
they even still exist) I have never written an explicit statement on these messages that
would label them strictly confidential.

This extraordinary vague response appears to have been enough to convince
the authorities at the University of East Anglia that a release of Ammann’s

correspondence would be legally actionable. This was clearly a very weak
position and Jones seems anyway to have wanted to be absolutely certain
that  nothing  was  going  to  be  revealed.  He  was  prepared  to  take  further
extraordinary steps to do so. This is an extract of an email he sent to Mann
at the end of May 2008:

Mike,
Can  you  delete  any  emails  you  may  have  had  with  Keith  re  AR4?  Keith  will  do
likewise. He’s not in at the moment – minor family crisis. Can you also email Gene
and get him to do the same? I don’t have his new email address. We will be getting
Caspar to do likewise.

Mann: 29 May 2008: 1212073451

Under  UK  Freedom  of  Information  (FOI)  legislation,  deleting  information
that has been requested under the legislation is a criminal offence. It is not
clear, however, whether anything was in fact deleted.

So  can  we  ever  know  who  provided  Briffa  with  a  copy  of  the  revised
comment in Climatic Change? Another set of emails appears to answer the
question. In July 2006, in the middle of the review process, we see Briffa
thanking Eugene Wahl for something.

Gene
Thanks a lot for this – I need to digest and I will come back to you.
Thanks again
Keith

Briffa: 21 July 2006: 1155402164

A  few  hours  later,  Wahl  responds,  apologising  that  there  is  so  much  to
digest,  but  also  providing  the  intriguing  new  detail  that  he  has  been
preparing  a  briefing  paper  on  the  Hockey  Stick  affair  for  ‘a  person  in
[Washington]  DC  who  is  working  on  all  this  with  regard  to  the  [Barton
Hearings].’r The briefing paper is very interesting but is slightly besides the
point  at  issue  here.  However,  one  extract  will  give  both  a  flavour  of  the
piece  and  a  feel  for  how  well  it  represented  the  debate.  This  covers  the
question  of  whether  the  bristlecones  are  valid  proxies  or  not.  (The
annotations and capitals are all Wahl’s.)

Wahl: 21 July 2006: 1155402164
Although  there  are  a  number  of  reasons  to  keep  the  bristlecone  data  in,  maybe  the
most  compelling  reason  they  are  a  NON-ISSUE  is  that,  over  the  common  period  of

overlap (1450–1980), the reconstruction based on using them from 1400–1980 is very
close to the reconstruction based on omitting them from 1450–1980. Since the issues
about the bristlecone response to climate are primarily about 1850 onwards, especially
1900 onwards [KEITH – PLEASE LET ME KNOW IF I AM NOT ACCURATE IN THIS], there is no
reason  to  expect  that  their  behavior  during  1400–1449  is  in  any  way  anomalous  to
their  behavior  from  1450–1850.  Thus,  THERE  IS  NO  REASON  TO  THINK  THAT  THE
BRISTLECONES  ARE  SOMEHOW  MAKING  THE  1400–1449  SEGMENT  OF  THE  MBH
RECONSTRUCTION BE INAPPROPRIATELY SKEWED.

Meanwhile, Briffa had been surveying whatever it was that Wahl had sent
him.

Briffa: 21 July 2006: 1155402164
Your  comments  have  been  really  useful  and  reassuring  that  I  am  not  doing  MM
[presumably Michael Mann, rather than McIntyre and McKitrick] a disservice. I will
use some sections of your text in my comments that will be eventually archived so
hope this is ok with you. I will keep the section in the chapter very brief – but will cite
all the papers to avoid claims of bias.

The next day, Wahl wrote back, somewhat concerned:

Wahl: 22 July 2006: 1155402164
If  I  could  get  a  chance  to  look  over  the  sections  of  my  text  you  would  post  to  the
comments  before  you  do,  I  would  appreciate  it.  If  this  is  a  burden/problem  let  me
know and we’ll work it out.

If it is anything from the [CC] paper, of course that is fine to use at once since it is
publicly available. There will only be exceedingly minor/few changes in the galleys,
including a footnote pointing to the extended RE benchmarking analysis contained in
the [revised comment].

What  I  am  concerned  about  for  the  time  being  is  that  nothing  in  the  [revised
comment] shows up anywhere. It is just going in, and confidentiality is important. The
only  exception  to  this  are  the  points  I  make  .  .  .  concerning  the  [McIntyre  and
McKitrick’s] way of benchmarking the RE statistic. Those comments are fine to repeat
at this point.

Fortunately for Wahl, Briffa appears to have been happy to play along:

Briffa: 24 July 2006: 1155402164
Here is where I am up to now with my responses (still a load to do) – you can see that
I have ‘borrowed (stolen)’ from 2 of your responses in a significant degree – please
assure me that this OK (and will not later be obvious) hopefully.

You will get the whole text (confidentially again) soon. You could also see that I
hope  to  be  fair  to  Mike  [Mann]  –  but  he  can  be  a  little  unbalanced  in  his  remarks
sometime – and I have had to disagree with his interpretations of some issues also.

Please do not pass these on to anyone at all.

It seems fair to say then that Eugene Wahl provided Briffa with a copy of
his unpublished Climatic  Change  comment  in  order  to  assist  in  rebutting
McIntyre,  and  that  contrary  to  IPCC  rules,  Briffa  used  this  information  to
inform his drafting work. It also appears clear that Jones was aware of what
had gone on.
On IPCC deadlines
There is an email from someone called ‘Mel’, working at the  IPCC TSU  to
Overpeck  and  Jansen,  the  coordinating  lead  authors  for  the  paleoclimate
chapter  which  demonstrates  that  Hegerl’s  paper  was  accepted  after  the
official  cutoff  date,  that  cutoff  date  being  retrospectively  changed  to
accommodate it and the other late breaking papers.

Mel: 10 August 2006: 1155497558
Although the deadline for additional accepted papers has now passed, this submission
comes from a [chapter lead author] (Gabi Hegerl) so am forwarding on.

On data withholding
One  interesting  email  exchange  that  is  of  direct  relevance  to  the  Hockey
Stick story concerns McIntyre’s attempts to get hold of Mann’s code for the
Hockey Stick papers.s The CRU email archive contains a message from one
of  the  Climatic  Change  editorial  board,  Professor  Christian  Azar  of  the
University of Goteborg, in which he indicates that he agrees with the rest of
the board that unless what Mann has already posted is sufficient to allow
reproduction of his results, then the code should be released. He adds that
releasing it would anyway be beneficial to the debate.

The day after Azar’s statement, Jones emailed the whole editorial board,

with a long plea that Mann should be allowed to withhold his code.

Jones: 16 January 2004: 1074277559
The papers that [McIntyre and McKitrick] refer [to] came out in Nature in 1998 and to
a lesser extent in GRL in 1999. These reviewers did not request the data . . . and the
code.  So,  acceding  to  the  request  for  this  to  do  the  review  is  setting  a  VERY
dangerous precedent. Mike [Mann] has made all the data series [available] and this is
all anyone should need. Making model code available is something else.

He goes on by explaining that the code is irrelevant to the debate and that
sceptics are picking on Mann:

Jones: 16 January 2004: 1074277559

I’m not sure how many of you realise how vicious the attack on him has been. I will
give you an example.

He then complains that the first Mann heard of McIntyre’s 2003 paper in
Energy and Environment was when the press told him about it. He claims
that  the  peer  review  of  McIntyre’s  paper  was  not  independent  and
complains  that  McIntyre  and  McKitrick’s  paper  had  a  figure  labelled
‘corrected  version’  which  he  felt  contradicted  their  position  that  they
weren’t  publishing  their  own  reconstruction.  Nothing  of  what  Jones  says,
however, could conceivably be labelled a ‘vicious attack’.

Closing off he addresses Schneider directly:

In trying to be scrupulously fair, Steve, you’ve opened up a whole can of worms.

Jones: 16 January 2004: 1074277559

An hour later he contacted Mann (capitals in original):

Jones: 16 January 2004: 1074277559
This is for YOURS EYES ONLY. Delete after reading – please ! I’m trying to redress the
balance. One reply . . . said you should make all available!!t . . . Told Steve separately
. . . to get more advice from a few others as well as [the publisher] and legal.

PLEASE DELETE – just for you, not even Ray and Malcolm.

The reasons why Jones asks so urgently for secrecy are not clear. It may be
that he was asked not to discuss the decision with Mann.

It  is  clear  from  the  email  archive  that  requests  for  data  are  seen  as
burdensome and irritating by the Hockey Team. They believe that there is
an  attempt  to  prevent  them  from  doing  their  work  by  tying  them  up  in
endless requests for information.

With the introduction of the UK Freedom of Information Act in 2005,
the scientists were clearly worried. Tom Wigley wrote to Jones wondering
if  it  meant  he  would  have  to  release  his  computer  code,  a  question  that
Jones  said  would  only  be  answered  in  the  fullness  of  time  once  legal
precedents began to be set on the subject. He reassured Wigley that as an
ex-employee of CRU he would probably not be covered by the Act, a theme
on which he expanded in a later message:

Jones: 21 January 2005: 1106338806

I wouldn’t worry about the code. If [the Freedom of Information Act] does ever get
used by anyone, there is also [intellectual property rights] to consider as well. Data is
covered by all the agreements we sign with [third parties], so I will be hiding behind
them.

However, by 2007, in an email to Tom Karl, Jones was reporting that he had
been able to persuade his FOI officers to help him out.

Jones: 19 June 2007: 1182255717
Think I’ve managed to persuade [University of East Anglia] to ignore all further [FOI]
requests if the people have anything to do with Climate Audit.

And  by  2008,  Phil  Jones  was  reporting  to  his  colleagues  that  CRU  was
coping well.

Jones: 20 August 2008: 1219239172
[Keith  Briffa  and  Tim  Osborn  are]  still  getting  FOI  requests  as  well  as  [the  Hadley
Centre  at  the  Met  Office  and  the  University  of  Reading].  All  our  FOI officers have
been in discussions and are now using the same exceptions not to respond – advice
they got from the Information Commissioner. . .

The . . . line we’re all using is this. IPCC is exempt from any country’s FOI – the

skeptics have been told this.

then, 

there 

the  Information
Extraordinarily 
Commissioner’s office had been providing a variety of public bodies with
advice on how to avoid public requests for information.

is  a  strong  hint 

Meanwhile,  other  members  of  the  Hockey  Team  were  less  happy  with
the way things were going. Ben Santer was one of these. He had been on
the receiving end of one of McIntyre’s requests for data, but his refusal to
comply had cause some problems:

that 

Santer: 2 December 2008: 1228258714
There  has  been  some  additional  fallout  from  the  publication  of  our  paper  in  the
International Journal of Climatology. After reading Steven McIntyre’s discussion of
our paper on climateaudit.com (and reading about my failure to provide McIntyre with
the  data  he  requested),  an  official  at  [The  US  Department  of  the  Environment]
headquarters has written to . . . [Lawrence Livermore National Laboratory, Santer’s
employer], claiming that my behavior is bringing [the lab’s] good name into disrepute.

The next day he expanded on his concerns:

Santer: 3 December 2008: 1228330629

One of the problems is that I’m caught in a real Catch-22 situation. At present, I’m
damned and publicly vilified because I refused to provide McIntyre with the data he
requested. But had I acceded to McIntyre’s initial request for climate model data, I’m
convinced  (based  on  the  past  experiences  of  [other  Hockey  Team  members])  that  I
would  have  spent  years  of  my  scientific  career  dealing  with  demands  for  further
explanations,  additional  data,  Fortran  code,  etc.  (Phil  has  been  complying  with  FOI
requests from McIntyre and his cronies for over two years). And if I ever denied a
single  request  for  further  information,  McIntyre  would  have  rubbed  his  hands
gleefully and written: ‘You see – he’s guilty as charged!’ on his website.

Jones tried to reassure the American:

Jones: 3 December 2008: 1228330629
When the FOI requests began here, the FOI person said we had to abide by the requests.
It took a couple of half hour sessions – one at a screen, to convince them otherwise,
showing them what [Climate Audit] was all about. Once they became aware of the
types  of  people  we  were  dealing  with,  everyone  at  UEA  (in  the  registry  and  in  the
Environmental Sciences school – the head of school and a few others) became very
supportive. I’ve got to know the FOI person quite well and the Chief Librarian – who
deals with appeals. The VCu is also aware of what is going on – at least for one of the
requests, but probably doesn’t know the number we’re dealing with. We are in double
figures.

Meanwhile,  it  was  clear  that,  just  as  they  had  done  on  the  possibility  of
journals  publishing  sceptic  papers,  the  Team  were  also  going  to  use  their
collective  influence  to  keep  the  journals  in  line  on  the  subject  of  data
availability.  Having  been  refused  data  by  Santer,  McIntyre  had  taken  the
issue  up  with 
(IJoC).
Unfortunately,  the  journal  editor  had  said  that  data  archiving  was  not
required by the journal.

International  Journal  of  Climatology 

the 

During the course of 2009, I corresponded with Professor Paul Hardaker,
the  chief  executive  of  the  Royal  Meteorological  Society  (RMS),  which
publishes IJoC, on the question of why the journal had no policy on making
data available. Professor Hardaker was very accommodating, and undertook
to  put  the  question  of  formulating  a  policy  to  the  society’s  publications
committee.  However,  shortly  after  I  had  made  my  first  approaches  to
Professor Hardaker, the Hockey Team seem to have got in touch with him
too. In March 2009. we see Phil Jones emailing Hardaker:

Jones: 19 March 2009: 1237496573
I had been meaning to email you about the RMS and IJoC issue of data availability for
numbers and data used in papers that appear in  RMS journals. This results from the

issue that arose with the paper by Ben Santer et al in IJoC last year. Ben has made the
data available that this complainant wanted. The issue is that this is intermediate data.
The raw data that Ben had used to derive the intermediate data was all fully available.

Santer,  meanwhile,  was  deeply  unimpressed  with  the  idea  of  having  to
make intermediate data available.

Santer: 19 March 2009: 1237496573
If the  RMS  is  going  to  require  authors  to  make  ALL  data  available  –  raw  data  PLUS
results from all intermediate calculations – I will not submit any further papers to RMS
journals.

Jones also seems to have had another issue with the RMS.

Jones: 19 March 2009: 1237496573
I’m having a dispute with the new editor of Weather.v I’ve complained about him to
[Hardaker]. If I don’t get him to back down, I won’t be sending any more papers to
any RMS journals and I’ll be resigning from the RMS.

Since that time, the Royal Meteorological Society’s publications committee
has  met  twice,  but  to  date  there  appears  to  be  no  new  policy  on  data
availability.

Other  scientists  couldn’t  quite  believe  the  Hockey  Team’s  approach  to
the subject. In 2006, Hans von Storch wrote to Keith Briffa having read an
article  that  recounted  some  of  McIntyre’s  problems  with  getting  hold  of
Briffa’s data. These requests, said von Storch, were entirely appropriate and
he quoted what had been written in the article.

Von Storch: 5 August 2006: 1155333435
‘The  issue  of  data  access  was  discussed  in  the  [dendroclimatology]  conference  in
Beijing – some people suggesting that withholding data was giving the trade a black
eye. Industry leaders, such as presumably Briffa, said that they were going to continue
stonewalling’.

Von Storch was incredulous:

Von Storch: 5 August 2006: 1155333435
I can not believe this claim, and I would greatly appreciate if you would help me to
diffuse any such suspicions . . . I am concerned if we do not apply a truly open data
and algorithm-policy, our credibility will be severely damaged, not only in the US but
also in Europe. ‘Open’ means also to provide data to groups which are hostile to our

work – we have done so with our [own] data, which resulted in two hostile comments
in Science, which were, however, useful as they helped to clarify some issues.

Briffa, however, merely brushed him aside:

Briffa: 11 August 2006: 1155333435
Just too bogged down with stuff to even read their crap – but I have no intention of
withholding anything. Will supply the stuff when I get five minutes!!

As we have seen Briffa’s data was only finally released three years later.

Mann may have refused to send his residual series to McIntyre but he
was quite happy to send them to trusted colleagues. In July 2003 he sent the
MBH99 figures to CRU’s Tim Osborn (emphasis added).

Mann: 31 July 2003: 1059664704

Tim,
Attached  are  the  calibration  residual  series  for  experiments  based  on  available
networks. . .

Basically, you’ll see that the residuals are pretty red for the first 2 cases, and then
not significantly red for the 3rd case – its even a bit better for the AD 1700 and 1820
cases,  but  I  can’t  seem  to  dig  them  up.w  In  any  case,  the  incremental  changes  are
modest  after  1600  –  it’s  pretty  clear  that  key  predictors  drop  out  before  AD 1600,
hence the redness of the residuals, and the notably larger uncertainties farther back . . .
You only want to look at the first column (year) and second column (residual) of
the  files.  I  can’t  even  remember  what  the  other  columns  are!  Let  me  know  if  that
helps.
Thanks,
Mike
P.S. I know I probably don’t need to mention this, but just to insure absolutely clarity
on  this,  I’m  providing  these  for  your  own  personal  use,  since  you’re  a  trusted
colleague. So please don’t pass this along to others without checking [with] me first.
This is the sort of ‘dirty laundry’ one doesn’t want to fall into the hands of those who
might potentially try to distort things. . .

On bristlecones
There is an interesting exchange of emails that was prompted by an series
of exchanges on Climate Audit between paleoclimatologist Martin Juckes
and McIntyre and his readers. Juckes had been trying to defend his use of
bristlecones  in  the  face  of  the  NAS  panel’s  conclusions  that  this  was  not
advisable, and he indicated in an email that he was going to contact Gerry
North to see if this was really what he meant. Unfortunately, North was not
willing to come off the fence, deferring instead to the panel member who
had  written  the  paragraph  on  bristlecones.  Juckes  however  felt  that  the
evidence for carbon dioxide fertilisation in bristlecones was weak

Briffa on the other hand was quite convinced that there were problems

with the species, if not necessarily to do with carbon dioxide fertilisation:

Briffa: 6 November 2006: 1163715685
In my opinion (as someone who has worked with the bristlecone data hardly at all!)
there  are  undoubtedly  problems  in  their  use  that  go  beyond  the  strip  bark  problem
(that I will come back to later). . .

The main one is an ambiguity in the nature and consistency of their sensitivity to
temperature variations . . . The bottom line though is that these trees likely represent a
mixed  temperature  and  moisture-supply  response  that  might  vary  on  longer
timescales.

He also said that stripbark problem meant that the bristlecones ‘will have
unpredictable  trends  as  a  consequence  of  aging  and  depending  on  the
precise nature of each tree’s structure’, and referred to Mann’s adjustment
for carbon dioxide fertilisation as ‘very arbitrary’. In fairness, he also noted
that  one  author  had  suggested  that  there  was  in  fact  no  carbon  dioxide
fertilisation at all.

Esper agreed, suggesting that the wider rings in recent years were due to

physical damage rather than temperature:

Esper: 6 November 2006: 1163715685
I didn’t visit the bristlecone sites yet, but the mechanism might be [physical damage].
I believe that over time the crown and root system are reduced, but not at the same
rate than the reduction in circumference covered by the cambium. This would be the
key for strip bark tree rings being wider than ‘normal’ rings.

Another  paleoclimatologist,  Rob  Wilson,  said  that  he  had  avoided  using
bristlecones after noting McIntyre’s findings. He went on to note that there
didn’t seem to be a correlation between the bristlecones and temperature,
although  he  thought  there  was  at  least  some  temperature  response  in  the
record.
On Yamal
Several  of  the  emails  touch  on  the  subject  of  Yamal.  It  is  revealed  that
Briffa was funding Stepan Shiyatov. In one email, Shiyatov asks Briffa to
send multiple small payments to his personal bank account:

Shiyatov: 7 March 1996: 0826209667
It is important for us if you can transfer the ADVANCEx money on the personal accounts
which we gave you earlier and the sum for one occasion transfer (for example, during

one day) will not be more than 10,000 USD. Only in this case we can avoid big taxes
and use money for our work as much as possible.

When  Briffa’s  data  was  finally  released,  there  was  clearly  consternation
among the scientists, a situation that was exacerbated by the fact that Briffa
was recovering from a serious illness. In his absence, Mann and Schmidt
approached  Briffa’s  colleague  Tim  Osborn  for  help  in  formulating  a
response.

But while Osborn had co-authored with Briffa in the past, he hadn’t been

involved with the Royal Society paper and wasn’t able to help much.

Osborn: 29 September 2009: 1254230232
Regarding Yamal, I’m afraid I know very little about the whole thing – other than that
I  am  100%  confident  that  ‘The  tree  ring  data  was  hand-picked  to  get  the  desired
result’ is complete crap. Having one’s integrity questioned like this must make your
blood  boil  (as  I’m  sure  you  know,  with  both  of  you  having  been  the  target  of
numerous such attacks) . . .

Apart from Keith, I think Tom Melvin here is the only person who could shed
light on the McIntyre criticisms of Yamal. But he can be a rather loose cannon and
shouldn’t be directly contacted about this . . .

Melvin, also from CRU, was a Briffa co-author on the Royal Society paper.
It appears, however, that the Hockey Team did not consider him as one of
their own. This must have been something of a problem, with Briffa out of
action  and  Melvin  not  trusted.  How  would  they  respond?  Fortunately,  by
the next day Briffa had agreed to pull himself from his sickbed, and Mann
and  Osborn  agreed  that  a  rebuttal  would  be  issued,  despite  the  fact  that
nobody had actually examined McIntyre’s work at that point.

Mann set straight to work, responding to a request for information from

the New York Times’ Andy Revkin with this:

Mann: 29 September 2009: 1254258663
The  preliminary  information  I  have  from  others  familiar  with  these  data  is  that  the
attacks  are  bogus  .  .  .  even  if  there  were  a  problem  [with]  these  data,  it  wouldn’t
matter as far as the key conclusions regarding past warmth are concerned. But I don’t
think there is any problem with these data, rather it appears that McIntyre has greatly
distorted the actual information content of these data.

Again, it is not clear that any detailed examination of McIntyre’s claims had
yet taken place. It is also odd that Revkin would ask Mann for information
about Briffa’s paper. Mann had not been involved in Briffa’s paper at all.

Meanwhile, other members of the Hockey Team were more concerned.

Tom Wigley in particular was very worried:

Wigley: 5 October 2009: 1254756944
Keith [Briffa] does seem to have got himself into a mess. As I pointed out in emails,
Yamal is insignificant. And you say that (contrary to what [McIntyre and McKitrick]
say) Yamal is not used in MBH, etc.y So these facts alone are enough to shoot down
[McIntyre and McKitrick] in a few sentences (which surely is the only way to go –
complex and wordy responses will be counter productive).

But,  more  generally,  (even  if  it  is  irrelevant)  how  does  Keith  explain  the
McIntyre plot that compares Yamal-12 with Yamal-all? And how does he explain the
apparent ‘selection’ of the less well-replicated chronology rather that the later (better
replicated) chronology?

Of course, I don’t know how often Yamal-12 has really been used in recent, post-
1995,  work.  I  suspect  from  what  you  say  it  is  much  less  often  that  [McIntyre  and
McKitrick] say – but where did they get their information? I presume they went thru
papers to see if Yamal was cited, a pretty foolproof method if you ask me.

Perhaps these things can be explained clearly and concisely – but I am not sure
Keith is able to do this as he is too close to the issue and probably quite pissed off.
And the issue of withholding data is still a hot potato, one that affects both you and
Keith  (and  Mann).  Yes,  there  are  reasons  –  but  many  good  scientists  appear  to  be
unsympathetic  to  these.  The  trouble  here  is  that  withholding  data  looks  like  hiding
something,  and  hiding  means  (in  some  eyes)  that  it  is  bogus  science  that  is  being
hidden.

I think Keith needs to be very, very careful in how he handles this. I’d be willing

to check over anything he puts together.

On the Hockey Stick
There is one lovely email from John Mitchell, who we met earlier in his
role of IPCC review editor. Here he is saying his piece to Eystein Jansen and
Jonathan Overpeck on the subject of the second order draft comments.

Mitchell: 21 June 2006: 1150923423
I am in Geneva . . . so I have not had a lot of time to look at the [Second Order Draft]
comments. I can not get to Bergen before Tuesday. I had a quick look at the comments
on  the  Hockey  Stick  and  include  below  the  questions  I  think  need  to  be  addressed
which I hope will help the discussions. I do believe we need a clear answer to the
skeptics. I have also copied these comments to Jean [Jouzel, the other review editor] .
. .

1. There needs to be a clear statement of why the instrumental and proxy data are
shown on the same graph. The issue of why we don’t show the proxy data for the last
few decades (they don’t show continued warming) but assume that they are valid for
early warm periods needs to be explained.

This is an extraordinary statement. Clearly, senior IPCC scientists knew that
the  proxy  records  showed  no  warming  in  recent  decades.  Mitchell  felt  it

needed to be explained. It is clear from the IPCC report however, that Jansen
and Overpeck did not take him up on this suggestion. The information that
proxy records do not now show any warming has been suppressed.

Mitchell: 21 June 2006: 1150923423
2 . There are number of methodological issues which need a clear response. There are
two aspects to this. First, in relation to the [Third Assessment Report and  MBH98],z
which seems to be the obsession of certain reviewers. Secondly (and this I believe this
is the main priority for us) in relation to conclusions we make in the chapter we should
make  it  clear  where  our  comments  apply  to  only  MBH  (if  that  is  appropriate),  and
where they apply to the overall findings of the chapter. Our response should consider
all  the  issues  for  both  MBH  and  the  overall  chapter  conclusions.  a.  The  role  of
bristlecone pine data: Is it reliable? Is it necessary to include this data to arrive at the
conclusion  that  recent  warmth  is  unprecedented?  b.  Is  the  [PC  analysis]  approach
robust? Are the results statistically significant? It seems to me that in the case of MBH
the answer in each is no. It is not clear how robust and significant the more recent
approaches are . . .

So  amazingly,  Professor  Mitchell  believed  that  the  Hockey  Stick  used  a
biased methodology and gave results that were not statistically significant
and yet signed off the paleoclimate chapter as ‘a reasonable assessment’ of
the evidence. Yet here is how the final version of the IPCC report explained
the Hockey Stick debate:

McIntyre and McKitrick . . . raised . . . concerns about the details of the Mann et al.
(1998)  method,  principally  relating 
the
reconstruction  against  19th-century  instrumental  temperature  data  and  to  the
extraction of the dominant modes of variability present in a network of western North
American  tree  ring  chronologies,  using  Principal  Components  Analysis.  The  latter
may have some theoretical foundation, but Wahl and Ammann (2006) also show that
the impact on the amplitude of the final reconstruction is very small.264

independent  verification  of 

to 

the 

Readers can judge for themselves whether Professor Mitchell should have
accepted this as a fair reflection of the dispute.

Professor Mitchell’s role in the IPCC review was to umpire disputes and
ensure that both sides of any argument were fairly represented in the report.
Yet here we see him engaged in an ongoing correspondence discussing not
how  to  represent  sceptic  positions  in  the  report,  but  how  to  give  them  ‘a
clear  answer’.  Disputes  were  supposed  to  be  reported  in  an  annex  to  the
report,  and  yet  there  is  no  sign  of  this  having  been  even  considered  by
Mitchell and Briffa.

Where do we stand now?
The initial shock of the leaking of the CRU emails seems to be dying away,
but it is clear that the reverberations will continue for months to come. The
emails  have  been  analysed  by  many  sets  of  eyes,  and  the  most  obvious
outrages are all now in the public realm. As those closer to the story survey
the evidence more closely in coming weeks, there can be little doubt that
further  revelations  will  be  made.  Analysis  has  also  begun  on  the  files  of
data and code that were released along with the emails. Already we have
seen the quality of computer programming come in for serious criticism.

The emails were released after completion of the text of this book. What
is extraordinary to me as a writer is how much of the content of the emails
entirely corroborates what I had written in the previous chapters. In light of
all I had learned while researching this book, the emails read exactly as I
might have imagined they would. The sceptic community, and particularly
McIntyre  and  McKitrick,  had  been  extraordinarily  insightful  in  their
analysis of what was happening behind the scenes of the global warming
movement. This means that everything I wrote in Chapter 15  still  stands.
However, the  CRU  emails  have  shown  us  that  the  situation  is  even  worse
than  was  thought.  For  the  purposes  of  this  book  there  are  two  clear
conclusions to be drawn from the emails. Firstly that senior climatologists
have sought to undermine the peer review process and bully journals into
suppressing dissenting views. This means that the scientific literature is no
longer a representation of the state of human knowledge about the climate.
It is a representation of what a small cabal of scientists feel is worthy of
discussion. Secondly, the IPCC reports represent the outcome of a process in
which  a  relatively  small  group  of  scientists  produce  a  biased  review  of  a
literature they themselves have colluded to distort through gatekeeping and
intimidation. The emails establish a pattern of behaviour that is completely
at odds with what the public has been told regarding the integrity of climate
science and the rigour of the IPCC report-writing process. It is clear that the
public can no longer trust what they have been told. What is less clear is
what we, as ordinary citizens, can do in the face of the powerful, relentless
forces of corrupted science, to set things right. Awareness, however, is the
essential first step.

a  See page 33.

b  Wigley’s evidence has been outlined above. The evidence of Danny Harvey, a professor at the

University of Toronto, does not appear in the email archive.

c  See page 230.
d  Professor of Climate Change at the University of East Anglia. In recent years Hulme has made
calls  for  climate  change  rhetoric  to  be  toned  down,262  so  it  is  interesting  to  see  that  he  was
apparently central to the plan to oust von Storch, thus suggesting that he has only recently taken up
his position as the voice of moderation.

e  A later email suggests that Wigley is referring to Mike Hulme rather than Mann.
f  The latter are all prominent climate sceptics.
g  We met David Appell in a later role as media outlet for the Hockey Team on page 65.
h  See page 65.
i  See page 70.
j  See page 63.
k  See page 65
l  The word ‘closet’ here may be a typing error, which should actually read ‘closest’.
m This may be a reference to the prominent sceptic Pat Michaels, a member of staff at the University

of Virginia.

n  See page 161.
o  It is notable that Overpeck misspells Deming’s name as ‘Deeming’ throughout this email. It is
possible that if he searched his emails for ‘Deeming’ he would have missed the relevant message’.

p  See page 167.
q  See page 278.
r  The first hearing had been held two days earlier.
s  See page 120.
t  Jones was not referring to the email from Azar, but to one of the other members of the board who

had expressed similar sentiments.

u  Apparently the Vice Chancellor, at that time Bill MacMillan.
v  Another RMS journal.
w  Residuals should be ‘white’, which is to say entirely random, rather than red. Mann is implying

that there is a problem with the calibration.

x  This appears to be a funding programme of some kind.
y  Although some members of the press made this erroneous allegation, I have been unable to locate
any instances of McIntyre doing so. It may be that Wigley is conflating McIntyre’s comments with
those of journalists.

z  Mitchell wrote MBA, which I assume is an error. Presumably he meant MBH.

APPENDIX A

A good trick to create a decline

In Chapter 11, we saw that during the review process for the IPCC’s Fourth
Assessment Report, McIntyre had suggested to the authors of the report’s
paleoclimate  chapter 
temperature
reconstructions  were  affected  by  carbon  dioxide  fertilisation.  In  his
response  Briffa  had  said  that  carbon  dioxide  fertilization  effect  had  been
adjusted for in MBH99; it was certainly true that Mann had said that this is
what he had done.

the  key  global 

that  some  of 

As we have seen, the MBH99 paper took the Hockey Stick reconstruction
right  back  to  the  start  of  the  millennium,  apparently  demonstrating  that
temperatures  had  declined  slowly  but  steadily  for  almost  nine  hundred
years. This decline, said Mann, was consistent with the Milankovitch cycle,
the tiny changes in the Earth’s orbit that are thought to cause the ice ages to
come and go. Then, at the start of the twentieth century, there had been a
sudden  uptick  in  temperatures  coinciding  with  the  onset  of  mass
industrialisation.  Since  the  long  decline  and  the  twentieth  century  uptick
were apparent even after carbon dioxide fertilization had been adjusted for,
it  seemed  that  the  findings  of  the  original  Hockey  Stick  paper  were
confirmed and strengthened.

Briffa’s  response,  however,  overlooked  two  inconvenient  facts.  Firstly,
none  of  the  other  reconstructions  presented  in  the  IPCC  report  had  been
corrected for carbon dioxide fertilisation, so there was a big question mark
over  their  reliability.  In  addition,  as  we  have  seen,  in  the  period  where
carbon  dioxide  fertilisation  was  an  issue  the  reconstruction  in  MBH99
appeared to be identical to the one in MBH98 (see p. 333). This appeared to
suggest that no adjustment had actually been made in practice.

Researching the adjustment
After  the  appearance  of  the  Fourth  Assessment  Report,  McIntyre  had
explained Mann’s strange adjustment to Climate Audit readers. In essence it
was  rather  simple,  although  in  practice  the  procedure  Mann  had  adopted

was rather complicated. As we have seen, temperature reconstructions tend
to use two different sets of tree ring records: those from high-elevation sites
and those from sites at the northern limit of the geographical range of the
species of tree concerned – it is only these groups where the tree rings are
expected to change in line with temperature. However, scientists also think
that it is only the high-elevation sites that are affected by carbon dioxide
fertilization,  and  using  this  insight  had  given  Mann  a  way  to  assess  the
magnitude  of  the  carbon  dioxide  fertilisation  effect  and  so  to  create  an
adjustment.

The wavy line is the residual – the size of gap between the two tree ring
series  –  and  as  Mann  showed  it,  this  roughly  tracked  the  carbon  dioxide
trend,  which  is  the  flat  line  at  the  left-hand  side,  which  then  shoots  off
skywards  at  the  top  of  the  graph.  However,  while  there  was  a  good
correlation between the residual and carbon dioxide for most of the record,
after  around  the  start  of  the  twentieth  century  the  match  seemed  to  fall
apart,  with  carbon  dioxide  still  trending  upwards  but  the  residuals  falling
away, only rising again after about 1950.

Mann had compared the high-elevation sites, including the bristlecones,
to  a  northern  tree  line  series  created  by  another  climatologist  –  Gordon
Jacoby of the Lamont-Doherty Earth Observatory at Columbia University
in  New  York.  When  overlaid,  the  graphs  of  the  two  series  tracked  each
other  remarkably  closely  for  most  of  the  length  of  the  record,  until  the
nineteenth  century,  when  the  bristlecone  record  started  shooting  upwards
while the northern treeline series was apparently unaffected. The gap that
emerged between the two series after 1800 was, in essence, a measure of
the carbon dioxide fertilization effect. This can be seen in Figure A1.

Now of course, the size of the gap (‘the residuals’) would be expected to
track  carbon  dioxide  concentrations  in  the  atmosphere  and  Mann  had
attempted to demonstrate that this was in fact the case by charting carbon
dioxide  concentration  and  the  residuals  on  the  same  chart,  as  shown  in
Figure A2.

FIGURE A1: The Jacoby coercion
The bristlecones (dotted line) have been forced to match the Jacoby record
(in black). The adjusted bristlecone record is the grey line.

FIGURE A2: Correlation of the residuals to carbon dioxide

Mann’s explanation for this phenomenon was as follows: some time after
1900  the  effect  became  ‘saturated’,  such  that  tree  growth  was  no  longer
fertilised  –  this  meant  that  further  rises  in  carbon  dioxide  produced  no
further  divergence  between  the  two  tree  ring  series.  In  support  of  his
position, he presented what he called the ‘secular trend’ in the residual – a
version of the residuals that had been smoothed over the very long period of
75 years. This is shown as the dotted line Fig A2, which, because of its long
smoothing filter, does indeed seem to tail off at the end. Careful observers
of his explanation might have pointed out that the underlying data, the wavy
line, was actually shooting upwards at the end of the record, suggesting that
this saturation hypothesis was actually mistaken, but this point appears to

have eluded the peer reviewers of Mann’s paper and so, for the moment at
least, his explanation stood.

With his explanation in hand, Mann had subtracted the secular trend –
the  smoothed  difference  between  the  two  types  of  trees  –  from  the
bristlecone series. This forced the bristlecone record down to the levels of
the Jacoby northern treeline series in the twentieth century. The result is the
dotted line shown in Figure A1.

When  McIntyre  started  to  examine  this  part  of  Mann’s  calculations,
however, things weren’t quite as they might have seemed from the text of
the original paper. Having obtained copies of the two tree ring series and
the  carbon  dioxide  record,  McIntyre  created  his  own  version  of  Mann’s
comparison of the trend and obtained a result that was strikingly different
(see Figure A3).

In  McIntyre’s  version  of  the  comparison,  there  was  virtually  no  match
between the carbon dioxide trend and the residual series or the smoothed
secular version of it. In fact it appeared as if the growth spurt in the high
altitude trees preceded the rise in carbon dioxide. The only explanation was
that Mann had rescaled the carbon dioxide record – stretching the vertical
axis to get a better match to the residuals.

FIGURE A3: McIntyre’s first attempt
The rise in tree growth (in grey) appears to precede the rise in carbon dioxide (in red).

Mann had given no indication of what he had done in practice, but the
obvious  way  to  achieve  such  a  rescaling  was  by  means  of  regression
analysis,  a  technique  that  would  calculate  a  mathematical  relationship
between the two records and from this derive a transformed version of the

carbon  dioxide  record  that  would  track  the  residuals  better.  McIntyre
therefore  prepared  his  own  regression  analysis  to  test  this  idea  and  his
results are shown in Figure A4.

As can be seen, the rescaled carbon dioxide line now passes through the
centre  of  all  the  waves  in  the  residuals.  However,  this  still  looks  nothing
like  Mann’s  own  version  (Figure A2),  where  the  two  lines  part  company
after 1900 AD. It looked almost as if Mann might have set the algorithm to
match the two lines only as far as the end of the nineteenth century. In order
to  test  this  new  idea,  McIntyre  performed  a  second  version  of  his
calculation, this time forcing a fit only over this shorter period (see Figure
A5).

FIGURE A4: McIntyre’s second attempt
By rescaling the carbon dioxide record, it can be made to track the residuals better. This was still
different to Mann’s paper, however.

While  not  identical  to  Mann’s  result,  McIntyre’s  new  calculations
certainly  gave  him  something  that  very  close  to  it,  suggesting  that  Mann
had used a slight variation on it in practice. If so, it raised a multitude of
questions about all the different steps Mann had gone through. Why smooth
the  residuals  over  75  years?  Did  such  an  adjustment  have  any  meaning,
particularly  since  the  underlying  tree  ring  series  had  already  been
smoothed? Wasn’t the levelling off in this ‘secular trend’ merely an artefact
of  the  smoothing,  which  mean  that  it  couldn’t  be  used  to  suggest  a
saturation of the carbon dioxide fertilization effect?

Then  again,  why  restrict  the  fit  of  the  carbon  dioxide  series  to  the
residuals to the end of the nineteenth century? Why not match them over

their full length? There were many other questions too: why use Jacoby’s
northern  treeline  series,  much  of  which  was  already  obsolete  by  the  time
Mann performed his calculations? What is more, Jacoby’s records had come
from  Northern  Canada,  thousands  of  miles  from  the  bristlecones  in  the
Western  USA.  Who  was  to  say  that  there  wasn’t  some  regional  effect
distorting the relationship between the two series?

FIGURE A5: The restricted regression

By restricting the match to the period 1400–1900,
McIntyre was able to get a close match to Mann’s paper.

Many of these questions remain unanswered, but in time McIntyre and
his Climate Auditors would get to the bottom of some of them and, as so
often  in  the  story  of  the  Hockey  Stick,  there  were  some  interesting  tales
uncovered.

A program
A few clues were unearthed among the files on Michael Mann’s University
of  Virginia  website  –  the  same  site  where  McIntyre  had  discovered  the
program Mann had used to calculate tree ring principal components (see p.
105).  Among  these  files,  McIntyre  had  discovered  a  computer  program
called  co2detrend.f,  which  appeared  to  be  the  actual  program  used  to
perform  the  carbon  dioxide  adjustment  –  certainly  the  comments  on  the
program suggested that this was the case:

c regress out co2-correlated trend (r=0.9 w/ co2)

c after 1800 from pc1 of ITRDB data
c remove co2-correlated portion (r=0.9) of 1800-1980
c corr= 0.9

The problem was that when McIntyre used co2detrend.f to process the tree
ring and carbon dioxide data, it turned out that it wasn’t the adjustment used
in  the  Hockey  Stick  paper.  Quite  what  it  did  and  where  it  was  used
remained a mystery.

Finlandia
The details of the carbon dioxide adjustment remained a mystery until the
sensational release of the Climategate emails at the end of 2009. Although
public  attention  has  been  focused  on  the  emails  released  from  CRU,  there
were also large numbers of data and program files in the CRU archive. While
the media storm was raging, experienced eyes were examining these to see
what they contained.

‘Jean  Sibelius’  is  the  pseudonym  for  a  professional  statistician  from
Finland.  A  regular  commenter  and  occasional  guest  poster  at  McIntyre’s
Climate Audit website, JEANS, as he is usually known, had been a valuable
foil to McIntyre’s work over the years, suggesting new lines of inquiry and
challenging  McIntyre’s  thinking.  Over  the  years  Sibelius  had  developed
considerable expertise in the intricacies of the Hockey Stick paper, and was
familiar with all of the remaining mysteries such as the confidence interval
calculations and of course the carbon dioxide adjustment.

One  of  the  problems  that  Sibelius  had  been  worrying  about  concerned
the apparent discrepancy between the adjustment as described by Mann in
the text of his paper and what appeared to have been done in practice. As
we saw above, the narrative of the paper described smoothing of both the
tree  ring  and  the  carbon  dioxide  records,  the  former  result  then  being
smoothed again  before  the  adjustment  was  calculated.  But  when  Sibelius
compared  the  original  PC1  to  the  adjusted  one,  the  difference,  while  very
similar to the secular trend, was not identical. Rather than being a smooth
line, it was stepped, or ‘piecewise linear’ in the jargon (see Figure A6). This
certainly ruled out co2detrend.f having been used to create the adjustment,
and even suggested that, contrary to the narrative in the paper, the secular
trend might not have been used in the adjustment at all.

FIGURE A6: Mann’s actual correction was stepwise linear

A new program
So  there  was  a  program  that  purported  to  do  the  adjustment  but  didn’t.
Moreover,  the  actual  adjustment  seemed  to  be  only  some  sort  of  an
approximation to the adjustment that was described in the paper. This was
extremely  strange.  However,  when  Sibelius  started  examining 
the
Climategate files, he stumbled across what turned out to be the solution to
this conundrum. Trawling through the leaked data, he noticed that amongst
the  disclosed  information  was  a  directory  of  files  relating  to  the  Hockey
Stick,  which  appeared  to  have  been  obtained  by  Tim  Osborn.  Examining
these files, Sibelius noticed that as well as co2detrend.f there was another
program called residualdetrend.f. This immediately stood out as being a file
that had not previously been discussed and so he set to work to see what it
contained.  To  his  surprise,  almost  the  first  thing  he  noticed  was  some
remarkably familiar text:

c regress out co2-correlated trend (r=0.9 w/ co2)
c after 1800 from pc1 of ITRDB data

The comment was almost identical to the one from co2detrend.f. This was
very strange. Why should Mann have created two programs to do the same
adjustment? As he read further, he realised that he might have solved the
problem. There, in the comments was a suggestion that this second program
might  create  an  adjustment  that  was  piecewise  linear,  as  the  final
adjustment appeared to be.

c linear segments describing approximate residuals
c relative to fit with respect to secular trend

And as he checked over the code, Sibelius realised that it did just what the
comments  suggested,  creating  a  piecewise  linear  adjustment  that  roughly
matched up with the secular trend in carbon dioxide shown in the original
paper. Why Mann should choose to create this approximation to the secular
trend, instead of simply using the secular trend itself, remained a mystery.

This  then  appeared  to  be  the  program  Mann  had  used  to  create  the
adjustment, but its existence then raised the uncomfortable question of what
the other program, co2detrend.f, was for?

Sibelius  examined  the  two  programs  side-by-side  and  quickly  noticed
that  they  both  output  their  results  to  the  same  file,  pc01-fixed.dat.  This,
together with the similarity of the comments and the file names, certainly
seemed to indicate that they were different attempts at the same adjustment.
The  suggestion  of  a  ‘trial  and  error’  approach  to  the  adjustment  was
disturbing, hinting at a search for a desired answer rather than a scientific
consideration of how the issue could best be corrected from a physical point
of view.

And another
At this point, Sibelius had another surprise. In the same directory, there was
another file that seemed relevant. This was a data file, the name of which –
pc1-fixed-old.dat – suggested that it might hold the output from the rejected
co2detrend.f  program.  However,  knowing  that  surmises  of  this  kind  are
often  unwarranted  when  studying  the  Hockey  Stick,  Sibelius  decided  to
make sure. To his surprise, it turned out that the data in the file was entirely
different and appeared to represent the results of a third attempt at creating
a carbon dioxide adjustment.

The  three  adjustments  –  co2detrend,  residualdetrend  and  the  newly

discovered ‘oldfix’ – are shown in Figure A7:

FIGURE A7: Three attempts to correct for carbon dioxide fertilization

FIGURE A8: The Hockey Stick with no fix for carbon dioxide fertilization.
The handle of the stick, from 1000 to 1900 is almost trendless.

Adjusting the Medieval Warm Period
Although  we  do  not  know  in  what  order  the  fixes  were  calculated,  it  is
probably fair to assume that Mann started from the unfixed graph and ended
up  with  the  version  that  he  finally  published.  If  so,  then  it  throws
considerable  light  on  the  reasons  for  Mann’s  multiple  smoothing  of  the
records  and  the  strange  restriction  of  the  residual–carbon  dioxide  match.
The first results of the MBH99 algorithm – before any fix for carbon dioxide
fertilization was applied – is shown in Figure A8:

To  anyone  used 

to 

this
immediately looks odd. For 900 years, from the beginning of the chart to
the  end  of  the  nineteenth  century,  there  is  essentially  no  trend  in  the
reconstructed  temperature  at  all,  giving  the  chart  a  very  artificial

temperature  reconstructions, 

looking  at 

appearance.  This  would  simply  never  be  accepted  by  the  climatological
community where it was widely accepted that temperatures are affected by
trends overmuch longer timescales than a century. The absence of any such
long-term  trend  would  immediately  raise  question  marks  over  Mann’s
result.

To  those  who  have  followed  the  Hockey  Stick  story  closely,  the
appearance  of  the  uncorrected  reconstruction  is  also  reminiscent  of  the
observation made by several sceptics that the effect of Mann’s calibration
algorithm had been to pick out series with twentieth century upticks. These
upticks then gave a blade to the Hockey Stick graph. But in the rest of the
series, these same series were essentially random and cancelled each other
out.  The  long  flat  blade  of  the  uncorrected  reconstruction  seemed  to  be
strong confirmation that this was the case.

The stepwise calculation
At  this  point  we  need  to  remind  ourselves  how  Mann  had  put  his
temperature reconstructions together.

The  essence  of  a  temperature  reconstruction  is  to  find  a  mathematical
relationship  between  the  proxies,  i.e.  the  tree  rings,  and  temperature.
Because so many of the tree rings are from one or two geographical areas
though, these first have to be summarised using PC analysis. Once this has
been  done  a  mathematical  model  can  be  worked  out  for  the  relationship
between, on the one hand, this summary and all the other data series in the
twentieth  century  and  on  the  other  hand  the  temperature  records  for  the
same  period.  These  mathematical  relationships  can  be  used  to  work  out
temperatures in earlier periods from the tree ring data.

When summarizing using PC analysis it is important to have no gaps in
the  data  and  this  should  have  been  a  problem  for  Mann  because,  as  the
bristlecone series went back in time, there were fewer and fewer trees and
therefore more and more gaps in the data series: as the earliest date for each
series was reached, any previous dates were seen as gaps. To get round this
problem  Mann  had  performed  the  PC  calculation  in  a  ‘stepwise’  fashion:
essentially he prepared several reconstructions and spliced sections of them
together to get his final graph.

Let’s look at a simplified example that should help you understand how
this process worked. In our example, all of the trees are at least 600 years
old. Some are even older than this, going back a further 250 years, and a

smaller  number  again  go  right  back  to  the  start  of  the  last  millennium.
Under the stepwise methodology, a reconstruction back to 1400 AD would
have  been  prepared  first  (the  top  line  in  Figure  9).  Then  a  second
reconstruction  would  be  prepared  on  the  subsample  of  the  trees  that
extended back to 1150 AD (the middle line) and finally the small number of
trees that were 1000 years old would be used to prepare a reconstruction
right back to 1000 AD.

The top line, having the most trees, is the most reliable reconstruction so
all of this graph is used in the final reconstruction. Between 1150 and 1400
AD,  however,  the  most  reliable  estimate  of  temperature  available  is  from
the  middle  graph,  so  the  section  of  this  line,  which  is  coloured  black  in
Figure  A9,  would  be  extracted  and  spliced  onto  the  top  line.  Finally  the
1000–1150  section  of  the  bottom  chart  would  be  used  for  the  earliest
section of the final Hockey Stick graph.

FIGURE A9: Example of a stepwise calculation
Reconstructions are calculated on different subsets of the tree ring data. The black sections of
each reconstruction are spliced together to make the final graph.

Now this is a simplified example and in practice Mann used steps that
were  rather  different.  It  is  worth  examining  these  as  they  are  somewhat
intriguing.

Step
1000–1399
1400–1449
1450–1499
1500–1599

Number of years

400
50
50
100

1600–1699
1700–1729
1730–1749
1750–1759
1760–1779
1780–1799
1800–1819
1820–1980

100
30
20
10
20
20
20
161

TABLE A1: Reconstruction steps in MBH99
These are equivalent to the black sections that would be spliced together in the final graph.

The varying length of the steps is very odd. Why should Mann choose to
have one step that was only ten years long but another that was 400 years
long?  This  remains  one  of  the  methodological  steps  of  the  Hockey  Stick
that  is  still  not  understood  by  outsiders,  but  the  impact  of  such  a  strange
procedure is certainly worth questioning.

Applying the adjustment
When McIntyre and Sibelius had started examining how Mann’s adjustment
worked,  they  had  had  another  surprise.  Mann  had  only  applied  the
correction  to  the  AD  1000  step  of  his  PC  analysis.  This  meant  that  the
correction  only  ended  up  affecting  the  very  early  sections  of  the  final
reconstruction  –  the  period  from  1000  to  1400  AD.  This  was  extremely
strange because of course Mann had ostensibly been trying to correct for
twentieth  century  carbon  dioxide 
somewhat
counterintuitive that an adjustment for this purpose should affect the shape
of the graph only in the first four centuries of the millennium, but that is
how Mann had chosen to do it. Assuming he had prepared the co2detrend
version  first,  he  would  have  obtained  a  reconstruction  that  looked  like
Figure A10.

fertilization. 

In this version, the Medieval Warm Period is relatively warm, reaching
levels in line with the proxy record at the end of the twentieth century. It

It 

is 

also had very poor verification statistics, rendering it unusable.

Although at first sight the charts Figs A8 and A10 are similar, the new
version showed a much faster rate of decline up to 1900. This would have
presented something of a problem because the Medieval Warm Period now
looked  rather  warm  –  in  touching  distance  of  the  levels  the  proxies  were
reaching at the end of the 1970s. A graph like this would have raised severe
question marks over whether there really was anything unprecedented about
recent temperatures. What was worse was that the verification statistics for
the revised version were dreadful and the results would therefore have been
laughed off as unreliable.

FIGURE A10: The Hockey Stick using the CO2detrend.f version of the fix.

FIGURE A11: The Hockey Stick with the oldfix version of the fix

The  oldfix  correction,  meanwhile,  had  somewhat  better  verification
statistics (although still short of the values achieved by the unfixed version)
but in the early sections of the reconstruction still had values that were too

high for comfort (see Figure A11). This version still suffers from very poor
verification statistics and was therefore not usable.

This seems to have brought Mann on to the residualdetrend version he
had  finally  settled  on.  With  all  the  strange  distortions  of  the  graph  –  the
repeated  smoothing  and  the  application  only  to  the  early  section  of  the
record – Mann appeared to get the result he wanted (see Figure A12). Here
the slope was now a gentle decline over 900 years that could be explained
away as the results of the Milankovitch cycle; the Medieval Warm Period
was satisfyingly cool, and the verification statistics were a whisker better
than the unfixed version of the graph, its artificial appearance now a thing
of the past.

FIGURE A12: The Hockey Stick with the residualdetrend version of the fix

This was the version used in the final published article.

